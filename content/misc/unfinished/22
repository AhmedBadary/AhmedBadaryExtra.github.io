Algorithms


Divide and Conquer

1. Divide and Conquer
    The divide and conquer strategy solves a problem by  
    1. Breaking it into subproblems that are themselves smaller instances of the same type of problem
    2. Recursively solving these subproblems
    3. Appropriately combining their answers
    

    The real work is done piecemeal in three different places  
    
    1. in the partitioning of problems into subproblems
    2. at the very tail end of the recursion when the subproblems are so small that they are solved outright
    3. and in the gluing together of partial answers.  

    These are held together and coordinated by the algorithm’s core recursive structure.  
    



4. Binary Search
    

5. Mergesort
    The problem of sorting a list of numbers lends itself immediately to a divide and conquer strategy split the list into two halves recursively sort each half and then merge the two sorted sublists.  

    

    Merge Function  
    Given two sorted arrays $$x1 . . . k$$ and $$y1 . . . l$$ how do we efficiently merge them into a single sorted array $$z1 . . . k + l$$?  
    

    Here  denotes concatenation.  

    Run Time  
    This merge procedure does a constant amount of work per recursive call provided the required array space is allocated in advance for a total running time of $$Ok + l$$. Thus merge’s are linear and the overall time taken by mergesort is  
    $$Tn=2 Tn / 2+On = On \log n$$  


    Iterative MergeSort  
    img4


6. $$n \log n$$ Lower Bound for comparison based Sorting Proof
    img5
    

7. Medians
    
    



 Reinforcement Learning


Intro  Reinforcement Learning

1. Reinforcement Learning
    



3. Mathematical Formulation of RL  Markov Decision Processes
    Markov Decision Process  
    
    Defined by $$\mathcal{S} \mathcal{A} \mathcal{R} \mathbb{P} \gamma$$  
    
     $$\mathcal{S}$$ set of possible states
     $$\mathcal{A}$$ set of possible actions
     $$\mathcal{R}$$ distribution of reward given state action pair
     $$\mathbb{P}$$ transition probability i.e. distribution over next state given state action pair
     $$\gamma$$ discount factor  

    MDPs Algorithm/Idea  
    
     At time step $$\mathrm{t}=0$$ environment samples initial state $$\mathrm{s} {0} \sim \mathrm{p}\left\mathrm{s} {0}\right$$
     Then for $$\mathrm{t}=0$$ until done
         Agent selects action $$at$$ 
         Environment samples reward $$\mathrm{r} {\mathrm{t}} \sim \mathrm{R}\left . \vert \mathrm{s}{\mathrm{t}} \mathrm{a} {\mathrm{t}}\right$$
         Environment samples next state $$\mathrm{s} {\mathrm{t}+1} \sim \mathrm{P}\left . \vert \mathrm{s} {\mathrm{t}} \mathrm{a} {\mathrm{t}}\right$$
         Agent receives reward $$\mathrm{r} {\mathrm{t}}$$ and next state $$\mathrm{s} {\mathrm{t}+1}$$

      A policy $$\pi$$ is a function from $$S$$ to $$A$$ that specifies what action to take in each state  
      Objective find policy $$\pi^{\ast}$$ that maximizes cumulative discounted reward  
    $$\sum{t \geq 0} \gamma^{t} r{t}$$  


    Optimal Policy $$\pi^{\ast}$$  
    We want to find optimal policy $$\mathbf{n}^{\ast}$$ that maximizes the sum of rewards.  
    We handle randomness  initial state transition probability... by Maximizing the expected sum of rewards.  
    Formally  
    $$\pi^{ }=\arg \max {\pi} \mathbb{E}\left\sum{t \geq 0} \gamma^{t} r{t} | \pi\right \quad$ \text{ with } $s{0} \sim p\lefts{0}\right a{t} \sim \pi\left\cdot | s{t}\right s{t+1} \sim p\left\cdot | s{t} a{t}\right$$  



    The Bellman Equations  
    Definition of “optimal utility” via expectimax recurrence gives a simple one step lookahead relationship amongst optimal utility values.  
    The Bellman Equations characterize optimal values    
    $$\begin{aligned} V^{  }s &= \max {a}\lefts^{}s a\right. 
                     Q^{  }s a &= \sum{s^{\prime}} T\lefts a s^{\prime}\right\leftR\lefts a s^{\prime}\right+\gamma V^{  }\lefts^{\prime}\right\right 
                     V^{  }s &= \max {a} \sum{s^{\prime}} T\lefts a s^{\prime}\right\leftR\lefts a s^{\prime}\right+\gamma V^{  }\lefts^{\prime}\right\right \end{aligned}$$  

    Value Iteration Algorithm  
    The Value Iteration algorithm computes the optimal values  
    $$V{k+1}s \leftarrow \max {a} \sum{s^{\prime}} T\lefts a s^{\prime}\right\leftR\lefts a s^{\prime}\right+\gamma V{k}\lefts^{\prime}\right\right$$   
      Value iteration is just a fixed point solution method.  
      It is repeated bellman equations.  

    Convergence  
    


    Issues  
    
     Problem 1 It’s slow   $$OS^2A$$ per iteration
     Problem 2 The “max” at each state rarely changes
     Problem 3 The policy often converges long before the values  
     Problem 4 Not scalable. Must compute $$Qs a$$ for every state action pair. If state is e.g. current game state pixels computationally infeasible to compute for entire state space  


    Policy Iteration  
    It is an Alternative approach for optimal values  
    Policy Iteration algorithm  
    
     Step \#1 Policy evaluation calculate utilities for some fixed policy not of to
    utilitiesl until convergence
     Step #2 Policy improvement update policy using one step look ahead with resulting but not optimall utilities af future values  
     Repeat steps until policy converges  

     Evaluation  
        For fixed current policy $$\pi$$ find values with policy evaluation  
         Iterate until values converge  
            $$V{k+1}^{\pi{i}}s \leftarrow \sum{s^{\prime}} T\lefts \pi{i}s s^{\prime}\right\leftR\lefts \pi{i}s s^{\prime}\right+\gamma V{k}^{\pi{i}}\lefts^{\prime}\right\right$$  
     Improvement  
        For fixed values get a better policy using policy extraction  
         One step look ahead  
            $$\pi{i+1}s=\arg \max {a} \sum{s^{\prime}} T\lefts a s^{\prime}\right\leftR\lefts a s^{\prime}\right+\gamma V^{\pi{i}}\lefts^{\prime}\right\right$$  

    Properties  
    
     It's still optimal
     Can can converge much faster under some conditions  


    Comparison  Value Iteration vs Policy Iteration  
    



    Q Learning \| Solving for Optimal Policy  
    A problem with value iteration was It is Not scalable. Must compute $$Qs a$$ for every state action pair.  
    Q Learning solves this by using a function approximator to estimate the action value function  
    $$Qs a ; \theta \approx Q^{ }s a$$  
    Deep Q learning the case where the function approximator is a deep neural net.  

    Training  


    


    Experience Replay  


    Deep Q learning with Experience Replay  Algorithm  
    



    
    Policy Gradients  
    An alternative to learning a Q function.  
    Q functions can be very complicated.  
    Example a robot grasping an object has a very high dimensional state => hard to learn exact value of every state action pair.  

      Define a class of parameterized policies  
    $$\Pi=\left\{\pi{\theta} \theta \in \mathbb{R}^{m}\right\}$$  
      For each policy define its value  
    $$J\theta=\mathbb{E}\left\sum{t \geq 0} \gamma^{t} r{t} | \pi{\theta}\right$$  
      Find the optimal policy $$\theta^{  }=\arg \max  {\theta} J\theta$$ by gradient ascent on policy parameters REINFORCE Algorithm   

    REINFORCE Algorithm  
    Expected Reward  
    $$\begin{aligned} J\theta &=\mathbb{E}{\tau \sim p\tau ; \theta}r\tau  &=\int{\tau} r\tau p\tau ; \theta \mathrm{d} \tau \end{aligned}$$  
    where $$r\tau$$ is the reward of a trajectory $$\tau=\lefts{0} a{0} r{0} s{1} \dots\right$$.  
    The Gradient  
    $$\nabla{\theta} J\theta=\int{\tau} r\tau \nabla{\theta} p\tau ; \theta \mathrm{d} \tau$$  
      The Gradient is Intractable. Gradient of an expectation is problematic when $$p$$ depends on $$\theta$$.  
      Solution  
     Trick  
        $$\nabla{\theta} p\tau ; \theta=p\tau ; \theta \frac{\nabla{\theta} p\tau ; \theta}{p\tau ; \theta}=p\tau ; \theta \nabla{\theta} \log p\tau ; \theta$$  
     Injecting Back  
        $$\begin{aligned} \nabla{\theta} J\theta &=\int{\tau}\leftr\tau \nabla{\theta} \log p\tau ; \theta\right p\tau ; \theta \mathrm{d} \tau  &=\mathbb{E}{\tau \sim p\tau ; \theta}\leftr\tau \nabla{\theta} \log p\tau ; \theta\right \end{aligned}$$  
     Estimating the Gradient Can estimate with Monte Carlo sampling.  
         The gradient does NOT depend on transition probabilities  
             $$p\tau ; \theta=\prod{t \geq 0} p\lefts{t+1} | s{t} a{t}\right \pi{\theta}\lefta{t} | s{t}\right$$  
             $$\log p\tau ; \theta=\sum{t \geq 0} \log p\lefts{t+1} | s{t} a{t}\right+\log \pi{\theta}\lefta{t} | s{t}\right$$  
                $$\implies$$ 
             $$\nabla{\theta} \log p\tau ; \theta=\sum{t \geq 0} \nabla{\theta} \log \pi{\theta}\lefta{t} | s{t}\right$$  
         Therefore when sampling a trajectory $$\tau$$ we can estimate $$J\theta$$ with  
            $$\nabla{\theta} J\theta \approx \sum{t \geq 0} r\tau \nabla{\theta} \log \pi{\theta}\lefta{t} | s{t}\right$$  
     Gradient Estimator  
        $$\nabla{\theta} J\theta \approx \sum{t \geq 0} r\tau \nabla{\theta} \log \pi{\theta}\lefta{t} | s{t}\right$$  
         Intuition/Interpretation  
             If $$\mathrm{r}\tau$$ is high push up the probabilities of the actions seen
             If $$\mathrm{r}\tau$$ is low push down the probabilities of the actions seen
        Might seem simplistic to say that if a trajectory is good then all its actions were good. But in expectation it averages out!  
         Variance  
             Issue This also suffers from high variance because credit assignment is really hard.  
             Variance Reduction  Two Ideas  
                1. Push up probabilities of an action seen only by the cumulative future reward from that state  
                    $$\nabla{\theta} J\theta \approx \sum{t \geq 0}\left\sum{t^{\prime} \geq t} r{t^{\prime}}\right \nabla{\theta} \log \pi{\theta}\lefta{t} | s{t}\right$$  
                2. Use discount factor $$\gamma$$ to ignore delayed effects  
                    $$\nabla{\theta} J\theta \approx \sum{t \geq 0}\left\sum{t^{\prime} \geq t} \gamma^{t^{\prime} t} r{t^{\prime}}\right \nabla{\theta} \log \pi{\theta}\lefta{t} | s{t}\right$$  

                  Problem The raw value of a trajectory isn’t necessarily meaningful. For example if rewards are all positive you keep pushing up probabilities of actions.  
                  What is important then Whether a reward is better or worse than what you expect to get.  
                  Solution Introduce a baseline function dependent on the state.  
                 Concretely estimator is now  
                $$\nabla{\theta} J\theta \approx \sum{t \geq 0}\left\sum{t^{\prime} \geq t} \gamma^{t^{\prime} t} r{t^{\prime}} b\lefts{t}\right\right \nabla{\theta} \log \pi{\theta}\lefta{t} | s{t}\right$$  
                 Choosing a Baseline  
                     Vanilla REINFORCE  
                        A simple baseline constant moving average of rewards experienced so far from all trajectories.  
                     Actor Critic  
                        We want to push up the probability of an action from a state if this action was better than the expected value of what we should get from that state.  
                        Intuitively we are happy with an action $$a{t}$$ in a state $$s{t}$$ if $$Q^{\pi}\lefts{t} a{t}\right V^{\pi}\lefts{t}\right$$ is large. On the contrary we are unhappy with an action if it's small.  
                        Now the estimator  
                        $$\nabla{\theta} J\theta \approx \sum{t \geq 0}\leftQ^{\pi{\theta}}\lefts{t} a{t}\right V^{\pi{\theta}}\lefts{t}\right\right \nabla{\theta} \log \pi{\theta}\lefta{t} | s{t}\right$$  
                         Learning $$Q$$ and $$V$$  
                            We learn $$Q V$$ using the Actor Critic Algorithm.  

    Actor Critic Algorithm  
    An algorithm to learn $$Q$$ and $$V$$.  
    We can combine Policy Gradients and Q learning by training both  
     Actor the policy and 
     Critic the Q function  

    Details  
     The actor decides which action to take and the critic tells the actor how good its action was and how it should adjust
     Also alleviates the task of the critic as it only has to learn the values of state action pairs generated by the policy
     Can also incorporate Q learning tricks e.g. experience replay
     Remark we can define by the advantage function how much an action was better than expected  

    Algorithm  
    



    
    Summary  
    
     Policy gradients very general but suffer from high variance so
    requires a lot of samples. Challenge sample efficiency
     Q learning does not always work but when it works usually more
    sample efficient. Challenge exploration  

     Guarantees  
         Policy Gradients Converges to a local minima of J𝜃 often good enough!
         Q learning Zero guarantees since you are approximating Bellman equation with a complicated function approximator







 Probability Theory  Mathematics of Deep Learning



Motivation

1. Uncertainty in General Systems and the need for a Probabilistic Framework
    1. Inherent stochasticity in the system being modeled  
        Take Quantum Mechanics most interpretations of quantum mechanics describe the dynamics of sub atomic particles as being probabilistic.  
    2. Incomplete observability  
        Deterministic systems can appear stochastic when we cannot observe all the variables that drive the behavior of the system.  
        i.e. Point of View determinism Monty Hall  
    3. Incomplete modeling  
        Building a system that makes strong assumptions about the problem and discards observed information result in uncertainty in the predictions.    
    

2. Bayesian Probabilities and Frequentist Probabilities
    Frequentist Probabilities describe the predicted number of times that a repeatable process will result in a given output in an absolute scale.  

    Bayesian Probabilities describe the degree of belief that a certain non repeatable event is going to result in a given output in an absolute scale.      
    
    We assume that Bayesian Probabilities behaves in exactly the same way as Frequentist Probabilities.  
    

3. Probability as an extension of Logic
    "Probability can be seen as the extension of logic to deal with uncertainty. Logic provides a set of formal rules for determining what propositions are implied to be true or false given the assumption that some other set of propositions is true or false. Probability theory provides a set of formal rules for determining the likelihood of a proposition being true given the likelihood of other propositions."  deeplearningbook p.54



Basics

0. Elements of Probability
     Sample Space $$\Omega$$ The set of all the outcomes of a stochastic experiment; where each outcome is a complete description of the state of the real world at the end of the experiment.  
     Event Space $${\mathcal {F}}$$ A set of events; where each event $$A \in \mathcal{F}$$ is a subset of the sample space $$\Omega$$  it is a collection of possible outcomes of an experiment.  
     Probability Measure $$\operatorname {P}$$ A function $$\operatorname {P} \mathcal{F} \rightarrow \mathbb{R}$$ that satisfies the following properties  
         $$\operatorname {P}A \geq 0 \ \forall A \in \mathcal{f}$$ 
         $$\operatorname {P}\Omega = 1$$ $$\operatorname {P}\emptyset = 0$$^1  
         $${\displaystyle \operatorname {P}\bigcupi Ai = \sumi \operatorname {P}Ai }$$ where $$A1 A2 ...$$ are disjoint events  

    Properties  
     $${\text { If } A \subseteq B \Longrightarrow PA \leq PB}$$   
     $${PA \cap B \leq \min PA PB} $$  
     Union Bound $${PA \cup B \leq PA+PB}$$  
     $${P\Omega \backslash A=1 PA}$$.  
     Law of Total Probability LOTB $$\text { If } A{1} \ldots A{k} \text { are a set of disjoint events such that } \cup{i=1}^{k} A{i}=\Omega \text { then } \sum{i=1}^{k} P\leftA{k}\right=1$$  
     Inclusion Exclusion Principle  
        $$\mathbb{P}\left\bigcup{i=1}^{n} A{i}\right=\sum{i=1}^{n} \mathbb{P}\leftA{i}\right \sum{i< j} \mathbb{P}\leftA{i} \cap A{j}\right+\sum{i< j < k} \mathbb{P}\leftA{i} \cap A{j} \cap A{k}\right \cdots+ 1^{n 1} \sum{i< \ldots< n} \mathbb{P}\left\bigcap{i=1}^{n} A{i}\right$$  


 
          

^1 Corresponds to "wanting" the probability of events that are certain to have p=1 and events that are impossible to have p=0  
                

1. Random Variables
    A Random Variable is a variable that can take on different values randomly.  
    Formally a random variable $$X$$ is a function that maps outcomes to numerical quantities labels typically real numbers
    $${\displaystyle X\colon \Omega \to \mathbb{R}}$$  

    Think of a R.V. as a numerical "summary" of an aspect of the experiment.  

    Types
     Discrete is a variable that has a finite or countably infinite number of states  
     Continuous is a variable that is a real value  

    Examples  
     Bernoulli A r.v. $$X$$ is said to have a Bernoulli distribution if $$X$$ has only $$2$$ possible values $$0$$ and $$1$$ and $$PX=1 = p PX=0 = 1 p$$; denoted $$\text{Bern}p$$.    
     Binomial The distr. of #successes in $$n$$ independent $$\text{Bern}p$$ trials and its distribution is $$PX=k = \left\begin{array}{l}{n}  {k}\end{array}\right p^k 1 p^{n k}$$; denoted $$\text{Bin}n p$$.          
    

2. Probability Distributions
    A Probability Distribution is a function that describes the likelihood that a random variable or a set of r.v. will take on each of its possible states.  
    Probability Distributions are defined in terms of the Sample Space.  
     Classes  
         Discrete Probability Distribution is encoded by a discrete list of the probabilities of the outcomes known as a Probability Mass Function PMF.  
         Continuous Probability Distribution is described by a Probability Density Function PDF.  
     Types  
         Univariate Distributions are those whose sample space is $$\mathbb{R}$$.  
        They give the probabilities of a single random variable taking on various alternative values 
         Multivariate Distributions also known as Joint Probability distributions  are those whose sample space is a vector space.   
        They give the probabilities of a random vector taking on various combinations of values.  


    A Cumulative Distribution Function CDF is a general functional form to describe a probability distribution  
    $${\displaystyle Fx=\operatorname {P} X\leq x\qquad {\text{ for all }}x\in \mathbb {R} .}$$  
    Because a probability distribution P on the real line is determined by the probability of a scalar random variable X being in a half open interval $$−\infty x$$ the probability distribution is completely characterized by its cumulative distribution function i.e. one can calculate the probability of any event in the event space  
    
 

3. Probability Mass Function
    A Probability Mass Function PMF is a function probability distribution that gives the probability that a discrete random variable is exactly equal to some value.  
    Mathematical Definition  
    Suppose that $$X S \rightarrow A \\\ A {\displaystyle \subseteq }  \mathbb{R}$$ is a discrete random variable defined on a sample space $$S$$. Then the probability mass function $$fX A \rightarrow 0 1$$ for $$X$$ is defined as   
    $$p{X}x=PX=x=P\{s\in SXs=x\}$$  
    The total probability for all hypothetical outcomes $$x$$ is always conserved  
    $$\sum {x\in A}p{X}x=1$$
    Joint Probability Distribution is a PMF over many variables denoted $$P\mathrm{x} = x \mathrm{y} = y$$ or $$Px y$$.  

    A PMF must satisfy these properties  
     The domain of $$P$$ must be the set of all possible states of $$\mathrm{x}$$.  
     $$\forall x \in \mathrm{x} \ 0 \leq Px \leq 1$$. Impossible events has probability $$0$$. Guaranteed events have probability $$1$$.  
     $${\displaystyle \sum{x \in \mathrm{x}} Px = 1}$$ i.e. the PMF must be normalized.  
    
            
4. Probability Density Function
    A Probability Density Function PDF is a function probability distribution whose value at any given sample or point in the sample space can be interpreted as providing a relative likelihood that the value of the random variable would equal that sample.  
    The PDF is defined as the derivative of the CDF  
    $$f{X}x = \dfrac{dF{X}x}{dx}$$  
    A Probability Density Function $$px$$ does not give the probability of a specific state directly; instead the probability of landing inside an infinitesimal region with volume $$\delta x$$ is given by $$px\delta x$$.  
    We can integrate the density function to find the actual probability mass of a set of points. Specifically the probability that $$x$$ lies in some set $$S$$ is given by the integral of $$px$$ over that set.  
    In the Univariate example the probability that $$x$$ lies in the interval $$a b$$ is given by $$\int{a b} pxdx$$  


    A PDF must satisfy these properties  
     The domain of $$P$$ must be the set of all possible states of $$x$$.  
     $$\forall x \in \mathrm{x} \ 0 \leq Px \leq 1$$. Impossible events has probability $$0$$. Guaranteed events have probability $$1$$.  
     $$\int pxdx = 1$$ i.e. the integral of the PDF must be normalized.  
    


44.Cumulative Distribution Function
    A Cumulative Distribution Function CDF is a function probability distribution of a real valued random variable $$X$$ or just distribution function of $$X$$ evaluated at $$x$$ is the probability that $$X$$ will take a value less than or equal to $$x$$.    
    $$F{X}x=\operatorname {P} X\leq x$$   
    The probability that $$X$$ lies in the semi closed interval $$a b$$ where $$a  <  b$$ is therefore  
    $${\displaystyle \operatorname {P} a<X\leq b=F{X}b F{X}a.}$$  
    
    Properties    
     $$0 \leq Fx \leq 1$$ 
     $$\lim{x \rightarrow  \infty} Fx = 0$$ 
     $$\lim{x \rightarrow \infty} Fx = 1$$ 
     $$x \leq y \implies Fx \leq Fy$$.  
    

5. Marginal Probability
    The Marginal Distribution of a subset of a collection of random variables is the probability distribution of the variables contained in the subset.  
    Two variable Case  
    Given two random variables $$X$$ and $$Y$$ whose joint distribution is known the marginal distribution of $$X$$ is simply the probability distribution of $$X$$ averaging over information about $$Y$$.
     Discrete    
        $${\displaystyle \PrX=x=\sum {y}\PrX=xY=y=\sum {y}\PrX=x\mid Y=y\PrY=y}$$    
     Continuous    
        $${\displaystyle p{X}x=\int {y}p{XY}xy\\mathrm {d} y=\int {y}p{X\mid Y}x\mid y\p{Y}y\\mathrm {d} y}$$  
     Marginal Probability as Expectation    
    $${\displaystyle p{X}x=\int {y}p{X\mid Y}x\mid y\p{Y}y\\mathrm {d} y=\mathbb {E} {Y}p{X\mid Y}x\mid y}$$  
    
    

    Marginalization the process of forming the marginal distribution with respect to one variable by summing out the other variable  

    Notes  
    
     Marginal Distribution of a variable is just the prior distr of the variable  
     Marginal Likelihood also known as the evidence or model evidence is the denominator of the Bayes equation. Its only role is to guarantee that the posterior is a valid probability by making its area sum to 1.  

     both terms above are the same  
     Marginal Distr VS Prior  

         Summary  
            Basically it's a conceptual difference.  
            The prior denoted $$p\theta$$ denotes the probability of some event 𝜔 even before any data has been taken.  
            A marginal distribution is rather different. You hold a variable value and integrate over the unknown values.  
            But in some contexts they are the same.  

            


            


6. Conditional Probability
    Conditional Probability is a measure of the probability of an event given that another event has occurred.  
    Conditional Probability is only defined when $$Px > 0$$  We cannot compute the conditional probability conditioned on an event that never happens.   
    Definition  
    $$PA|B={\frac {PA\cap B}{PB}} = {\frac {PA B}{PB}}$$  

    Intuitively it is a way of updating your beliefs/probabilities given new evidence. It's inherently a sequential process.  



7. The Chain Rule of Conditional Probability
    Any joint probability distribution over many random variables may be decomposed into conditional distributions over only one variable.  
    The chain rule permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities    
    $$\mathrm {P} \left\bigcap {k=1}^{n}A{k}\right=\prod {k=1}^{n}\mathrm {P} \leftA{k}\{\Bigg |}\\bigcap {j=1}^{k 1}A{j}\right$$  

8. Independence and Conditional Independence
    Two random variables $$x$$ and $$y$$ or events  are independent if their probability distribution can be expressed as a product of two factors one involving only $$x$$ and one involving only $$y$$  
    $$\mathrm{P}A \cap B = \mathrm{P}A\mathrm{P}B$$  

    Two random variables $$A$$ and $$B$$ are conditionally independent given a random variable $$Y$$ if the conditional probability distribution over $$A$$ and $$B$$ factorizes in this way for every value of $$Y$$  
    $$\PrA\cap B\mid Y=\PrA\mid Y\PrB\mid Y$$  
    or equivalently  
    $$\PrA\mid B\cap Y=\PrA\mid Y$$  
    In other words $$A$$ and $$B$$ are conditionally independent given $$Y$$ if and only if given knowledge that $$Y$$ occurs knowledge of whether $$A$$ occurs provides no information on the likelihood of $$B$$ occurring and knowledge of whether $$B$$ occurs provides no information on the likelihood of $$A$$ occurring.  


    Pairwise VS Mutual Independence  
     Pairwise  
        $$\mathrm{P}\leftA{m} \cap A{k}\right=\mathrm{P}\leftA{m}\right \mathrm{P}\leftA{k}\right$$  
     Mutual Independence
        $$\mathrm{P}\left\bigcap{i=1}^{k} B{i}\right=\prod{i=1}^{k} \mathrm{P}\leftB{i}\right$$  
        for all subsets of size $$k \leq n$$  

    Pairwise independence does not imply mutual independence but the other way around is TRUE by definition.  



    Notation  
     $$A$$ is Independent from $$B$$  $$A{\perp}B$$
     $$A$$ and $$B$$ are conditionally Independent given $$Y$$  $$A{\perp}B \\vert Y$$  

    Notes  
    
     Unconditional Independence is very rare there is usually some hidden factor influencing the interaction between the two events/variables  
     Conditional Independence is the most basic and robust form of knowledge about uncertain environments  
            
    
                
9. Expectation
    The expectation or expected value of some function $$fx$$ with respect to a probability distribution $$Px$$ is the "theoretical" average or mean value that $$f$$ takes on when $$x$$ is drawn from $$P$$.  
    The Expectation of a R.V. is a weighted average of the values $$x$$ that the R.V. can take   $$\operatorname {E}X = \sum{x \in X} x \cdot px$$  
     Discrete case  
        $${\displaystyle \operatorname {E}{x \sim P} fX=fx{1}px{1}+fx{2}px{2}+\cdots +fx{k}px{k}} = \sumx Pxfx$$             
     Continuous case  
    $${\displaystyle \operatorname {E} {x \sim P} fX = \int pxfxdx}$$   
    Linearity of Expectation  
    $${\displaystyle {\begin{aligned}\operatorname {E} X+Y&=\operatorname {E} X+\operatorname {E} Y6pt\operatorname {E} aX&=a\operatorname {E} X\end{aligned}}}$$   
    Independence   
    If $$X$$ and $$Y$$ are independent $$\implies \operatorname {E} XY = \operatorname {E} X \operatorname {E} Y$$  
    

10.Variance
    Variance is the expectation of the squared deviation of a random variable from its mean.  
    It gives a measure of how much the values of a function of a random variable $$x$$ vary as we sample different values of $$x$$ from its probability distribution  
    $$\operatorname {Var} fx=\operatorname {E} \leftfx \mu ^{2}\right = \sum{x \in X} x  \mu^2 \cdot px$$  
    Variance expanded  
    $${\displaystyle {\begin{aligned}\operatorname {Var} X&=\operatorname {E} \leftX \operatorname {E} X^{2}\right
        &=\operatorname {E} \leftX^{2} 2X\operatorname {E} X+\operatorname {E} X^{2}\right
        &=\operatorname {E} \leftX^{2}\right 2\operatorname {E} X\operatorname {E} X+\operatorname {E} X^{2}
        &=\operatorname {E} \leftX^{2}\right \operatorname {E} X^{2}\end{aligned}}}$$     
    Variance as Covariance 
    Variance can be expressed as the covariance of a random variable with itself 
    $$\operatorname {Var} X=\operatorname {Cov} XX$$   
    
    Properties  
     $$\operatorname {Var} a = 0 \forall a \in \mathbb{R}$$ constant $$a$$  
     $$\operatorname {Var} afX = a^2 \operatorname {Var} fX$$ constant $$a$$
     $$\operatorname {Var} X + Y = a^2 \operatorname {Var} X + \operatorname {Var} Y + 2 \operatorname {Cov} X Y$$.  
    

11.Standard Deviation
    The Standard Deviation is a measure that is used to quantify the amount of variation or dispersion of a set of data values.  
    It is defined as the square root of the variance  
    $${\displaystyle {\begin{aligned}\sigma &={\sqrt {\operatorname {E} X \mu ^{2}}}&={\sqrt {\operatorname {E} X^{2}+\operatorname {E}  2\mu X+\operatorname {E} \mu ^{2}}}&={\sqrt {\operatorname {E} X^{2} 2\mu \operatorname {E} X+\mu ^{2}}}&={\sqrt {\operatorname {E} X^{2} 2\mu ^{2}+\mu ^{2}}}&={\sqrt {\operatorname {E} X^{2} \mu ^{2}}}&={\sqrt {\operatorname {E} X^{2} \operatorname {E} X^{2}}}\end{aligned}}}$$  
    
    Properties  
     68% of the data points lie within $$1 \cdot \sigma$$s from the mean
     95% of the data points lie within $$2 \cdot \sigma$$s from the mean
     99% of the data points lie within $$3 \cdot \sigma$$s from the mean
    

12.Covariance
    Covariance is a measure of the joint variability of two random variables.  
    It gives some sense of how much two values are linearly related to each other as well as the scale of these variables  
    $$\operatorname {cov} XY=\operatorname {E} { {\big }X \operatorname {E} XY \operatorname {E} Y{ \big } }$$   
    Covariance expanded  
    $${\displaystyle {\begin{aligned}\operatorname {cov} XY&=\operatorname {E} \left\leftX \operatorname {E} \leftX\right\right\leftY \operatorname {E} \leftY\right\right\right&=\operatorname {E} \leftXY X\operatorname {E} \leftY\right \operatorname {E} \leftX\rightY+\operatorname {E} \leftX\right\operatorname {E} \leftY\right\right&=\operatorname {E} \leftXY\right \operatorname {E} \leftX\right\operatorname {E} \leftY\right \operatorname {E} \leftX\right\operatorname {E} \leftY\right+\operatorname {E} \leftX\right\operatorname {E} \leftY\right&=\operatorname {E} \leftXY\right \operatorname {E} \leftX\right\operatorname {E} \leftY\right.\end{aligned}}}$$   
    when $${\displaystyle \operatorname {E} XY\approx \operatorname {E} X\operatorname {E} Y} $$ this last equation is prone to catastrophic cancellation when computed with floating point arithmetic and thus should be avoided in computer programs when the data has not been centered before.  

    Covariance of Random Vectors  
    $${\begin{aligned}\operatorname {cov} \mathbf {X} \mathbf {Y} &=\operatorname {E} \left\mathbf {X}  \operatorname {E} \mathbf {X} \mathbf {Y}  \operatorname {E} \mathbf {Y} ^{\mathrm {T} }\right&=\operatorname {E} \left\mathbf {X} \mathbf {Y} ^{\mathrm {T} }\right \operatorname {E} \mathbf {X} \operatorname {E} \mathbf {Y} ^{\mathrm {T} }\end{aligned}}$$   

    The Covariance Matrix of a random vector $$x \in \mathbb{R}^n$$ is an $$n \times n$$ matrix such that    
    $$ \operatorname {cov} X {ij} = \operatorname {cov}xi xj 
        \operatorname {cov}xi xj = \operatorname {Var} xi$$   
    Interpretations  
     High absolute values of the covariance mean that the values change very much and are both far from their respective means at the same time.
     The sign of the covariance   
        The sign of the covariance shows the tendency in the linear relationship between the variables  
         Positive  
            the variables tend to show similar behavior
         Negative  
            the variables tend to show opposite behavior  
         Reason  
        If the greater values of one variable mainly correspond with the greater values of the other variable and the same holds for the lesser values i.e. the variables tend to show similar behavior the covariance is positive. In the opposite case when the greater values of one variable mainly correspond to the lesser values of the other i.e. the variables tend to show opposite behavior the covariance is negative.  

    Covariance and Variance  
    $$\operatorname{Var}X+Y=\operatorname{Var}X+\operatorname{Var}Y+2 \operatorname{Cov}X Y$$  

    Covariance and Independence  
    If $$X$$ and $$Y$$ are independent $$\implies \operatorname{cov}X Y=\mathrm{E}X Y \mathrm{E}X \mathrm{E}Y = 0$$.  
     Independence $$\Rightarrow$$ Zero Covariance  
     Zero Covariance $$\nRightarrow$$ Independence

    Covariance and Correlation  
    If $$\operatorname{Cov}X Y=0 \implies $$ $$X$$ and $$Y$$ are Uncorrelated.  


13.Mixtures of Distributions
    It is also common to define probability distributions by combining other simpler probability distributions. One common way of combining distributions is to construct a mixture distribution.    
    A Mixture Distribution is the probability distribution of a random variable that is derived from a collection of other random variables as follows  a random variable is selected by chance from the collection according to given probabilities of selection and then the value of the selected random variable is realized.    
    On each trial the choice of which component distribution should generate the sample is determined by sampling a component identity from a multinoulli distribution  
    $$Px = \sumi Px=iPx \vert c=i$$    
    where $$Pc$$ is the multinoulli distribution over component identities.    

14.Bayes' Rule
    Bayes' Rule describes the probability of an event based on prior knowledge of conditions that might be related to the event.    
    $${\displaystyle PA\mid B={\frac {PB\mid A\PA}{PB}}}$$  
    where   
    $$PB =\sumA PB \vert A PA$$  


15.Common Random Variables
    Discrete RVs    
     Bernoulli  
     Binomial  
     Geometric  
     Poisson  

    Continuous RVs  
     Uniform  
     Exponential  
     Normal/Gaussian  
            
            
16.Summary of Distributions


17.Formulas
     $$\overline{X} = \hat{\mu}$$  
     $$\operatorname {E}\overline{X}=\operatorname {E}\left\frac{X{1}+\cdots+X{n}}{n}\right = \mu$$  
     $$\operatorname{Var}\overline{X}=\operatorname{Var}\left\frac{X{1}+\cdots+X{n}}{n}\right = \dfrac{\sigma^2}{n}$$    
     $$\operatorname {E}\leftX{i}^{2}\right=\operatorname {Var} X+\operatorname {E} X^{2} = \sigma^{2}+\mu^{2}$$  
     $$\operatorname {E}\left\overline{X}^{2}\right=\operatorname {E}\left\hat{\mu}^{2}\right=\frac{\sigma^{2}}{n}+\mu^{2}\$$ ^2  

    


18.Correlation
    In the broadest sense correlation is any statistical association though it commonly refers to the degree to which a pair of variables are linearly related.  

    There are several correlation coefficients often denoted $${\displaystyle \rho }$$ or $$r$$ measuring the degree of correlation  


    It is a measure of the linear correlation between two variables $$X$$ and $$Y$$.  
    $$\rho{X Y}=\frac{\operatorname{cov}X Y}{\sigma{X} \sigma{Y}}$$  
    where $${\displaystyle \sigma{X}}$$ is the standard deviation of $${\displaystyle X}$$ and $${\displaystyle \sigma{Y}}$$  is the standard deviation of $${\displaystyle Y}$$ and $$\rho \in  1 1$$.   



    Correlation and Independence  
    1. Uncorrelated $$\nRightarrow$$ Independent  
    2. Independent $$\implies$$ Uncorrelated  

    Zero correlation will indicate no linear dependency however won't capture non linearity. Typical example is uniform random variable $$x$$ and $$x^2$$ over $$ 11$$ with zero mean. Correlation is zero but clearly not independent.  

     

19.Probabilistic Inference
    Probabilistic Inference compute a desired probability from other known probabilities e.g. conditional from joint.  

    We generally compute Conditional Probabilities  
    
     $$p\text{sun} \vert T=\text{12 pm} = 0.99$$  
     These represent the agents beliefs given the evidence  

    Probabilities change with new evidence  
     
     $$p\text{sun} \vert T=\text{12 pm} C=\text{Stockholm} = 0.85$$  
    $$\longrightarrow$$  
     $$p\text{sun} \vert T=\text{12 pm} C=\text{Stockholm} M=\text{Jan} = 0.40$$  
     Observing new evidence causes beliefs to be updated

    Inference by Enumeration  
    


          

    Problems  
     Worst case time complexity $$\mathrm{O}\left\mathrm{d}^{n}\right$$
     Space complexity $$\mathrm{O}\left\mathrm{d}^{n}\right$$ to store the joint distribution  

    Inference with Bayes Theorem  
     Diagnostic Probability from Causal Probability  
        $$P\text { cause } | \text { effect }=\frac{P\text { effect } | \text { cause } P\text { cause }}{P\text { effect }}$$  




^2 Comes from $$\operatorname{Var}\overline{X}=\operatorname {E}\left\overline{X}^{2}\right \{\operatorname {E}\overline{X}\}^{2}$$  



Discrete Distributions

1. Uniform Distribution
       

2. Bernoulli Distribution
       A distribution over a single binary random variable.  
        It is controlled by a single parameter $$\phi \in 0 1$$ which fives the probability of the r.v. being equal to $$1$$.  
        It models the probability of a single experiment with a boolean outcome e.g. coin flip $$\rightarrow$$ {heads 1 tails 0}  
       PMF  
       $${\displaystyle Px={\begin{cases}p&{\text{if }}p=1q=1 p&{\text{if }}p=0.\end{cases}}}$$  
       Properties  
        $$PX=1 = \phi$$
        $$PX=0 = 1  \phi$$
        $$PX=x = \phi^x 1  \phi^{1 x}$$
        $$\operatorname {E}X = \phi$$
        $$\operatorname {Var}X = \phi 1  \phi$$

3. Binomial Distribution
    $${\binom {n}{k}}={\frac {n!}{k!n k!}}$$ is the number of possible ways of getting $$x$$ successes and $$n x$$ failures



110

1. Problems


          
        Sol Inclusion Exclusion  


          

       

       


    

Notes Tips and Tricks

 It is more practical to use a simple but uncertain rule rather than a complex but certain one even if the true rule is deterministic and our modeling system has the fidelity to accommodate a complex rule.  
    For example the simple rule “Most birds fly” is cheap to develop and is broadly useful while a rule of the form “Birds fly except for very young birds that have not yet learned to fly sick or injured birds that have lost the ability to fly flightless species of birds including the cassowary ostrich and kiwi. . .” is expensive to develop maintain and communicate and after all this effort is still brittle and prone to failure.

 Disjoint Events Mutually Exclusive{ .bodyContents10 } are events that cannot occur together at the same time
    Mathematically  
     $$Ai \cap Aj = \varnothing$$ whenever $$i \neq j$$  
     $$pAi Aj = 0$$  

 Complexity of Describing a Probability Distribution  
    A description of a probability distribution is exponential in the number of variables it models.  
    The number of possibilities is exponential in the number of variables.  

 Probability VS Likelihood  
    Probabilities are the areas under a fixed distribution  
    $$pr$$data$$|$$distribution$$$$  
    i.e. probability of some data left hand side given a distribution described by the right hand side  
    Likelihoods are the y axis values for fixed data points with distributions that can be moved..  
    $$L$$distribution$$|$$observation/data$$$$  

    Likelihood is basically a specific probability that can only be calculated after the fact of observing some outcomes. It is not normalized to $$1$$ it is not a probability. It is just a way to quantify how likely a set of observation is to occur given some distribution with some parameters; then you can manipulate the parameters to make the realization of the data more "likely" it is precisely meant for that purpose of estimating the parameters; it is a function of the parameters.  
    Probability on the other hand is absolute for all possible outcomes. It is a function of the Data.  

 Maximum Likelihood Estimation  
    A method that tries to find the optimal value for the mean and/or stdev for a distribution given some observed measurements/data points.

 Variance  
    When $$\text{Var}X = 0 \implies X = EX = \mu$$. not interesting  

 Reason we sometimes prefer Biased Estimators  
        


 Optimization Problems


Geometry and Lin Alg Hyper Planes

1. Minimum Distance from a point to a hyperplane/Affine set?
       $$d = \dfrac{\| w \cdot x0 + b \|}{\|w\|}$$  
       where we have an n dimensional hyperplane $$w \cdot x + b = 0$$ and a point $$x0$$.
    Also known as The Signed Distance.  
     Proof.  
         Suppose we have an affine hyperplane defined by $$w \cdot x + b$$ and a point $$x0$$.
         Suppose that $$\vec{v} \in \mathbf{R}^n$$ is a point satisfying $$w \cdot \vec{v} + b = 0$$ i.e. it is a point on the plane.
         We construct the vector $$x0−\vec{v}$$ which points from $$\vec{v}$$ to $$x0$$ and then project it onto the unique vector perpendicular to the plane i.e. $$w$$  

            $$d=\| \text{proj}{w} x0 \vec{v}\| = \left\| \frac{x0 \vec{v}\cdot w}{w \cdot w} w \right\| = \|x0 \cdot w  \vec{v} \cdot w\|\frac{\|w\|}{\|w\|^2} = \frac{\|x0 \cdot w  \vec{v} \cdot w\|}{\|w\|}.$$

         We chose $$\vec{v}$$ such that $$w\cdot \vec{v}= b$$ so we get  

            $$d=\| \text{proj}{w} x0 \vec{v}\| = \frac{\|x0 \cdot w +b\|}{\|w\|}$$

2. Every symmetric positive semi definite matrix is a covariance matrix
     Proof.  
         Suppose $$M$$ is a $$p\times p$$ positive semidefinite matrix.  

         From the finite dimensional case of the spectral theorem it follows that $$M$$ has a nonnegative symmetric square root that can be denoted by $$M^{1/2}$$.  

         Let $${\displaystyle \mathbf {X} }$$ be any $$p\times 1$$ column vector valued random variable whose covariance matrix is the $$p\times p$$ identity matrix.   

         Then   

            $${\displaystyle \operatorname {var} \mathbf {M} ^{1/2}\mathbf {X} =\mathbf {M} ^{1/2}\operatorname {var} \mathbf {X} \mathbf {M} ^{1/2}=\mathbf {M} \}$$



Statistics


2. Every symmetric positive semi definite matrix is a covariance matrix
     Proof.  
         Suppose $$M$$ is a $$p\times p$$ positive semidefinite matrix.  

         From the finite dimensional case of the spectral theorem it follows that $$M$$ has a nonnegative symmetric square root that can be denoted by $$M^{1/2}$$.  

         Let $${\displaystyle \mathbf {X} }$$ be any $$p\times 1$$ column vector valued random variable whose covariance matrix is the $$p\times p$$ identity matrix.   

         Then   

            $${\displaystyle \operatorname {var} \mathbf {M} ^{1/2}\mathbf {X} =\mathbf {M} ^{1/2}\operatorname {var} \mathbf {X} \mathbf {M} ^{1/2}=\mathbf {M} \}$$


Inner Products Over Balls

1. Extrema of inner product over a ball
       Let $$y \in \mathbf{R}^n$$ be  a  given non null vector and let $$\chi = \left\{x \in \mathbf{R}^n \  \|x\|2 \leq r\right\}$$  
    where $$r$$ is some given positive number.
    1. Determine the optimal value $$p1^\ast$$ and the optimal set of the problem $$\min{x \in \chi} \; |y^Tx|$$    
        The minimum value of $$\min{x \in \chi} \; |y^Tx|$$ is $$p1^\ast = 0$$.  
        This value is attained either by $$x = 0$$ or by any vector $$x\in\chir$$ orthogonal to $$y$$.  
        The optimal set $$\chi{opt} = \left\{x  \ x = Vz \|z\|2 \leq r\right\}$$        
    2. Determine the optimal value $$p2^\ast$$ and the optimal set of the problem $$\max{x\in \chi} \; |y^Tx|$$
        The optimal value of $$\max{x\in \chi} \; |y^Tx|$$ is attained for any $$x = \alpha y$$ with $$\|x\|2 = r$$.  
        Thus for $$|\alpha| = \dfrac{r}{\|y\|2}$$ for which we have $$p2^\ast∗ = r\|y\|2.$$  
        The optimal set contains two points $$\chi{opt} = \left\{x  \  x = \alpha y \alpha = ± \dfrac{r}{\|y\|2} \right\}$$.
    3. Determine the optimal value $$p3^\ast$$ and the optimal set of the problem $$\min{x\in \chi} \; y^Tx$$  
        We have $$p3^\ast = −r\|y\|2$$ which is attained at the unique optimal poin   
        $$x^\ast = −\dfrac{r}{\|y\|2} y.$$ 

    4. Determine the optimal value $$p4^\ast$$ and the optimal set of the problem $$\max{x\in\chi} \; y^Tx$$  
        We have $$p4^\ast = r \|y\|2$$ which is attained at the unique optimal point  
         $$x^\ast = \dfrac{r}{\|y\|2} y.$$


Gradients and Derivatives

1. Gradient of log sum exp function
       Find the gradient at $$x$$ of the function $$lse  \  \mathbf{R}^n \rightarrow \mathbf{R}$$ defined as  
       $$
        lsex = \log{\sum{i=1}^n e^{xi}}.
        $$

       Solution.  
        $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \nablax \  lsex$$
       $$
        \begin{align}
        & \ = \nablax \log\sum{i=1}^n e^{xi} 
        & \ = \dfrac{\dfrac{d}{dxi} \sum{i=1}^n e^{xi}}{\log\sum{i=1}^n e^{xi}} 
        & \ = \dfrac{e^{xi}}{lsex} 
        & \ = \dfrac{e^{x1} \  e^{x2} \  \ldots \  e^{xn}^T}{lsex}
        \end{align}
        $$



 Optimization <br > Cheat Sheet




1. Functions
     Graph of a function $$f  \mathbf{R}^n \rightarrow \mathbf{R}$$ is the set of input output pairs that $$f$$ can attain that is  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ Gf = \left \{ xfx \in \mathbf{R}^{n+1}  x \in \mathbf{R}^n \right \}.$$ 
    It is a subset of $$\mathbf{R}^{n+1}$$.
     Epigraph of a function $$f$$ describes the set of input output pairs that $$f$$ can achieve as well as "anything above"  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathop{\bf epi} f = \left \{ xt \in \mathbf{R}^{n+1} ~~ x \in \mathbf{R}^n \ \  t \ge fx \right \}.$$
     Level sets  is the set of points that achieve exactly some value for the function $$f$$.  
    For $$t \in \mathbf{R}$$ the $$t $$level set of the function $$f$$ is defined as  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{L}tf = \left\{ x \in \mathbf{R}^{n} ~~ x \in \mathbf{R}^n \ \  t = fx \right \}.$$
     Sub level sets is the set of points that achieve at most a certain value for  $$f$$ or below  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \mathbf{S}tf = \left\{ x \in \mathbf{R}^{n} ~~ x \in \mathbf{R}^n \ \  t \ge fx \right\}.$$  



2. Optimization Problems
     Functional Form  An optimization problem is a problem of the form
    $$\\\\\\\$$ $$p^\ast = \displaystyle\minx f0x   fix \le 0 \ \  i=1\ldots m$$  
    where $$x \in \mathbf{R}^n$$ is the decision variable;  $$f0  \mathbf{R}^n \rightarrow \mathbf{R}$$ is the objective function or cost;  $$fi  \mathbf{R}^n \rightarrow \mathbf{R} \ \  i=1 \ldots m$$ represent the constraints;  $$p^\ast$$ is the optimal value.
     Feasibility Problems  Sometimes an objective function is not provided. This means that we are just interested in finding a feasible point or determine that the problem is infeasible. 
    By convention we set $$f0$$ to be a constant in that case to reflect the fact that we are indifferent to the choice of a point x as long as it is feasible.
                

3. Optimality
     Feasible Set  
    $$\\\\\\\$$ $$ \mathbf{X} =  \left\{ x \in \mathbf{R}^n ~~  fix \le 0 \ \  i=1 \ldots m \right\}.$$  
     Optimal Value  
    $$\\\\\\\$$$$\\\\\\\$$  $$p^\ast = \minx  f0x ~~ fix \le 0 \ \  i=1 \ldots m.$$   
     Optimal Set  The set of feasible points for which the objective function achieves the optimal value  
    $$\\\\\\\$$$$\\\\\\\$$  $$ \mathbf{X}^{\rm opt} =  \left\{ x \in \mathbf{R}^n ~~  f0x = p^\ast \ \ fix \le 0 \ \  i=1\ldots m \right\} = \mathrm{arg min}{x \in \mathbf{X}}  f0x$$  
    By convention the optimal set is empty if the problem is not feasible.  
    A point $$x$$ is said to be optimal if it belongs to the optimal set.  
    If the optimal value is ONLY attained in the limit then it is NOT in the optimal set.    
     Suboptimality  the $$\epsilon$$ suboptimal set is defined as  
    $$\\\\\\\$$$$\\\\\\\$$  $$ \mathbf{X}\epsilon = \left\{ x \in \mathbf{R}^n ~~ fix \le 0 \ \  i=1 \ldots m \ \  f0x \le p^\ast + \epsilon \right\}.$$  
    $$\implies \  \mathbf{X}0 = \mathbf{X}{\rm opt}$$.  
     Local Optimality  
        A point $$z$$ is Locally Optimal if there is a value $$R>0$$ such that $$z$$ is optimal for the following problem  
    $$\\\\\\\$$ $$\\\\\\\$$ $$minx  f0x ~~ fix \le 0 \ \ i=1 \ldots m  \ \ \|z x\|2 \le R$$.  
    i.e. a local minimizer $$x$$ minimizes $$f0$$ but only for nearby points on the feasible set.  
     Global Optimality  
        A point $$z$$ is Globally Optimal if it is the optimal value of the original problem on all of the feasible region.   
            

4. Problem Classes
 east squares
           $$\minx \;\;\;\; \sum{i=1}^m \left \sum{j=1}^n A{ij} . xj  bi \right^2$$
           where $$A{ij} \  bi \  1 \le i \le m  \ 1 \le j \le n$$ are given numbers and $$x \in \mathbf{R}^n$$ is the variable.

    Linear Programming
           $$ \min \sum{j=1}^n cjxj ~~ \sum{j=1}^n A{ij} . xj  \le bi  \;\; i=1 \ldots m $$ 
           where $$ cj bi$$ and $$A{ij} \  1 \le i \le m \  1 \le j \le n$$ are given real numbers.  

    Quadratic Programming
           $$\minx \;\;\;\; \displaystyle\sum{i=1}^m \left\sum{j=1}^n C{ij} . xj+di\right^2 + \sum{i=1}^n cixi \;\;\;\;\; \sum{j=1}^m A{ij} . xj \le bi \;\;\;\; i=1\ldotsm.$$  
        Includes a sum of squared linear functions in addition to a linear term in the objective.  

    Nonlinear optimization
           A broad class that includes Combinatorial Optimization.

        One of the reasons for which non linear problems are hard to solve is the issue of local minima.

    Convex optimization
            A generalization of QP where the objective and constraints involve "bowl shaped" or convex functions.

        They are easy to solve because they do not suffer from the "curse" of local minima.

    Combinatorial optimization
           In combinatorial optimization some or all the variables are boolean or integers reflecting discrete choices to be made.

        Combinatorial optimization problems are in general extremely hard to solve. Often they can be approximately solved with linear or convex programming.

    NON Convex Optimization Problems Examples
         Boolean/integer optimization some variables are constrained to be Boolean or integers.  
        Convex optimization can be used for getting sometimes good approximations.
         Cardinality constrained problems we seek to bound the number of non zero elements in a vector variable.  
        Convex optimization can be used for getting good approximations.
         Non linear programming usually non convex problems with differentiable objective and functions.  
        Algorithms provide only local minima.  



Linear Algebra

1. Basics
    Linear Independence
           A set of vectors $$\{x1 ...  xm\} \in {\mathbf{R}}^n i=1 \ldots m$$ is said to be independent if and only if the following condition on a vector $$\lambda \in {\mathbf{R}}^m$$  
           $$\sum{i=1}^m \lambdai xi = 0 \ \ \ \implies  \lambda = 0.$$

            i.e. no vector in the set can be expressed as a linear combination of the others.

    Subspace
           A subspace of $${\mathbf{R}}^n$$ is a subset that is closed under addition and scalar multiplication. Geometrically subspaces are "flat" like a line or plane in 3D and pass through the origin.  

         A Subspace $$\mathbf{S}$$ can always be represented as the span of a set of vectors $$xi \in {\mathbf{R}}^n i=1 \ldots m$$ that is as a set of the form  
        $$\mathbf{S} = \mbox{ span}x1 \ldots xm = \left\{ \sum{i=1}^m \lambdai xi ~~ \lambda \in {\mathbf{R}}^m \right\}.$$
        $$$$ 

    Affine Sets Cosets | Abstract Algebra
           An affine set is a translation of a subspace — it is "flat" but does not necessarily pass through 0 as a subspace would. 
           An affine set $$\mathbf{A}$$ can always be represented as the translation of the subspace spanned by some vectors
           $$\\\\\\\$$ $$ \mathbf{A} = \left\{ x0 + \sum{i=1}^m \lambdai xi ~~ \lambda \in {\mathbf{R}}^m \right\}\ \ \ $$  
        for some vectors $$x0 x1 \ldots xm.$$  
        $$\implies \mathbf{A} = x0 + \mathbf{S}.$$

         Special case lines When $$\mathbf{S}$$ is the span of a single non zero vector the set $$\mathbf{A}$$ is called a line passing through the point $$x0$$. Thus lines have the form
        $$\left\{ x0 + tu ~~ t \in \mathbf{R} \right\}$$  
        where $$u$$ determines the direction of the line and $$x0$$ is a point through which it passes.

    Basis
            A basis of $${\mathbf{R}}^n$$ is a set of $$n$$ independent vectors. If the vectors $$u1 \ldots un$$ form a basis we can express any vector as a linear combination of the $$ui$$'s
           $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ x = \sum{i=1}^n \lambdai ui \ \ \ \text{for appropriate numbers } \lambda1 \ldots \lambdan$$.


    Dimension
           The number of vectors in the span of the sub space.

2. Norms and Scalar Products
    Scalar Product
       The scalar product or inner product or dot product between two vectors $$xy \in \mathbf{R}^n$$ is the scalar denoted $$x^Ty$$ and defined as 
       $$x^Ty = \sum{i=1}^n xi yi.$$ 

    Norms
       A measure of the "length" of a vector in a given space.
       Theorem. A function from $$\chi$$ to $$\mathbf{R}$$ is a norm if  
        1. $$\|x\| \geq 0 \ \forall x \in \chi$$ and $$\|x\| = 0 \iff x = 0$$.
        2. $$\|x+y\| \leq \|x\| + \|y\|$$ for any $$x y \in \chi$$ triangle inequality.
        3. $$\|\alpha x\| = \|\alpha\| \|x\|$$ for any scalar $$\alpha$$ and any $$x\in \chi$$.

    $$lp$$ Norms
       $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  $$\ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  $$\|x\|p = \left \sum{k=1}^n \|xk\|p \right^{1/p} \ 1 \leq p < \infty$$.


    The $$l1 norm$$
       $$ \|x\|1 = \sum{i=1}^n \| xi \| $$   
       Corresponds to the distance travelled on a rectangular grid to go from one point to another.  
        Induces a diamond shape

    The $$l2 norm$$ Euclidean Norm
       $$  \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \| x \|2 = \sqrt{ \sum{i=1}^n xi^2 } = \sqrt{x^Tx} $$.  
       Corresponds to the usual notion of distance in two or three dimensions.
    The $$l2 norm$$ is invariant under orthogonal transformations     
        i.e. $$\|x\|2 = \|Vz\|2 = \|z\|2$$ where $$V$$ is an orthogonal matrix. 
    The set of points with equal l2 norm is a circle in 2D a sphere in 3D or a hyper sphere in higher dimensions. 

    The $$l\infty norm$$
       $$ \| x \|\infty = \displaystyle\max{1 \le i \le n} \| xi \|$$  
    useful in measuring peak values.  
        Induces a square

    The Cardinality
       The Cardinality of a vector $$\vec{x}$$ is often called the $$l0$$ pseudo norm and denoted with  
       $$\|\vec{x}\|0$$.
    Defined as the number of non zero entries in the vector.


    Cauchy Schwartz inequality
       For any two vectors $$x y \in \mathbf{R}^n$$ we have  
       $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  $$x^Ty \le \|x\|2 \cdot \|y\|2$$.
    The above inequality is an equality if and only if $$x y$$ are collinear  
         $$ {\displaystyle \max{x  \ \|x\|2 \le 1} \ x^Ty = \|y\|2}$$ 
        with optimal $$x$$ given by  
        $$x^\ast = \dfrac{y}{\|y\|2} \ $$ if $$y$$ is non zero.

    Angles between vectors
       When none of the vectors xy is zero we can define the corresponding angle as theta such that
        $$\cos\  \theta = \dfrac{x^Ty}{\|x\|2 \|y\|2} .$$ 


3. Notes
     Norms and Metrics  
    Norms induce metrics on topological spaces but not the other way around.  
    Technically a norm is a metric with two additional properties 1 Translation Invariance 2 Absolute homogeneouity/scalability  
    We can always define a metric from a norm $$dxy = \|x  y \|$$  
    A metric is a function of two variables and a norm is a function of one variable.
     Collinearity  In geometry collinearity of a set of points is the property of their lying on a single line  
            






 The Generalized Max Sub Array Problem


# Final Thought and Conclusions

Rarding my answers during the Interview

 During the interview I was thinking of the dynamic programming approach of trying out the matrices and growing them in sizes after having precomputed their sum values.  
 I also tried exlporing the $$\mathcal{O}n^3$$ after you discusses the 1D approach.  
 The branch and bound method is interesting but solves the problem from a different perspective.  

Fther Development

 I believe that the $$\mathcal{O}n^3$$ utilizing Kadane algorithm could be improved by calling the algorithm only in the  loop not the  by smartly computing the overlapping values and going across cols then rows instead two runs i.e. constant.  
    This will lead the algorithm to be $$\mathcal{O}n^2$$ instead but the idea needs further exploration.  

 Another approach would be to rely on looking at the distribution of the numbers in the matrix linear then to sample smartly using an ML approach perhaps by fitting a hough transform that detects large sum "chunks".  

Fal Comments

 I will be updating this post whenever I have time.  
 Code has been Unit Tested and most but not all has been stress tested with edge cases.

Please note that all code and descriptions here were completely written by me.  However credit was given for the "C++" implementation of the "Box Struct".  All code descriptions and explanations are under a public license  Copyright C 2017 MIT




 1.1 | 1.2  Optimization Models


Mathematical Background

1. Maps
       We reserve the term map to refer to vector valued functions. That is maps are
    functions which return more than a single value.

2. Graph
    Consider a function $$f  \mathbf{R}^n \rightarrow \mathbf{R}$$.  
    The graph of $$f$$ is the set of input output pairs that $$f$$ can attain that is
    $$Gf = \left \{ xfx \in \mathbf{R}^{n+1}  x \in \mathbf{R}^n \right \}.$$ 
    It is a subset of $$\mathbf{R}^{n+1}$$.

3. Epigraph
    Consider a function $$f  \mathbf{R}^n \rightarrow \mathbf{R}$$.  
    The epigraph denoted $$\mathop{\bf epi} f$$ describes the set of input output pairs that $$f$$ can achieve as well as "anything above"  
    $$\mathop{\bf epi} f = \left \{ xt \in \mathbf{R}^{n+1} ~~ x \in \mathbf{R}^n \ \  t \ge fx \right \}.$$
    epi in Greek means "above"  

    
    


4. Level and Sub level Sets
    Level and sub level sets correspond to the notion of contour of a function. Both are indexed on some scalar value $$t$$.  

     Level sets is simply the set of points that achieve exactly some value for the function $$f$$.  
    For $$t \in \mathbf{R}$$ the $$t $$level set of the function $$f$$ is defined as  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{L}tf = \left\{ x \in \mathbf{R}^{n} ~~ x \in \mathbf{R}^n \ \  t = fx \right \}.$$

     Sub level sets is the set of points that achieve at most a certain value for  $$f$$ or below  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \mathbf{S}tf = \left\{ x \in \mathbf{R}^{n} ~~ x \in \mathbf{R}^n \ \  t \ge fx \right\}.$$  

    
    



Mathematical Formulation Standard Forms

1. Functional Form
       An optimization problem is a problem of the form
    $$p^\ast = \displaystyle\minx f0x   fix \le 0 \ \  i=1\ldots m$$  
    where  
         $$x \in \mathbf{R}^n$$ is the decision variable;

         $$f0  \mathbf{R}^n \rightarrow \mathbf{R}$$ is the objective function or cost; 

         $$fi  \mathbf{R}^n \rightarrow \mathbf{R} \ \  i=1 \ldots m$$ represent the constraints;

         $$p^\ast$$ is the optimal value.  



        

2. Epigraph form
       TODO

3. Other Standard Forms
       TODO


Nomenclature

1. Feasible set
       $$ \mathbf{X} =  \left\{ x \in \mathbf{R}^n ~~  fix \le 0 \ \  i=1 \ldots m \right\}.$$  

2. Solution
       In an optimization problem we are usually interested in computing the optimal value of the objective function and also often a minimizer which is a vector which achieves that value if any.

3. Feasibility problems
       Sometimes an objective function is not provided. This means that we are just interested in finding a feasible point or determine that the problem is infeasible.  
    By convention we set $$f0$$ to be a constant in that case to reflect the fact that we are indifferent to the choice of a point x as long as it is feasible.

4. Optimal value
        $$p^\ast = \minx  f0x ~~ fix \le 0 \ \  i=1 \ldots m.$$   

    Denoted $$p^\ast$$.

5. Optimal set
       The set of feasible points for which the objective function achieves the optimal value  
    $$ \mathbf{X}^{\rm opt} =  \left\{ x \in \mathbf{R}^n ~~  f0x = p^\ast \ \ fix \le 0 \ \  i=1\ldots m \right\}$$.  
    Equivalently  
    $$ \mathbf{X}^{\rm opt} = \mathrm{arg min}{x \in \mathbf{X}}  f0x$$.  

    We take the convention that the optimal set is empty if the problem is not feasible.  

    A point $$x$$ is said to be optimal if it belongs to the optimal set.

    If the optimal value is ONLY attained in the limit then it is NOT in the optimal set.

6. When is a problem "Attained"?
       If the optimal set is not empty we say that the problem is attained.

7. Suboptimality
       The $$\epsilon$$ suboptimal set is defined as  
       $$ \mathbf{X}\epsilon = \left\{ x \in \mathbf{R}^n ~~ fix \le 0 \ \  i=1 \ldots m \ \  f0x \le p^\ast + \epsilon \right\}.$$  

    $$\implies \  \mathbf{X}0 = \mathbf{X}{\rm opt}$$.

8. Local and Global Optimality
     A point $$z$$ is Locally Optimal if there is a value $$R>0$$ such that $$z$$ is optimal for the following problem
    $$minx  f0x ~~ fix \le 0 \ \ i=1 \ldots m  \ \ \|z x\|2 \le R$$.  
    i.e. a local minimizer $$x$$ minimizes $$f0$$ but only for nearby points on the feasible set.

     A point $$z$$ is Globally Optimal if it is the optimal value of the original problem on all of the feasible region.   


Problem Classes

1. Least squares
       $$\minx \;\;\;\; \sum{i=1}^m \left \sum{j=1}^n A{ij} . xj  bi \right^2$$
       where $$A{ij} \  bi \  1 \le i \le m  \ 1 \le j \le n$$ are given numbers and $$x \in \mathbf{R}^n$$ is the variable.

2. Linear Programming
       $$ \min \sum{j=1}^n cjxj ~~ \sum{j=1}^n A{ij} . xj  \le bi  \;\; i=1 \ldots m $$ 
       where $$ cj bi$$ and $$A{ij} \  1 \le i \le m \  1 \le j \le n$$ are given real numbers.  

    This corresponds to the case where the functions $$fii=0 \ldots m$$ in the standard problem are all affine that is linear plus a constant term.  

    Denoted $$LP$$.

3. Quadratic Programming
       $$\minx \;\;\;\; \displaystyle\sum{i=1}^m \left\sum{j=1}^n C{ij} . xj+di\right^2 + \sum{i=1}^n cixi \;\;\;\;\; \sum{j=1}^m A{ij} . xj \le bi \;\;\;\; i=1\ldotsm.$$  

    Includes a sum of squared linear functions in addition to a linear term in the objective.  

    QP's are popular in finance where the linear term in the objective refers to the expected negative return on an investment and the squared terms corresponds to the risk or variance of the return.  

    QP was introduced by "Markowitz"

4. Nonlinear optimization
       A broad class that includes Combinatorial Optimization.

    One of the reasons for which non linear problems are hard to solve is the issue of local minima.

5. Convex optimization
        A generalization of QP where the objective and constraints involve "bowl shaped" or convex functions.

    They are easy to solve because they do not suffer from the "curse" of local minima.

6. Combinatorial optimization
       In combinatorial optimization some or all the variables are boolean or integers reflecting discrete choices to be made.

    Combinatorial optimization problems are in general extremely hard to solve. Often they can be approximately solved with linear or convex programming.

7. NON Convex Optimization Problems Examples
     Boolean/integer optimization some variables are constrained to be Boolean or integers.  
    Convex optimization can be used for getting sometimes good approximations.
     Cardinality constrained problems we seek to bound the number of non zero elements in a vector variable.  
    Convex optimization can be used for getting good approximations.
     Non linear programming usually non convex problems with differentiable objective and functions.  
    Algorithms provide only local minima.  

    Most but not all non convex problems are hard!


 1.2  Introduction to Optimization



Nomenclature

1. Feasible set
       $$ \mathbf{X} =  \left\{ x \in \mathbf{R}^n ~~  fix \le 0 \ \  i=1 \ldots m \right\}.$$  

2. Solution
       In an optimization problem we are usually interested in computing the optimal value of the objective function and also often a minimizer which is a vector which achieves that value if any.

3. Feasibility problems
       Sometimes an objective function is not provided. This means that we are just interested in finding a feasible point or determine that the problem is infeasible.  
    By convention we set $$f0$$ to be a constant in that case to reflect the fact that we are indifferent to the choice of a point x as long as it is feasible.

4. Optimal value
        $$p^\ast = \minx  f0x ~~ fix \le 0 \ \  i=1 \ldots m.$$   

    Denoted $$p^\ast$$.

5. Optimal set
       The set of feasible points for which the objective function achieves the optimal value  
    $$ \mathbf{X}^{\rm opt} =  \left\{ x \in \mathbf{R}^n ~~  f0x = p^\ast \ \ fix \le 0 \ \  i=1\ldots m \right\}$$.  
    Equivalently  
    $$ \mathbf{X}^{\rm opt} = \mathrm{arg min}{x \in \mathbf{X}}  f0x$$.  

    We take the convention that the optimal set is empty if the problem is not feasible.  

    A point $$x$$ is said to be optimal if it belongs to the optimal set.

    If the optimal value is ONLY attained in the limit then it is NOT in the optimal set.

6. When is a problem "Attained"?
       If the optimal set is not empty we say that the problem is attained.

7. Suboptimality
       The $$\epsilon$$ suboptimal set is defined as  
       $$ \mathbf{X}\epsilon = \left\{ x \in \mathbf{R}^n ~~ fix \le 0 \ \  i=1 \ldots m \ \  f0x \le p^\ast + \epsilon \right\}.$$  

    $$\implies \  \mathbf{X}0 = \mathbf{X}{\rm opt}$$.

8. Local and Global Optimality
     A point $$z$$ is Locally Optimal if there is a value $$R>0$$ such that $$z$$ is optimal for the following problem
    $$minx  f0x ~~ fix \le 0 \ \ i=1 \ldots m  \ \ \|z x\|2 \le R$$.  
    i.e. a local minimizer $$x$$ minimizes $$f0$$ but only for nearby points on the feasible set.

     A point $$z$$ is Globally Optimal if it is the optimal value of the original problem on all of the feasible region.   


Problem Classes

1. Least squares
       $$\minx \;\;\;\; \sum{i=1}^m \left \sum{j=1}^n A{ij} . xj  bi \right^2$$
       where $$A{ij} \  bi \  1 \le i \le m  \ 1 \le j \le n$$ are given numbers and $$x \in \mathbf{R}^n$$ is the variable.

2. Linear Programming
       $$ \min \sum{j=1}^n cjxj ~~ \sum{j=1}^n A{ij} . xj  \le bi  \;\; i=1 \ldots m $$ 
       where $$ cj bi$$ and $$A{ij} \  1 \le i \le m \  1 \le j \le n$$ are given real numbers.  

    This corresponds to the case where the functions $$fii=0 \ldots m$$ in the standard problem are all affine that is linear plus a constant term.  

    Denoted $$LP$$.

3. Quadratic Programming
       $$\minx \;\;\;\; \displaystyle\sum{i=1}^m \left\sum{j=1}^n C{ij} . xj+di\right^2 + \sum{i=1}^n cixi \;\;\;\;\; \sum{j=1}^m A{ij} . xj \le bi \;\;\;\; i=1\ldotsm.$$  

    Includes a sum of squared linear functions in addition to a linear term in the objective.  

    QP's are popular in finance where the linear term in the objective refers to the expected negative return on an investment and the squared terms corresponds to the risk or variance of the return.  

    QP was introduced by "Markowitz"

4. Nonlinear optimization
       A broad class that includes Combinatorial Optimization.

    One of the reasons for which non linear problems are hard to solve is the issue of local minima.

5. Convex optimization
        A generalization of QP where the objective and constraints involve "bowl shaped" or convex functions.

    They are easy to solve because they do not suffer from the "curse" of local minima.

6. Combinatorial optimization
       In combinatorial optimization some or all the variables are boolean or integers reflecting discrete choices to be made.

    Combinatorial optimization problems are in general extremely hard to solve. Often they can be approximately solved with linear or convex programming.

7. NON Convex Optimization Problems Examples
     Boolean/integer optimization some variables are constrained to be Boolean or integers.  
    Convex optimization can be used for getting sometimes good approximations.
     Cardinality constrained problems we seek to bound the number of non zero elements in a vector variable.  
    Convex optimization can be used for getting good approximations.
     Non linear programming usually non convex problems with differentiable objective and functions.  
    Algorithms provide only local minima.  

    Most but not all non convex problems are hard!


 1.1  Background


Mathematical Background

1. Maps
       We reserve the term map to refer to vector valued functions. That is maps are
    functions which return more than a single value.

2. Graph
    Consider a function $$f  \mathbf{R}^n \rightarrow \mathbf{R}$$.  
    The graph of $$f$$ is the set of input output pairs that $$f$$ can attain that is
    $$Gf = \left \{ xfx \in \mathbf{R}^{n+1}  x \in \mathbf{R}^n \right \}.$$ 
    It is a subset of $$\mathbf{R}^{n+1}$$.

3. Epigraph
    Consider a function $$f  \mathbf{R}^n \rightarrow \mathbf{R}$$.  
    The epigraph denoted $$\mathop{\bf epi} f$$ describes the set of input output pairs that $$f$$ can achieve as well as "anything above"  
    $$\mathop{\bf epi} f = \left \{ xt \in \mathbf{R}^{n+1} ~~ x \in \mathbf{R}^n \ \  t \ge fx \right \}.$$
    epi in Greek means "above"  

    
    


4. Level and Sub level Sets
    Level and sub level sets correspond to the notion of contour of a function. Both are indexed on some scalar value $$t$$.  

     Level sets is simply the set of points that achieve exactly some value for the function $$f$$.  
    For $$t \in \mathbf{R}$$ the $$t $$level set of the function $$f$$ is defined as  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{L}tf = \left\{ x \in \mathbf{R}^{n} ~~ x \in \mathbf{R}^n \ \  t = fx \right \}.$$

     Sub level sets is the set of points that achieve at most a certain value for  $$f$$ or below  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \mathbf{S}tf = \left\{ x \in \mathbf{R}^{n} ~~ x \in \mathbf{R}^n \ \  t \ge fx \right\}.$$  

    
    



Mathematical Formulation Standard Forms

1. Functional Form
       An optimization problem is a problem of the form
    $$p^\ast = \displaystyle\minx f0x   fix \le 0 \ \  i=1\ldots m$$  
    where  
         $$x \in \mathbf{R}^n$$ is the decision variable;

         $$f0  \mathbf{R}^n \rightarrow \mathbf{R}$$ is the objective function or cost; 

         $$fi  \mathbf{R}^n \rightarrow \mathbf{R} \ \  i=1 \ldots m$$ represent the constraints;

         $$p^\ast$$ is the optimal value.  



        

2. Epigraph form
       TODO

3. Other Standard Forms
       TODO


 Convex Optimization


Point Set Topology

1. Open Set
       A set $$\chi \subseteq \mathbf{R}^n$$ is said to be open if for any point $$x \in \chi$$ there exist a ball centered in $$x$$ which is contained in $$\chi$$. 
       Precisely for any $$x \in \mathbf{R}^n$$ and $$\epsilon > 0$$ define the Euclidean ball of radius $$r$$ centered at $$x$$
       $$B\epsilonx = {z  \|z − x\|2 < \epsilon}$$
       Then $$\chi \subseteq \mathbf{R}^n$$ is open if
       $$\forall x \ \epsilon \ \chi \\ \exists \epsilon > 0  B\epsilonx \subset \chi .$$
       Equivalently
        A set $$\chi \subseteq \mathbf{R}^n$$ is open if and only if $$\chi = int\; \chi$$.
        An open set does not contain any of its boundary points.
        A closed set contains all of its boundary points. 
        Unions and intersections of open resp. closed sets are open resp. closed.

2. Closed Set
       A set $$\chi \subseteq \mathbf{R}^n$$ is said to be closed if its complement $$ \mathbf{R}^n \text{ \ } \chi$$ is open.

3. Interior of a Set
       The interior of a set $$\chi \subseteq \mathbf{R}^n$$ is defined as 
       $$int\ \chi = \{z \in \chi  B\epsilonz \subseteq \chi \\ \text{for some } \epsilon > 0 \}$$

4. Closure of a Set
       The closure of a set $$\chi \subseteq \mathbf{R}^n$$ is defined as
       $$\bar{\chi} = \{z ∈ \mathbf{R}^n  \ z = \lim{k\to\infty} xk \ xk \in \chi  \ \forall k\}$$  
    i.e. the closure of $$\chi$$ is the set of limits of sequences in $$\chi$$.

5. Boundary of a Set
       The boundary of X is defined as
       $$\partial \chi = \bar{\chi} \text{ \ }  int\ \chi$$

6. Bounded Set
       A set $$\chi \subseteq \mathbf{R}^n$$ is said to be bounded if it is contained in a ball of finite radius that is if there exist $$x \in \mathbf{R}^n$$ and $$r > 0$$ such that $$\chi \subseteq Brx$$.

7. Compact Set
       A set $$\chi \subseteq \mathbf{R}^n$$ is compact $$\iff$$ it is Closed and Bounded.

8. Relative Interior $$\operatorname{relint}$$
       We define the relative interior of the set $$\chi$$ denoted $$\operatorname{relint} \chi$$ as its interior relative to $$\operatorname{aff} C$$
       $$\operatorname{relint} \chi = \{x \in \chi  \ Bx r \cap \operatorname{aff} \chi \subseteq \chi \text{ for some } r > 0\}$$
       where $$Bx r = \{y  ky − xk \leq r\}$$ the ball of radius $$r$$ and center $$x$$ in the norm $$\| · \|$$.

9. Relative Boundary
       We can then define the relative boundary of a set $$\chi$$ as $$\mathbf{cl}  \chi \text{ \ } \operatorname{relint} \chi$$ where $$\mathbf{cl} \chi$$ is the closure of $$\chi$$.


Sets Combinations and Hulls

1. Lines and Line Segments Linear Sets
       Suppose $$x1 \ne x2$$ are two points in $$\mathbf{R}^n$$
       Points of the form 
       $$y = \theta x1 + 1 − \thetax2$$
       where $$\theta \in \mathbf{R}$$ form the line passing through $$x1$$ and $$x2$$. 
       The parameter value $$\theta = 0$$ corresponds to $$y = x2$$ and the parameter value $$\theta = 1$$ corresponds to $$y = x1$$.
       Values of the parameter $$\theta$$ between 0 and 1 correspond to the closed line segment between $$x1$$ and $$x2$$.

2. Affine Sets
       An affine set is a translation of a subspace — it is "flat" but does not necessarily pass through 0 as a subspace would. 
        Think for example of a line or a plane that does not go through the origin.
       An affine set $$\mathbf{A}$$ can always be represented as the translation of the subspace spanned by some vectors
       $$ \mathbf{A} = \left\{ x0 + \sum{i=1}^m \lambdai xi ~~ \lambda \in {\mathbf{R}}^m \right\}\ \ \ $$ for some vectors $$x0 x1 \ldots xm.$$  

    $$\implies \mathbf{A} = x0 + \mathbf{S}.$$

     Special case lines When $$\mathbf{S}$$ is the span of a single non zero vector the set $$\mathbf{A}$$ is called a line passing through the point $$x0$$. Thus lines have the form
    $$\left\{ x0 + tu ~~ t \in \mathbf{R} \right\}$$  
    where $$u$$ determines the direction of the line and $$x0$$ is a point through which it passes.

     
    
  

3. Cones Cone Sets
       A set $$C$$ is a cone if $$x \in C$$ then $$\alpha x \in C$$ for every $$\alpha \geq 0$$. 
       A set C is said to be a convex cone if it is convex and it is a cone.  
        The conic hull of a set is a convex cone.

77.Linear Combination
       A Linear Combination is an expression constructed from a set of terms by multiplying each term by a constant and adding the results.
       $$ \sum{i=1}^n \lambdai xi $$

88.Affine Combination
       An Affine Combination of the points is a special type of linear combination in which
    the coefficients $$\lambdai$$ are restricted to sum up to one that is
       $$\sum{i=1}^n \lambdai xi \  \\ \sum{i=1}^m \lambdai = 1$$
    Intuitively a convex combination is a weighted average of the points with weights
    given by the $$\lambdai$$ coefficients.

99.Conical Combination
       A Conical Combination of the points is a special type of linear combination in which
    the coefficients $$\lambdai$$ are restricted to be nonnegative that is
       $$\sum{i=1}^n \lambdai xi \  \\ \lambdai \geq 0 \ \text{for all } i$$

10.Convex Combination
       A Convex Combination of the points is a special type of linear combination in which
    the coefficients $$\lambdai$$ are restricted to be nonnegative and to sum up to one that is
       $$\lambdai \geq 0 \ \text{for all }  i \ \text{ and } \sum{i=1}^m \lambdai = 1$$
    Intuitively a convex combination is a weighted average of the points with weights
    given by the $$\lambdai$$ coefficients.

8. Linear Hull
       Given a set of points vectors $$\in  \mathbf{R}^n$$  
       $$P = \{x^{1}  . . .  x^{m} \}$$
       The linear hull subspace generated by these points is the set of all possible linear
    combinations of the points
       $$x=\lambda1x^{1} + \cdots + \lambdamx^{m} \\ \text{for } \lambdai \in \mathbf{R} \ i \in \{1 \cdots m\}$$

9. Affine Hull
       The affine hull $$\operatorname{aff}\ P$$ of $$P$$ is the set generated by taking all possible linear
    combinations of the points in $$P$$ under the restriction that the coefficients $$\lambdai$$ sum up to one that is $$\sum{i=1}^m \lambdai = 1$$.
       $$\operatorname{aff}\ P$$ is the smallest affine set containing $$P$$.
        Props.  
             It is the smallest affine set containing $$\chi$$. 
             or The intersection of all affine sets containing $$\chi$$.
             $${\displaystyle \mathrm {aff} \mathrm {aff} S=\mathrm {aff} S}$$
             $${\mathrm{aff}}S$$ is a closed set
             $${\displaystyle \mathrm {aff} S+F=\mathrm {aff} S+\mathrm {aff} F}$$
             Affine Hull is bigger than or equal to the convex hull.
             The linear span of $$\chi$$ contains the affine hull of $$\chi$$.
        Examples  
             The affine hull of a singleton a set made of one single element is the singleton itself.
             The affine hull of a set of two different points is the line through them.
             The affine hull of a set of three points not on one line is the plane going through them.
             The affine hull of a set of four points not in a plane in $$\mathbf{R}^3$$ is the entire space $$\mathbf{R}^3$$.

11.Convex Hull
       The set of all possible convex combination is called the convex hull of the point set $$\chi$$ in the Euclidean plane or in a Euclidean space or more generally in an affine space over the reals is the smallest convex set that contains $$\chi$$
       $$\mathbf{conv} x^{1} \cdots x^{m} = \left\{\sum{i=1}^m \lambdai x^{i}  \ \lambdai \geq 0 \ i \in \{1 \cdots m\}; \\ \sum{i=1}^m \lambdai = 1\right\}$$
        Props.  
             The convex hull of the given points is identical to the set of all their convex combinations.
             It is the intersection of all convex sets containing $$\chi$$.
             or The set of all convex combinations of points in $$\chi$$.
             or The unique minimal convex set containing $$\chi$$.
             or The union of all simplices with vertices in $$\chi$$.
             The algorithmic problem of finding the convex hull of a finite set of points in the plane or other low dimensional Euclidean spaces is one of the fundamental problems of computational geometry.
             The convex hull of a finite point set $${\displaystyle S\subsetneq \mathbb {R} ^{n}}$$ forms a convex polygon when $$n = 2$$
             or more generally a convex polytope in $${\displaystyle \mathbb {R} ^{n}}$$.

12.Conic Hull
       The set of all possible conical combinations is called the conic hull of the point set
       $$\mathbf{conic} x^{1} \cdots x^{m} = \left\{\sum{i=1}^m \lambdai x^{i}  \ \lambdai \geq 0 \ i \in \{1 \cdots m\} \right\}$$
        Props.  
             The conical hull of a set $$\chi$$ is a convex set.
             In fact it is the intersection of all convex cones containing $$\chi$$ plus the origin.
             If $$\chi$$ is a compact set in particular when it is a finite non empty set of points then the condition "plus the origin" is unnecessary.
             If we discard the origin we can divide all coefficients by their sum to see that a conical combination is a convex combination scaled by a positive factor.
             Conical combinations and hulls may be considered as convex combinations and convex hulls in the projective space.
             The conic hull of a closed set is not even necessarily a closed set.
             While the convex hull of a compact set is a compact set as well this is not so for the conical hull the latter is Unboudned.


Convex Set

0. Convex Set
       A subset $$\mathbf{C}$$ of $$\mathbf{R}^n$$ is said to be convex if and only if it contains the line segment between any two points in it  
     $$ \forall  x1 x2 \in \mathbf{C} \;\; \forall  \lambda \in 01 \\ \lambda x1 + 1 \lambda  x2 \in \mathbf{C}$$ 

11.Strictly Convex Set
       A set C is said to be strictly convex if it is convex and 
       $$x1 \ne x2 \in C \ \lambda \in 0 1 \implies \lambda x1 + 1 − \lambdax2 \in \mathbf{relint} C$$

2. Strongly Convex
       A function $$ f \mathbf{R}^n \rightarrow \mathbf{R}$$ is strongly convex if there exist a $$m > 0$$ such that $$\tilde{f}x = fx − \dfrac{m}{2}\|x\|2^2$$ is convex that is if
       $$ f\theta x + 1 \theta y \leq \theta fx + 1 \theta fy  \dfrac{m}{2}\theta1 \theta \|x y\|2^2$$ 

1. Diminsion
       The dimension d of a convex set $$C \subseteq \mathbf{R}^n$$ is defined as the dimension of its affine hull. 
       It can happen that $$d < n$$.  
        e.g. $$C = \left\{x = \left\alpha 0\right^T  \; \alpha ∈ 0 1\right\}$$ is a convex subset of $$\mathbf{R}^2$$  with affine dimension $$d = 1$$.



Prominant Examples

1. Convex Examples
       Subspaces and affine sets such as lines and hyperplanes are obviously convex as
    they contain the entire line passing through any two points. 
       Half spaces are also convex.
            
    

            
    





Operators and Convexity

1. Intersection
       The intersection of a possibly infinite family of convex sets is convex. This property can be used to prove convexity for a wide variety of situations.
       Ex An halfspace $$H = \{x \in \mathbf{R}^n  \ c^Tx \leq d\} c \ne 0$$ is a convex set. The intersection of $$m$$ halfspaces $$Hi i = 1 \cdots m$$ is a convex set called a polyhedron.
            
    

            
    


2. Affine Transformation
       If a map $$f \ \mathbf{R}^n \rightarrow \mathbf{R}^m$$ is affine and $$\mathbf{C}$$ is convex then the set
       $$f\mathbf{C} \ = \left\{ fx  \ x \in \mathbf{C} \right\}$$
       is convex.  
        In particular the projection of a convex set on a subspace is convex.
       
    

        
    


3. Composition w/ Affine Function
       The composition with an affine function preserves convexity 
       If $$A \in \mathbf{R}^{m \times n} b \in \mathbf{R}^m \text{ and } f  \mathbf{R}^m \rightarrow \mathbf{R}$$
       is convex then the function $$g  \mathbf{R}^n \rightarrow \mathbf{R}$$ with values $$gx = fAx+b$$ is convex.

4. Point Wise Maximum
       The pointwise maximum of a family of convex functions is convex  
       If $$f\alpha{\alpha \in {\cal A}}$$ is a family of convex functions index by $$\alpha$$ then the function
       $$fx = \max{\alpha \in {\cal A}}  f\alphax$$
       is convex. 
       
    

        
    
        
    


5. Nonnegative Weighted Sum
       The nonnegative weighted sum of convex functions is convex.
        
    
       
    



6. Partial Minimum
       If $$f$$ is a convex function in $$x=yz$$ then the function 
       $$gy = \minz \ fyz $$
       is convex.
    Note that joint convexity in $$yz$$ is essential.
        
    

7. Composition W/ Monotone Convex Functions.
       The composition with another function does not always preserve convexity. However if f = h circ g with hg convex and h increasing then f is convex.
       Indeed the condition $$fx \le t$$ is equivalent to the existence of $$y$$ such that
       $$hy \le t \;\; gx \le y$$ 
       The condition above defines a convex set in the space of $$xyt$$ variables. 
       The epigraph of $$f$$ is thus the projection of that convex set on the space of $$xt$$ variables hence it is convex.
        
    

 Further Analysis  
       More generally if the functions $$gi  \mathbf{R}^n \rightarrow \mathbf{R} i=1 \ldots k$$ are convex and $$h  \mathbf{R}^k \rightarrow \mathbf{R}$$ is convex and non decreasing in each argument with $$\mathbf{dom}gi = \mathbf{dom} h = \mathbf{R}$$ then
       $$x \rightarrow h \circ gx \ = hg1x \ldots gkx $$
       is convex.
    For example if $$gi$$'s are convex then  $$log \sumi \exp{gi}$$ also is.

Seperation Theorems
Separation theorems are one of the most important tools in convex optimization. They convey the intuitive idea that two convex sets that do not intersect can be separated by a straight line.

1. Theorem. Supporting Hyperplane
       If $$\mathbf{C} \subseteq \mathbf{R}^n$$ is convex and non empty then for any $$x0$$ at the boundary of $$\mathbf{C}$$ there exist a supporting hyperplane to $$\mathbf{C}$$ at $$x0$$   
    meaning that there exist $$a \in \mathbf{R}^n \ a \ne 0 $$ such that $$a^Tx x0 \le 0$$ for every $$x \in \mathbf{C}$$.
        
    


2. Theorem. Separating Hyperplane
       If $$\mathbf{C} \mathbf{D}$$ are two convex subsets of $$\mathbf{R}^n$$ that do not intersect then there is an hyperplane that separates them  
    that is $$\exists a \in \mathbf{R}^n \ a \ne 0 $$ and $$b \in \mathbf{R}$$ such that   $$a^Tx \le b$$ for every $$x \in \mathbf{C}$$ and $$a^Tx \ge b$$ for every $$x \in \mathbf{D}$$.
       Equivalently 
       Let C D ⊆ Rn be nonempty convex disjoint sets i.e. $$C \cap D = \varnothing$$.
       Then there exists a hyperplane separating these sets i.e. 
       $$ \exists  a \in \mathbf{R}^n \ a \ne 0$$ such that
       $$  \sup{x\in C} a^Tx \ \leq \  \sup{z\in D}a^Tz$$  
        
    

    When two convex sets do not intersect it is possible to find a hyperplane that separates them.

3. Theorem. Strictly Separating Hyperplane
       Let $$C D \subseteq \mathbf{R}^n$$ be nonempty convex disjoint sets.
       Assume that $$C − D$$ is closed. Then there exists a hyperplane strictly. separating the sets i.e. $$\exists \ a \in \mathbf{R}^n \ a \ne 0$$ such that
       $$  \sup{x\in C} a^Tx \ < \  \sup{z\in D}a^Tz$$  
     When is $$C − D$$ closed?   
          > One of conditions $$C$$ is closed and $$D$$ is compact.

4. Farkas lemma
       Let $$A \in \mathbf{R}^{m \times n}$$ and $$y \in \mathbf{R}^m$$. Then one and only one of the following two conditions is satisfied  
        1. The system of linear equations $$Ax = y$$ admits a nonnegative solution $$x \geq 0$$.
        2. There exist $$z \in \mathbf{R}^m$$ such that $$z^TA \geq 0 \ z^Ty < 0$$.
        Equivalent Formulation statement 2 above implies the negation of statement 1 and vice versa. Thus the following two statements are equivalent   
        1. There exist $$x \geq 0$$ such that $$Ax = y$$.
        2. $$z^Ty \geq 0 \\ \forall z  \ z^TA \geq 0$$.
        Interpretation in terms of systems of linear inequalities
            Let $$ai \in \mathbf{R}^m i = 1 \cdots n$$ be the columns of $$A$$ then
       $$y^Tz \geq 0 \forall z  ai^Tz \geq 0 i = 1 \cdots n$$
       if and only if there exist multipliers $$xi \geq 0 i = 1 \cdots n$$ such that $$y$$ is a conic combination of the $$ai$$’s
       $$ \exists xi \geq 0 i = 1 \cdots m  \ y = a1x1 + \cdots + anxn.$$




Convex Functions

1. Domain
       The domain of a function $$f \mathbf{R}^n \rightarrow \mathbf{R}$$ is the set $$\mathbf{dom} f \subseteq \mathbf{R}^n$$ over which $$f$$ is well defined in other words
       $$\mathbf{dom} f \ = \{ x \in \mathbf{R}^n  \  \infty < fx < +\infty\}.$$

2. Convex Function
       A function $$f  \mathbf{R}^n \rightarrow \mathbf{R}$$ is convex if
       1 Its domain $$\mathbf{dom} f$$ is convex.
       2 And $$\forall \\ x y \in \mathbf{dom} f  \;\; \forall \theta \in 01  \\ f\theta x + 1 \theta y \le \theta fx + 1 \theta fy.$$
    Note that the convexity of the domain is required.

3. Concave Function
       A function $$f$$ is concave if the function $$ f$$ is convex.

4. Convexity and the Epigraph
       A function $$f  \mathbf{R}^n \rightarrow \mathbf{R}$$ is convex if and only if its epigraph
       $$ \mathbf{ epi } f \=  \ \left\{ xt \in \mathbf{R}^{n+1}  \ t \ge fx \right\} $$
       is convex.
       Example We can us this result to prove for example that the largest eigenvalue function $$\lambda{\rm max}  \mathcal{S}^n \rightarrow \mathbf{R}$$ which to a given $$n \times n$$ symmetric matrix $$X$$ associates its largest eigenvalue is convex since the condition $$\lambda{\rm max}X \le t$$ is equivalent to the condition that $$t I  X \in \mathcal{S}+^n$$.

5.  order condition
       If f is differentiable that is $$\mathbf{dom} f$$ is open and the gradient exists everywhere on the domain then $$f$$ is convex if and only if
       $$ \forall  x y  \ fy \ge fx + \nabla fx^Ty x .$$
    The geometric interpretation is that the graph of $$f$$ is bounded below everywhere by anyone of its tangents.
       
    


6. Restriction to a line
       The function $$f$$ is convex if and only if its restriction to any line is convex meaning that 
       for every $$x0 \in \mathbf{R}^n$$ and $$v \in \mathbf{R}^n$$ the function $$gt = fx0+tv$$ is convex. 
       Note that the "if" part is a direct consequence of the "composition with an affine function" result below.
       
    

       
    


7.  order Condition
       If $$f$$ is twice differentiable then it is convex if and only if its Hessian $$\nabla^2 f$$ is positive semi definite everywhere on the domain of $$f$$.
    This is perhaps the most commonly known characterization of convexity.
       Also If $$f$$ is twice differentiable then it is Strictly convex if and only if its Hessian $$\nabla^2 f$$ is positive definite everywhere on the domain of $$f$$.
       Finally If $$f$$ is twice differentiable then it is Strongly convex if and only if its Hessian $$\nabla^2 f \succeq ml$$ for some $$m > 0$$ and for all $$x \in \mathbf{dom} f$$.
       
    

       
    

       
    

       
    




 3.6  Robust Linear Programming



Introduction

1. Robust Linear Programming
       Robust Linear Programming addresses linear programming problems where the data is uncertain and a solution which remains feasible despite that uncertainty is sought.
       The robust counterpart to an LP is not an LP in general but is always convex. The figure on the left illustrates the feasible set of the "robust counterpart" of an LP after we take into account uncertainty in the facets' directions.

2. Uncertainty Models
       We have three models for Tractable cases of uncertainty  
        1. Scenario uncertainty
        2. Box uncertainty
        3. Ellipsoidal uncertainty


Tractable Cases

1. Scenario Uncertainty
        Uncertainty model In the scenario uncertainty model the uncertainty on a coefficient vector a is described by a finite set of points
        $$\mathbf{U} = \left\{ a^1 \ldots a^K \right\}$$
       where each vector $$a^k \in \mathbf{R}^n k=1 \ldots K$$ corresponds to a particular “scenario”.
        The robust counterpart to a half space constraint
       $$\forall \ a \in \mathbf{U}  \;\; a^Tx \le b $$
       can be simply expressed as a set of $$K$$ affine inequalities
       $$ a^k^Tx \le b \;\; k= 1 \ldots K. $$
    Note that the scenario model actually enforces more than feasibility at the “scenario” points $$a^k$$.   
          > In fact for any $$a$$ that is in the convex hull of the set $$\mathbf{U}$$ the robust counterpart holds.
       Indeed if the above holds then for any set of nonnegative weights $$\lambda1 \ldots \lambdaK$$ summing to one we have  
       $$ \sum{k=1}^K \lambdak a^k^Tx \le b \;\; k= 1 \ldots K. $$
       $$\implies$$
        The robust counterpart to the original LP
       $$\minx \ c^Tx ~~ \forall \ ai \in \mathbf{U}i  \;\; ai^Tx \le bi  \;\; i= 1 \ldots m$$
       with $$\mathbf{U}i = \{ ai^1 \ldots ai^{Ki} \} i= 1 \ldots m$$ becomes
       $$\minx \ c^Tx ~~ ai^k^Tx \le bi \;\; k=1 \ldots Ki \;\; i= 1 \ldots m$$
       where this is an LP with a total of $$K1+...+Km$$ constraints where $$Ki$$ is the number of elements in the finite set $$\mathbf{U}i$$ and $$m$$ is the number of constraints in the original nominal LP.
        The scenario model is attractive for its simplicity. However the number of scenarios can result in too large a problem.
 
2. Box Uncertainty
        Uncertainty model The box uncertainty model assumes that every coefficient vector $$ai$$ lies in a "box" or more generally a hyper rectangle $$\in \mathbf{R}^n$$ but is otherwise unknown.
        In its simplest case the uncertainty model has the following form
       $$ \mathbf{U} = \left\{ a ~~ |a \hat{a}|\infty \le \rho \right\}$$
       where $$\rho \ge 0$$ is a measure of the size of the uncertainty and $$\hat{a}$$ represents a "nominal" vector.   
        This describes a "box" of half diameter $$\rho$$ around the center $$\hat{a}$$.
       























 3.2  Linear Programming


Polyhedra

1. Linear Programs
       A linear program LP is an optimization problem in standard form in which all the functions involved are affine. The feasible set is thus a polyhedron that is an intersection of half spaces.

2. Polyhedral function
       Polyhedral functions are functions with a polyhedral epigraph and include maxima or sums of maxima of linear or affine functions. Such functions can be minimized via LP.

3. Half spaces
       A half space is a set defined by a single affine inequality. Precisely a half space $$\in \mathbf{R}^n$$ is a set of the form
 $$mathbf{H} = left{ x ~~ a^Tx le b right}$$ 
where $$a in mathbf{R}^n b in mathbf{R}$$.



 2.1  Basics and Definitions


Definitions

1. Linear Independence
       A set of vectors $$\{x1 ...  xm\} \in {\mathbf{R}}^n i=1 \ldots m$$ is said to be independent if and only if the following condition on a vector $$\lambda \in {\mathbf{R}}^m$$  
       $$\sum{i=1}^m \lambdai xi = 0 \ \ \ \implies  \lambda = 0.$$

        i.e. no vector in the set can be expressed as a linear combination of the others.


2. Subspace
       A subspace of $${\mathbf{R}}^n$$ is a subset that is closed under addition and scalar multiplication. Geometrically subspaces are "flat" like a line or plane in 3D and pass through the origin.  

     A Subspace $$\mathbf{S}$$ can always be represented as the span of a set of vectors $$xi \in {\mathbf{R}}^n i=1 \ldots m$$ that is as a set of the form  
    $$\mathbf{S} = \mbox{ span}x1 \ldots xm = \left\{ \sum{i=1}^m \lambdai xi ~~ \lambda \in {\mathbf{R}}^m \right\}.$$


3. Affine Sets Cosets | Abstract Algebra
       An affine set is a translation of a subspace — it is "flat" but does not necessarily pass through 0 as a subspace would. 
        Think for example of a line or a plane that does not go through the origin.
       An affine set $$\mathbf{A}$$ can always be represented as the translation of the subspace spanned by some vectors
       $$ \mathbf{A} = \left\{ x0 + \sum{i=1}^m \lambdai xi ~~ \lambda \in {\mathbf{R}}^m \right\}\ \ \ $$ for some vectors $$x0 x1 \ldots xm.$$  

    $$\implies \mathbf{A} = x0 + \mathbf{S}.$$

     Special case lines When $$\mathbf{S}$$ is the span of a single non zero vector the set $$\mathbf{A}$$ is called a line passing through the point $$x0$$. Thus lines have the form
    $$\left\{ x0 + tu ~~ t \in \mathbf{R} \right\}$$  
    where $$u$$ determines the direction of the line and $$x0$$ is a point through which it passes.

     
    



4. Basis
        A basis of $${\mathbf{R}}^n$$ is a set of $$n$$ independent vectors. If the vectors $$u1 \ldots un$$ form a basis we can express any vector as a linear combination of the $$ui$$'s
       $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ x = \sum{i=1}^n \lambdai ui \ \ \ \text{for appropriate numbers } \lambda1 \ldots \lambdan$$.


5. Dimension
       The number of vectors in the span of the sub space.


Norms and Scalar Products

1. Scalar Product
       The scalar product or inner product or dot product between two vectors $$xy \in \mathbf{R}^n$$ is the scalar denoted $$x^Ty$$ and defined as 
       $$x^Ty = \sum{i=1}^n xi yi.$$ 



        

2. Norms
       A measure of the "length" of a vector in a given space.
       Theorem. A function from $$\chi$$ to $$\mathbf{R}$$ is a norm if  
        1. $$\|x\| \geq 0 \ \forall x \in \chi$$ and $$\|x\| = 0 \iff x = 0$$.
        2. $$\|x+y\| \leq \|x\| + \|y\|$$ for any $$x y \in \chi$$ triangle inequality.
        3. $$\|\alpha x\| = \|\alpha\| \|x\|$$ for any scalar $$\alpha$$ and any $$x\in \chi$$.

$$lp$$ Norms
    $$\|x\|p = \left \sum{k=1}^n \|xk\|^p \right^{1/p} \ 1 \leq p < \infty$$


4. The $$l1 norm$$
       $$ \|x\|1 = \sum{i=1}^n \| xi \| $$   
       Corresponds to the distance travelled on a rectangular grid to go from one point to another.  

3. The $$l2 norm$$ Euclidean Norm
       $$  \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \| x \|2 = \sqrt{ \sum{i=1}^n xi^2 } = \sqrt{x^Tx} $$.  
       Corresponds to the usual notion of distance in two or three dimensions.
    The $$l2 norm$$ is invariant under orthogonal transformations     
        i.e. $$\|x\|2 = \|Vz\|2 = \|z\|2$$ where $$V$$ is an orthogonal matrix. 
    The set of points with equal l2 norm is a circle in 2D a sphere in 3D or a hyper sphere in higher dimensions.  

5. The $$l\infty norm$$
       $$ \| x \|\infty = \displaystyle\max{1 \le i \le n} \| xi \|$$  
    useful in measuring peak values.  

0. The Cardinality
       The Cardinality of a vector $$\vec{x}$$ is often called the $$l0$$ pseudo norm and denoted with  
       $$\|\vec{x}\|0$$.
    Defined as the number of non zero entries in the vector.


6. Cauchy Schwartz inequality
       For any two vectors $$x y \in \mathbf{R}^n$$ we have  
       $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  $$x^Ty \le \|x\|2 \cdot \|y\|2$$.
    The above inequality is an equality if and only if $$x y$$ are collinear  
     $$ {\displaystyle \max{x  \ \|x\|2 \le 1} \ x^Ty = \|y\|2}$$ 
    with optimal $$x$$ given by  
    $$x^\ast = \dfrac{y}{\|y\| 2} \ $$ if $$y$$ is non zero.

7. Angles between vectors
       When none of the vectors xy is zero we can define the corresponding angle as theta such that
        $$\cos\  \theta = \dfrac{x^Ty}{\|x\| 2 \|y\| 2} .$$ 



        

Notes  
 $$L^q$$ for $$q \in 01$$ are no longer Norms.  
     They have non convex contours; thus using them makes the optimization much harder  
     They however induce more sparsity than $$L^1$$  
     $$L^1$$ is the best sparse norm convex approximation to the $$L^q$$ for $$q \in 01$$  


Orthogonality

1. Orthogonal Vectors
       We say that two vectors $$x y \in \mathbf{R}^n$$ are orthogonal if $$x^Ty = 0.$$



Projections

1. Line
       A line in $$\mathbf{R}^n$$ passing through $$x0 \in \mathbf{R}^n$$ and with direction $$u \in \mathbf{R}^n$$
       $$\left\{ x0 + tu ~~ t \in \mathbf{R} \right\}$$  

    Re Written  
    A line in $$\mathbf{R}^n$$ passing through the point $$x0 \in \mathbf{R}^n$$ and with direction $$\mathbf{u} \in \mathbb{R}^n$$  
    $$\left\{ x0 + c \mathbf{u} ~~ c \in \mathbb{R} \right\}$$  


2. Projection on a line
       The projection of a given point $$\vec{x}$$ on the line is a vector $$\vec{z}$$ located on the line that is closest to $$\vec{x}$$ in Euclidean norm. This corresponds to a simple optimization problem  
       $$\mint \ \|x  x0  tu\| 2^2$$. 
    This particular problem is part of a general class of optimization problems known as least squares.  
    It is also a special case of a Euclidean projection on a general set.  

    Re Written  
    The projection of a given point $$\mathbf{v}$$ on the line is a vector $$\tilde{\mathbf{v}}$$ located on the line that is closest distance wise to $$\mathbf{v}$$ in Euclidean norm. This corresponds to a simple optimization problem  
    $$\minc \ \|\mathbf{v}  x0  c \mathbf{u}\| 2^2$$  

3. The Projection
       Assuming that $$\vec{u}$$ is normalized so that $$\|\vec{u}\|2 = 1$$ the objecive function of the projection problem reads after squaring  
       $$\|x  x0  tu\|2^2 = t^2  2t u^Tx x0 + \|x x0\|2^2 = t  u^Tx x0^2 + \mbox{constant}.$$
       $$\implies $$ the optimal solution to the projection problem is  
       $$ t^\ast = u^Tx x0$$
       and the expression for the projected vector is
       $$ z^\ast = x0 + t^\ast u = x0 + u^Tx x0 u.$$
    The scalar product $$u^Tx x0$$ is the component of $$x x0$$ along $$\vec{u}$$.
    In the case when u is not normalized the expression is obtained by replacing $$\vec{u}$$ with its scaled version $$\dfrac{\vec{u}}{\|\vec{u}\|2}$$.
       The General Solution  
       $$\vec{z}^\ast = \vec{x0} + \dfrac{\vec{u}^T\vec{x} \vec{x0}}{\vec{u}^T\vec{u}} \vec{u} .$$

4. Interpreting the scalar product
       In general the scalar product $$u^Tx$$ is simply  
     the component of $$x$$ along the normalized direction $$\dfrac{\vec{u}}{\|\vec{u}\|2}$$ defined by $$\vec{u}$$.  


5. Projection
    A Projection is a linear transformation $$P$$ from a vector space to itself such that the matrix $$P$$ is idempotent  
    $$P^2 = P$$  
    It leaves its image unchanged.  

    Mathematically  
    A Projection on a vector space $${\displaystyle V}$$ is a linear operator $${\displaystyle PV\mapsto V}$$ such that $${\displaystyle P^{2}=P}$$  

    Properties  
     The Eigenvalues of a projection matrix must be $$0$$ or $$1$$  
        From the equation $$P^2 = P \iff x^2 = x = xx 1$$ has roots $$0 1$$  

     $${\displaystyle P}$$ is always a positive semi definite matrix  
        Follows from the fact that the eigenvalues are either $$0$$ or $$1$$  
     The corresponding eigenspaces are respectively the kernel and range of the projection  
     If a projection is nontrivial it has minimal polynomial $${\displaystyle x^{2} x=xx 1}$$ which factors into distinct roots and thus $${\displaystyle P}$$ is diagonalizable  
     The product of projections is not in general a projection even if they are orthogonal.  
         If projections commute then their product is a projection.  


    Notes  




6. Orthogonal Projections
    An Orthogonal Projection is a projection $$P$$ from a vector space to itself such that the matrix $$P$$ is symmetric  
    $$P = P^T$$  

    Mathematically  
     When $${\displaystyle V}$$ has an inner product and is complete i.e. when $${\displaystyle V}$$ is a Hilbert space the concept of orthogonality can be used.  
    Then $${\displaystyle P}$$ is called an orthogonal projection if it satisfies $${\displaystyle \langle Pxy\rangle =\langle xPy\rangle }$$ for all $${\displaystyle xy\in V}$$  
     A projection on a Hilbert space that is not orthogonal is called an oblique projection.  
     A square matrix $${\displaystyle P}$$ is called an orthogonal projection matrix if $${\displaystyle P^{2}=P=P^{\mathrm {T} }}$$  
     The range $${\displaystyle U}$$ and the null space $${\displaystyle V}$$ are orthogonal subspaces  
        $$\langle xPy\rangle =\langle PxPy\rangle =\langle Pxy\rangle$$  
     An orthogonal projection is a bounded operator.  
        By Cauchy Schwartz  
        $${\displaystyle \|Pv\|^{2}=\langle PvPv\rangle =\langle Pvv\rangle \leq \|Pv\|\cdot \|v\|}  \iff  {\displaystyle \|Pv\|\leq \|v\|}$$  


    Orthogonal Projection onto a Line  
    If $$\hat{u}$$ is a unit vector on the line then the projection is given by the outer product  
    $$P{\hat{u}} = \hat{u}\hat{u}^T$$  
    Orthogonal Projection onto Subspaces  
    Generalize the above definition if $${\displaystyle \hat{u}{1}\ldots \hat{u}{k}}$$ are an orthonormal basis of the subspace $$U$$ and $$A$$ is the $$n \times k$$ matrix with columns $${\displaystyle \hat{u}{1}\ldots \hat{u}{k}}$$ then the projection is given by  
    $$PA = AA^T$$  
    Equivalently  
    $$P{A}=\sum {i}\langle u{i}\cdot \rangle u{i}$$  
    Dropping the Orthonormality condition on the basis we get  
    $$P{A}=AA^{\mathrm {T} }A^{ 1}A^{\mathrm {T} }$$  
            



Hyperplanes

1. Hyperplanes
       A hyperplane is a set described by a single scalar product equality. Precisely a hyperplane $$\in \mathbf{R}^n$$ is a set of the form  
       $$\mathbf{H} = \left\{ x ~~ a^Tx = b \right\}$$ 
       where a $$\in \mathbf{R}^n 
    a \ne 0$$ and $$b \in \mathbf{R}$$ are given. 
    When $$b=0$$ the hyperplane is simply the set of points that are orthogonal to $$a$$.
    when $$b \ne 0$$ the hyperplane is a translation along direction $$a$$ of that set.
    If $$x0 \in \mathbf{H}$$ then for any other element $$x \in \mathbf{H}$$ we have  
       $$ b = a^Tx0 = a^Tx.$$ 
       Hence the hyperplane can be characterized as the set of vectors $$x$$ such that $$x x0$$ is orthogonal to $$a$$ 
       $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{H} = \left\{ x ~~ a^Tx x0=0 \right\}$$. 

2. Hyper Planes as Affine Sets
       Hyper planes are affine sets of degree $$n 1$$.
    
    


    Thus they generalize the usual notion of a plane $$\in \mathbf{R}^3$$. 
    Hyperplanes are very useful because they allows to separate the whole space in two regions.

3. Geometry of Hyperplanes
       Geometrically an hyperplane $$\mathbf{H} =  \left\{ x ~~ a^Tx = b \right\}$$ with $$\|a\|2 = 1$$ is a
        Translation of the set of vectors orthogonal to a.
        The Direction of the translation is determined by a and the amount by b.
        $$absb$$ is Precisely the length of the closest point $$x0$$ on $$\mathbf{H}$$ from the origin.
        The sign of $$b$$ determines if $$\mathbf{H}$$ is away from the origin along the direction $$a$$ or $$ a$$.
        The magnitude of $$b$$ determines the shifting of the hyperplane as follows 
             Increasing the magnitude shifts the hyperplane further away along $$\pm a$$ depending on the sign of $$b$$.
             Decreasing the magnitude shifts the hyperplane closer along $$\pm a$$ depending on the sign of $$b$$.

    In the image below the scalar b is positive as $$x0$$ and a point to the same direction.  


Half Spaces

1. Half Space
       A half space is a subset of $$\mathbf{R}^n$$ defined by a single inequality involving a scalar product. Precisely a half space $$\in \mathbf{R}^n$$ is a set of the form  
       $$ \mathbf{H} = \left\{ x ~~ a^Tx \ge b \right\}$$  
       where $$a \in \mathbf{R}^n a \ne 0$$ and $$b \in \mathbf{R}$$ are given.

2. Geometric Interptation
       Geometrically the half space above is  
        The set of points such that $$a^Tx x0 \ge 0$$.
    i.e. The angle between $$x x0$$ and $$a$$ is acute $$\in  90^\circ +90^\circ$$.  
        $$x0$$ is the point closest to the origin on the hyperplane defined by the equality $$a^Tx = b$$. 
    When $$a$$ is normalized as in the picture $$x0 = ba$$.  


Linear Functions and Transformations and Maps

1. Linear Functions
        Linear functions are functions which preserve scaling and addition of the input argument.
    Formally
         A function $$f \mathbf{R}^n \rightarrow \mathbf{R}$$ is linear if and only if $$f$$ preserves scaling and addition of its arguments  
        for every $$x \in \mathbf{R}^n$$ and $$\alpha \in \mathbf{R} \ f\alpha x = \alpha fx$$; and
        for every $$x1 x2 \in \mathbf{R}^n fx1+x2 = fx1+fx2$$.

2. Affine Functions
       Affine functions are linear functions plus constant functions.
       Formally  
       A function f is affine if and only if the function $$\tilde{f} \mathbf{R}^n \rightarrow \mathbf{R}$$ with values $$\tilde{f}x = fx f0$$ is linear. $$\diamondsuit$$
    Equivalently
       A map $$f  \mathbf{R}^n \rightarrow \mathbf{R}^m$$ is affine if and only if the map $$g  \mathbf{R}^n \rightarrow \mathbf{R}^m$$ with values $$gx = fx  f0$$ is linear.


3. Equivalent Definitions of Linear Functions Theorem
       A map $$f  \mathbf{R}^n \rightarrow \mathbf{R}^m$$ is linear if and only if either one of the following conditions hold
        $$f$$ preserves scaling and addition of its arguments
              for every $$x \in \mathbf{R}^n$$ and $$\alpha \in \mathbf{R}  f\alpha x = \alpha fx$$; and
             for every $$x1 x2 \in \mathbf{R}^n fx1+x2 =  fx1+fx2.$$
        $$f$$ vanishes at the origin
             $$f0 = 0$$ and
             It transforms any line segment $$\in \mathbf{R}^n$$ into another segment $$\in \mathbf{R}^m$$
            $$\forall \ x y \in \mathbf{R}^n \; \forall \ \lambda \in 01 ~~ f\lambda x + 1 \lambda y = \lambda fx + 1 \lambda fy$$.  
                 $$f$$ is differentiable vanishes at the origin and the matrix of its derivatives is constant.
                 There exist $$A \in \mathbf{R}^{m \times n}$$ such that $$\ \forall  x \in \mathbf{R}^n ~~ fx = Ax$$. 
    
    


4. Vector Form and the scalar product
    Theorem Representation of affine function via the scalar product.  
    $$\ \ \ \ \ \ \ \ $$    A function $$f \mathbf{R}^n \rightarrow \mathbf{R}$$ is affine if and only if it can be expressed via a scalar product  
        $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ \ \ \ \ \ \ \ \ $$  $$fx = a^Tx + b$$   
        $$\ \ \ \ \ \ \ \ $$ for some unique pair $$ab$$ with $$a \in \mathbf{R}^{n}$$ and $$b \in \mathbf{R}$$ given by $$ai = fei f0$$ with $$ei$$ $$\ \ \ \ \ \ \ \ \ $$the $$i th$$ unit vector $$\in \mathbf{R}^n i=1 \ldots n$$ and $$\ b = f0$$.  
    The function is linear $$\iff b = 0$$.  

    The theorem shows that a vector can be seen as a linear function from the "input" space $$\mathbf{R}^n$$ to the "output" space $$\mathbf{R}$$.  

    Both points of view matrices as simple collections of numbers or as linear functions are useful.

0. Gradient of a Linear Function
    

5. Gradient of an Affine Function
       The gradient of a function $$f  \mathbf{R}^n \rightarrow \mathbf{R}$$ at a point $$x$$ denoted $$\nabla fx$$ is the vector of  derivatives with respect to $$x1 \ldots xn$$.
    When $$n=1$$ there is only one input variable the gradient is simply the derivative.  
       An affine function $$f  \mathbf{R}^n \rightarrow \mathbf{R}$$ with values $$fx = a^Tx+b$$ has the gradient
       $$\nabla fx = a$$.  
    i.e. For all Affine Functions the gradient is the constant vector $$a$$.

6. Interpreting $$a$$ and $$b$$
        The $$b=f0$$ is the constant term. For this reason it is sometimes referred to as the bias or intercept.  
            as it is the point where $$f$$ intercepts the vertical axis if we were to plot the graph of the function.
        The terms $$aj j=1 \ldots n$$ which correspond to the gradient of $$f$$ give the coefficients of influence of $$xj$$ on $$f$$. 
            For example if $$a1 >> a3$$ then the  component of $$x$$ has much greater influence on the value of $$fx$$ than the .

7.  order approximation of non linear functions
        One dimensional case  
        Consider a function of one variable $$f  \mathbf{R} \rightarrow \mathbf{R}$$ and assume it is differentiable everywhere.  
        Then we can approximate the values function at a point $$x$$ near a point $$x0$$ as follows  
       $$ fx \simeq lx = fx0 + f'x0 x x0  $$
       $$\ \ \ \ \  \ \ \ $$ where $$f'x$$ denotes the derivative of $$f$$ at $$x$$.
        Multi dimensional  
        Let us approximate a differentiable function $$f  \mathbf{R}^n \rightarrow \mathbf{R}$$ by a linear function $$l$$ so that $$f$$ and $$l$$ coincide up and including to the  derivatives.  
        The approximate function l must be of the form  
       $$lx = a^Tx + b $$  
       $$\ \ \ \ \  \ \ \ $$ where $$a \in \mathbf{R}^n$$ and $$b \in \mathbf{R}$$.  
    The corresponding approximation $$l$$ is called the  order approximation to $$f$$ at $$x0$$.  

        Our condition that $$l$$ coincides with $$f$$ up and including to the  derivatives shows that we must have  
       $$  \nabla lx = a = \nabla fx0 \;\; a^Tx0 + b = fx0 $$  
       $$\ \ \ \ \  \ \ \ $$   where $$\nabla fx0$$ is the gradient of $$f$$ at $$x0$$. 

8.  order Expansion of a function Theorem
       The  order approximation of a differentiable function $$f$$ at a point $$x0$$ is of the form  
       $$fx \approx lx = fx0 + \nabla fx0^T x x0$$   
       where $$\nabla fx0 \in \mathbf{R}^n$$ is the gradient of $$f$$ at $$x0$$.
    
    


Matrices

0. Matrix Transpose
       $$ A{ij} =  A{ji}^T \; \forall i j \in \mathbf{F}$$  
     Properties  
         $$AB^T = B^TA^T.$$  

1. Matrix vector product
        $$Axi = \sum{j=1}^n A{ij}xj  \;\; i=1 \ldots m. $$
        Where the Matrix is $$\in {\mathbf{R}}^{m \times n}$$ and the vector is $$ \in {\mathbf{R}}^m$$.
        Interpretations  
           1. A linear combination of the columns of $$A$$    
            $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \   Ax = \left \begin{array}{c} a1^Tx  \ldots  am^Tx \end{array} \right^T$$ .   
            where the columns of $$A$$ are given by the vectors $$ai i=1 \ldots n$$ so that $$A = a1  \ldots an$$.

            2. Scalar Products of Rows of $$A$$ with $$x$$    
            $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  Ax = \sum{i=1}^n xi ai$$ .   
            where the rows of $$A$$ are given by the vectors $$ai^T i=1 \ldots m$$
            $$A = \left \begin{array}{c} a1^T  \ldots  am^T \end{array} \right^T$$.

    
    


2. Left Product
        If $$z \in \mathbf{R}^m$$ then the notation $$z^TA$$ is the row vector of size $$n$$ equal to the transpose of the column vector $$A^Tz \in \mathbf{R}^n$$  
       $$ z^TAj = \sum{i=1}^m A{ij}zi  \;\; j=1 \ldots n. $$
    
    



3. Matrix matrix product
       $$  AB{ij} = \sum{k=1}^n A{ik} B{kj}$$.  
       where $$A \in \mathbf{R}^{m \times n}$$ and $$B \in \mathbf{R}^{n \times p}$$ and the notation $$AB$$ denotes the $$m \times p$$ matrix given above.
        Interpretations  
           1. Transforming the columns of $$B$$ into $$Abi$$    
            $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \    AB = A \left \begin{array}{ccc} b1 & \ldots & bn \end{array} \right =  \left \begin{array}{ccc} Ab1 & \ldots & Abn \end{array} \right$$ .   
            where the columns of $$B$$ are given by the vectors $$bi i=1 \ldots n$$ so that $$B = b1  \ldots bn$$.  
            2. Transforming the Rows of $$A$$ into $$ai^TB$$      
            $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  AB = \left\begin{array}{c} a1^T  \vdots  am^T \end{array}\right B = \left\begin{array}{c} a1^TB  \vdots  am^TB \end{array}\right$$.   
            where the rows of $$A$$ are given by the vectors $$ai^T i=1 \ldots m$$
            $$A = \left \begin{array}{c} a1^T  \ldots  am^T \end{array} \right^T$$.

4. Block Matrix Products

5. Outer Products

6. Trace
       The trace of a square $$n \times n$$ matrix $$A$$ denoted by $$\mathbf{Tr} A$$ is the sum of its diagonal elements  
       $$ \mathbf{Tr} A = \sum{i=1}^n A{ii}$$.  
     Properties  
         $$\mathbf{Tr} A = \mathbf{Tr} A^T$$.  
         $$\mathbf{Tr} AB = \mathbf{Tr} BA$$.
         $$\mathbf{Tr}XYZ = \mathbf{Tr}ZXY = \mathbf{Tr}YZX$$.
         $${\displaystyle \operatorname{tr} A+B = \operatorname{tr} A+\operatorname{tr} B}$$.
         $${\displaystyle \operatorname{tr} cA = c\operatorname{tr} A}$$.
         $${\displaystyle \operatorname{tr} \leftX^{\mathrm {T} }Y\right=\operatorname{tr} \leftXY^{\mathrm {T} }\right=\operatorname{tr} \leftY^{\mathrm {T} }X\right=\operatorname{tr} \leftYX^{\mathrm {T} }\right=\sum {ij}X{ij}Y{ij}}$$.
         $${\displaystyle \operatorname{tr} \leftX^{\mathrm {T} }Y\right=\sum {ij}X\circ Y{ij}}\ \ \ \ $$ The Hadamard product.
         Arbitrary permutations of the product of matrices is not allowed. Only cyclic permutations are.
            However if products of three symmetric matrices are considered any permutation is allowed.
         The trace of an idempotent matrix $$A$$ is the dimension of A.
         The trace of a nilpotent matrix is zero.
         If $$fx = x − \lambda1^{d1} \cdots x − \lambdak^{dk}$$ is the characteristic polynomial of a matrix $$A$$ then $${\displaystyle \operatorname{tr} A=d{1}\lambda{1} + \cdots + d{k} \lambda{k}}$$.
         When both $$A$$ and $$B$$ are $$n \times n$$ the trace of the ring theoretic commutator of $$A$$ and $$B$$ vanishes $$\mathbf{tr}A B = 0$$; one can state this as "the trace is a map of Lie algebras $${\displaystyle \mathbf{GL{n}} \to k}$$ from operators to scalars" as the commutator of scalars is trivial it is an abelian Lie algebra.
         The trace of a projection matrix is the dimension of the target space.
            $${\displaystyle 
            P{X} = X\leftX^{\mathrm {T} }X\right^{ 1}X^{\mathrm {T} } 
            \Rightarrow 
            \operatorname {tr} \leftP{X}\right = \operatorname {rank} \leftX\right}$$



7. Scalar Product
       $$\langle A B \rangle = \mathbf{Tr}A^TB = \displaystyle\sum{i=1}^m\sum{j=1}^n A{ij}B{ij}.$$  
    The above definition is Symmetric  
       $$\implies \langle AB \rangle =  \mathbf{Tr} A^TB = \mathbf{Tr} A^TB^T =  \mathbf{Tr} B^TA = \langle BA \rangle .$$  
    We can interpret the matrix scalar product as the vector scalar product between two long vectors of length $$mn$$ each obtained by stacking all the columns of $$A B$$ on top of each other.

8. Special Matrices
     Diagonal matrices/workfiles/research/la/symmat are square matrices $$A$$ with $$A{ij} = 0$$ when $$i \ne j$$.  
     Symmetric matrices are square matrices that satisfy $$A{ij} = A{ji} $$for every pair $$ij$$.
     Triangular matrices are square matrices that satisfy $$A{ij} = A{ji} $$for every pair $$ij$$.    


Matrix Norms

1. Norm
       A matrix norm is a functional  
       $${\displaystyle \|\cdot \|K^{m\times n}\to \mathbf{R} }$$  
       on the vector space $${\displaystyle K^{m\times n}} $$ that must satisfy the following properties
       For all scalars $${\displaystyle \alpha }  \in {\displaystyle K} $$ and for all matrices $${\displaystyle A} $$ and $${\displaystyle B}  \in {\displaystyle K^{m\times n}}$$  
        $$\|\alpha A\|=|\alpha| \|A\|$$ 
        i.e. being absolutely homogeneous
        $${\displaystyle \|A+B\|\leq \|A\|+\|B\|}$$
        i.e. being sub additive or satisfying the triangle inequality 
        $${\displaystyle \|A\|\geq 0} $$
        i.e. being positive valued 
        $${\displaystyle \|A\|=0} \iff {\displaystyle A=0{mn}}$$
        i.e. being definite
        $${\displaystyle \|AB\|\leq \|A\|\|B\|}$$ for all square matrices $${\displaystyle A}$$ and $${\displaystyle B} \in {\displaystyle K^{n\times n}}.$$
        Submultiplicativity.
          > Not satisfied by all Norms.

2. $$l{pq}$$ norms
       $${\displaystyle \Vert A\Vert {pq}=\left\sum {j=1}^{n}\left\sum {i=1}^{m}|a{ij}|^{p}\right^{q/p}\right^{1/q}}$$

     $$l{21}$$  
       $${\displaystyle \Vert A\Vert {21}= \sum {j=1}^{n}\left\sum {i=1}^{m}|a{ij}|^{2}\right^{1/2}}$$

3. $$l{22}$$ Frobenius norm
       $${\displaystyle \|A\|{\rm {F}}={\sqrt {\sum {i=1}^{m}\sum {j=1}^{n}|a{ij}|^{2}}}={\sqrt {\operatorname {trace} A^{\dagger }A}}={\sqrt {\sum {i=1}^{\min\{mn\}}\sigma {i}^{2}A}}} $$  
       where $${\displaystyle A^{\dagger }}$$ denotes the conjugate transpose of $${\displaystyle A}$$ and $${\displaystyle \sigma {i}A}$$ are the singular values of $${\displaystyle A}$$.

     Properties  
        1. Submultiplicative.

        2. Invariant under rotations.  
            i.e. $${\displaystyle \|A\|{\rm {F}}^{2}=\|AR\|{\rm {F}}^{2}=\|RA\|{\rm {F}}^{2}} {\displaystyle \|A\|{\rm {F}}^{2}=\|AR\|{\rm {F}}^{2}=\|RA\|{\rm {F}}^{2}}$$ for any rotation matrix $$R$$.

        3. Invariant under a unitary transformation for complex matrices.

        4. $${\displaystyle \|A^{\rm {T}}A\|{\rm {F}}=\|AA^{\rm {T}}\|{\rm {F}}\leq \|A\|{\rm {F}}^{2}}$$.

        5. $${\displaystyle \|A+B\|{\rm {F}}^{2}=\|A\|{\rm {F}}^{2}+\|B\|{\rm {F}}^{2}+2\langle AB\rangle {\mathrm {F} }}$$.


4. $$l{\infty\infty}$$ Max Norm
       $$ \|A\|{\max} = \max{ij} |a{ij}|.$$

     Properties  
        1. NOT Submultiplicative.

5. The Spectral Norm
       $${\displaystyle \|A\|{2}={\sqrt {\lambda {\max }A^{^{}}A}}=\sigma {\max }A} = {\displaystyle \max{\|x\|2!=0}\|Ax\|2/\|x\|2}.$$  
    The spectral norm of a matrix $${\displaystyle A} $$ is the largest singular value of $${\displaystyle A}$$. 
    i.e. the square root of the largest eigenvalue of the positive semidefinite matrix $${\displaystyle A^{}A}.$$

     The Spectral Radius of $$A \ $$  denoted $$\rhoA$$
       $$ \lim{r\rightarrow\infty}\|A^r\|^{1/r}=\rhoA.$$

     Properties  
        1. Submultiplicative.

        2. Satisfies $${\displaystyle \|A^{r}\|^{1/r}\geq \rho A}$$ where $$\rhoA$$ is the spectral radius of $$A$$.

        3. It is an "induced vector norm".



8. Equivalence of Norms
    
    


8. Applications
    1. RMS Gain Frobenius Norm.

    2. Peak Gain Spectral Norm.

    3. Distance between Matrices Frobenius Norm.
        
        


    4. Direction of Maximal Variance Spectral Norm.
        
        







NOTES

 Distance between 2 vectors from $$y$$ to $$x$$  
    $$d = \|x y\|2^2$$ 


 2.1  Basics and Definitions


Definitions

6. Span
    
    

1. Linear Independence



2. Subspace
     Definition  
     Geometrical Interpretation  
     Mathematical Representation  
            

3. Affine Sets and Subspaces Cosets  Abstract Algebra
     Definition  
     Geometrical Interpretation  
     Mathematical Representation  
     Special Case of a single basis vector  
     Find the Affine Subspace Corresponding to the following set  
        The set $$\boldsymbol{L}$$ in $$\mathbb{R}^3$$ defined by  
        $$x{1} 13 x{2}+4 x{3}=2 \quad 3 x{2} x{3}=9$$  
        $$5x{1} 8x{2}+17 x{3}=2 \quad 6 x{2} 2x{3}=13$$  

     Mathematical Representation of a line  


4. Basis


5. Dimension


Norms and Scalar Products

1. Scalar/Inner/Dot Product


2. Norms
     Definition + Theorem properties  
            

00.$$lp$$ Norms


4. The $$l1 norm$$
     Geometrically Corresponds to  



3. The $$l2 norm$$ Euclidean Norm
     Geometrically Corresponds to  
     Properties  

5. The $$l\infty norm$$
     Geometrically Corresponds to  
     Application  

            

0. The Cardinality


6. Cauchy Schwartz inequality

7. Angles between vectors



Orthogonality

1. Orthogonal Vectors

2. Orthogonal Matrix



Projections

1. Line
     Definition  
     Mathematical Representation  
            

2. Projection on a line
     Set up Equation  
            

3. The Projection
     Solve Equation  


4. Interpreting the scalar product
            


Hyperplanes

1. Hyperplanes
     Two definitions  

    

2. Hyper Planes as Affine Sets
     How are they useful?  


3. Geometry of Hyperplanes
    


Half Spaces

1. Half Space

2. Geometric Interptation

Linear Functions and Transformations and Maps

1. Linear Functions

2. Affine Functions


3. Equivalent Definitions of Linear Functions Theorem
    


4. Vector Form and the scalar product
    

0. Gradient of a Linear Function
    

5. Gradient of an Affine Function

6. Interpreting $$a$$ and $$b$$

7.  order approximation of non linear functions
        


8.  order Expansion of a function Theorem


Matrices

0. Matrix Transpose


1. Matrix vector product
   
            

2. Left Product


3. Matrix matrix product
   
           

4. Block Matrix Products

5. Outer Products

6. Trace


7. Scalar Product

8. Special Matrices


Matrix Norms

1. Norm

2. $$l{pq}$$ norms


3. $$l{22}$$ Frobenius norm


4. $$l{\infty\infty}$$ Max Norm


5. The Spectral Norm




8. Equivalence of Norms

8. Applications



 PCA  Principle Compnent Analysis


PCA

1. What?
       It is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.

2. Goal?
       Given points $$ \in \mathbf{R}^d$$ find k directions that capture most of the variation.

3. Why?
       1. Find a small basis for representing variations in complex things.
            e.g. faces genes.
        2. Reducing the number of dimensions makes some computations cheaper.
        3. Remove irrelevant dimensions to reduce overfitting in learnging algorithms.
            Like "subset selection" but the features are not axis aligned.  
            They are linear combinations of input features.

4. Finding Principle Components
     Let '$$X$$' be an $$n \times d$$ design matrix centered with mean $$\hat{x} = 0$$.
     Let '$$w$$' be a unit vector.
     The Orthogonal Projection of the point '$$x$$' onto '$$w$$' is $$\tilde{x} = x.ww$$.
        Or $$\tilde{x} = \dfrac{x.w}{\|w\|2^2}w$$ if $$w$$ is not a unit vector.
     Let '$$X^TX$$' be the sample covariance matrix  
        $$0 \leq \lambda1 \leq \lambda2 \leq \cdots \leq \lambdad$$ be its eigenvalues  and let $$v1 v2 \cdots vd$$ be the corresponding Orthogonal Unit Eigen vectors.
     Given Orthonormal directions vectors $$v1 v2 \ldots vk$$ we can write   

        $$\tilde{x} = \sum{i=1}^k x.vivi.$$  

    The Principle Components are precisely the eigenvectors of the data's covariance matrix. 

5. Total Variance and Error Measurement
         The Total Variance of the data can be expressed as the sum of all the eigenvalues
       $$
        \mathbf{Tr} \Sigma = \mathbf{Tr} U \Lambda U^T = \mathbf{Tr} U^T U \Lambda = \mathbf{Tr} \Lambda = \lambda1 + \ldots + \lambdan. 
        $$
         The Total Variance of the Projected data is
       $$
         \mathbf{Tr} P \Sigma P^T  = \lambda1 + \lambda2 + \cdots + \lambdak. 
        $$
         The Error in the Projection could be measured with respect to variance.
             We define the ratio of variance "explained" by the projected data as
       $$
        \dfrac{\lambda1 + \ldots + \lambdak}{\lambda1 + \ldots + \lambdan}. 
        $$
    If the ratio is high we can say that much of the variation in the data can be observed on the projected plane.


Derivation 1. Fitting Gaussians to Data with MLE

1. What?
    1. Fit a Gaussian to data with MLE
    2. Choose k Gaussian axes of greatest variance.
    Notice MLE estimates a covariance matrix; $$\hat{\Sigma} = \dfrac{1}{n}X^TX$$.

2. Algorithm
    1. Center $$X$$
    2. Normalize $$X$$.
        Optional. Should only be done if the units of measurement of the features differ.
    3. Compute the unit Eigen values and Eigen vectors of $$X^TX$$
    4. Choose '$$k$$' based on the Eigenvalue sizes
        Optional. Top to bottom.
    5. For the best k dim subspace pick Eigenvectors $$v{d k+1} \cdots vd$$.
    6. Compute the coordinates '$$x.vi$$' of the trainning/test data in PC Space.


Derivation 2. Maximizing Variance

1. What?
    1. Find a direction '$$w$$' that maximizes the variance of the projected data.
    2. Maximize the variance

2. Derivation
       $$\max{w  \|w\|2=1} \ Var\left\{\tilde{x1} \tilde{x2} \cdots \tilde{xn} \right\}$$
       $$
        \begin{align}
        & \ = \max{w  \|w\|2=1}  \dfrac{1}{n} \sum{i=1}{n}xi.\dfrac{w}{\|w\|}^2 
        & \ = \max{w  \|w\|2=1}  \dfrac{1}{n} \dfrac{\|xw\|^2}{\|w\|^2} 
        & \ = \max{w  \|w\|2=1}  \dfrac{1}{n} \dfrac{w^TX^TXw}{w^Tw}
        \end{align}
        $$
       Where $$\dfrac{1}{n}\dfrac{w^TX^TXw}{w^Tw}$$ is the Rayleigh Quotient.
       For any Eigen vector $$vi$$ the Rayleigh Quotient is $$ = \lambdai$$.
       $$\implies$$ the vector $$vd$$ with the largest $$\lambdad$$ achieves the maximum variance $$\dfrac{\lambdad}{n}.$$
       Thus the maximum of the Rayleigh Quotient is achieved at the Eigen vector that has the highest correpsonding Eigen value.
       We find subsequent vectors by finding the next biggest $$\lambdai$$ and choosing its corresponding Eigen vector.

3. Another Derivation from Statistics
       The data matrix has points $$xi$$; its component along a proposed axis $$u$$ is $$x · u$$.
       The variance of this is $$Ex · u − Ex · u^2$$
       and the optimization problem is
       $$
        \begin{align}
        \max{x  \|x\|2=1} \ Ex · u − Ex · u^2 & 
        & \ = \max{u  \|u\|2=1} \  Eu \cdot x − Ex^2 
        & \ = \max{u  \|u\|2=1} \  uEx − Ex \cdot x − Ex^Tu 
        & \ = \max{u  \|u\|2=1} \  u^T \Sigma u
        \end{align}
        $$
       where the matrix $${\displaystyle \Sigma \= \dfrac{1}{n} \sum{j=1}^n xj \hat{x}xj \hat{x}^T}.$$
       Since $$\Sigma$$ is symmetric the $$u$$ that gives the maximum value to $$u^T\Sigma u$$ is the eigenvector of $$\Sigma$$ with the largest eigenvalue.
       The  and subsequent principal component axes are the other eigenvectors sorted by eigenvalue.


Derivation 3. Minimize Projection Error

1. What?
    1. Find direction '$$w$$' that minimizes the Projection Error.

2. Derivation
       $$
        \begin{align}
        \min{\tilde{x}  \|\tilde{x}\|2 = 1} \; \sum{i=1}^n \|xi  \tilde{xi}\|^2 & 
        & \ = \min{w  \|w\|2 = 1} \; \sum{i=1}^n \|xi  \dfrac{xi \cdot w}{\|w\|2^2}w\|^2 
        & \ = \min{w  \|w\|2 = 1} \; \sum{i=1}^n \left\|xi\|^2  xi \cdot \dfrac{w}{\|w\|2}^2\right 
        & \ = \min{w  \|w\|2 = 1} \; c  n\sum{i=1}^nxi \cdot \dfrac{w}{\|w\|2}^2 
        & \ = \min{w  \|w\|2 = 1} \; c  nVar\left\{\tilde{x1} \tilde{x2} \cdots \tilde{xn} \right\} 
        & \ = \max{w  \|w\|2 = 1} \; Var\left\{\tilde{x1} \tilde{x2} \cdots \tilde{xn} \right\}
        \end{align}
        $$
       Thus minimizing projection error is equivalent to maximizing variance.



 2.4  Singular StuffValues





The Singular Value Decomposition

1. What?
       Recall from here that any matrix $$A \in \mathbf{R}^{m \times n}$$ with rank one can be written as   $$A = \sigma u v^T$$  where $$u \in \mathbf{R}^m v \in \mathbf{R}^n$$ and $$\sigma >0.$$
       It turns out that a similar result holds for matrices of arbitrary rank $$r$$.     That is we can express any matrix $$A \in \mathbf{R}^{m \times n}$$ as sum of rank one matrices
       $$
        A = \sum{i=1}^r \sigmai ui vi^T  
        $$
       where $$u1 \ldots ur$$ are mutually orthogonal $$v1 \ldots vr$$ are also mutually orthogonal and the $$\sigmai$$’s are positive numbers called the singular values of $$A$$.

2. The SVD Theorem
       An arbitrary matrix $$A \in \mathbf{R}^{m \times n}$$ admits a decomposition of the form
       $$
        A = \sum{i=1}^r \sigmai ui vi^T = U \tilde{ {S}} V^T \;\; \tilde{ {S}} = \left \begin{array}{cc}  {S} & 0  0 & 0 \end{array} \right   
        $$
       where $$U \in \mathbf{R}^{m \times m} V \in \mathbf{R}^{n \times n}$$ are both orthogonal matrices and the matrix $$S$$ is diagonal 
       $$S = \mathbf{diag}\sigma1  \ldots \sigmar   $$
       where  
         The positive numbers $$\sigma1 \ge \ldots \ge \sigmar > 0$$ are unique and are called the singular values of A.  
         The number $$r \le minmn$$ is equal to the rank of $$A$$.  
         The triplet $$U \tilde{ {S}} V$$ is called a singular value decomposition SVD of $$A$$.  
         The  $$r$$ columns of $$U ui i=1 \ldots r$$ resp. $$V vi  i=1 \ldots r$$ are called left resp. right singular vectors of $$A$$ and satisfy  
       $$
        Avi = \sigmai ui \;\;\;\; A^Tui = \sigmai vi \;\;\;\; i=1...r.
        $$

       
    

    Notes  
     $$\begin{array}{l}{U^{T} U=I}  {V^{T} V=I}\end{array}$$ are orthogonal matrices with orthonormal eigenvector basis  
     If $$\boldsymbol{v}$$ is eigenvector of $$X^TX$$ then $$X\boldsymbol{v}$$ is eigenvector of $$XX^T$$  

    

3. Computing the SVD
       To find the SVD of a matrix $$A$$ we solve the following equation
       $$
        \begin{align}
        & 1\  A^TA = V\Lambda^T\Lambda V^T 
        & 2\  AV\  = U \Lambda
        \end{align}
        $$

4. Complexity of the SVD
    1. Normal Matrices the complexity grows as $$\mathcal{O}nm \ minnm$$. 
    2. Sparse Matrices good approximations can be calculated very efficiently.

5. Geometric Interpretation
       The theorem allows to decompose the action of A on a given input vector as a sequence of three elementary transformations.
        1.  we form $$\tilde{x} = V^Tx \in \mathbf{R}^n$$.
            $$V$$ orthogonal $$\implies \tilde{x}$$ is a rotated version of $$x$$ which still lies in the input space.
        2. Then we act on the rotated vector $$\tilde{x}$$ by scaling its elements
            The  $$k$$ elements of $$\tilde{x}$$ are scaled by the singular values $$\sigma1 \ldots \sigmar$$; the remaining $$n r$$ elements are set to zero.  
            This step results in a new vector $$\tilde{y}$$ which now belongs to the output space $$\mathbf{R}^m$$.
        3. Finally we rotate the vector $$\tilde{y}$$ by the orthogonal matrix $$U$$ which results in $$y = U\tilde{y} = Ax$$.   
        Notice also $$\tilde{x}  = V^Tx \ x = V\tilde{x}.$$
       Summary  
        1. A rotation in the input space
        2. A scaling that goes from the input space to the output space
        3. A rotation in the output space. 
        In contrast with symmetric matrices input and output directions are different.
       
    

6. Link with the Spectral Theorem
       If $$A$$ admits an SVD then the matrices $$AA^T$$ and $$A^TA$$ has the following SEDs
       $$
        AA^T = U \Lambdam U^T \;\; A^TA = V \Lambdan V^T  
        $$
       where $$\Lambdam = \tilde{ {S}}\tilde{ {S}}^T = \mathbf{ diag}\sigma1^2 \ldots \sigmar^2 0 \ldots 0$$ is $$m \times m$$ so it has $$m r$$ trailing zeros   
        and $$\Lambdan = \tilde{ {S}}^T\tilde{ {S}} = \mathbf{ diag}\sigma1^2 \ldots \sigmar^2 0 \ldots 0$$ is $$n \times n$$ so it has $$n r$$ trailing zeros. 
    The eigenvalues of $$AA^T$$ and $$A^TA$$ are the same and equal to the squared singular values of $$A$$.  
        The corresponding eigenvectors are the left and right singular vectors of $$A$$.


Matrix Properties via SVD

1. Nullspace
       The SVD allows to compute an orthonormal basis for the nullspace of a matrix.

2. Theorem Nullspace via SVD
       The nullspace of a matrix A with SVD
       $$
        A = U \tilde{S} V^T \;\; \tilde{S} = \left \begin{array}{cc}S & 0  0 & 0 \end{array} \right  \;\; S = \mathbf{diag}\sigma1  \ldots \sigmar  
        $$
       where $$U \in \mathbf{R}^{m \times m} V \in \mathbf{R}^{n \times n}$$ are both orthogonal matrices admits the last $$n r$$ columns of $$V$$ as an orthonormal basis.


3. Full Column Rank Matrices
       ne to one or full column rank matrices are the matrices with nullspace reduced to {0}. If the dimension of the nullspace is zero then we must have n=r. Thus full column rank matrices are ones with SVD of the form
       $$
        A = U \left \begin{array}{c}  {S}  0 \end{array} \right V^T. 
        $$

4. Theorem Range and Rank via SVD
       The range of a matrix $$A$$ with SVD   
       $$
        A = U \tilde{ {S}} V^T \;\; \tilde{ {S}} = \mathbf{diag}\sigma1 \ldots \sigmar 0 \ldots 0
        $$  
       where $$U \in \mathbf{R}^{m \times m} V \in \mathbf{R}^{n \times n}$$ are both orthogonal matrices admits the  $$r$$ columns of $$U$$ as an orthonormal basis.

5. Full Row Rank Matrices.
       An onto or full row rank matrix has a range $$r=m$$.  
        These matrices are characterized by an SVD of the form  
       $$
        A = U \left \begin{array}{cc}  {S} & 0 \end{array} \right V^T.
        $$ 

6. Fundamental theorem of linear algebra
       Let $$A \in \mathbf{R}^{m \times n}$$. The sets $$\mathbf{N} A$$ and $$\mathbf{R} A^T$$ form an orthogonal decomposition of $$\mathbf{R}^n$$ in the sense that any vector  $$x \in \mathbf{R}^n$$ can be written as   
       $$
        x = y + z \;\; y \in \mathbf{N} A \;\; z \in \mathbf{R} A^T \;\; y^Tz = 0.
        $$
       In particular we obtain that the condition on a vector $$x$$ to be orthogonal to any vector in the nullspace implies that it must be in the range
       $$
        x^Ty = 0 \mbox{ whenever } Ay = 0 \Longleftrightarrow \exists \ \lambda \in \mathbf{R}^m ~~ x = A^T\lambda.
        $$ 
       
    

7. Matrix Norms
       Matrix norms which are useful to measure the size of a matrix can be interpreted in terms of input output properties of the corresponding linear map; for example the Frobenius norm measure the average response to unit vectors while the largest singular LSV norm measures the peak gain.  
        These two norms can be easily read from the SVD.

        Frobenius Norm 
       $$
        \|A\|F  = \sqrt{\mathbf{Tr} A^TA}
        $$
       $$\\\\\\\\\$$    Using the SVD $$U \tilde{ {S}} V$$ of $$A$$ we obtain
       $$
        \|A\|F^2 = \mathbf{Tr} V \tilde{ {S}}^T \tilde{ {S}} V^T = \mathbf{Tr} V^TV \tilde{ {S}}^T \tilde{ {S}} = \mathbf{Tr} \tilde{ {S}}^T \tilde{ {S}} = \sum{i=1}^r \sigmai^2.  
        $$  

        Hence the squared Frobenius norm is nothing else than the sum of the squares of the singular values.
        Largest Singular Value Norm. measures a matrix size based on asking the maximum ratio of the norm of the output to the norm of the input. When the norm used is the Euclidean norm the corresponding quantity
       $$
        \|A\|{\rm LSV} = \maxx  \|Ax\|2 ~~ \|x\|2 \le 1 = \max{x \\\ \|x\|2 \le 1}  \|Ax\|2 = \sigma1A
        $$
       $$\\\\\\\\\$$ where $$\sigma1A$$ is the largest singular value of $$A$$ is called the largest singular value LSV norm.  
    Any left singular vector associated with the largest singular value achieves the maximum in the above.

8. Condition Number
       The condition number of an invertible n times n matrix A is the ratio between the largest and the smallest singular values 
       $$
        \kappaA = \frac{\sigma1}{\sigman}  = \|A\| \cdot \|A^{ 1}\|.  
        $$  
     Provides a measure of the sensitivity of the solution of a linear equation to changes in $$A$$.




 2.3  Eigen Stuff


Quadratic Functions

1. What?
       A function $$q  \mathbf{R}^n \rightarrow \mathbf{R}$$ is said to be a quadratic function if it can be expressed as
       $$
        qx = \sum{i=1}^n \sum{j=1}^n A{ij} xi xj + 2 \sum{i=1}^n bi xi + c 
        $$  
       for numbers $$A{ij} bi$$ and $$c i j \in {1 \ldots n}$$.
        A quadratic function is thus an affine combination of the $$\ xi$$'s and all the "cross products" $$xixj$$.  
    We observe that the coefficient of $$xixj$$ is $$A{ij} + A{ji}$$.  
    The function is said to be a quadratic form if there are no linear or constant terms in it $$bi = 0 c=0.$$
    The Hessian of a quadratic function is always constant.

2. Link between Quadratic Func's & Symmetric Matrices
       Indeed any quadratic function $$q  \mathbf{R}^n \rightarrow \mathbf{R}$$ can be written as
       $$
        qx = \left \begin{array}{c} x  1 \end{array} \right^T \left \begin{array}{cc} A & b  b^T & c \end{array} \right \left \begin{array}{c} x  1 \end{array} \right = x^TAx + 2 b^Tx + c 
        $$
       for an appropriate symmetric matrix $$A \in \mathbf{S}^{n}$$ vector $$b \in \mathbf{R}^n$$ and scalar $$c \in \mathbf{R}$$. 
    $$A{ii}$$ is the coefficient of $$xi^2$$ in q;   
        $$2A{ij}$$ for $$i \ne j$$ is the coefficient of the term $$xixj$$ in q;  
        $$2bi$$ is that of $$xi$$;  
        $$c$$ is the constant term $$q0$$.  
     If q is a quadratic form then $$b=0 c=0$$ and we can write $$qx = x^TAx$$ where now $$A \in \mathbf{S}^n$$.

3.  order approximations 1 D
        If $$f  \mathbf{R} \rightarrow \mathbf{R}$$ is a twice differentiable function of a single variable then the  order approximation or  order Taylor expansion of $$f$$ at a point $$x0$$ is of the form 
       $$
        fx \approx qx = fx0 + fx0' x x0 + \dfrac{1}{2} f''x0x x0^2 
        $$
       where $$f'x0$$ is the  derivative and f''x0 the  derivative of $$f$$ at $$x0$$.  
        We observe that the quadratic approximation $$q$$ has the same value derivative and  derivative as $$f$$ at $$x0$$.

4.  order approximations n D
       Let us approximate a twice differentiable function $$f  \mathbf{R}^n \rightarrow \mathbf{R}$$ by a quadratic function $$q$$ so that $$f$$ and $$q$$ coincide up and including to the  derivatives.
       The function $$q$$ must be of the form
       $$qx = x^TAx + 2b^Tx + c $$  
       where $$A \in \mathbf{S}^n b \in \mathbf{R}^n\text{ and } c \in \mathbf{R}$$. Our condition that q coincides with f up and including to the  derivatives shows that we must have
       $$
        1\ \ \ \ \ \ \  \nabla^2 qx\ \  =\ \  2 A =\ \  \nabla^2 fx0 
        2\ \nabla qx = 2Ax0+b = \nabla fx0 
        3\ \ \ \ \ \ x0^TAx0 + 2b^Tx0 + c = fx0. 
        $$  
       Solving for Abc we obtain the following result
       $$
        fx \approx qx = fx0 + \nabla fx0^T x x0 + \dfrac{1}{2} x x0^T \nabla^2 fx0 x x0 
        $$
       where $$\nabla fx0 \in \mathbf{R}^n$$ is the gradient of $$f$$ at $$x0$$ and the symmetric matrix $$\nabla^2 fx0$$ is the Hessian of $$f$$ at $$x0$$. 


Basics and Definitions

1. Eigenvalue
       A real scalar $$\lambda$$ is said to be an eigenvalue of a matrix $$A$$ if there exist a non zero vector $$v \in \mathbf{R}^n$$ such that
       $$ A v = \lambda u. $$
    The interpretation of $$v$$ is that it defines a direction along $$A$$ behaves just like scalar multiplication. The amount of scaling is given by $$\lambda$$. 

2. Eigenvector
       








Eigen Stuff of Symmetric Matrices

1. The Spectral Theorem for Symmetric Matrices
       We can decompose any symmetric matrix $$A \in \mathbf{S}^n$$ with the symmetric eigenvalue decomposition SED
       $$
        A = \sum{i=1}^n \lambdai uiui^T  = U \Lambda U^T \;\; \Lambda = \mathbf{diag}\lambda1 \ldots \lambdan . 
        $$
       where the matrix of $$U = u1  \ldots un$$ is orthogonal that is $$U^TU=UU^T = In$$ and contains the eigenvectors of $$A$$ while the diagonal matrix Lambda contains the eigenvalues of $$A$$.  
    The SED provides a decomposition of the matrix in simple terms namely dyads.

        
    

2. Spectral Decomposition
       $$ Auj = \sum{i=1}^n \lambdai uiui^Tuj = \lambdaj uj \;\; j=1 \ldots n. $$

3. Rayleigh Quotients
       Given a symmetric matrix $$A$$ we can express the smallest and largest eigenvalues of $$A$$ denoted $$\lambda{\rm min}$$ and $$\lambda{\rm max}$$ respectively in the so called variational form
       $$
        \lambda{\rm min}A  = \min{x}  \left\{ x^TAx ~~ x^Tx = 1 \right\}   \lambda{\rm max}A  = \max{x}  \left\{ x^TAx ~~ x^Tx = 1 \right\} . 
        $$
    The term "variational" refers to the fact that the eigenvalues are given as optimal values of optimization problems which were referred to in the past as variational problems.  
        Variational representations exist for all the eigenvalues but are more complicated to state.  
       
    

        Interptation   
            The interpretation of the above identities is that the largest and smallest eigenvalues is a measure of the range of the quadratic function $$x \rightarrow x^TAx$$ over the unit Euclidean ball.  
            The quantities above can be written as the minimum and maximum of the so called Rayleigh quotient $$\dfrac{x^TAx}{x^Tx}$$.



        




Positive Definitness

0. Associated Quadratic Form
       For a given symmetric matrix $$A \in \mathbf{R}^{n \times n}$$ the associated quadratic form is the function $$q  \mathbf{R}^n \rightarrow \mathbf{R}$$ with values $$qx = x^TAx.$$

1. Positive Definite Matrices
       A symmetric matrix $$A$$ is said to be positive definite PD notation $$A \succ 0$$ if and only if the associated quadratic form $$q$$ is positive everywhere
       $$
        qx > 0 \mbox{ for every } x \in \mathbf{R}^n. 
        $$

2. Positive Semi Definite Matrices
       A symmetric matrix $$A$$ is said to be positive semi definite PSD notation $$A \succeq 0$$ if and only if the associated quadratic form $$q$$ is non negative everywhere
       $$
        qx \ge 0 \mbox{ for every } x \in \mathbf{R}^n. 
        $$

3. Definite Matrices
       When $$q = 0$$.

0. Diagonal Matrices and Positive Definitness

       Diagonal matrices. A diagonal matrix is PSD resp. PD if and only if all of its diagonal elements are non negative resp. positive.

4. Theorem. Spectral Decomposition of PSD Matrices
       A quadratic form $$qx = x^TAx$$ with $$A \in \mathbf{S}^n$$ is non negative resp. positive definite if and only if every eigenvalue of the symmetric matrix A is non negative resp. positive.

    
    

5. Square Roots of PSD Matrices
        If A is PSD there exist a unique PSD matrix denoted $$A^{1/2}$$ such that $$A = A^{1/2}^2$$. 
       We can express this matrix square root in terms of the SED of $$A = U\Lambda U^T$$ as $$A^{1/2} = U \Lambda^{1/2} U^T$$ where $$\Lambda^{1/2}$$ is obtained from $$\Lambda$$ by taking the square root of its diagonal elements. 
       If $$A$$ is PD then so is its square root.



6. The Cholesky Decomposition
       Any PSD matrix can be written as a product $$A = LL^T$$ for an appropriate matrix $$L$$. 
       The decomposition is not unique and $$L = A^{1/2}$$ is only a possible choice the only PSD one. 
       Another choice in terms of the SED of $$A = U^T \Lambda U$$ is $$L = U^T \Lambda^{1/2}$$.
       If $$A$$ is positive definite then we can choose $$L$$ to be lower triangular and invertible. The decomposition is then known as the Cholesky decomposition of $$A$$.

7. Ellipsoids and PSDs
       Definition. We define an ellipsoid to be affine transformation of the unit ball for the Euclidean norm
       $$
        \mathbf{E} = \left\{ \hat{x} + L z ~~ \|z\|2 \le 1 \right\}  
        $$
       where $$L \in \mathbf{R}^{n \times n}$$ is an arbitrary non singular invertible matrix. 
       We can express the ellipsoid as
       $$
        \mathbf{E} = \left\{ x ~~ \|L^{ 1}x \hat{x}\|2 \le 1 \right\}  =  \left\{ x ~~ x \hat{x}^T A^{ 1} x \hat{x} \le 1 \right\}  
        $$
       where  $$A=LL^T$$ is PD.
    
8. Geometric Interpretation via SED
       We interpret the eigenvectors and associated eigenvalues of A in terms of geometrical properties of the ellipsoid as follows.
       Consider the SED of $$A A = U \Lambda U^T$$ with $$U^TU = I$$ and $$\Lambda$$ diagonal with diagonal elements positive.
       The SED of its inverse is $$A^{ 1} = L L^T = U \Lambda^{ 1} U^T$$.
       Let $$\tilde{x} = U^Tx \hat{x}$$.
       We can express the condition $$x \in \mathbf{E}$$ as
       $$
        \tilde{x}^T\Lambda^{ 1}\tilde{x} = \displaystyle\sum{i=1}^n \frac{\tilde{x}i^2}{\lambdai} \le 1.
        $$
        Now set $$\bar{x}i = \tilde{x}i/\sqrt{\lambdai}  i=1 \ldots n$$.
         The above writes $$\bar{x}^T\bar{x} \le 1 \in \bar{x} $$space the ellipsoid is simply an unit ball. 
         In $$\tilde{x} $$space the ellipsoid corresponds to scaling each $$\bar{x} $$axis by the square roots of the eigenvalues.
         The ellipsoid has principal axes parallel to the coordinate axes in $$\tilde{x} $$space. 
         We then apply a rotation and a translation to get the ellipsoid in the original x space. 
         The rotation is determined by the eigenvectors of $$A^{ 1}$$ which are contained in the orthogonal matrix $$U$$.
         Thus the geometry of the ellipsoid can be read from the SED of the PD matrix $$A^{ 1} = LL^T  \implies$$
        1 The eigenvectors give the principal directions and  
        2 The semi axis lengths are the square root of the eigenvalues.
       
    
    It is possible to define degenerate ellipsoids which correspond to cases when the matrix B in the above or its inverse A is degenerate. For example cylinders or slabs intersection of two parallel half spaces are degenerate ellipsoids.



 Topology and Smooth Manifolds


Introduction and Definitions

1. Topology
       is a mathematical field concerned with the properties of space that are preserved under continuous deformations such as stretching crumpling and bending but not tearing or gluing

2. Topological Space
       is defined as a set of points $$\mathbf{X}$$ along with a set of neighbourhoods sub sets $$\mathbf{T}$$ for each point satisfying the following set of axioms relating points and neighbourhoods  
         $$\mathbf{T}$$ is the Open Sets     
            1. The Empty Set $$\emptyset$$ is in $$\mathbf{T}$$
            2. $$\mathbf{X}$$ is in $$\mathbf{T}$$
            3. The Intersection of a finite number of Sets in $$\mathbf{T}$$ is also in $$\mathbf{T}$$
            4. The Union of an arbitrary number of Sets in $$\mathbf{T}$$ is also in $$\mathbf{T}$$  
         $$\mathbf{T}$$ is the Closed Sets     
            1. The Empty Set $$\emptyset$$ is in $$\mathbf{T}$$
            2. $$\mathbf{X}$$ is in $$\mathbf{T}$$
            3. The Intersection of an arbitrary number of Sets in $$\mathbf{T}$$ is also in $$\mathbf{T}$$
            4. The Union of a finite number of Sets in $$\mathbf{T}$$ is also in $$\mathbf{T}$$

3. Homeomorphism
       Intuitively a Homeomorphism or Topological Isomorphism or bi continuous Function is a continuous function between topological spaces that has a continuous inverse function.  
       Mathematically a function $${\displaystyle fX\to Y}$$ between two topological spaces $${\displaystyle X{\mathcal {T}}{X}}$$ and $${\displaystyle Y{\mathcal {T}}{Y}}$$ is called a Homeomorphism if it has the following properties  
         $$f$$ is a bijection one to one and onto  
         $$f$$ is continuous
         the inverse function $${\displaystyle f^{ 1}}$$ is continuous $${\displaystyle f}$$ is an open mapping.  
    i.e. There exists a continuous map with a continuous inverse

4. Maps and Spaces
       | Map | Space | Preserved Property |  
        | Linear Map | Vector Space | Linear Structure $$faw+v = afw+fv$$ |  
        | Group Homomorphism | Group | Group Structure $$fx \ast y = fx \ast fy$$ |  
        | Continuous Map | Topological Space | Openness/Closeness $$f^{ 1}\{\text{open}\} \text{ is open}$$ |  
        | Smooth Map | Topological Space | 

5. Smooth Maps
       
         Continuous 
         Unique Limits       

6. Hausdorff

 

Point Set Topology

1. Open Set
       A set $$\chi \subseteq \mathbf{R}^n$$ is said to be open if for any point $$x \in \chi$$ there exist a ball centered in $$x$$ which is contained in $$\chi$$. 
       Precisely for any $$x \in \mathbf{R}^n$$ and $$\epsilon > 0$$ define the Euclidean ball of radius $$r$$ centered at $$x$$
       $$B\epsilonx = {z  \|z − x\|2 < \epsilon}$$
       Then $$\chi \subseteq \mathbf{R}^n$$ is open if
       $$\forall x \ \epsilon \ \chi \\ \exists \epsilon > 0  B\epsilonx \subset \chi .$$
       Equivalently
        A set $$\chi \subseteq \mathbf{R}^n$$ is open if and only if $$\chi = int\; \chi$$.
        An open set does not contain any of its boundary points.
        A closed set contains all of its boundary points. 
        Unions and intersections of open resp. closed sets are open resp. closed.

2. Closed Set
       A set $$\chi \subseteq \mathbf{R}^n$$ is said to be closed if its complement $$ \mathbf{R}^n \text{ \ } \chi$$ is open.

3. Interior of a Set
       The interior of a set $$\chi \subseteq \mathbf{R}^n$$ is defined as 
       $$int\ \chi = \{z \in \chi  B\epsilonz \subseteq \chi \\ \text{for some } \epsilon > 0 \}$$

4. Closure of a Set
       The closure of a set $$\chi \subseteq \mathbf{R}^n$$ is defined as
       $$\bar{\chi} = \{z ∈ \mathbf{R}^n  \ z = \lim{k\to\infty} xk \ xk \in \chi  \ \forall k\}$$  
    i.e. the closure of $$\chi$$ is the set of limits of sequences in $$\chi$$.

5. Boundary of a Set
       The boundary of X is defined as
       $$\partial \chi = \bar{\chi} \text{ \ }  int\ \chi$$

6. Bounded Set
       A set $$\chi \subseteq \mathbf{R}^n$$ is said to be bounded if it is contained in a ball of finite radius that is if there exist $$x \in \mathbf{R}^n$$ and $$r > 0$$ such that $$\chi \subseteq Brx$$.

7. Compact Set
       A set $$\chi \subseteq \mathbf{R}^n$$ is compact $$\iff$$ it is Closed and Bounded.

8. Relative Interior $$\operatorname{relint}$$
       We define the relative interior of the set $$\chi$$ denoted $$\operatorname{relint} \chi$$ as its interior relative to $$\operatorname{aff} C$$
       $$\operatorname{relint} \chi = \{x \in \chi  \ Bx r \cap \operatorname{aff} \chi \subseteq \chi \text{ for some } r > 0\}$$
       where $$Bx r = \{y  ky − xk \leq r\}$$ the ball of radius $$r$$ and center $$x$$ in the norm $$\| · \|$$.

9. Relative Boundary
       We can then define the relative boundary of a set $$\chi$$ as $$\mathbf{cl}  \chi \text{ \ } \operatorname{relint} \chi$$ where $$\mathbf{cl} \chi$$ is the closure of $$\chi$$.


Manifolds

1. Manifold
       is a topological space that locally resembles Euclidean space near each point  
        i.e. around every point there is a neighborhood that is topologically the same as the open unit ball in $$\mathbb{R}^n$$  
       

2. Smooth Manifold
       A topological space $$M$$ is called a $$n$$ dimensional smooth manifold if  
         Is is Hausdorff
         It is  Countable
         It comes with a family $$\{U\alpha \phi\alpha\}$$ with  
             Open sets $$U\alpha \subset\text{open} M$$ 
             Homeomorphisms $$\phi\alpha  U\alpha \rightarrow \mathbb{R}^n$$   
    such that $${\displaystyle M = \bigcup\alpha U\alpha}$$  
    and given $${\displaystyle U\alpha \cap U\beta \neq \emptyset}$$ the map $$\phi\beta \circ \phi\alpha^{ 1}$$ is smooth




 Topology and Smooth Manifolds


Introduction and Definitions

1. Power Sets
       Lemma the size of the power set of a set of size $$n$$ is $$2^n$$
       Proof  
        Assume that we n

2. Topological Space



 Topology and Smooth Manifolds


Introduction and Definitions

1. Topology
       is a mathematical field concerned with the properties of space that are preserved under continuous deformations such as stretching crumpling and bending but not tearing or gluing

2. Topological Space
       is defined as a set of points $$\mathbf{X}$$ along with a set of neighbourhoods sub sets $$\mathbf{T}$$ for each point satisfying the following set of axioms relating points and neighbourhoods  
         $$\mathbf{T}$$ is the Open Sets     
            1. The Empty Set $$\emptyset$$ is in $$\mathbf{T}$$
            2. $$\mathbf{X}$$ is in $$\mathbf{T}$$
            3. The Intersection of a finite number of Sets in $$\mathbf{T}$$ is also in $$\mathbf{T}$$
            4. The Union of an arbitrary number of Sets in $$\mathbf{T}$$ is also in $$\mathbf{T}$$  
         $$\mathbf{T}$$ is the Closed Sets     
            1. The Empty Set $$\emptyset$$ is in $$\mathbf{T}$$
            2. $$\mathbf{X}$$ is in $$\mathbf{T}$$
            3. The Intersection of an arbitrary number of Sets in $$\mathbf{T}$$ is also in $$\mathbf{T}$$
            4. The Union of a finite number of Sets in $$\mathbf{T}$$ is also in $$\mathbf{T}$$

3. Homeomorphism
       Intuitively a Homeomorphism or Topological Isomorphism or bi continuous Function is a continuous function between topological spaces that has a continuous inverse function.  
       Mathematically a function $${\displaystyle fX\to Y}$$ between two topological spaces $${\displaystyle X{\mathcal {T}}{X}}$$ and $${\displaystyle Y{\mathcal {T}}{Y}}$$ is called a Homeomorphism if it has the following properties  
         $$f$$ is a bijection one to one and onto  
         $$f$$ is continuous
         the inverse function $${\displaystyle f^{ 1}}$$ is continuous $${\displaystyle f}$$ is an open mapping.  
    i.e. There exists a continuous map with a continuous inverse

4. Maps and Spaces
       | Map | Space | Preserved Property |  
        | Linear Map | Vector Space | Linear Structure $$faw+v = afw+fv$$ |  
        | Group Homomorphism | Group | Group Structure $$fx \ast y = fx \ast fy$$ |  
        | Continuous Map | Topological Space | Openness/Closeness $$f^{ 1}\{\text{open}\} \text{ is open}$$ |  
        | Smooth Map | Topological Space | 

5. Smooth Maps
       
         Continuous 
         Unique Limits       

6. Hausdorff
       

       

       


Point Set Topology

1. Open Set
       A set $$\chi \subseteq \mathbf{R}^n$$ is said to be open if for any point $$x \in \chi$$ there exist a ball centered in $$x$$ which is contained in $$\chi$$. 
       Precisely for any $$x \in \mathbf{R}^n$$ and $$\epsilon > 0$$ define the Euclidean ball of radius $$r$$ centered at $$x$$
       $$B\epsilonx = {z  \|z − x\|2 < \epsilon}$$
       Then $$\chi \subseteq \mathbf{R}^n$$ is open if
       $$\forall x \ \epsilon \ \chi \\ \exists \epsilon > 0  B\epsilonx \subset \chi .$$
       Equivalently
        A set $$\chi \subseteq \mathbf{R}^n$$ is open if and only if $$\chi = int\; \chi$$.
        An open set does not contain any of its boundary points.
        A closed set contains all of its boundary points. 
        Unions and intersections of open resp. closed sets are open resp. closed.

2. Closed Set
       A set $$\chi \subseteq \mathbf{R}^n$$ is said to be closed if its complement $$ \mathbf{R}^n \text{ \ } \chi$$ is open.

3. Interior of a Set
       The interior of a set $$\chi \subseteq \mathbf{R}^n$$ is defined as 
       $$int\ \chi = \{z \in \chi  B\epsilonz \subseteq \chi \\ \text{for some } \epsilon > 0 \}$$

4. Closure of a Set
       The closure of a set $$\chi \subseteq \mathbf{R}^n$$ is defined as
       $$\bar{\chi} = \{z ∈ \mathbf{R}^n  \ z = \lim{k\to\infty} xk \ xk \in \chi  \ \forall k\}$$  
    i.e. the closure of $$\chi$$ is the set of limits of sequences in $$\chi$$.

5. Boundary of a Set
       The boundary of X is defined as
       $$\partial \chi = \bar{\chi} \text{ \ }  int\ \chi$$

6. Bounded Set
       A set $$\chi \subseteq \mathbf{R}^n$$ is said to be bounded if it is contained in a ball of finite radius that is if there exist $$x \in \mathbf{R}^n$$ and $$r > 0$$ such that $$\chi \subseteq Brx$$.

7. Compact Set
       A set $$\chi \subseteq \mathbf{R}^n$$ is compact $$\iff$$ it is Closed and Bounded.

8. Relative Interior $$\operatorname{relint}$$
       We define the relative interior of the set $$\chi$$ denoted $$\operatorname{relint} \chi$$ as its interior relative to $$\operatorname{aff} C$$
       $$\operatorname{relint} \chi = \{x \in \chi  \ Bx r \cap \operatorname{aff} \chi \subseteq \chi \text{ for some } r > 0\}$$
       where $$Bx r = \{y  ky − xk \leq r\}$$ the ball of radius $$r$$ and center $$x$$ in the norm $$\| · \|$$.

9. Relative Boundary
       We can then define the relative boundary of a set $$\chi$$ as $$\mathbf{cl}  \chi \text{ \ } \operatorname{relint} \chi$$ where $$\mathbf{cl} \chi$$ is the closure of $$\chi$$.


Manifolds

1. Manifold
       is a topological space that locally resembles Euclidean space near each point  
        i.e. around every point there is a neighborhood that is topologically the same as the open unit ball in $$\mathbb{R}^n$$  
       

2. Smooth Manifold
       A topological space $$M$$ is called a $$n$$ dimensional smooth manifold if  
         Is is Hausdorff
         It is  Countable
         It comes with a family $$\{U\alpha \phi\alpha\}$$ with  
             Open sets $$U\alpha \subset\text{open} M$$ 
             Homeomorphisms $$\phi\alpha  U\alpha \rightarrow \mathbb{R}^n$$   
    such that $${\displaystyle M = \bigcup\alpha U\alpha}$$  
    and given $${\displaystyle U\alpha \cap U\beta \neq \emptyset}$$ the map $$\phi\beta \circ \phi\alpha^{ 1}$$ is smooth




 TensorFlow 


 Graph slices holding one var constant is important for understanding partial derivatives   

 Vector Field basically a way of visualizing functions that have the same number of inputs as outputs outputs are vectors.  

 Partial Derivative 
   Interpret as how does a tiny change in the input in the $$x$$OR$$y$$ direction influence the output $$f$$; keeping the other variable constant. technically you then take the ratio of the nudges   
   On a graph interpret as slicing the graph at the constant variable and then looking at the slope on the projected/sliced graph.  
   Properties  
     $$f{x y} = f{y x}$$  

 Tangent Hyperplanes  
  The tangent hyperplane to a curve at a given point $$\mathbf{x}$$ is the best linear approximation of the curve at that point.  
   Tangent Line  
    $$y y{0}=f^{\prime}\leftx{0}\right\leftx x{0}\right$$   
   Tangent Plane  
    $$y y{0}=f^{\prime}\leftx{0}\right\leftx x{0}\right$$  
    of the surface $$z=fx y$$  at the point $$P\leftx{0} y{0} z{0}\right$$  

        
 Gradients  
   Properties  
     Always normal to contour lines  





































 Single Variable Calculus






Notes  
 The derivative is the best constant approximation of the rate of change.  
Not the instantaneous rate of change.    





































 The Essence of Linear Algebra







Definitions and Intuitions

1. Linear Algebra
    Linear Algebra is about two operations on a list of numbers  
    1. Scalar Multiplication
    2. Vector Addition

2. Vectors
    Think of each element in the vector as a scalar that scales the corresponding basis vectors.  
    Meaning think about how each one stretches or squishes vectors in this case the basis vectors $$\hat{i} \hat{j}$$  

    $$\begin{bmatrix}
            x 
            y 
        \end{bmatrix} = \begin{bmatrix}
            1 & 0  
            0 & 1 
        \end{bmatrix}   \begin{bmatrix}
                            x 
                            y 
                        \end{bmatrix} = \color{red} x \color{red} {\underbrace{\begin{bmatrix}
            1 
            0 
        \end{bmatrix}} {\hat{i}}} + \color{red} y \color{red} {\underbrace{\begin{bmatrix}
            0 
            1 
        \end{bmatrix}} {\hat{j}}} = \begin{bmatrix}
            1\times x + 0 \times y 
            0\times x + 1 \times y 
        \end{bmatrix}
    $$  

3. Span
    The Span of two vectors $$\mathbf{v} \mathbf{w}$$ is the set of all linear combinations of the two vectors 
    $$a \mathbf{v} + b \mathbf{w} \ ; \ ab \in \mathbb{R}$$  


4. Linearly Dependent Vectors
    If one vector is in the span of the other vectors.  
    Mathematically  
    $$a \vec{v}+b \vec{w}+c \vec{u}=\overrightarrow{0} \implies a=b=c=0$$  

The Basis
    The Basis of a vector space is a set of linearly independent vectors that span the full space.  

5. Matrix as a Linear Transformation
    Summary
    1. Each column  is a transformed version of the basis vectors e.g. $$\hat{i} \hat{j}$$  
    2. The result of a Matrix Vector Product is the linear combination of the vectors with the appropriate transformed coordinate/basis vectors  
        i.e. Matrix Vector Product is a way to compute what the corresponding linear transformation does to the given vector.  


    Matrix as a Linear Transformation  
    Always think of Matrices as Transformations of Space    
     A Matrix represents a specific linear transformation
         Where the columns represent the coordinates of the transformed basis vectors   
     & Multiplying a matrix by a vector is EQUIVALENT to Applying the transformation to that vector  
    The word "transformation" suggests that you think using movement  
    If a transformation takes some input vector to some output vector we imagine that input vector moving over to the output vector.  
    Then to understand the transformation as a whole we might imagine watching every possible input vector move over to its corresponding output vector.  
    This transformation/"movement" is Linear if it keeps all the vectors parallel and evenly spaced and fixes the origin.  


    Matrices and Vectors | The Matrix Vector Product  
    Again We think of each element in a vector as a scalar that scales the corresponding basis vectors.  
     Thus if we know how the basis vectors get transformed we can then just scale them by multiplying with our vector elements.  
        Mathematically we think of the vector   
        $$\mathbf{v} = \begin{bmatrix}x y \end{bmatrix} = x\hat{i} + y\hat{j}$$
        and its transformed version  
        $$\text{Transformed } \mathbf{v} = x \text{Transformed } \hat{i} + y \text{Transformed }  \hat{j}$$  
        $$\implies$$ we can describe where any vector $$\mathbf{v}$$ go by describing where the basis vectors will land.  
        If you're given a two by two matrix describing a linear transformation and some specific vector and you want to know where that linear transformation takes that vector you can 1 take the coordinates of the vector 2 multiply them by the corresponding columns of the matrix 3 then add together what you get.  
        This corresponds with the idea of adding the scaled versions of our new basis vectors.  


    $$ \mathbf{v} = 
        \begin{bmatrix}
            x 
            y 
        \end{bmatrix} = \begin{bmatrix}
            1 & 0  
            0 & 1 
        \end{bmatrix}   \begin{bmatrix}
                            x 
                            y 
                        \end{bmatrix} = \color{red} x \color{red} {\underbrace{\begin{bmatrix}
            1 
            0 
        \end{bmatrix}} {\hat{i}}} + \color{red} y \color{red} {\underbrace{\begin{bmatrix}
            0 
            1 
        \end{bmatrix}} {\hat{j}}} = \begin{bmatrix}
            1\times x + 0 \times y 
            0\times x + 1 \times y 
        \end{bmatrix} = x\hat{i} + y\hat{j}
    $$  

    The Matrix Vector Product  


    Non Square Matrices $$N \times M$$  
    Map vectors from $$\mathbb{R}^M \rightarrow \mathbb{R}^N$$.  
    They are transformations between dimensions.   


6. The Product of Two Matrices
    The Product of Two Matrices Corresponds to the composition of the transformations being applied from right to left.  
    This is very important intuition  
    e.g. do matrices commute?  
    if you think of the matrices as transformations of space then answer quickly is no.  
    Equivalently Are matrices associative?  Yes function composition is associative $$f \circg \circ h = f \circ g \circ h$$  


7. Linear Transformations
    Linear Transformations are transformations that preserve the following properties  
    1. All vectors that are parallel remain parallel 
    2. All vectors are evenly spaced 
    3. The origin remains fixed  



          


8. The Determinant
    The Determinant of a transformation is the "scaling factor" by which the transformation changed any area in the vector space.  

    The Negative Determinant determines the orientation.  

    Linearity of the Determinant  
    $$\text{det}AB = \text{det}A \text{det}B$$  


10.Solving Systems of Equations
    The Equation $$A\mathbf{x} = \mathbf{b}$$ finds the vector $$\mathbf{x}$$ that lands on the vector $$\mathbf{b}$$ when the transformation $$A$$ is applied to it.    

    Again the intuition is to think of a linear system of equations geometrically as trying to find a particular vector that once transformed/moved lands on the output vector $$\mathbf{b}$$.  
     This becomes more important when you think of the different properties of that transformation/function encoded now in the matrix $$A$$  
         When the $$detA \neq 0$$ we know that space is preserved and from the properties of linearity we know there will always be one unique vector that would land on $$\mathbf{b}$$ once transformed and you can find it by "playing the transformation in reverse" i.e. the inverse matrix.  
         When $$detA = 0$$ then the space is squished down to a lower representation resulting in information loss.  


9. The Inverse of a matrix
    The Inverse of a matrix is the matrix such that if we "algebraically" multiply the two matrices we get back to the original coordinates the identity.  
    It is basically the transformation applied in reverse.  

    Why inverse transformation/matrix DNE when det is Zero i.e. space is squished  
    To do so is equivalent to transforming a line into a plane which would require mapping each individual vector into a "whole line full of vectors" multiple vectors; which is not something a Function can do.  
    Functions map single input to single output.  


11.The Determinants Inverses & Solutions to Equations
    When the $$\text{det} = 0$$ the area gets squashed to $$0$$ and information is lost. Thus   
    1. The Inverse DNE
    2. A unique solution DNE  
    i.e. there is no function that can take a line onto a plane; info loss


12.The Rank
    The Rank is the dimensionality of the output of a transformation.  
    Viewed as a Matrix it is the number of independent vectors as columns that make up the matrix.  


13.The Column Space
    The Column Space is the set of all possible outputs of a transformation/matrix.  
     View each column as a basis vector; there span is then all the possible outputs
     The Zero Vector origin is always in the column space corresponds to preserving the origin
        
    The Column Space allows us to understand when a solution exists.  
    For example even when the matrix is not full rank det=0 a solution might still exist; if when $$A$$ squishes space onto a line the vector $$\mathbf{b}$$ lies on that line in the span of that line.  
     Formally solution exists if $$\mathbf{b}$$ is in the column space of $$A$$.   


14.The Null Space
    The Null Space is the set of vectors that get mapped to the origin; also known as The Kernel.   

    The Null Space allows us to understand what the set of all possible solutions look like.   

     Rank and the Zero Vector  
    A Full Rank matrix maps only the origin to itself.  
    A non full rank det=0 rank=n 1 matrix maps a whole line to the origin rank=n 2 a plane to the origin etc.  


15.The Dot Product/Scalar Product
    for vectors $$\mathbf{u} \mathbf{v}$$  
    $$\mathbf{u} \cdot \mathbf{v} = \mathbf{u}^T \mathbf{v}$$   
     Geometrically  
        1. project 
            

16.Cramer's Rule


          



17.Coordinate Systems

    A Coordinate System is just a way to formalize the basis vectors and their lengths. All coordinate systems agree on where the origin is.  
    Coordinate Systems are a way to translate between vectors defined w.r.t. basis vectors being scaled and added in space and sets of numbers the elements of the vector/list/array of numbers which we define.  



          
        Imp 647  



          

    Implicit Assumptions in a Coordinate System  
     Direction of each basis vector our vector is scaling
     The unit of distance  


18.Eigenstuff



          



          


    Notes  
     Complex Eigenvalues generally correspond to some kind of rotation in the transformation/matrix think multiplication by $$i$$ in $$\mathbb{C}$$ is a $$90^{\deg}$$ rotation.  
     For a diagonal matrix all the basis vectors are eigenvectors and the diagonal entries are their eigenvalues.  






 Classes of Matrices







Diagonal Matrices

1. Definition.
    Diagonal matrices are square matrices $$A$$ with $$A{ij} = 0 \text{ when } i \ne j.$$  
    

2. Properties
    1. Diagonal matrices correspond to quadratic functions that are simple sums of squares of the form  
        $$qx = \sum{i=1}^n \lambdai xi^2 = x^T \mathbf{diag}\lambda x.$$  
    2. They are easy to invert $$A^{ 1} {ii} = 1/A {ii} \ \forall i \in 1 n$$  
    3. The pseudo inverse is easy to compute keep zero diagonal elements as zero  
    4. Allow easier matrix multiplication and powers  
    



Symmetric Matrices

1. Definition
    Symmetric matrices are square matrices that satisfy $$A{ij} = A{ji}$$ for every pair $$ij.$$  

    The set of symmetric $$n \times n$$ matrices is denoted $$\mathbf{S}^n$$. This set is a subspace of $$\mathbf{R}^{n \times n}$$.  
    

2. Properties
     Has orthonormal eigenvectors even with repeated eigenvalues 
     Its inverse is also symmetric
     All eigenvalues of a symmetric matrix are real 
     $$A^TA$$ is invertible iff columns of $$A$$ are linearly independent  
     Every symmetric matrix $$S$$ can be diagonalized factorized with $$Q$$ formed by the orthonormal eigenvectors $$vi$$ of $$S$$ and $$\Lambda$$ is a diagonal matrix holding all the eigenvalues  
        $$\begin{aligned} S&=Q \Lambda Q^{T} 
            &= \lambda{1} v{1} v{1}^{T}+\ldots+\lambda{n} v{n} v{h}^{T} = \sum{i=1}^n \lambdai vi vi^T
            \end{aligned}$$  
     One can create a symmetric matrix from any matrix by  
        $$M = {\displaystyle {\tfrac {1}{2}}\leftA+A^{\textsf {T}}\right}$$  
    

3. Examples


        



        


        


        
    
 

Skew Symmetric Matrices

1. Definition.

2. Properties



Covariance Matrices

1. Definition.
    Standard Form  
    $$\Sigma =\mathrm {E} \left\left\mathbf {X}  \mathrm {E} \mathbf {X} \right\left\mathbf {X}  \mathrm {E} \mathbf {X} \right^{\rm {T}}\right$$  
    $$ \Sigma = \dfrac{1}{m} \sum{k=1}^m xk  \hat{x}xk  \hat{x}^T. $$  


    Matrix Form  
    $$ \Sigma = \dfrac{X^TX}{n} $$  

    valid only for 1 $$X$$ w/ samples in rows and variables in columns  2 $$X$$ is centered mean=0  



2. Properties
    1. The sample covariance matrix allows to find the variance along any direction in data space.

    2. The diagonal elements of $$\Sigma$$ give the variances of each vector in the data.

    3. The trace of $$\Sigma$$ gives the sum of all the variances.

    4. The matrix $$\Sigma$$ is positive semi definite since the associated quadratic form $$u \rightarrow u^T \Sigma u$$ is non negative everywhere.

    4. It is Symmetric.

    4. Every symmetric positive semi definite matrix is a covariance matrix.  


        

    5. The sample variance along direction $$u$$ can be expressed as a quadratic form in $$u$$  
        $$ \sigma^2u = \dfrac{1}{n} \sum{k=1}^n u^Txk \hat{x}^2 = u^T \Sigma u$$  
    6. The diminsion of the matrix is $$n \times n$$ where $$n$$ is the number of variables/features/columns.

    7. The inverse of this matrix $${\displaystyle \Sigma ^{ 1}}$$ if it exists is the inverse covariance matrix also known as the concentration matrix or precision matrix.

    7. If a vector of $$n$$ possibly correlated random variables is jointly normally distributed or more generally elliptically distributed then its probability density function can be expressed in terms of the covariance matrix.

    8. $$\Sigma =\mathrm {E} \mathbf {XX^{\rm {T}}}  {\boldsymbol {\mu }}{\boldsymbol {\mu }}^{\rm {T}}$$.

    9. $${\displaystyle \operatorname {var} \mathbf {AX} +\mathbf {a} =\mathbf {A} \\operatorname {var} \mathbf {X} \\mathbf {A^{\rm {T}}} }$$.

    10. $$\operatorname {cov} \mathbf {X} \mathbf {Y} =\operatorname {cov} \mathbf {Y} \mathbf {X} ^{\rm {T}}$$.

    11. $$\operatorname {cov} \mathbf {X} {1}+\mathbf {X} {2}\mathbf {Y} =\operatorname {cov} \mathbf {X} {1}\mathbf {Y} +\operatorname {cov} \mathbf {X} {2}\mathbf {Y} $$.  

    12. If $$p = q$$ then $$\operatorname {var} \mathbf {X} +\mathbf {Y} =\operatorname {var} \mathbf {X} +\operatorname {cov} \mathbf {X} \mathbf {Y} +\operatorname {cov} \mathbf {Y} \mathbf {X} +\operatorname {var} \mathbf {Y} $$.

    13. $$\operatorname {cov} \mathbf {AX} +\mathbf {a} \mathbf {B} ^{\rm {T}}\mathbf {Y} +\mathbf {b} =\mathbf {A} \\operatorname {cov} \mathbf {X} \mathbf {Y} \\mathbf {B}$$.

    14. If $${\displaystyle \mathbf {X} }$$  and $${\displaystyle \mathbf {Y} }$$  are independent or somewhat less restrictedly if every random variable in $${\displaystyle \mathbf {X} }$$ is uncorrelated with every random variable in $${\displaystyle \mathbf {Y} }$$ then $${\displaystyle \operatorname {cov} \mathbf {X} \mathbf {Y} =\mathbf {0} }$$.

    15. $$\operatorname {var} \mathbf {b} ^{\rm {T}}\mathbf {X} = \mathbf {b} ^{\rm {T}}\operatorname {cov} \mathbf {X} \mathbf {b} = \operatorname {cov} \mathbf{b}^T\mathbf {X} \mathbf{b}^T\mathbf {X}  \geq 0\$$.
        This quantity is NON Negative because it's variance.  
        Maybe $$= \mathbf {b} ^{\rm {T}}\operatorname {var} \mathbf {X} \mathbf {b}$$??  
    

    16. An identity covariance matrix $$\Sigma = I$$ has variance $$= 1$$ for all variables.
 
    17. A covariance matrix of the form $$\Sigma=\sigma^2I$$ has variance $$= \sigma^2$$ for all variables.

    18. A diagonal covariance matrix has variance $$\sigmai^2$$ for the $$i th$$  variable.

    19. When the mean $$\hat{x}$$ is not known the denominator of the "SAMPLE COVARIANCE MATRIX" should be $$n 1$$ and not $$n$$.

    where
     $${\displaystyle \mathbf {X} \mathbf {X} {1}}$$ and $${\displaystyle \mathbf {X} {2}}$$ are random $$p\times 1$$ vectors $${\displaystyle \mathbf {Y} }$$  is a random $$q\times 1$$ vector $${\displaystyle \mathbf {a} }$$  is a $$q\times 1$$ vector $${\displaystyle \mathbf {b} }$$ is a $$p\times 1$$ vector and $${\displaystyle \mathbf {A} }$$ and $${\displaystyle \mathbf {B} }$$  are $$q\times p$$ matrices of constants.

3. $$\Sigma$$ as a Linear Operator
     Applied to one vector the covariance matrix maps a linear combination $$c$$ of the random variables $$X$$ onto a vector of covariances with those variables   

    $${\displaystyle \mathbf {c} ^{\rm {T}}\Sigma =\operatorname {cov} \mathbf {c} ^{\rm {T}}\mathbf {X} \mathbf {X} }$$

     Treated as a bilinear form it yields the covariance between the two linear combinations  

    $${\displaystyle \mathbf {d} ^{\rm {T}}\Sigma \mathbf {c} =\operatorname {cov} \mathbf {d} ^{\rm {T}}\mathbf {X} \mathbf {c} ^{\rm {T}}\mathbf {X} }$$

     The variance of a linear combination is then its covariance with itself

    $${\displaystyle \mathbf {c} ^{\rm {T}}\Sigma \mathbf {c} }$$  

     The pseudo inverse covariance matrix provides an inner product  $${\displaystyle \langle c \mu \|\Sigma ^{+}\| c \mu \rangle }$$  which induces the Mahalanobis distance a measure of the "unlikelihood" of $$c$$.


4. Applications Examples

    allows one to find an optimal basis for representing the data in a compact way.












    


Positive Semi Definite Matrices

1. Definition
    A symmetric $${\displaystyle n\times n}$$ real matrix $${\displaystyle M}$$ is said to be positive semi definite if the scalar $${\displaystyle z^{\textsf {T}}Mz}$$ is non negative for every non zero column vector $${\displaystyle z}$$  of $$n$$ real numbers.  

    Mathematically  
    $$M \text { positive semi definite } \Longleftrightarrow \quad x^{\top} M x \geq 0 \text { for all } x \in \mathbb{R}^{n}$$  
    

2. Properties
     $$AA^T$$ and $$A^TA$$ are PSD  
     $$M$$ is positive definite if All pivots > 0      
     $$M$$ is positive definite if and only if all of its eigenvalues are non negative.  
     Covariance Matrices $$\Sigma$$ are PSD  
    




Positive Definite Matrices

1. Definition
    A symmetric $${\displaystyle n\times n}$$ real matrix $${\displaystyle M}$$ is said to be positive definite if the scalar $${\displaystyle z^{\textsf {T}}Mz}$$ is strictly positive for every non zero column vector $${\displaystyle z}$$  of $$n$$ real numbers.  


    Mathematically  
    $$M \text { positive definite } \Longleftrightarrow x^{\top} M x>0 \text { for all } x \in \mathbb{R}^{n} \backslash \mathbf{0}$$  


2. Properties
     $$M$$ is positive definite if and only if all of its eigenvalues are positive  
     The matrix $${\displaystyle M}$$ is positive definite if and only if the bilinear form $${\displaystyle \langle zw\rangle =z^{\textsf {T}}Mw}$$ is positive definite  
     A symmetric matrix $${\displaystyle M}$$ is positive definite if and only if its quadratic form is a strictly convex function  
     Every positive definite matrix is invertible and its inverse is also positive definite.  
     $$M$$ is positive definite if All pivots > 0  
     Covariance Matrices $$\Sigma$$ are positive definite unless one variable is an exact linear function of the others. Conversely every positive semi definite matrix is the covariance matrix of some multivariate distribution  
     Any quadratic function from $${\displaystyle \mathbb {R} ^{n}}$$ to $${\displaystyle \mathbb {R} }$$ can be written as $${\displaystyle x^{\textsf {T}}Mx+x^{\textsf {T}}b+c}$$ where $${\displaystyle M}$$ is a symmetric $${\displaystyle n\times n}$$ matrix $$b$$ is a real $$n$$ vector and $$c$$ a real constant. This quadratic function is strictly convex and hence has a unique finite global minimum if and only if $${\displaystyle M}$$ is positive definite.  
    




Orthogonal Matrices

1. Definition.
    Orthogonal or unitary complex matrices are square matrices such that the columns form an orthonormal basis.  
    

2. Properties
    1. If $$U = u1 \ldots un$$ is an orthogonal matrix then  
        $$ui^Tuj = \left\{ \begin{array}{ll} 1 & \mbox{if } i=j   0 & \mbox{otherwise.} \end{array} \right. $$  

    2. $$U^TU = UU^T = In$$.  

    3. Easy inverse  
        $$U^{ 1} = U^T$$  

    4. Geometrically orthogonal matrices correspond to rotations around a point or reflections around a line passing through the origin.  
        i.e. they preserve length and angles!  
        Proof. Part 5 and 6.  

    5. For all vectors $$\vec{x}$$  
        $$ \|Ux\|2^2 = Ux^TUx = x^TU^TUx = x^Tx = \|x\|2^2 .$$  
        Known as the rotational invariance of the Euclidean norm.  
        Thus If we multiply x with an orthogonal matrix the errors present in x will not be magnified. This behavior is very desirable for maintaining numerical stability.  

    6. If $$x y$$ are two vectors with unit norm then the angle $$\theta$$ between them satisfies $$\cos \theta = x^Ty$$  
    while the angle $$\theta'$$ between the rotated vectors $$x' = Ux y' = Uy$$ satisfies $$\cos \theta' = x'^Ty'$$.  
    Since $$Ux^TUy = x^T U^TU y = x^Ty$$ we obtain that the angles are the same.  
    


3. Examples


        
    


Dyads

1. Definition.
    A matrix $$A \in \mathbf{R}^{m \times n}$$ is a dyad if it is of the form $$A = uv^T$$ for some vectors $$u \in \mathbf{R}^m v \in \mathbf{R}^n$$.  
    
    The dyad acts on an input vector $$x \in \mathbf{R}^n$$ as follows  
    $$ Ax = uv^T x = v^Tx u.$$  
    

2. Properties
    1. The output always points in the same direction $$u$$ in output space $$\mathbf{R}^m$$ no matter what the input $$x$$ is.

    2. The output is always a simple scaled version of $$u$$.

    3. The amount of scaling depends on the vector $$v$$ via the linear function $$x \rightarrow v^Tx$$.  
    

3. Examples


        

    

4. Normalized dyads
       We can always normalize the dyad by assuming that both uv are of unit Euclidean norm and using a factor to capture their scale.  
       That is any dyad can be written in normalized form  
       $$ A = uv^T = \|u\|2 \cdot |v|2  \cdot \dfrac{u}{\|u\|2}  \dfrac{v}{\|v\|2} ^T = \sigma \tilde{u}\tilde{v}^T$$  
       where $$\sigma > 0$$ and $$\|\tilde{u}\|2 = \|\tilde{v}\|2 = 1.$$

5. Symmetric dyads
        Another important class of symmetric matrices is that of the form $$uu^T$$ where $$u \in \mathbf{R}^n$$.
    The matrix has elements $$uiuj$$ and is symmetric.
    If $$\|u\|2 = 1$$ then the dyad is said to be normalized.

       $$
        uu^T = \left\begin{array}{ccc} u1^2  & u1u2  & u1u3  
        u1u2 & u2^2   & u2u3  
        u1u3 & u2u3  & u3^2  \end{array} \right
        $$  
        Properties
            1. Symmetric dyads corresponds to quadratic functions that are simply squared linear forms  
            $$qx = u^Tx^2$$
            2. When the vector $$u$$ is normalized unit then  
            $$\mathbf{Tr}uu^T = \|u\|2^2 = 1^2 = 1$$  
            This follows from the fact that the diagonal entries of a symmetric dyad are just $$ui^2 \forall i \in 1 n$$
            3. 


Correlation matrix

1. Definition.
       $${\text{corr}}\mathbf {X} =\left{\text{diag}}\Sigma \right^{ {\frac {1}{2}}}\\Sigma \\left{\text{diag}}\Sigma \right^{ {\frac {1}{2}}}$$

2. Properties
    0. It is the matrix of "Pearson product moment correlation coefficients" between each of the random variables in the random vector $${\displaystyle \mathbf {X} }$$.

    1. The correlation matrix can be seen as the covariance matrix of the standardized random variables $${\displaystyle X{i}/\sigma X{i}}$$ for $${\displaystyle i=1\dots n}$$.

    2. Each element on the principal diagonal of a correlation matrix is the correlation of a random variable with itself which always equals 1.

    3. Each off diagonal element is between 1 and  1 inclusive.












Extrapolation

1. What?
     Extrapolation is used to 
        
        
     Extrapolation can be applied whenever 
        
        
     Suppose that for each number $$h \neq 0$$ we have a formula $$N1h$$ that approximates an
    unknown constant $$ \ \ \ \ \ \ \ \ $$ and that the truncation error involved with the approximation has the
    form  
    $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ 
     The truncation error is $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  
        where  
        1  
        2  

    and in general  
    $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ 
     The object of extrapolation is 
        

        

2. Why?
        



3. The $$\mathcal{O}h$$ formula for approximating $$M$$
    The  Formula  
    

    


    The  Formula  
    

    
 

4. The $$\mathcal{O}h^2$$ approximation formula for M
    


    

     Derivation  
        



        


5. When to apply Extrapolation?
 
6. The $$\mathcal{O}h^4$$ formula for approximating $$M$$
    

    


7. The $$\mathcal{O}h^6$$ formula for approximating $$M$$
    

    

8. The $$\mathcal{O}h^{2j}$$ formula for approximating $$M$$
    



9. The Order the Approximations Generated
    


    

9. How to actually calculate a derivative using the Extrapolation formula
    

    


Deriving n point Formulas with Extrapolation

1. Deriving Five point Formula
    

Numerical Quadrature

1. What?
     

2. How?
     

3. Based on
     

4. Method
    


    

     Derivation  
        





        


5. The Quadrature Formula
    
    


6. The Error
    
    


The Trapezoidal Rule

1. What?
    

    


1. Precision
    
    

2. The Trapezoidal Rule
    


    

     Derivation  
        


        

3. Error
    

    


Simpson’s Rule

1. What?
    

    

2. Simpson's Rule
    

    

     Derivation  
        




        


1. Precision
    
    

3. Error
    

    


Measuring Precision

1. What?
    

    

2. Precision degree of accuracy
    

    

3. Precision of Quadrature Formulas
     The degree of precision of a quadrature formula is $$ \mathcal{O}$$
        
        
     The Trapezoidal and Simpson’s rules are examples of $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  
     Types of Newton Cotes formulas 
         

Closed Newton Cotes Formulas

1. What?
    
    

     It is called closed because  
        
        

2. Form of the Formula
    

    

    where  
        
        

3. The Error Analysis
    

    


4. Degree of Preceision
     Even n the degree of precision is $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  
     Odd n the degree of precision is 

5. Closed Form Formulas
     $$n = 1$$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  rule  
        
        
        
     $$n = 2$$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  rule   
        
        
        
     $$n = 3$$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  rule   
        
        
        
     n = 4  
        

        


Open Newton Cotes Formulas

1. What?
     They $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  
     They use 
        
        
     This implies that 
        
        
     Open formulas contain $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ 

2. Form of the Formula
    

    
    where  
        
        

3. The Error Analysis
    

    

4. Degree of Preceision
     Even n 
     Odd n 
5. Open Form Formulas
     $$n = 0$$ PUT NAME HERE  
        
        
     $$n = 1$$   
        
        
     $$n = 2$$   
        
        
     n = 3  
        
        







Composite Rules

1. What?
    
    

2. Why?
    1.  
        
        
    2.  
        
        
    3.  
        
        

3. Notice
    
    


Composite Simpson’s rule

1. Composite Simpson’s rule
    

    

2. Error in Comoposite Simpson's Rule
    
    
    Error  

3. Theorem Rule and Error
    

    

4. Algorithm
    

    


Composite Newton Cotes Rules

1. Composite Trapezoidal rule
    

    

2. Composite Midpoint rule
    

    


Round Off Error Stability

1. Stability Property
    

    

2. Proof
    


    






Main Idea

1. What?
    

    

2. Why?
    
    
3. Error in Composite Trapezoidal rule
    

    
    This implies that $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ 

4. Extrapolation Formula
    Extrapolation then is used to produce $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ $$  approximations by  
        
        

    and according to this table  
        
        

    Calculate the Romberg table this way


5. Algorithm
    




     







Main Idea

1. What?
       1.  
            
            
        2.  
            
            
2. Why?
    
    

3. Approximation Formula
    $$\int{a}^{b} fx dx = $$  
        
        

     Derivation  
        


        

4. Error Bound
     Error relative to Composite Approximations  
        
        
     Error relative to True Value  
        
        

     ERROR DERIVATION  
        


         

    This implies 
        
        

5. Procedure
    When the approximations in 4.38 
        
        
    
    Then we use the error estimation procedure to 
        
          

    If the approximation on one of the subintervals fails to be within the tolerance $$\ \ \ \ \ \ \ \ $$ then
        
        

7. Algorithm
    


    

8. Derivation
    


    






Main Idea

1. What?
       1. 
            
            
        2.  
            

            

        3.  
            

              

        
        To Measure Accuracy 
            

            

         
        The Coefficients $$c1 c2 ...  cn$$ in the approximation formula are $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$    
        and   
        The Nodes $$x1 x2 ...  xn$$ are restricted by/to  $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  
        This gives  
        The number of Parameters to choose is $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ 

         
        If the coefficients of a polynomial are considered parameters 
            
              
        This then is 
            
            

2. Why?
    
    


Legendre Polynomials

1. What?
    
    

9. Why?
    

    

2. Properties
    1.  
    2.  
    3. The roots of these polynomials are 
           
           
           
           
           

3. The  Legendre Polynomials
    $$
    P0x = \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  P1x =  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  P2x = \ \ \ \ \ \ \ \ \ \ 
    $$  
    $$
    P3x = \ \ \ \ \ \ \ \  \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \   P4x = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
    $$
4. Determining the nodes
    

    

     PROOF  
        

        

    The nodes $$x1 x2 ...  xn$$ needed to
        
        


Gaussian Quadrature on Arbitrary Intervals

1. What?
    
    

2. The Change of Variables
    
    

3. Gaussian quadrature arbitrary interval
    
    
    







Approximating Double Integral

1. What?
    
    

2. Why?
    
    

3. Comoposite Trapezoidal Rule for Double Integral
    $$ \ \  \iintR fxy \dA \  = $$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  
    

    

     DERIVATION  
        


        

4. Comoposite Simpsons' Rule for Double Integral
     Rule  
        

        

     Error  
        

        

     Derivation  
        


        


Gaussian Quadrature for Double Integral Approximation

1. What?
    

    

2. Why?
    
    
3. Example
    

    


Non Rectangular Regions

1. What?
    
    

    Form  
        
          
        or  
        
        

2. How?
     We use  
     Step Size
         x  
         y   

3. Simpsons' Rule for Non Rect Regions
    


    

4. Simpsons' Double Integral Algorithm
    


    

5. Gaussian Double Integral Algorithm
    



    


Triple Integral Approximation

1. What?
        On what?  
        Form  
            

            

2. Gaussian Triple Integral Algorithm
    


    










Binary Machine Numbers

1. Representing Real Numbers
       A $$\ \ \ \ \ \ \ \ \ \ \ \ \$$ binary digit representation is used for a real number. 
     The  bit is  
     Followed by  
     and a $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \$$ called the  
     The base for the exponent is   
2. Floating Point Number Form
    

    

3. Smallest Normalized positive Number
     When  
     Equivalent to  

4. Largest Normalized positive Number
     When  
     Equivalent to  

5. UnderFlow
     When numbers occurring in calculations have  

6. OverFlow
     When numbers occurring in calculations have

7. Representing the Zero
     There are $$ \ \ \ \ \ $$ Representations of the number zero
        

        


Decimal Machine Numbers

1. What?
    

    

2. k digit Decimal Machine Numbers
    

    

3. Normalized Form
    

    

4. Floating Point Form of a Decimal Machine Number
     The floating point form of y denoted $$fly$$ is obtained by
        

        

5. Termination
    There are two common ways of performing this termination  
    1. $$ \ \ \ \ \ \ \ \ \ \ $$ 
        
        This produces the floating point form  

    2. $$ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ $$ which 
        

        This produces the floating point form   
          > For rounding when $$d{k+1} \geq 5$$ we  
          > When $$d{k+1} < 5$$ we  
          > If we round down then $$\deltai =$$   
          > However if we round up  

6. Approximation Errors
    

    
     The Absolute Error $$ \ \ \ \ \ \ \ $$.  

     The Relative Error $$ \ \ \ \ \ \ \ $$.

7. Significant Digits
    

    

8. Error in using Floating Point Repr.
     Chopping
        The Relative Error =   
        The Machine Repr. for k decimial digits =  
           $$ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$.  

        $$ \implies $$  
        

        
        Bound $$ \ \ \ \implies \ \  \ \ \ \ \ \ \ \ \ \$$.

     Rounding
        In a similar manner a bound for the relative error when using k digit rounding arithmetic is   

        Bound $$ \ \ \ \implies \ \  \ \ \ \ \ \ \ \ \ \ \ \ $$.

9. Distribution of Numbers
    The number of decimal machine numbers in $$\ \ \ \ \ \ \ \ \ \ \ $$ is  $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ for   


Finite Digit Arithmetic

1. Values
       $$ x = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$
       $$ y = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$

2. Operations
    

    

3. Error producing Calculations
       
        

        
       
        

        

4. Avoiding Round Off Error
       
        

        
       
        

           
        $$ 
        \implies \ \ \ \ \ \ \ \ \ \ \ \ \ \  x1 = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  
        x2 = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
        $$


Nested Arithmetic

1. What?
    

       
    Remember that chopping or rounding is performed   

    
      $$  \ \ \ \ \ \ \$$
    
    

    Polynomials should always be expressed $$  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  becasue $$  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$

2. Why?
    

      






Main Idea

1. Algorithm
    
    


Characterizing Algorithms

1. Stability
     Stable Algorithm 
    
    
     Conditionally Stable Algorithm
    
    
2. Error Growth
    

    
3. Stability and Error Growth
     Stable Algorithm  
     UnStable Algorithm 


Rates of Convergence

1. Rate of Convergence
    

    
    $$\betan \  = \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \  \ \ \ \ $$ for 

2. Big Oh Notation
    

    











Fixed Point Problems

1. Fixed Point
    

    

2. Root finding problems and Fixed point problems
    Root Finding and Fixed point problems are  
    
    
    
    

3. Why?
    

    

4. Existence and Uniqueness of a Fixed Point.
    

    


Fixed Point Iteration

1. Approximating Fixed Points
    
    
2. Algorithm
    
           
    
    

3. Convergence
     Fixed Point Theorem 
        
        
     Error bound in using $$pn$$ for $$p$$ 
        
        

        Notice 
            
            
4. Using Fixed Points
    Question $$ \ \ \ \ \ $$ 
        
        
    Answer     
        
        

5. Newton's Method as a Fixed Point Problem
    
    
    
    







Newton’s Method

1. What?
     Newton’s or the Newton Raphson method is
        
        
        
        
        
        
        
        

2. Derivation
    
    
    
    
    
    
3. Algorithm
    
    
    
    
    
    

4. Stopping Criterions
    
    


Convergence using Newton’s Method

1. Convergence Theorem
    Theorem  
    
    
    The crucial assumption is
        
        

    

    Theorem 2.6 states that 
    1  
        
        
    2 
        
        

The Secant Method

1. What?
    In Newton's Method 
    We approximate $$f' pn−1$$ as
        
        
    To produce 
        
        

2. Why?
    $$\ \ \ \ \ \ \ \ \ $$ 
        
        
      > Frequently 
            
            

    Note 
        
        

3. Algorithm
    
    
    
    


4. Convergence Speed
    
    


The Method of False Position

1. What?
    
    

2. Why?
    
    
3. Method
    
    
    
    
4. Algorithm
    
    
    
    







Order of Convergence 

1. Order of Convergence
    
    
2. Important Two cases of order
    
    
3. An arbitrary technique that generates a convergent sequences does so only linearly
    
    
    Theorem 2.8 implies 
         

4. Conditions to ensure Quadratic Convergence
    
    

5. Theorems 2.8 and 2.9 imply
    i
        
        
    ii
        
        

5. Newtons' Method Convergence Rate
    
    
    
    

Multiple Roots 

1. Problem
    
    

2. Zeros and their Multiplicity
    

    

3. Identifying Simple Zeros
     Theorem  
    
    
     Generalization of Theorem 2.11
        
        

        The result in Theorem 2.12 implies 
            

            

4. Why Simple Zeros
    
    
    Example
        
        


5. Handling the problem of multiple roots
     We $$  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \$$  
     We define $$\ \ \ \ \ \ \ \ \ \ $$ as 
        
        

     Derivation  
        
        

     Properties
         
            
            
         
            
            
         
            
            
           
            
            







Aitken’s $$ \Delta^2 $$ Method 

1. What?
    
    

2. Why?
    
    

0. Derivation
    


    

3. Del Forward Difference
    
    

4. $$\hat{p}n$$ Formula
    
    

5. Generating the Sequence Formula
    
    


Steffensen’s Method

1. What?
    

    

2. Zeros and their Multiplicity
    
    

3. Difference from Aitken's method
     Aitken's method  
        
        
     Steffensen’s method  
        

        

    Notice 
        
        

4. Algorithm
    
    

5. Convergance of Steffensen’s Method
    
    







Algebraic Polynomials

1. Fundamental Theorem of Algebra
    
    
2. Existance of Roots
    
    
3. Polynomial Equivalence
    
    
    This result implies 
        
        


Horner’s Method

1. What?
    

    

2. Why?
    
    

3. Horner's Method
    
        
    
    

4. Algorithm
    
    
    

    

5. Horner's Derivatives
    
    

6. Deflation
    
    

5. MatLab Implementation
    
    

Complex Zeros Müller’s Method

1. What?
     It is a 
        

        
     Müller’s method uses
        

        


2. Why?
    1.   
        
    2.   
        
        If the initial approximation is a real number 

3. Complex Roots
    
    

4. Algorithm
    


    
    
5. Calculations and Evaluations
    Müller’s method can  
        
        



Binary Machine Numbers

1. Representing Real Numbers
       A $$\ \ \ \ \ \ \ \ \ \ \ \ \$$ binary digit representation is used for a real number. 
     The  bit is  
     Followed by  
     and a $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \$$ called the  
     The base for the exponent is   
2. Floating Point Number Form
    

    

3. Smallest Normalized positive Number
     When  
     Equivalent to  

4. Largest Normalized positive Number
     When  
     Equivalent to  

5. UnderFlow
     When numbers occurring in calculations have  

6. OverFlow
     When numbers occurring in calculations have

7. Representing the Zero
     There are $$ \ \ \ \ \ $$ Representations of the number zero
        

        


Decimal Machine Numbers

1. What?
    

    

2. k digit Decimal Machine Numbers
    

    

3. Normalized Form
    

    

4. Floating Point Form of a Decimal Machine Number
     The floating point form of y denoted $$fly$$ is obtained by
        

        

5. Termination
    There are two common ways of performing this termination  
    1. $$ \ \ \ \ \ \ \ \ \ \ $$ 
        
        This produces the floating point form  

    2. $$ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ $$ which 
        

        This produces the floating point form   
          > For rounding when $$d{k+1} \geq 5$$ we  
          > When $$d{k+1} < 5$$ we  
          > If we round down then $$\deltai =$$   
          > However if we round up  

6. Approximation Errors
    

    
     The Absolute Error $$ \ \ \ \ \ \ \ $$.  

     The Relative Error $$ \ \ \ \ \ \ \ $$.

7. Significant Digits
    

    

8. Error in using Floating Point Repr.
     Chopping
        The Relative Error =   
        The Machine Repr. for k decimial digits =  
           $$ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$.  

        $$ \implies $$  
        

        
        Bound $$ \ \ \ \implies \ \  \ \ \ \ \ \ \ \ \ \$$.

     Rounding
        In a similar manner a bound for the relative error when using k digit rounding arithmetic is   

        Bound $$ \ \ \ \implies \ \  \ \ \ \ \ \ \ \ \ \ \ \ $$.

9. Distribution of Numbers
    The number of decimal machine numbers in $$\ \ \ \ \ \ \ \ \ \ \ $$ is  $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ for   


Finite Digit Arithmetic

1. Values
       $$ x = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$
       $$ y = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$

2. Operations
    

    

3. Error producing Calculations
       
        

        
       
        

        

4. Avoiding Round Off Error
       
        

        
       
        

           
        $$ 
        \implies \ \ \ \ \ \ \ \ \ \ \ \ \ \  x1 = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  
        x2 = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
        $$


Nested Arithmetic

1. What?
    

       
    Remember that chopping or rounding is performed   

    
      $$  \ \ \ \ \ \ \$$
    
    

    Polynomials should always be expressed $$  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  becasue $$  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$

2. Why?
    

      






Main Idea

1. Algorithm
    
    


Characterizing Algorithms

1. Stability
     Stable Algorithm 
    
    
     Conditionally Stable Algorithm
    
    
2. Error Growth
    

    
3. Stability and Error Growth
     Stable Algorithm  
     UnStable Algorithm 


Rates of Convergence

1. Rate of Convergence
    

    
    $$\betan \  = \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \  \ \ \ \ $$ for 

2. Big Oh Notation
    

    










Divided Differences

1. What?
    
    

2. Form of the Polynomial
     $$Pnx = $$
    
    
     Evaluated at $$x0$$ 
    

     Evaluated at $$x1$$ 
    

    $$\implies   a1 = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$.
3. The divided differences
     The zeroth divided difference of the function f with respect to $$xi$$
         Denoted 
         Defined 
        $$fxi =  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$.
    The remaining divided differences are defined 

     The  divided difference of $$f$$ with respect to $$xi$$ and $$x{i+1}$$
         Denoted 
         Defined 
            

            
     The  divided difference of $$f$$ with respect to $$xi$$ $$x{i+1}$$ and $$x{i+2}$$
         Denoted 
         Defined 
            

            
     The Kth divided difference of $$f$$ with respect to $$xi$$ $$x{i+1}...x{i+k 1}x{i+k}$$
         Denoted 
         Defined 
            

            
    The process ends with 
     The nth divided difference of $$f$$ with respect to $$xi$$ $$x{i+1}...x{i+k 1}x{i+k}$$
         Denoted 
         Defined 
            
            

4. The Interpolating Polynomial
    $$Pnx = $$

5. Newton’s Divided Difference
    $$Pnx = $$
    
    
    The value of $$fx0x1...xk$$ is 
         

6. Generation of Divided Differences
    

    

7. Algorithm
    
    
    

    


Forward Differences

1. Forward Difference
    

    

2. The divided differences with del notation
    
      
    $$
    fx0x{1} = 
    fx0x{1}x2 = 
    $$
    and in general  
    $$
    
    fx0x{1}...x{k 1}x{k} 
    $$
    
    

3. Newton Forward Difference Formula
    

      


Backward Differences

1. Backward Difference
    

    

2. The divided differences
    
    

    and in general 

    
    

    Consequently the Interpolating Polynomial 
    

    

    If we extend the binomial coefficient notation to 
         
    

    
    then 
    

    

3. Newton Backward Difference Formula
    

    

Centered Differences

1. What?
     
2. Why?
    
    
3. Stirling's Formula
     If $$n = 2m + 1$$ is odd 
        
        
     If $$n = 2m$$ is even we use the same formula but delete the last line
        
        
4. Table of Entries
    
    
    




























FOURTH











 6.5  TITLE



































FOURTH











 6.1  Linear Systems of Equations


Linear System of Equations

1. 

2. Linear System of Equations

3. Linear Operations
    1. Equation $$Ei$$ can be multiplied by any nonzero constant $$\lambda$$ with the resulting equation used in place of $$Ei$$. This operation is denoted $$\lambda Ei \rightarrow Ei$$.

    2. Equation $$Ej$$ can be multiplied by any constant $$\lambda$$ and added to equation $$Ei$$ with the resulting equation used in place of $$Ei$$. This operation is denoted $$Ei + \lambda Ej \rightarrow Ei$$.

    3. Equations $$Ei$$ and $$Ej$$ can be transposed in order. This operation is denoted $$Ei \leftrightarrow Ej$$.



Matrices and Vectors

1. Matrix

2. Gaussian Elemenation

3. Algorithm

    
    


Operation Counts


2. Multiplications/divisions for each i
       $$
        n − i + n − in − i + 1 = n − in − i + 2
        $$

3. Additions/subtractions for each i
       $$
        n − in − i + 1
        $$

4. Summing the operations in Steps 5 and 6


5. Multiplications/divisions Gauss Elem
       $$
        n − i + n − in − i + 1 = n − in − i + 2
        $$

6. Additions/subtractions Gauss Elem

    $$y' =  y + ty^{1/2}\  2 \leq t \leq 3\  y2 = 2 $$ with $$ h = 0.25 $$
    $$\dfrac{d}{dy}fty = $$$$\dfrac{d}{dy}  y + $$$$ty^{1/2}$$
    $$ =  \dfrac{t}{2  \sqrt{y}}  1$$
    $$\implies \vert \dfrac{d}{dy} fty \vert = \vert \dfrac{t}{2 \cdot \sqrty}  1 \vertl$$  
    Now since $$ t \in 23 $$ we know that this is maximized at $$ t = 3 $$
    $$
    \implies Max{f'} = \vert \dfrac{t}{2\dot \sqrty}  1 \vert  $$
    However since $$y \in  \infty \infty$$ at $$y=0$$ we get   
    $$ \vert \dfrac{t}{2\dot \sqrty}  1 \vert = \vert \dfrac{t}{2\dot \sqrt0}  1 \vert 
    = \vert \dfrac{t}{0}  1 \vert = \infty $$
    Thus this problem is ill posed and doesn't satisfy lipschitz condition.  




9. ...
    For a total of $$\approx \dfrac{n^3}{3}$$ operations $$ \implies \in \mathcal{O}n^3$$.


 6.4  TITLE



































FOURTH











 1.2  Round off Errors and Computer Arithmetic


Binary Machine Numbers

1. Representing Real Numbers
       A 64 bit binary digit representation is used for a real number. 
     The  bit is a sign indicator denoted $$s$$.  
     Followed by an 11 bit exponent $$c$$ called the characteristic  
     and a 52 bit binary fraction $$f$$  called the mantissa.  
     The base for the exponent is 2.
2. Floating Point Number Form
       $$ −1^s\ \  2^{c−1023} \ 1 + f$$
3. Smallest Normalized positive Number
    When  $$s = 0 \ \  c = 1\ \ $$ and $$\ \ f = 0$$.  
    Equivalent to  $$2^{−1022}\  \ \dot \ \ 1 + 0 \ \approx \ 0.22251 \ \dot \ 10^{−307}$$.
4. Largest Normalized positive Number
    When  $$s = 0\ \  c = 2046\ \ $$ and $$\ \ f = 1  2^{ 52}$$.  
    Equivalent to  $$2^{1023}\  \dot \  2  2^{ 52} \  \approx 0.17977  × 10^{309}$$.

5. UnderFlow
       When numbers occurring in calculations have a magnitude less than  
        $$2^{ 1022}$$.

6. OverFlow
       When numbers occurring in calculations have a magnitude greater than  
        $$2^{1023} \dot 2  2^{ 52}$$.

7. Representing the Zero
     There are Two Representations of the number zero
        1. A positive 0 when $$s = 0 \ \ c = 0 \ \ $$ and $$ \ f = 0$$. 
        2. A negative 0 when $$s = 1 \ \ c = 0 \ \ $$ and $$ \ f = 0$$. 


Decimal Machine Numbers

1. What?
       We assume that machine numbers are represented in the normalized decimal
        floating point form.
2. k digit Decimal Machine Numbers
       $$±0.d1d2 ... dk × 10^n  1 \leq d1 \leq 9 \text{and } 0 \leq di \leq 9 $$  
       $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \text{for each } i = 2 ...  k$$.
3. Normalized Form
       $$ y = 0.d1d2 ... dk × 10^n $$
4. Floating Point Form of a Decimal Machine Number
    The floating point form of y denoted $$fly$$ is obtained by terminating the mantissa of $$y$$ at k decimal digits.
5. Termination
    There are two common ways of performing this termination  
     Chopping is to simply chop off the digits $$d{k+1}d{k+2}$$.
    This produces the floating point form $$fly = 0.d1d2 ... dk × 10^n$$  
     Rounding adds $$5 × 10^{n−k+1}$$ to $$y$$ and then chops the result
    This produces the floating point form $$fly = 0.\delta1\delta2 ... \deltak × 10^n$$.  
      > For rounding when $$d{k+1} \geq 5$$ we add $$1$$ to $$dk$$ to obtain $$fly$$; that is we round up.
      > When $$d{k+1} < 5$$ we simply chop off all but the  k digits; so we round down.
      > If we round down then $$\deltai = di$$ for each $$i = 1 2 ...  k$$.  
      > However if we round up the digits and even the exponent might change.
6. Approximation Errors
     The Absolute Error $$ \ \ \ \ \ \ \ \|p − p^∗\|$$.  

     The Relative Error $$ \ \ \ \ \ \ \ \dfrac{\|p − p^∗\|}{\|p\|}$$.

7. Significant Digits

8. Error in using Floating Point Repr.
     Chopping
        The Relative Error = $$|\dfrac{y  fly}{y}|$$  
        The Machine Repr. for k decimial digits =  
           $$y = 0.d1d2 ... dkd{k+1} ... × 10^n$$.  

        $$ \implies $$  
        Bound $$ \ \ \ \implies \ \ |\dfrac{y  fly}{y}| \leq \dfrac{1}{0.1} \times 10^{ k} = 10^{ k+1}$$.

     Rounding
        In a similar manner a bound for the relative error when using k digit rounding arithmetic is   

        Bound $$ \ \ \ \implies \ \ \|\dfrac{y  fly}{y}\| \leq 0.5 × 10^{−k+1}$$.

9. Distribution of Numbers
    The number of decimal machine numbers in $$10^n 10^{n+1}$$ is constant for all integers $$n$$.


Finite Digit Arithmetic

1. Values
       $$ x = flx \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 1$$  
       $$ y = fly \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 2$$

2. Operations
3. Error producing Calculations
     Cancelation of significant digits due to the subtraction of nearly equal numbers.  
    
    
     Dividing by a number with small magnitude / Multiplying by a number with large magnitude.  
    
    

4. Avoiding Round Off Error
    The loss of accuracy due to round off error can often be avoided by a reformulation of
    the calculations.

    We change the form of the quadratic formula by rationalizing the numerator
    $$ 
    \implies \ \ \ \ \ \ \ \ \ \ \ \ \ \  x1 = \dfrac{−2c}{b + \sqrt{b^2 − 4ac}} \ \ \ \ \ \ \ \ \ \ \ \ \ \  
    x2 = \dfrac{−2c}{b  \sqrt{b^2 − 4ac}}
    $$


Nested Arithmetic

1. What?
       Rearranging calculations to reduce the number of computations.  
    Remember that chopping or rounding is performed after each calculation.  

       
    Polynomials should always be expressed in nested form before performing an evaluation
    because this form minimizes the number of arithmetic calculations.
2. Why?
       Accuracy loss due to round off error can also be reduced by rearranging calculations to reduce the number of computations.


 1.3  Algorithms and Convergence


Main Idea

1. Algorithm
       An Algorithm is a procedure that describes in an
        unambiguous manner a finite sequence of steps to be performed in a specified order


Characterizing Algorithms

1. Stability
     Stable Algorithm an algorithm where small changes in the initial data produce correspondingly small changes in the final results.
     Conditionally Stable Algorithm an algorithm that is stable only for certain choices of initial data.
2. Error Growth
3. Stability and Error Growth
     Stable Algorithm an algorithm that exhibits linear growth of error.
     UnStable Algorithm an algorithm that exhibits exponential error growth.


Rates of Convergence

1. Rate of Convergence
    $$\betan \  = \  \dfrac{1}{n^p} \ \ \ \ $$ for the largest number $$p > 0$$.

2. Big Oh Notation

3. Example determining rate of convergence
    
    


 1.1  Calculus


Limits and Continuity

1. Limit of Function

2. Continuity

3. Limit of Sequence

4. Convergence and Continuity Correspondance


Differentiability

1. Differentiablity

2. Differentiablity and Continuity Correspondance

3. Rolle’s Theorem

6. Generalized Rolle’s Theorem

4. Mean Value Theorem
    Proof.  
    $$
    \begin{align}
    &\ fa = ga\  \ \ \  \  
    & \ fb = gb 
    & \ hx = fx  gx\  \text{   define }hx 
    & \iff ha = hb = 0 
    & \implies  h'x = f'x  g'x
    & \implies  h'x = f'x  g'x = 0 \ \ \text{for some } x = c
    & \implies  f'c = g'c = \dfrac{gb  ga}{b a}
    & \implies  f'x = \dfrac{fb  fa}{b a}
    \end{align}
    $$

5. Extreme Value Theorem

7. Intermediate Value Theorem


Integration

1. The Riemann Integral
    Or for equally spaced intervals  

2. Integrability and Continuity Correspondance
       A function f that is continuous on an interval $$a b$$ is also Riemann   integrable on
        $$a b$$

3. Weighted Mean Value Theorem for Integrals
    When $$gx ≡ 1$$ Theorem 1.13 is the usual Mean Value Theorem for Integrals.  
    It gives
    the average value of the function $$f$$ over the interval $$a b$$  
    $$fc\  = \   \dfrac{1}{b − a} \inta^b fx \  dx.$$


Taylor Polynomials and Series

1. Taylor’s Theorem
    $$Pnx$$ is called the nth Taylor polynomial for $$f$$ about $$x0$$.  

    $$Rnx$$ is called the truncation error or remainder term associated with $$Pnx$$. 

    Since the number $$ξx$$ in the truncation error $$Rnx$$ depends on the value of x at which the polynomial $$Pnx$$ is being evaluated it is a function of the variable $$x$$.  

    Taylor’s Theorem only ensures that such a function $$ξx$$ exists and that its value lies between $$x$$ and $$x0$$ and not how to determine the function $$ξx$$.

2. Polynomials
     Taylor's Polynomial The polynomial definied by  
     Maclaurin Polynomial The special case Taylors Polynomial with $$x0 = 0$$.  

3. Series
     Taylor's Series The infinite series obtained by taking the limit of $$Pnx\text{as }\ n \rightarrow \inf$$.
     Maclaurin Series The special case Taylors series with $$x0 = 0$$.  

4. Truncation Error
       Refers to the error involved in using a truncated or finite summation to approximate  
        the sum of an infinite series.


 4.5  Romberg Integration


Main Idea

1. What?
       Richardson extrapolation applied to results from the
        Composite Trapezoidal rule can be used for accurate results.
2. Why?
       The technique can be used to obtain high accuracy approximations with little
        computational cost.
3. Error in Composite Trapezoidal rule
    This implies that Richardsons' Extrapolation is applicable here.
4. Extrapolation Formula
    Extrapolation then is used to produce $$\mathcal{O}hk^{2j}$$ approximations by  

    and according to this table  

    Calculate the Romberg table one complete row at a time.
5. Algorithm



 4.1  Numerical Differentiation


The derivative

1. Derivative
    $$f'x0 = \lim{h\to\infty} \ \ \dfrac{fx0 + h − fx0}{h}$$

2. The forward/backward difference formula
    Derivative formulat at $$x = x0$$
    This formula is known as the forward difference formula if $$h > 0$$
    and the backward difference formula if $$h < 0$$.  

    
    



    Error Bound
        For small values of h the difference quotient $$\dfrac{fx0 + h − fx0}{h}$$ can be used to approximate $$fx0$$ with an error bounded by $$M\dfrac{|h|}{2}$$ where $$M$$ is a bound on $$|f''x|$$ for $$x$$ between $$x0$$ and $$x0 + h$$.

3. The $$n + 1$$ point formula to approximate $$f'xj$$
     Derivation  
    
    


4. Three point Formula
    for each $$j = 0 1 2$$ where the notation $$\zetaj$$ indicates that this point depends on $$xj$$.
     Derivation  
    
    



Three Point Formulas

1. Equally Spaced nodes
    The formulas from Eq. 4.3 become especially useful if the nodes are equally spaced that
    is when $$x1 = x0 + h$$ and $$x2 = x0 + 2h$$ for some $$h \neq 0$$.  
    We will assume equally spaced nodes throughout the remainder of this section.

2. Three Point Endpoint Formula
    The approximation in Eq. 4.4 is useful near the ends of an interval because information about f outside the interval may not be available.  

    
    

    Errors the errors in both Eq. 4.4 and Eq. 4.5 are $$Oh^2$$
3. Three Point Midpoint Formula
    Errors Although the errors in both Eq. 4.4 and Eq. 4.5 are $$Oh^2$$ the error in Eq. 4.5 is approximately half the error in Eq. 4.4.  
      > This is because Eq. 4.5 uses data on both sides of $$x0$$ and Eq. 4.4 uses data  
        on only one side. Note also that f needs to be evaluated at only two points in Eq. 4.5 whereas in Eq. 4.4 three evaluations are needed.


Five Point Formulas

1. What?
    They are five point formulas that involve evaluating the function at two additional points to the three point formulas.
2. Why?
    One common five point formula is used to determine approximations for the derivative at the midpoint.
3. Error
    The error term for these formulas is $$Oh^4$$.  

4. Five Point Midpoint Formula

    Used for approximation at Mid Points  

5. Five Point Endpoint Formula

    Used for approximation at End Points  

    Left endpoint approximations are found using this formula with $$h > 0$$ and right endpoint approximations with $$h < 0$$.  

    The five point endpoint formula is particularly useful for the
    clamped cubic spline interpolation of Section 3.5.  


Approximating Higher Derivatives

1. Approximations to  Derivatives
    Derivation below
    

2.  Derivative Midpoint Formula
    
    

    Error Bound If $$f^{4}$$ is continuous on $$x0 − h x0 + h$$ it is also bounded and the approximation is $$Oh^2$$.  


Round Off Error Instability

1. Form of Error
     We assume that our computations actually use the values $$\tilde{f}x0 + h$$ and $$\tilde{f}x0 − h$$   
     which are related to the true values $$fx0 + h$$ and $$fx0 − h$$ by  
    $$ fx0 + h = \tilde{f}x0 + h + ex0 + h \ \ $$  &  
    $$fx0 − h = \tilde{f}x0 − h + ex0 − h $$
2. The Total Error
    It is due both to round off error the  part and to truncation error.  

3. Error Bound
    If we assume that the round off errors $$ex0 ± h$$ are bounded by some number $$ε > 0$$and that the  derivative of $$f$$ is bounded by a number $$M > 0$$ then  
4. Reducing Truncation Error
     How? To reduce the truncation error $$\dfrac{h2}{6}M$$ we need to reduce $$h$$. 
     Effect of reducing $$h$$ But as $$h$$ is reduced the roundoff error $$\dfrac{ε}{h}$$ grows.

5. Conclusion
     It is seldom advantageous to let $$h$$ be too small because in that case the round off error will dominate the calculations.  
     But we must remain aware that reducing the step size will not always improve the approximation.
     As approximation methods numerical differentiation is unstable.


 4.4  Composite Numerical Integration


Composite Rules

1. What?
    A piecewise approach to numerical integration that uses the
    low order Newton Cotes formulas.
2. Why?
     The Newton Cotes formulas are generally unsuitable for use over large integration intervals.  
     High degree formulas would be required and the values of the coefficients in these
    formulas are difficult to obtain.  
     Newton Cotes formulas are based on interpolatory
    polynomials that use equally spaced nodes a procedure that is inaccurate over large
    intervals because of the oscillatory nature of high degree polynomials.
3. Notice
        $$h = \dfrac{b − a}{n}$$ and $$xj = a + jh$$


Composite Simpson’s rule

1. Composite Simpson’s rule
2. Error in Comoposite Simpson's Rule
    Error $$\ \ \in \ \ \  \mathcal{O}h^4$$
3. Theorem Rule and Error
4. Algorithm


Composite Newton Cotes Rules

1. Composite Trapezoidal rule
2. Composite Midpoint rule


Round Off Error Stability

1. Stability Property
    An important property shared by all the composite integration techniques is a stability with respect to round off error.  
    The round off error does not depend on the number of calculations performed.
2. Proof
    
    


 4.3  Elements of Numerical Integration


Numerical Quadrature

1. What?
       The basic method involved in approximating $$\int{a}^{b} fx dx$$.


2. How?
           It uses a sum $$\sum{i=0}^{n} ai fxi$$ to approximate $$\int{a}^{b} fx dx$$.
3. Based on
       The methods of quadrature in this section are based on the interpolation polynomials
    given in Chapter 3.
4. Method
     Select a set of distinct nodes $${x0 ...  xn}$$ from the
    interval $$a b$$.  
     Then integrate the Lagrange interpolating polynomial  
    $$Pnx = \sum{i=0}^{n} fxiLix$$  
    and its truncation error term over $$a b$$ to obtain  
    
    
5. The Quadrature Formula

6. The Error


The Trapezoidal Rule

1. What?
       Approximation method for integrals produced by using  linear Lagrange polynomials with equally spaced nodes.
2. The Trapezoidal Rule
    
    

3. Error
    The error term for the Trapezoidal rule involves $$f$$  so the rule gives the exact
    result when applied to any function whose  derivative is identically zero that is any
    polynomial of degree one or less.


Simpson’s Rule

1. What?
    A method to approximate an integral that results from integrating over $$a b$$ the  Lagrange polynomial with equally spaced nodes $$x0 = a x2 = b$$ and $$x1 = a + h$$ where $$h = \dfrac{b − a}{2}$$.
2. Simpson's Rule
    
    
3. Error
    The error term in Simpson’s rule involves the fourth derivative of $$f$$  so it gives exact
    results when applied to any polynomial of degree three or less.


Measuring Precision

1. What?
    The standard derivation of quadrature error formulas is based on determining the class of
    polynomials for which these formulas produce exact results.
2. Precision degree of accuracy
    Definition 4.1 implies that the Trapezoidal and Simpson’s rules have degrees of precision
    one and three respectively.
3. Precision of Quadrature Formulas
     The degree of precision of a quadrature formula is n if and only if the error is zero for
    all polynomials of degree $$k = 0 1 ...  n$$ but is not zero for some polynomial of degree $$n + 1$$.
     The Trapezoidal and Simpson’s rules are examples of a class of methods known as Newton Cotes formulas.
     Types of Newton Cotes formulas There are 2 types Open and Closed.


Closed Newton Cotes Formulas

1. What?
     The $$n+1$$ point closed Newton Cotes formula uses nodes $$xi = x0 +ih$$ for $$i = 0 1 ...  n$$ where $$x0 = a xn = b$$ and $$h = \dfrac{b − a}{n}$$ .
     It is called closed because the endpoints of the closed interval a b are included as nodes.
2. Form of the Formula
    where  
3. The Error Analysis

4. Degree of Preceision
     Even n the degree of precision is $$n + 1$$ although the interpolation polynomial is of degree at most n
     Odd n the degree of precision is only $$n$$.

5. Closed Form Formulas
     $$n = 1$$ Trapezoidal rule  
     $$n = 2$$ Simpson’s rule   
     $$n = 3$$ Simpson’s Three Eighths rule   
     n = 4  


Open Newton Cotes Formulas

1. What?
     They do not include the endpoints of $$a b$$ as nodes.
     They use the nodes $$xi = x0 + ih$$ for each $$i = 0 1 ...  n$$ where $$h = \dfrac{b − a}{n + 2}$$ and $$x0 = a + h$$.
     This implies that $$xn = b − h$$ so we label the endpoints by setting $$x{−1} = a$$ and $$x{n+1} = b$$.
     Open formulas contain all the nodes used for the approximation within the open interval $$a b$$
2. Form of the Formula
    where  

3. The Error Analysis

4. Degree of Preceision
     Even n Higher.
     Odd n Lower.
5. Open Form Formulas
     $$n = 0$$ Midpoint rule  
     $$n = 1$$   
     $$n = 2$$   
     n = 3  



 4.7  Gaussian Quadrature


Main Idea

1. What?
        A technique that is used to determine the nodes and coefficients  
        for formulas that give exact results for higher degree polynomials.  
         Gaussian quadrature chooses the points for evaluation in an optimal rather than    
        equally spaced way.
         The nodes $$x1 x2 ...  xn$$ in the interval $$a b$$ and coefficients $$c1 c2 ...  cn$$ are chosen to minimize the expected error obtained in the approximation  $$\int{a}^{b} fx dx  = \sum{i=1}^{n} ci fxi.$$  

        
        To Measure Accuracy we assume that the best choice of these values produces the exact result for the largest class of polynomials that is the choice that gives the greatest degree of precision.  

         
        The Coefficients $$c1 c2 ...  cn$$ in the approximation formula are arbitrary  
        and   
        The Nodes $$x1 x2 ...  xn$$ are restricted only by the fact that they must lie in $$a b$$ the interval of integration.  
        This gives  
        The number of Parameters to choose is $$2n$$.  

         
        If the coefficients of a polynomial are considered parameters the class of polynomials of degree at most $$2n − 1$$ also contains $$2n$$ parameters.  
        This then is The Largest Class of Polynomials for which it is reasonable to
        expect a formula to be exact.

2. Why?
       The fact that Newton Cotes Formulas  use values at equally spaced nodes can
        significantly decrease the accuracy of the approximation.


Legendre Polynomials

1. What?
       A series of solutions to "Legendre's differential equation" that form a polynomial    
        sequence of orthogonal polynomials.

9. Why?
       The roots of the nth deg Legnedre Polynomial are the nodes needed for the approximation formula that gives exact results for any polynomial of degree less than $$2n$$.
2. Properties
    1. For each $$n$$ $$Pnx$$ is a monic polynomial of degree $$n$$.
    2. $$\int{ 1}^{1} Px Pnx dx = 0 $$ whenever $$Px$$ is a polynomial of degree less than $$n$$.
    3. The roots of these polynomials are 
         Distinct 
         lie in the interval $$−1 1$$
         have a symmetry with respect to the origin
         the correct choice for determining the parameters that give us the nodes and coefficients for our quadrature method.
3. The  Legendre Polynomials
    $$
    P0x = 1 \ \ \ \ \ \ \ \ \ \ \ \ \ \  P1x = x \ \ \ \ \ \ \ \ \ \ \ \ \ \  P2x = x^2 − \dfrac{1}{3}
    $$  
    $$
    P3x = x^3 − \dfrac{3}{5}x\ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \   P4x = x^4 − \dfrac{6}{7}x^2 +\dfrac{3}{35}.
    $$
4. Determining the nodes
    
    

    The nodes $$x1 x2 ...  xn$$ needed to produce an integral approximation formula that
    gives exact results for any polynomial of degree less than $$2n$$ are the roots of the nth degree Legendre polynomial.


Gaussian Quadrature on Arbitrary Intervals

1. What?
     Change of Intervals
        An integral $$\int{a}^{b} fx dx$$ over an arbitrary $$a b$$ can be transformed into an integral over $$−1 1$$ by using a change of variables.
2. The Change of Variables
    $$ \ \ \ t = \dfrac{2x − a − b}{b − a} \ \ \ \iff \ \ \ x = \dfrac{1}{2}b − at + a + b.$$
3. Gaussian quadrature arbitrary interval



 4.6  Adaptive Quadrature Methods


Main Idea

1. What?
        Efficient techniques for calculating integrals in intervals with high functional variations.  
         They predict the amount of functional variation and adapt the step size as necessary.
2. Why?
       The composite formulas suffer because they require the use of equally spaced nodes.  
        This is inappropriate when integrating a function on an interval that contains both regions with large functional variation and regions with small functional variation.
3. Approximation Formula
    $$\int{a}^{b} fx dx = $$  


    
    

4. Error Bound
     Error relative to Composite Approximations  
     Error relative to True Value  
    
    

    This implies that this procedure approximates the integral about 15 times better than it agrees with the computed value $$Sa b$$.

5. Procedure
    When the approximations in 4.38 differ by more than $$15\epsilon$$ we can apply the Simpson’s rule technique individually to the subintervals $$a\dfrac{a + b}{2}$$ and $$\dfrac{a + b}{2} b$$.  
    
    Then we use the error estimation procedure to determine if the approximation to the integral on each subinterval is within a tolerance of $$\epsilon/2$$. If so we sum the approximations to produce an approximation to $$\int{a}^{b} fx dx$$ within the tolerance $$\epsilon$$.  

    If the approximation on one of the subintervals fails to be within the tolerance $$\epsilon/2$$ then
    that subinterval is itself subdivided and the procedure is reapplied to the two subintervals to determine if the approximation on each subinterval is accurate to within $$\epsilon/4$$. This halving procedure is continued until each portion is within the required tolerance.

7. Algorithm
    
    

8. Derivation
    
    



 4.2  Richardson's Extrapolation


Extrapolation

1. What?
     Extrapolation that is used to generate high accuracy results while using low order
    formulas.
     Extrapolation can be applied whenever it is known that an approximation technique
    has an error term with a predictable form one that depends on a parameter usually the step
    size $$h$$.
     Suppose that for each number $$h \neq 0$$ we have a formula $$N1h$$ that approximates an
    unknown constant $$M$$ and that the truncation error involved with the approximation has the
    form  
    $$ M − N1h = K1h + K2h^2 + K3h^3 +··· $$  
    for some collection of unknown constants $$K1 K2 K3 ...$$ .  
     The truncation error is $$Oh$$ so unless there was a large variation in magnitude among the constants $$K1 K2 K3 ... $$  
    $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ M − N10.1 \approx 0.1K1\ \ \ \ \ \ \ \ M − N10.01 \approx 0.01K1 $$  
    and in general  
    $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ M − N1h \approx K1h$$ .
     The object of extrapolation is to find an easy way to combine these rather inaccurate
    $$Oh$$ approximations in an appropriate way to produce formulas with a higher order
    truncation error.
2. Why?
     We can combine the $$N1h$$ formulas to produce an $$\mathcal{O}h^2$$
    approximation formula $$N2h$$ for $$M$$ with  
    $$M − N2h = \hat{K}2h^2 + \hat{K}3h^3 +···$$   
    for some again unknown collection of constants $$\hat{K}2 \hat{K}3 ... $$.  
    Then we would have  
    $$M − N20.1 \approx 0.01\hat{K}2 M − N20.01 \approx 0.0001\hat{K}2$$  
     If the constants $$K1$$ and $$\hat{K}2$$ are roughly of the same magnitude then the $$N2h$$ approximations would be much better than the corresponding $$N1h$$ approximations.  
  


3. The $$\mathcal{O}h$$ formula for approximating $$M$$
    The  Formula  

    The  Formula  

4. The $$\mathcal{O}h^2$$ approximation formula for M
    
    

5. When to apply Extrapolation?
    Extrapolation can be applied whenever the truncation error for a formula has the form  

    for a collection of constants $$Kj$$ and when $$\alpha1 < \alpha2 < \alpha3 < ··· < \alpham$$.  

    The extrapolation is much more effective than when all powers of $$h$$ are present because the averaging process produces results with errors $$\mathcal{O}h^2 \mathcal{O}h^4 \mathcal{O}h^6 ... $$ with essentially no increase in computation over the results with errors $$\mathcal{O}h \mathcal{O}h^2 \mathcal{O}h^3 ...$$ .
6. The $$\mathcal{O}h^4$$ formula for approximating $$M$$
    Derivation below  
7. The $$\mathcal{O}h^6$$ formula for approximating $$M$$
    Derivation below
8. The $$\mathcal{O}h^{2j}$$ formula for approximating $$M$$
    
    

9. The Order the Approximations Generated
    It is conservatively assumed that the true result is accurate at least to within the agreement of the bottom two results in the diagonal in this case to within  
    $$|N3h − N4h|$$.  



Deriving n point Formulas with Extrapolation

1. Deriving Five point Formula
    
    


 4.9  TITLE



































FOURTH











 4.8  Multiple Integrals


Approximating Double Integral

1. What?
       The techniques discussed in the previous sections can be modified for use in the approximation of multiple integrals.
2. Why?

3. Comoposite Trapezoidal Rule for Double Integral
    $$ \ \  \iintR fxy \dA \  = \  \int{a}^{b} \ \big \ \int{c}^{d} \ \  fxy \ \ dy \ \ \big \  dx \ \ \ $$  

    $$ \approx \dfrac{b − ad − c}{16} \biggfac+fad + fbc + fbd+ $$
    $$\ \ \ \ \ \ \ \ 2\Bigf\big\dfrac{a + b}{2}  c\big + 
     f\big\dfrac{a + b}{2}  d\big + f\biga \dfrac{c + d}{2}\big + f\bigb \dfrac{c + d}{2}\big\Big + 4f\big\dfrac{a + b}{2} \dfrac{c + d}{2}\big\bigg$$

    
    
4. Comoposite Simpsons' Rule for Double Integral
     Rule  
    
    
     Error  
    
    
     Derivation  
    
    


Gaussian Quadrature for Double Integral Approximation

1. What?
       More efficient methods such as Gaussian
        quadrature Romberg integration or Adaptive quadrature can be incorporated in place of the Newton Cotes formulas.
2. Why?
       To reduce the number of functional evaluations.
3. Example
    
    


Non Rectangular Regions

1. What?
       Regions that don't have a rectangular shape.  
       Form  
       $$ \ \int{a}^{b} \bigg \int{cx}^{dx} fxy dy \bigg dx \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 4.42$$
       or  
       $$ \int{c}^{d} \bigg \int{ay}^{by} fxy dx \bigg dy \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 4.43$$
2. How?
     We use Simpson's Rule for Approximation.
     Step Size
         x $$\ \  h = \dfrac{b − a}{2} $$
         y $$\ \ kx = \dfrac{dx − cx}{2}$$
3. Simpsons' Rule for Non Rect Regions
    
    
4. Simpsons' Double Integral Algorithm
    
    
5. Gaussian Double Integral Algorithm
    
    


Triple Integral Approximation

1. What?
        Triple integrals.  
        Form  
       $$\ \int{a}^{b} \  \int{cx}^{dx} \   \int{\alphax}^{\betax} fxy dz \  dy \   dx $$
2. Gaussian Triple Integral Algorithm
    
    



 3.4  Hermite Interpolation



































FOURTH











 3.5  TITLE



































FOURTH











 3.1  Interpolation and the Lagrange Polynomial


Algebraic Polynomials

1. What?
       Set of Functions of the form
       $$Pnx = anx^n + a{n−1}x^{n−1} +···+ a1x + a0$$
2. Why?
       Polynomials uniformly approximate continuous functions. By this we mean that
        given any function defined and continuous on a closed and bounded interval there exists a polynomial that is as “close” to the given function as desired.
3. Weierstrass Approximation Theorem
    i.e. Polynomials uniformly approximate continuous functions.
4. Taylor Polynomials
    Taylor Polynomials are generally bad at approximating functions anywhere but at a certain point $$x0$$.
    To approximate an interval we do not use Taylors Polynomials.


Lagrange Interpolating Polynomials

1. The linear Lagrange interpolating polynomial

2. The nth Lagrange interpolating polynomial
3. The error term bound

    
    




 3.2   Data Approximation and Neville’s Method


Neville’s Method

1. What?
       A recursive method definition used to generate successively higher degree   
        polynomial approximations at a specific point.
2. Why?
       A practical difficulty with Lagrange interpolation is that the error term is    
        difficult to apply so the degree of the polynomial needed for the desired accuracy is generally not known
    until computations have been performed.
3. The lagrange Polynomial of the point $$x{mi}$$
4. Method to recursively generate Lagrange polynomial
     Method  
     Examples
        $$ 
        P{01} = \dfrac{1}{x1 − x0}x − x0P1 − x − x1P0 
        P{12} = \dfrac{1}{x2 − x1}x − x1P2 − x − x2P1 
        P{012} = \dfrac{1}{x2 − x0}x − x0P{12} − x − x2P{01}
        $$
     Generated according to the following Table

5. Notation and subscripts
     Proceeding down the table corresponds to using consecutive points $$xi$$ with larger i and proceeding to the right corresponds to increasing the degree of the interpolating polynomial.

     To avoid the multiple subscripts we let $$Q{ij}x$$ for $$0 ≤ j ≤ i$$ denote the interpolating polynomial of degree j on the j + 1 numbers $$x{i−j} x{i−j+1} ...  x{i−1} xi$$; that is
       $$Q{ij} = P{i−j}{i−j+1}...{i−1}i$$
6. Algorithm
7. Stopping Criterion
     Criterion  
    $$|Q{ii} − Q{i−1i−1}| < \epsilon$$
     If the inequality is true $$Q{ii}$$ is a reasonable approximation to $$fx$$.
     If the inequality is false a new interpolation point $$x{i+1}$$ is added.

0. OMG  
    $$ 
    P{j..i} = \dfrac{1}{xi − xj}x − xjP{j+1..i} − x − xiP{j..i 1}         
    Q{ij} = \dfrac{1}{x{i} − x{i j}}x − x{i j}Q{ij 1} − x − xiQ{i 1j 1} 
    $$


 3.3  Divided Differences


Divided Differences

1. What?
       A recursive method definition used to successively generate the approximating   
        polynomials.
2. Form of the Polynomial
     $$Pnx = a0 + a1x − x0 + a2x − x0x − x1 +···+ anx − x0···x − x{n−1}\ \ \  3.5$$
     Evaluated at $$x0$$ $$\ Pnx0 = a0 = fx0$$

     Evaluated at $$x1$$ $$\ Pnx1 = fx0 + a1x1 − x0 = fx1$$

    $$\implies \ \ \ \ \ \ \ \  a1 = \dfrac{fx1 − fx0}{x1 − x0}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 3.6$$.
3. The divided differences
     The zeroth divided difference of the function f with respect to $$xi$$
         Denoted $$fxi$$
         Defined as the value of $$f$$ at $$xi$$
        $$fxi = fxi \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 3.7$$.
    The remaining divided differences are defined recursively.

    
     The  divided difference of $$f$$ with respect to $$xi$$ and $$x{i+1}$$
         Denoted $$fxix{i+1}$$
         Defined as
        $$fxix{i+1} = \dfrac{fx{i+1} − fxi}{x{i+1} − xi} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 3.8$$.
     The  divided difference of $$f$$ with respect to $$xi$$ $$x{i+1}$$ and $$x{i+2}$$
         Denoted $$fxix{i+1}x{i+2}$$
         Defined as
        $$fxix{i+1}x{i+2} = \dfrac{fx{i+1}x{i+2} − fxix{i+1}}{x{i+2} − xi} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ 3.85$$.
     The Kth divided difference of $$f$$ with respect to $$xi$$ $$x{i+1}...x{i+k 1}x{i+k}$$
         Denoted $$fxix{i+1}...x{i+k 1}x{i+k}$$
         Defined as
        $$fxix{i+1}...x{i+k 1}x{i+k} = \dfrac{fx{i+1}x{i+2}...x{i+k} − fxix{i+1}...x{i+k 1}}{x{i+k} − xi} \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ 3.9$$
    The process ends with the nth divided difference
     The nth divided difference of $$f$$ with respect to $$xi$$ $$x{i+1}...x{i+k 1}x{i+k}$$
         Denoted $$fx0x1...xn$$
         Defined as
        $$fx0x1...xn = \dfrac{fx1x2...xn − fx0x1...x{n 1}}{xn − x0} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  3.95$$.


4. The Interpolating Polynomial
    $$Pnx = fx0 + fx0 x1x − x0 + a2x − x0x − x1+···+ anx − x0x − x1···x − x{n−1}$$

5. Newton’s Divided Difference
    $$Pnx = fx0+ \sum^n{k=1}fx0 x1 ...  xkx x0···x − x{k−1} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  3.10$$
    The value of $$fx0x1...xk$$ is independent of the order of the numbers $$x0 x1 ... xk$$

6. Generation of Divided Differences
    
    

7. Algorithm


Forward Differences

1. Forward Difference
     Check here/workfiles/school/128a/25 

2. The divided differences with del notation
    and in general 
3. Newton Forward Difference Formula

    

   

Backward Differences

1. Backward Difference

2. The divided differences
    and in general 

    Consequently the Interpolating Polynomial 

    If we extend the binomial coefficient notation to include all real values of s by letting 
    then 

3. Newton Backward Difference Formula

Centered Differences

1. What?

2. Why?
       The Newton forward and backward difference formulas are not appropriate for    
        approximating $$fx$$ when x lies near the center of the table because neither will permit the highest order difference to have $$x0$$ close to x.
3. Stirling's Formula
     If $$n = 2m + 1$$ is odd 
     If $$n = 2m$$ is even we use the same formula but delete the last line
    


4. Table of Entries
    




0. OMG  
    $$ 
    F{ij} = \dfrac{1}{x{i} − x{i j}}F{ij 1} − F{i 1j 1} 
    Q{ij} = \dfrac{1}{x{i} − x{i j}}Q{ij 1} − Q{i 1j 1} 
    $$


 2.1  The Bisection Method


Bisection Technique

1. What?
       The  technique based on the Intermediate Value Theorem is called the Bisection or
        Binary search method.

0. Why?
       Can be used to accelerate the convergence of a sequence that is linearly convergent    
        regardless of its origin or application.

2. Method

3. Algorithm
    
    

4. Drawbacks
    It is relatively slow to converge 
    that is N may become quite large before $$| p − pN|$$ is sufficiently
    small 
    and a good intermediate approximation might be inadvertently discarded


5. Stopping Criterions
    The best criterion is 2.2

6. Convergence
    It Always converges to a solution!

7. Rate of Convergence \ Error Bound

8. The problem of Percision
    We use 

9. The Signum Function
    We use 

10.MatLab Implementation


 2.5  Accelerating Convergence 



Aitken’s $$ \Delta^2 $$ Method 

1. What?

    Derivation can be found here!/mainfiles/128a/2/2.5/derivation.png

2. Why?
    Can be used to accelerate the convergence of a sequence that is linearly convergent regardless of its origin or application.

3. Del Forward Difference

4. $$\hat{p}n$$ Formula

5. Generating the Sequence Formula


Steffensen’s Method

1. What?
        By applying a modification of Aitken’s $$ \Delta^2 $$ method to a linearly convergent  
        sequence obtained from fixed point iteration we can accelerate the convergence to quadratic.

2. Zeros and their Multiplicity
!definition/mainfiles/128a/2/2.4/7.png

3. Difference from Aitken's method
     Aitken's method Constructs the terms in order
     Steffensen’s method constructs the same
     four terms $$p0 p1 p2$$ and $$\hat{p}0$$. However at this step we assume that $$\hat{p}0$$ is a better
    approximation to $$p$$ than is $$p2$$ and apply fixed point iteration to $$\hat{p}0$$ instead of $$p2$$

    Notice 
    Every  term of the Steffensen sequence is generated by Eq. 2.15;
    the others use fixed point iteration on the previous term.

4. Algorithm
    
    

5. Convergance of Steffensen’s Method

    Steffensen’s Method gives quadratic convergence without evaluating a derivative.

6. MatLab Implementation




 2.4  Error Analysis For Iterative Methods 



Order of Convergence 

1. Order of Convergence
!definition/mainfiles/128a/2/2.4/1.png

2. Important Two cases of order


3. An arbitrary technique that generates a convergent sequences does so only linearly
    Theorem 2.8 implies that higher order convergence for fixed point methods of the form
    $$ gp = p $$ can occur only when $$ g'p = 0 $$.

4. Conditions to ensure Quadratic Convergence


5. Theorems 2.8 and 2.9 imply
    i
    ii

5. Newtons' Method Convergence Rate
    
    

Multiple Roots 

1. Problem
    Newton’s method and the Secant method will generally give
    problems if $$ f' p = 0$$ when $$f  p = 0 $$.

2. Zeros and their Multiplicity
!definition/mainfiles/128a/2/2.4/7.png

3. Identifying Simple Zeros
    !Thm/mainfiles/128a/2/2.4/8.png
     Generalization of Theorem 2.11
    !Thm/mainfiles/128a/2/2.4/9.png

        The result in Theorem 2.12 implies that an interval about p exists where Newton’s
        method converges quadratically to p for any initial approximation $$ p0 = p$$ provided that p
        is a simple zero.

4. Why Simple Zeros
    Quadratic convergence might not occur if the zero is not simple
    Example
    Let $$f x = e^x − x − 1$$ 
    Notice that Newton’s method with $$p0 = 1$$ converges to the zero $$x=0$$ but not quadratically


5. Handling the problem of multiple roots
    We Modify Newton's Method 
    We define $$gx$$ as 

    Derivation can be found here!/mainfiles/128a/2/2.4/derivation.png

     Properties
         If g has the required continuity conditions functional iteration applied to $$g$$ will be
        quadratically convergent regardless of the multiplicity of the zero of $$f$$ .
         Theoretically the only drawback to this method is the additional calculation of $$f
        x$$ and the more laborious procedure of calculating the iterates.
         In practice multiple roots can cause serious round off problems because the denominator of 2.13 consists of the difference of two numbers that are both close to 0.
         In the case of a simple zero the original Newton’s method requires substantially less computation.



 2.3  Newton’s Method and Its Extensions


Newton’s Method

1. What?
     Newton’s or the Newton Raphson method is one of the most powerful and well known
    numerical methods for solving a root finding problem
        
        


2. Derivation
    Derivation can be found here/mainfiles/128a/2/2.3/derivation.png

3. Algorithm
    
    

4. Stopping Criterions

5. MatLab Implementation


Convergence using Newton’s Method

1. Convergence Theorem
    The crucial assumption is that the
    term involving $$ p − p0^2$$ is by comparison with $$| p − p0|$$ so small that it can be deleted

    Theorem 2.6 states that 
    1 Under reasonable assumptions Newton’s method converges
      provided a sufficiently accurate initial approximation is chosen. 
    2 It also implies that the constant k that bounds the derivative of g and consequently indicates the speed of convergence
      of the method decreases to 0 as the procedure continues.

The Secant Method

1. What?
    In Newton's Method 
    We approximate $$f' pn−1$$ as
    To produce 

2. Why?
    Newton's Method Weakness 
    the need to know the value of the derivative of f at each approximation.
      > Frequently $$f'x$$ is harder and needs more arithmetic operations to calculate than $$fx$$.

    Note only one function evaluation is needed per step for the Secant method after $$p2$$ has been 
    determined. In contrast each step of Newton’s method requires an evaluation of both the function and its derivative.
3. Algorithm
    
    

4. Convergence Speed
    Generally 
    The convergence of the Secant method is much faster than functional iteration but slightly slower than Newton’s method.

The Method of False Position

1. What?
    The method of False Position also called Regula Falsi generates approximations
    in the same manner as the Secant method but it includes a test to ensure that the root is
    always bracketed between successive iterations.

2. Why?
    Root bracketing is not guaranteed for either Newton’s method or the Secant method.

3. Method

4. Algorithm
    
    




 2.2  Fixed Point Iteration


Fixed Point Problems

1. Fixed Point
   A fixed point for a function is a number at which the value of the function does not change
    when the function is applied.

2. Root finding problems and Fixed point problems
    Root Finding and Fixed point problems are equivalent in the following sense 


3. Why?
    Although the problems we wish to solve are in the root finding form the fixed point
    form is easier to analyze and certain fixed point choices lead to very powerful root finding
    techniques.

4. Existence and Uniqueness of a Fixed Point.


Fixed Point Iteration

1. Approximating Fixed Points

2. Algorithm

3. Convergence
     Fixed Point Theorem 
     Error bound in using $$pn$$ for $$p$$ 

        Notice 
        The rate of convergence depends on the factor $$k^n$$. The smaller the
        value of $$k$$ the faster the convergence which may be very slow if $$k$$ is close to 1.
4. Using Fixed Points
    Question. How can we find a fixed point problem that produces a sequence that reliably
    and rapidly converges to a solution to a given root finding problem?
    
    Answer. Manipulate the root finding problem into a fixed point problem that satisfies the
    conditions of Fixed Point Theorem 2.4 and has a derivative that is as small as possible
    near the fixed point.

5. Newton's Method as a Fixed Point Problem

6. Convergence Example
    
    

7. MatLab Implementation



 2.6  Zeros of Polynomials and Müller’s Method



Algebraic Polynomials

1. Fundamental Theorem of Algebra
    !definition/mainfiles/128a/2/2.6/1.png

2. Existance of Roots
    !definition/mainfiles/128a/2/2.6/2.png

3. Polynomial Equivalence
    !definition/mainfiles/128a/2/2.6/3.png
    This result implies that to show that two polynomials of degree less than or equal to $$n$$ are the same we only need to show that they agree at $$n + 1$$ values.


Horner’s Method

1. What?
    Horner’s method incorporates the Section 1.2/workfiles/school/128a/2/12 nesting technique and
    as a consequence requires only n multiplications and n additions to evaluate an arbitrary
    nth degree polynomial.

2. Why?
    To use Newton’s method to locate approximate zeros of a polynomial Px we need to
    evaluate $$Px$$ and $$P'x$$ at specified values Which could be really tedious.

3. Horner's Method

4. Algorithm

5. Horner's Derivatives

6. Deflation

5. MatLab Implementation


Complex Zeros Müller’s Method

1. What?
     A synthetic division involving quadratic polynomials can be devised to approximately
    factor the polynomial so that one term will be a quadratic polynomial whose complex roots
    are approximations to the roots of the original polynomial
     Müller’s method uses three initial approximations
    $$p0 p1$$ and $$p2$$ and determines the next approximation $$p3$$ by considering the intersection
    of the x axis with the parabola through $$ p0\ f  p0 \ \  p1\ f  p1$$ and $$\ \  p2\ f  p2$$

    Derivation can be found here!/mainfiles/128a/2/2.6/derivation.jpg

2. Why?
    Newton's Method/Secant/False Postion Weakness
    The possibility that the polynomial having complex roots even when all the coefficients are real numbers.
      > If the initial approximation is a real number all subsequent approximations
        will also be real numbers.

3. Complex Roots
    !Thm1/mainfiles/128a/2/2.6/7.png


4. Algorithm
    
    
    
5. Calculations and Evaluations
    HERE!/mainfiles/128a/2/2.6/Evaluations.png
    Müller’s method can approximate the roots of polynomials
    with a variety of starting values.


 5.4  Runge Kutta Methods


Runge Kutta methods

1. What?

2. Why?
       The Taylor methods outlined in the previous section have the desirable property of highorder local truncation error but the disadvantage of requiring the computation and evaluation of the derivatives of $$ft y$$. This is a complicated and time consuming procedure for most problems so the Taylor methods are seldom used in practice.

    The Runge Kutta methods have the high order local truncation error of the Taylor methods but eliminate the need to compute and evaluate the derivatives of $$ft y$$

3. Taylor’s Theorem 2 variables


Runge Kutta Methods of Order Two

1. What?

2. Midpoint Method
     2nd order  
     Higher order  
        The fact that 5.21 has four parameters however gives a flexibility in their choice so a number of $$Oh^2$$ methods can be derived.  
    
    


3. Modified Euler Method
    One of the most important is the Modified
    Euler method which corresponds to choosing $$a1 = a2 = \dfrac{1}{2}$$
    and $$\alpha2 = \delta2 = h$$. It has the following difference equation form  

4. Error Order
    The order of error for this new method is the same as that of the Taylor method of order two.


Higher Order Runge Kutta Methods

1. What?
    The term $$T^{3}t y$$ can be approximated with error $$Oh^3$$ by an expression of the form
    $$ft + \alpha1 y + \delta1 ft + \alpha2 y + \delta2 ft y$$
    involving four parameters the algebra involved in the determination of $$\alpha1 \delta1 \alpha2$$ and $$\delta2$$ is quite involved.
2. Heun’s method

     Local Truncation Error $$ \mathcal{O}h^3$$.

3. Runge Kutta Order Four
     Local Truncation Error $$ \mathcal{O}h^4$$.
        Provided the solution $$yt$$ has five continuous derivatives.

    Algorithm
        
        


Computational Comparisons

1. What is Compuatation Heavy?
       The main computational effort in applying the Runge Kutta methods is the evaluation of $$f$$.
2. Function Evaluations
      Order | 2 function evaluations per step | Error of order $$\mathcal{O}h^2$$.
     Fourth Order | 4 function evaluations per step | Error of order $$\mathcal{O}h^4$$.
3. Relationship between number of evaluations and order of truncation error
    This indicates why the methods of order less than five with
smaller step size are used in preference to the higher order methods using a larger step size.   

    This is because..

4. Comparing lower order Runge Kutta Methods
    The Runge Kutta method of order four requires four evaluations per step whereas Euler’s
    method requires only one evaluation. Hence if the Runge Kutta method of order four is
    to be superior it should give more accurate answers than Euler’s method with one fourth
    the step size. Similarly if the Runge Kutta method of order four is to be superior to the  order Runge Kutta methods which require two evaluations per step it should
    give more accuracy with step size h than a  order method with step size h/2.

    This indeed holds true.


 5.11  TITLE



































FOURTH











 5.1  The Elementary Theory of Initial Value Problems


Theory of IVP

1. Lipschitz Condition

2. Convex Set

3. Lipschitz Condition on Convex Sets

4. Existence and Uniqueness Theorem for  Order ODEs


Well Posed Problems

1. What?
       It is the property that small changes or perturbations in the statement of the problem introduce correspondingly small changes in the solution.

2. Why?
       Due to round off errors and measurment error we need to make sure the result of such small errors is minuscule.

3. A Well Posed Problem
    The problem specified by 5.3 is called a perturbed problem associated with the
    original problem 5.2.  

    It assumes the possibility of an error being introduced in the statement of the differential equation as well as an error δ0 being present in the initial condition.

4. Conditions for being Well Posed



 5.5  Error Control and the Runge Kutta Fehlberg Method


Adaptive Methods

1. What?
    Techniques used to control the error of a difference equation method in an efficient manner by the appropriate choice of mesh points.  
    By using methods of differing order we can predict the local truncation error
    and using this prediction choose a step size that will keep it and the global error in check.

2. Why?
       Adaptive Methods incorporate in the step size procedure an estimate of
    the truncation error that does not require the approximation of the higher derivatives of the function.  
       They adapt the number and position of the nodes used in the approximation to ensure that the truncation error is kept within a specified bound.

3. Derivation
    
    
    

Runge Kutta Fehlberg Method

1. What?

2. Why?
    An advantage to this method is that only six evaluations of f are required per step.  

    As opposed to requiring at least four evaluations of $$f$$ for the fourth order method and an additional six for the fifth order method for a total of at least ten function evaluations.  

    $$\implies$$ This Method has at least a $$40\%$$ decrease in the number of function evaluations over the use of a pair of arbitrary fourth and fifth order methods.

3. Error Bound Order
    $$\mathcal{O}h^5$$

4. The choice of "q"
     The value of q determined at the ith step is used for two purposes
        1. When $$q < 1$$ to reject the initial choice of $$h$$ at the ith step and repeat the calculations using $$qh$$ and
        2. When $$q \geq 1$$ to accept the computed value at the ith step using the step size $$h$$ but change the step size to $$qh$$ for the i + 1st step.

    Because of the penalty in terms of function evaluations that must be paid if the steps are repeated q tends to be chosen conservatively.

     The choice of q for the "Runge Kutta Fehlberg"

5. Algorithm
    
    
     Notice
        1. Step 9 is added to eliminate large modifications in step size.
        2. This is done to avoid spending too much time with small step sizes in regions with irregularities in the derivatives of y and to avoid large step sizes which can result in skipping sensitive regions between the steps.
        3. The step size increase procedure could be omitted completely from the algorithm.
        4. The step size decrease procedure used only when needed to bring the error under control.



 5.10  TITLE



































FOURTH











 5.2  Euler’s Method


Eulers Method

1. What?
        The object of Euler’s method is to obtain approximations to the well posed initial value problem  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \dfrac{dy}{dt} = fty \ \ \ \ \ \  a \leq b \ \ \ \ \ \  ya = \alpha \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \  5.6$$
        A continuous approximation to the solution $$yt$$ will not be obtained; instead approximations to $$y$$ will be generated at various values called mesh points in the interval $$a b$$.  
        Once the approximate solution is obtained at the points the approximate solution at other points in the interval can be found by interpolation.


2. Mesh Points
       $$ti = a + ih \ \ \  \text{for each } i = 0 1 2 ...  N $$  

    The mesh points are equally distributed throughout the interval $$a b$$.

3. Step Size
       $$
    h = \dfrac{b − a}{N} = t{i+1} − t
    $$

4. Euler's Method

    Equation $$5.8$$ is called the difference equation associated with Euler’s method.

5. Derivation
    
    

6. Algorithm
    
    

7. Geometric Interpetation
    To interpret Euler’s method geometrically note that when $$wi$$ is a close approximation to $$yti$$ the assumption that the problem is well posed implies that
    $$
    fti wi \approx yti = fti yti
    $$.  
    i.e. each step corresponds to correcting the path by the approximation to the derivative slope.


Error Bounds for Euler’s Method

1. Comparison Lemmas
    1. Lemma 1  
    
    

    2. Lemma 2  
    
    

2. Error Bound
    
    

3. Properties of the Error Bound Theorem
    1. The Weakness of Theorem 5.9 lies in the requirement that a bound be known for the  derivative of the solution.  
    2. However if $$\dfrac{\partial f}{\partial t}$$ and $$\dfrac{\partial f}{\partial y}$$ both exist the chain rule for partial differentiation implies that  
    So it is at times possible to obtain an error bound for $$y''t$$ without explicitly knowing $$yt$$.  
    3. The Principal Importance of the error bound formula given in Theorem 5.9 is that the bound depends linearly on the step size h.  
    4. Consequently diminishing the step size should give correspondingly greater accuracy to the approximations.


Finite Digit Approximations

1. Euler Method Finite Digit Approximations
    Where $$\deltai$$ denotes the round off error associated with $$ui$$.

2. Error Bound for fin dig approx. to $$yi$$ given by Euler’s method

3. Properties of the Error Bound on Finit digit Approximations
    1. The error bound 5.13 is no longer linear in h.
    2. In fact since  
    $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \lim{h\to 0} \ \dfrac{hM}{2} + \dfrac{\delta}{h} = \infty$$  
    the error would be expected to become large for sufficiently small values of h.
    3. Calculus can be used to determine a lower bound for the step size h  
    4. The Minimal value of $$Eh$$ occurs when  
       $$
    h = \sqrt{\dfrac{2\delta}{M}}  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ 5.14
    $$ 
    5. Decreasing h beyond this value tends to increase the total error in the approximation; however normally $$\delta$$ is so small that the lower bound for h doesn't affect Euler's Method.



 5.6  Multistep Methods


Multi Step Methods

1. What?
       Methods that use the information produced at the steps $$t0 t1 ..  t{i 1} ti$$ to approximate $$t{i+1}$$.

2. Why?
       Since the error $$\mid wj − ytj \mid$$ increases with each step we know that the previously computed values of $$t$$ are actually more accurate than those coming up next.  
       Thus it makes sense to use these more accurate values to produce the next result.

3. m step Multistep Method

4. Types of Methods
    1. Open / Explicit Methods If $$bm = 0$$  because Eq. $$5.24$$ then gives $$w{i+1}$$ explicitly in terms of previously determined values. 

    2. Closed / Implicit Methods If $$bm \neq 0$$ because $$w{i+1}$$ occurs on both sides of Eq. $$5.24$$ so $$w{i+1}$$ is specified only implicitly. 

5. Open vs Closed / Explicit vs Implicit
    In general the coefficients of the terms involving $$f$$ in the local truncation error are smaller for the implicit methods than for the explicit methods.

6. Starting Values
       The starting values must be specified generally by assuming $$w0 = \alpha$$ and generating the remaining values by either a Runge Kutta or Taylor method.

7. Deriving Multi Step Methods

    
    

8. Example Deriving three step Adams Bashforth
    
    


Adams Bashforth Explicit Methods

1. Adams Bashforth Two Step Explicit Method

2. Adams Bashforth Three Step Explicit Method

3. Adams Bashforth Four Step Explicit Method

4. Adams Bashforth Five Step Explicit Method


Adams Moulton Implicit Methods

1. What?
       Implicit methods are derived by using $$t{i+1} ft{i+1} yt{i+1}$$ as an additional interpolation node in the approximation of the integral  
      $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ \ \ \ \ \ \ $$  $$  \int{t{i+1}}^{ti} ft yt dt$$.


2. Adams Moulton Two Step Implicit Method

3. Adams Moulton Three Step Implicit Method

4. Adams Moulton Four Step Implicit Method

5. DrawBacks
    To apply an implicit method we must solve the implicit equation for $$w{i+1}$$.  
    This is not always possible and even when it can be done the solution for $$w{i+1}$$
    may not be unique.


Predictor Corrector Methods

1. What?
       The combination of an explicit method to predict and an implicit to improve the
    prediction.

2. Why?
       Even though the implicit methods are better than the explicit methods they have the inherent weakness of  having to convert the method algebraically to an explicit representation for $$w{i+1}$$.  
       This procedure is not always possible  
    
    


3. Derivation
    
    

4. Algorithm
    
    

5. Milne’s method
    Derived by integrating an interpolating polynomial over $$t{i−3} t{i+1}$$.  

6. Implicit Simpson's Method
    Derived by integrating an interpolating polynomial over $$t{i−1} t{i+1}$$.  

7. Another Predictor Corrector Method
     Milne’s method is occasionally used as a predictor for the implicit Simpson’s method.

     Properties
        1. The local truncation error involved with a predictor corrector method of the Milne Simpson type is generally smaller than that of the Adams Bashforth Moulton method.
        2. However the technique has limited use because of round off error problems which do not occur with the Adams procedure.



 5.7  Variable Step Size Multistep Methods


Variable Step Multistep Methods

1. What?
       A Predictor Corrector Method that uses variable step sizes for error control.

2. Why?
       Predictor corrector techniques always generate two approximations at each step so they are natural candidates for error control adaptation.

3. Derivation
    
    

4. Choosing '$$q$$'
     $$q$$ is generally chosen conservatively  

5. Properties
    1. A change in step size for a multistep method is more costly in terms of function evaluations than for a one step method because new equally spaced starting values must be computed.
    2. Consequently we ignore the step size change whenever the local truncation error is between $$\dfrac{\epsilon}{10}$$ and $$\epsilon$$ that is when
    3. $$q$$ is given an upper bound to ensure that a single unusually accurate approximation does not result in too large a step size.

6. Algorithm
    
    


 5.3  Higher Order Taylor Methods


Local Truncation Error

1. What?
       The local truncation error at a specified step measures the amount by which the exact
    solution to the differential equation fails to satisfy the difference equation being used for the approximation at that step.

2. Why?
    We need a means for comparing the efficiency of various approximation methods.  

    The local truncation will serve quite well to determine not only the local error of a method but the actual approximation error.

3. Definition


4. Why local?
    This error is a local error because it measures the accuracy of the method at a specific step assuming that the method was exact at the previous step.

5. What does it depend on?
    As such it depends on the differential equation the step size and the particular step in the approximation.

6. Euler Method Truncation Error
    
    

7. How to select difference equations methods?
    One way to select difference equation methods for solving ordinary differential equations
    is in such a manner that their local truncation errors are Ohp for as large a value
    of p as possible while keeping the number and complexity of calculations of the methods
    within a reasonable bound.


Talors Method

1. Taylors Method of order n
    
    

2. Approximation Theorem
    
    

3. Using Hermite Polynomials to evaluate a differential equations at a midpoint
    
    



 5.9  Higher Order Equations and Systems of Differential Equations


Systems of  Order Differential Equations

1. An mth order System

2. Lipschitz condition several variables


3. Existence and Uniqueness


Methods to Solve Systems of Equations

1. What?
       Methods to solve systems of  order differential equations are generalizations of the methods for a single  order equation presented earlier in this chapter.


2. Generalizing Runge Kutta Order Four
    
    

3. Algorithm
    
    

4. Example Kirchoffs Law
    
    


Higher Order Differential Equations

1. What?
    New techniques are not required for solving initial value problems whose equations  have orders higher than one.  
    By relabeling the variables we can reduce a higher order differential equation into a system of  order differential equations and then apply one of the methods we have already discussed.

2. Converting an mth order IVP to 1st order System of Equations

3. Example
    
    



 PCA  Principal Component Analysis









PCA

1. What?
    It is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.  
    

2. Goal?
    Given points $$\mathbf{x} i \in \mathbf{R}^d$$ find k directions that capture most of the variation.  
    

3. Why?
    1. Find a small basis for representing variations in complex things.
        e.g. faces genes.  

    2. Reducing the number of dimensions makes some computations cheaper.  
    3. Remove irrelevant dimensions to reduce over fitting in learning algorithms.
        Like "subset selection" but the features are not axis aligned.  
        They are linear combinations of input features.  

    4. Represent the data with fewer parameters dimensions  
    

4. Finding Principal Components
     Let '$$X$$' be an $$n \times d$$ design matrix centered with mean $$\hat{x} = 0$$.
     Let '$$w$$' be a unit vector.
     The Orthogonal Projection of the point '$$x$$' onto '$$w$$' is $$\tilde{x} = x.ww$$.
        Or $$\tilde{x} = \dfrac{x.w}{\|w\|2^2}w$$ if $$w$$ is not a unit vector.
     Let '$$X^TX$$' be the sample covariance matrix  
        $$0 \leq \lambda1 \leq \lambda2 \leq \cdots \leq \lambdad$$ be its eigenvalues  and let $$v1 v2 \cdots vd$$ be the corresponding Orthogonal Unit Eigen vectors.
     Given Orthonormal directions vectors $$v1 v2 \ldots vk$$ we can write   

        $$\tilde{x} = \sum{i=1}^k x.vivi.$$  

    The Principal Components are precisely the eigenvectors of the data's covariance matrix. Read More#pcvspd  

    

5. Total Variance and Error Measurement
     The Total Variance of the data can be expressed as the sum of all the eigenvalues
    $$
        \mathbf{Tr} \Sigma = \mathbf{Tr} U \Lambda U^T = \mathbf{Tr} U^T U \Lambda = \mathbf{Tr} \Lambda = \lambda1 + \ldots + \lambdan. 
        $$
     The Total Variance of the Projected data is
    $$
         \mathbf{Tr} P \Sigma P^T  = \lambda1 + \lambda2 + \cdots + \lambdak. 
        $$
     The Error in the Projection could be measured with respect to variance.
         We define the ratio of variance "explained" by the projected data equivalently the ratio of information "retained" as  
    $$
        \dfrac{\lambda1 + \ldots + \lambdak}{\lambda1 + \ldots + \lambdan}. 
        $$  
    If the ratio is high we can say that much of the variation in the data can be observed on the projected plane.  

    

8. Mathematical Formulation
    PCA is mathematically defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the  coordinate called the  principal component the  greatest variance on the  coordinate and so on.  

    Consider a data matrix $$X$$ with column wise zero empirical mean the sample mean of each column has been shifted to zero where each of the $$n$$ rows represents a different repetition of the experiment and each of the $$p$$ columns gives a particular kind of feature say the results from a particular sensor.  

    Mathematically the transformation is defined by a set of $$p$$ dimensional vectors of weights or coefficients $${\displaystyle \mathbf {v} {k}=v{1}\dots v{p} {k}}$$ that map each row vector $${\displaystyle \mathbf {x} {i}}$$ of $$X$$ to a new vector of principal component scores $${\displaystyle \mathbf {t} {i}=t{1}\dots t{l} {i}}$$ given by  
    $${\displaystyle {t{k}}{i}=\mathbf {x} {i}\cdot \mathbf {v} {k}\qquad \mathrm {for} \qquad i=1\dots n\qquad k=1\dots l}$$  
    in such a way that the individual variables $${\displaystyle t{1}\dots t{l}}$$  of $$t$$ considered over the data set successively inherit the maximum possible variance from $$X$$ with each coefficient vector $$v$$ constrained to be a unit vector where $$l$$ is usually selected to be less than $${\displaystyle p}$$ to reduce dimensionality.  
    
    The Procedure and what it does  
    
     Finds a lower dimensional subspace PCs that Minimizes the RSS of projection errors  
     Produces a vector 1st PC with the highest possible variance each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.  
     Results in an uncorrelated orthogonal basis set.  
     PCA constructs new axes that point to the directions of maximal variance in the original variable space  
    


9. Intuition
    PCA can be thought of as fitting a p dimensional ellipsoid to the data where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small then the variance along that axis is also small and by omitting that axis and its corresponding principal component from our representation of the dataset we lose only a commensurately small amount of information.  

     Its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data.  
    

11.PCA Algorithm
     Data Preprocessing  
         Training set $$x^{1} x^{2} \ldots x^{m}$$ 
         Preprocessing feature scaling + mean normalization  
             mean normalization  
                $$\mu{j}=\frac{1}{m} \sum{i=1}^{m} x{j}^{i}$$  
                Replace each $$x{j}^{i}$$ with $$xj^{i}  \muj$$  
             feature scaling  
                If different features on different scale features to have comparable range  
                $$sj = S.DXj$$ the standard deviation of feature $$j$$  
                Replace each $$x{j}^{i}$$ with $$\dfrac{xj^{i}  \muj}{sj}$$    
     Computing the Principal Components  
         Compute the SVD of the matrix $$X = U S V^T$$  
         Compute the Principal Components  
            $$T = US = XV$$  
            Note The $$j$$ th principal component is $$Xvj$$  
         Choose the top $$k$$ components singular values in $$S = Sk$$  
         Compute the Truncated Principal Components  
            $$Tk = USk$$  
     Computing the Low rank Approximation Matrix $$Xk$$  
         Compute the reconstruction matrix  
            $$Xk = TkV^T = USkV^T$$  
            
    
    Results and Definitions  
    
     Columns of $$V$$ are principal directions/axes  
     Columns of $$US$$ are principal components "scores"


    NOTE the analysis above is valid only for 1 $$X$$ w/ samples in rows and variables in columns  2 $$X$$ is centered mean=0  
    

10.Properties and Limitations
    Limitations  
     PCA is highly sensitive to the relative scaling of the data; no consensus on best scaling.  
    
            

12.Optimality
    Optimal for Finding a lower dimensional subspace PCs that Minimizes the RSS of projection errors  
    

6. How does PCA relate to CCA
    CCA defines coordinate systems that optimally describe the cross covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset.  
    

7. How does PCA relate to ICA
    Independent component analysis ICA is directed to similar problems as principal component analysis but finds additively separable components rather than successive approximations.  
    


13.What's the difference between PCA estimate and OLS estimate
    

Notes  

 Variance is the measure of spread along only one axis   
 SVDX vs Spectral Decomposition$$\Sigma = X^TX$$  
    SVD is better $$\iff$$ more numerically stable $$iff$$ faster  
 When are the PCs independent?  
    



Derivation 1. Fitting Gaussians to Data with MLE





1. What?
    1. Fit a Gaussian to data with MLE
    2. Choose k Gaussian axes of greatest variance.
    Notice MLE estimates a covariance matrix; $$\hat{\Sigma} = \dfrac{1}{n}X^TX$$.

2. Algorithm
    1. Center $$X$$
    2. Normalize $$X$$.
        Optional. Should only be done if the units of measurement of the features differ.
    3. Compute the unit Eigen values and Eigen vectors of $$X^TX$$
    4. Choose '$$k$$' based on the Eigenvalue sizes
        Optional. Top to bottom.
    5. For the best k dim subspace pick Eigenvectors $$v{d k+1} \cdots vd$$.
    6. Compute the coordinates '$$x.vi$$' of the trainning/test data in PC Space.


Derivation 2. Maximizing Variance

1. What?
    1. Find a direction '$$w$$' that maximizes the variance of the projected data.
    2. Maximize the variance

2. Derivation
       $$\max{w  \|w\|2=1} \ Var\left\{\tilde{x1} \tilde{x2} \cdots \tilde{xn} \right\}$$
       $$
        \begin{align}
        & \ = \max{w  \|w\|2=1}  \dfrac{1}{n} \sum{i=1}^{n}xi.\dfrac{w}{\|w\|}^2 
        & \ = \max{w  \|w\|2=1}  \dfrac{1}{n} \dfrac{\|xw\|^2}{\|w\|^2}  
        & \ = \max{w  \|w\|2=1}  \dfrac{1}{n} \dfrac{w^TX^TXw}{w^Tw} 
        \end{align}
        $$
       where $$\dfrac{1}{n}\dfrac{w^TX^TXw}{w^Tw}$$ is the Rayleigh Quotient.
       For any Eigen vector $$vi$$ the Rayleigh Quotient is $$ = \lambdai$$.
       $$\implies$$ the vector $$vd$$ with the largest $$\lambdad$$ achieves the maximum variance $$\dfrac{\lambdad}{n}.$$
       Thus the maximum of the Rayleigh Quotient is achieved at the Eigenvector that has the highest corresponding Eigenvalue.
       We find subsequent vectors by finding the next biggest $$\lambdai$$ and choosing its corresponding Eigenvector.



          
    

3. Another Derivation from Statistics
     we note that The sample variance along direction $$u$$ can be expressed as a quadratic form in $$u$$  
    $$ \sigma^2u = \dfrac{1}{n} \sum{k=1}^n u^Txk \hat{x}^2 = u^T \Sigma u$$  

    The data matrix has points $$xi$$; its component along a proposed axis $$u$$ is $$x · u$$.  
    The variance of this is $$Ex · u − Ex · u^2$$  
    and the optimization problem is
    $$
        \begin{align}
        \max{x  \|x\|2=1} \ Ex · u − Ex · u^2 & 
        & \ = \max{u  \|u\|2=1} \  Eu \cdot x − Ex^2 
        & \ = \max{u  \|u\|2=1} \  uEx − Ex \cdot x − Ex^Tu 
        & \ = \max{u  \|u\|2=1} \  u^T \Sigma u
        \end{align}
        $$
    where the matrix $${\displaystyle \Sigma \= \dfrac{1}{n} \sum{j=1}^n xj \hat{x}xj \hat{x}^T}.$$  
    Since $$\Sigma$$ is symmetric the $$u$$ that gives the maximum value to $$u^T\Sigma u$$ is the eigenvector of $$\Sigma$$ with the largest eigenvalue.  
    The  and subsequent principal component axes are the other eigenvectors sorted by eigenvalue.  

    Proof of variance along a direction  
    $$\boldsymbol{u}^{\top} \operatorname{cov}\boldsymbol{X} \boldsymbol{u}=\boldsymbol{u}^{\top} \mathbb{E}\left\boldsymbol{X} \mathbb{E}\boldsymbol{X}\boldsymbol{X} \mathbb{E}\boldsymbol{X}^{\top}\right \boldsymbol{u}=\mathbb{E}\left\langle\boldsymbol{u} \boldsymbol{X} \mathbb{E}\boldsymbol{X}\rangle^{2}\right \geq 0  \implies  
    \operatorname{var}\langle\boldsymbol{u} \boldsymbol{X}\rangle=\mathbb{E}\left\langle\boldsymbol{u} \boldsymbol{X} \mathbb{E} \boldsymbol{X}\rangle^{2}\right=\boldsymbol{u}^{\top} \operatorname{cov}\boldsymbol{X} \boldsymbol{u}$$  




Derivation 3. Minimize Projection Error

1. What?
    1. Find direction '$$w$$' that minimizes the Projection Error.

2. Derivation
    $$
        \begin{align}
        \min{\tilde{x}  \|\tilde{x}\|2 = 1} \; \sum{i=1}^n \|xi  \tilde{xi}\|^2 & 
        & \ = \min{w  \|w\|2 = 1} \; \sum{i=1}^n \|xi  \dfrac{xi \cdot w}{\|w\|2^2}w\|^2 
        & \ = \min{w  \|w\|2 = 1} \; \sum{i=1}^n \left\|xi\|^2  xi \cdot \dfrac{w}{\|w\|2}^2\right 
        & \ = \min{w  \|w\|2 = 1} \; c  n\sum{i=1}^nxi \cdot \dfrac{w}{\|w\|2}^2 
        & \ = \min{w  \|w\|2 = 1} \; c  nVar\left\{\tilde{x1} \tilde{x2} \cdots \tilde{xn} \right\} 
        & \ = \max{w  \|w\|2 = 1} \; Var\left\{\tilde{x1} \tilde{x2} \cdots \tilde{xn} \right\}
        \end{align}
        $$  
    Thus minimizing projection error is equivalent to maximizing variance.  



 The Naive Bayes Classifier








Introduction and the Naive Bayes Classifier

0. Naive Bayes
    Naive Bayes is a simple technique for constructing classifiers.  

1. Naive Bayes Classifiers
    

    The Assumptions  
    
    1. Naive Independence the feature probabilities are indpendenet given a class $$c$$.   
    2. Bag of Words we assume that the position of the words does not matter.  
    


    Notes  
    
     Not a Bayesian Method the name only references the use of Bayes' theorem in the classifier's decision rule  
     The Naive Bayes Classifier is a Bayes Classifier i.e. minimizes the prob of misclassification  
    

2. The Probabilistic Model Naive Bayes Probability/Statistical Model
    Abstractly naive Bayes is a conditional probability model  
    given a problem instance to be classified represented by a vector $${\displaystyle \ \mathbf{x} =x{1}\dots x{n}}$$ representing some $$n$$ features independent variables it assigns to this instance probabilities  
    $${\displaystyle pC{k}\mid x{1}\dots x{n} = pC{k}\mid \mathbf {x}}$$  
    for each of the $$k$$ possible classes $$Ck$$.  

    Using Bayes' Theorem we decompose the conditional probability as  
    $${\displaystyle pC{k}\mid \mathbf {x} ={\frac {pC{k}\ p\mathbf {x} \mid C{k}}{p\mathbf {x} }}\}$$  

    Notice that the numerator is equivalent to the joint probability distribution  
    $$p\leftC{k}\right p\left\mathbf{x} | C{k}\right = p\leftC{k} x{1} \ldots x{n}\right$$  

    Using the Chain Rule for repeated application of the conditional probability the joint probability model can be rewritten as  
    $$pC{k}x{1}\dots x{n}\ = px{1}\mid x{2}\dots x{n}C{k}px{2}\mid x{3}\dots x{n}C{k}\dots px{n 1}\mid x{n}C{k}px{n}\mid C{k}pC{k}$$  

    Using the Naive Conditional Independence assumptions  
    $$p\leftx{i} | x{i+1} \ldots x{n} C{k}\right=p\leftx{i} | C{k}\right$$  
    Thus we can write the joint model as  
    $${\displaystyle {\begin{aligned}pC{k}\mid x{1}\dots x{n}&\varpropto pC{k}x{1}\dots x{n}&=pC{k}\ px{1}\mid C{k}\ px{2}\mid C{k}\ px{3}\mid C{k}\ \cdots &=pC{k}\prod {i=1}^{n}px{i}\mid C{k}\\end{aligned}}}$$  

    Finally the conditional distribution over the class variable $$C$$ is  
    $${\displaystyle pC{k}\mid x{1}\dots x{n}={\frac {1}{Z}}pC{k}\prod {i=1}^{n}px{i}\mid C{k}}$$   
    where $${\displaystyle Z=p\mathbf {x} =\sum {k}pC{k}\ p\mathbf {x} \mid C{k}}$$ is a constant scaling factor a dependent only on the known feature variables $$xi$$s.  
    

3. The Classifier
    We can construct the classifier from the probabilistic model above.  
    The Naive Bayes Classifier combines this model with a decision rule.  

    The Decision Rule  


    The Classifier is the function that assigns a class label $$\hat{y} = Ck$$ for some $$k$$ as follows  
    $${\displaystyle {\hat {y}}={\underset {k\in \{1\dots K\}}{\operatorname {argmax} }}\ pC{k}\displaystyle \prod {i=1}^{n}px{i}\mid C{k}.}$$  

    It basically maximizes the probability of the class given an input $$\boldsymbol{x}$$.  
    

4. Naive Bayes Estimate VS MAP Estimate
    MAP Estimate  
    $${\displaystyle {\hat {y}{\text{MAP}}}={\underset {k\in \{1\dots K\}}{\operatorname {argmax} }}\ pC{k}\ p\mathbf {x} \mid C{k}}$$  
    Naive Bayes Estimate  
    $${\displaystyle {\hat {y}{\text{NB}}}={\underset {k\in \{1\dots K\}}{\operatorname {argmax} }}\ pC{k}\displaystyle \prod {i=1}^{n}px{i}\mid C{k}}$$  

5. Estimating the Parameters of the Classifier
    Parameters to be Estimated  
     The prior probability of each class 
        $$pC{k}$$  
     The conditional probability of each feature word given a class  
        $$px{i}\mid C{k} \\ \forall i \in {1 .. n}$$  

    We generally use Maximum Likelihood Estimates for the parameters.  

    The MLE Estimates for the Parameters  
     $$\hat{P}Ck = \dfrac{\text{doc count}C=Ck}{N\text{doc}}$$  
    
     $$\hat{P}xi | Ci = \dfrac{\text{count}xiCj}{\sum{x \in V} \text{count}x Cj}$$  
    

6. MLE Derivation of the Parameter Estimates

    
    The Likelihood of the observed data TBC  

7. Motivation
    To estimate the parameters of the "true" MAP estimate we need a prohibitive number of examples ~ $$\mathcal{O}\vert x\vert^n \cdot \vert C\vert$$.  

8. Notes





 KNN  K Nearest Neighbor


K Nearest Neighbors k NN

1. KNN
    KNN is a non parametric method used for classification and regression.  
    It is based on the Local Constancy Smoothness Prior/workfiles/research/dl/theory/dlbookpt1 which states that "the function we learn should not change very much within a small region." for generalization.  
     
  

    

2. Structure
    In both classification and regression the input consists of the $$k$$ closest training examples in the feature space. The output depends on whether k NN is used for classification or regression  
    
     In k NN classification the output is a class membership. An object is classified by a plurality vote of its neighbors with the object being assigned to the class most common among its $$k$$ nearest neighbors $$k$$ is a positive integer typically small. If $$k = 1$$ then the object is simply assigned to the class of that single nearest neighbor.  
     In k NN regression the output is the property value for the object. This value is the average of the values of $$k$$ nearest neighbors.  
    

3. Formal Description  Statistical Setting
    Suppose we have pairs $${\displaystyle X{1}Y{1}X{2}Y{2}\dots X{n}Y{n}}$$ taking values in $${\displaystyle \mathbb {R} ^{d}\times \{12\}}$$ where $$Y$$ is the class label of $$X$$ so that $${\displaystyle X|Y=r\sim P{r}}$$ for $${\displaystyle r=12}$$ and probability distributions $${\displaystyle P{r}}$$. Given some norm $${\displaystyle \|\cdot \|}$$ on $${\displaystyle \mathbb {R} ^{d}}$$ and a point $${\displaystyle x\in \mathbb {R} ^{d}}$$ let $${\displaystyle X{1}Y{1}\dots X{n}Y{n}}$$ be a reordering of the training data such that $${\displaystyle \|X{1} x\|\leq \dots \leq \|X{n} x\|}$$.  
    

33.Choosing $$k$$
    Nearest neighbors can produce very complex decision functions and its behavior is highly dependent on the choice of $$k$$  


    Choosing $$k = 1$$ we achieve an optimal training error of $$0$$ because each training point will classify as itself thus achieving $$100\%$$ accuracy on itself.  
    However $$k = 1$$ overfits to the training data and is a terrible choice in the context of the bias variance tradeoff.  

    Increasing $$k$$ leads to increase in training error but a decrease in testing error and achieves better generalization.  

    At one point if $$k$$ becomes too large the algorithm will underfit the training data and suffer from huge bias.  

    In general we select $$k$$ using cross validation.  


       $$\text{Training and Test Errors as a function of } k \\\\\\\\\\\\$$
    

    

44.Bias Variance Decomposition of k NN
    
    


    

4. Properties
     Computational Complexity  
         We require $$\mathcal{O}n$$ space to store a training set of size $$n$$. There is no runtime cost during training if we do not use specialized data structures to store the data.  
            However predictions take $$\mathcal{O}n$$ time which is costly.  
         There has been research into Approximate Nearest Neighbors ANN procedures that quickly find an approximation for the nearest neighbor  some common ANN methods are Locality Sensitive Hashing and algorithms that perform dimensionality reduction via randomized Johnson Lindenstrauss distance preserving projections.  
         k NN is a type of instance based learning or "lazy learning" where the function is only approximated locally and all computation is deferred until classification.  
     Flexibility  
         When $$k>1$$ k NN can be modified to output predicted probabilities $$PY \vert X$$ by defining $$PY \vert X$$ as the proportion of nearest neighbors to $$X$$ in the training set that have class $$Y$$.  
         k NN can also be adapted for regression — instead of taking the majority vote take the average of the $$y$$ values for the nearest neighbors.  
         k NN can learn very complicated non linear decision boundaries highly influenced by choice of $$k$$.  
     Non Parametricity  
        k NN is a non parametric method which means that the number of parameters in the model grows with $$n$$ the number of training points. This is as opposed to parametric methods for which the number of parameters is independent of $$n$$.  
     High dimensional Behavior  
         k NN does NOT behave well in high dimensions.  
            As the dimension increases data points drift farther apart so even the nearest neighbor to a point will tend to be very far away.  
         It is sensitive to the local structure of the data in any/all dimension/s.  
     Theoretical Guarantees/Properties  
        $$1$$ NN has impressive theoretical guarantees for such a simple method  
         Cover and Hart 1967 prove that as the number of training samples $$n$$ approaches infinity the expected prediction error for $$1 \mathrm{NN}$$ is upper bounded by $$2 \epsilon^{}$$ where $$\epsilon^{}$$ is the Bayes optimal error.  
         Fix and Hodges 1951 prove that as $$n$$ and $$k$$ approach infinity and if $$\frac{k}{n} \rightarrow 0$$ then the $$k$$ nearest neighbor error approaches the Bayes error.  
    

5. Algorithm and Computational Complexity
    Training  
     Algorithm To train this classifier we simply store our training data for future reference.  
        Sometimes we store the data in a specialized structure called a k d tree. This data structure usually allows for faster average case $$\mathcal{O}\log n$$ nearest neighbors queries.  
        For this reason k NN is sometimes referred to as “lazy learning”.  
     Complexity $$\\\\\mathcal{O}1$$   

    Prediction  
    
     Algorithm  
        1. Compute the $$k$$ closest training data points "nearest neighbors" to input point $$\boldsymbol{z}$$.  
            "Closeness" is quantified using some metric; e.g. Euclidean distance.  
        2. Assignment Stage  
             Classification Find the most common class $$y$$ among these $$k$$ neighbors and classify $$\boldsymbol{z}$$ as $$y$$ majority vote   
             Regression Take the average label of the $$k$$ nearest points.  
     Complexity $$\\\\\mathcal{O}N$$   
    


    Notes  
    
     We choose odd $$k$$ for binary classification to break symmetry of majority vote  
    


6. Behavior in High Dimensional Space  Curse of Dimensionality
    As mentioned k NN does NOT perform well in high dimensional space. This is due to the "Curse of Dimensionality".  

    Curse of Dimensionality CoD
    To understand CoD we  need to understand the properties of metric spaces. In high dimensional spaces much of our low dimensional intuition breaks down  

    Geometry of High Dimensional Space  
    Consider a ball in $$\mathbb{R}^d$$ centered at the origin with radius $$r$$ and suppose we have another ball of radius $$r  \epsilon$$ centered at the origin. In low dimensions we can visually see that much of the volume of the outer ball is also in the inner ball.  
    In general the volume of the outer ball is proportional to $$r^{d}$$ while the volume of the inner ball is proportional to $$r \epsilon^{d}$$.  
    Thus the ratio of the volume of the inner ball to that of the outer ball is  
    $$\frac{r \epsilon^{d}}{r^{d}}=\left1 \frac{\epsilon}{r}\right^{d} \approx e^{ \epsilon d / r} \underset{d \rightarrow \infty}{\longrightarrow} 0$$  
    Hence as $$d$$ gets large most of the volume of the outer ball is concentrated in the annular region $$\{x  r \epsilon < x < r\}$$ instead of the inner ball.  



    Concentration of Measure  
    High dimensions also make Gaussian distributions behave counter intuitively. Suppose $$X \sim$$ $$\mathcal{N}\left0 \sigma^{2} I\right$$. If $$X{i}$$ are the components of $$X$$ and $$R$$ is the distance from $$X$$ to the origin then $$R^{2}=\sum{i=1}^{d} X{i}^{2}$$. We have $$\mathbb{E}\leftR^{2}\right=d \sigma^{2}$$ so in expectation a random Gaussian will actually be reasonably far from the origin. If $$\sigma=1$$ then $$R^{2}$$ is distributed chi squared with $$d$$ degrees of freedom.  
    One can show that in high dimensions with high probability $$1 \mathcal{O}\lefte^{ d^{\epsilon}}\right$$ this multivariate Gaussian will lie within the annular region $$\left\{X \left|R^{2} \mathbb{E}\leftR^{2}\right\right| \leq d^{1 / 2+\epsilon}\right\}$$ where $$\mathbb{E}\leftR^{2}\right=d \sigma^{2}$$ one possible approach is to note that as $$d \rightarrow \infty$$ the chi squared approaches a Gaussian by the CLT and use a Chernoff bound to show exponential decay. This phenomenon is known as Concentration of Measure.  

    Without resorting to more complicated inequalities we can show a simple weaker result  
    $$\bf{\text{Theorem}}$$ $$\text{If } X{i} \sim \mathcal{N}\left0 \sigma^{2}\right i=1 \ldots d \text{  are independent and } R^{2}=\sum{i=1}^{d} X{i}^{2} \text{ then for every } \epsilon>0  
     \text{the following holds } $$  
    $$\lim{d \rightarrow \infty} P\left\left|R^{2} \mathbb{E}\leftR^{2}\right\right| \geq d^{\frac{1}{2}+\epsilon}\right=0$$  
    Thus in the limit the squared radius is concentrated about its mean

    


    Thus a random Gaussian will lie within a thin annular region away from the origin in high dimensions with high probability even though the mode of the Gaussian bell curve is at the origin. This illustrates the phenomenon in high dimensions where random data is spread very far apart.  

    The k NN classifier was conceived on the principle that nearby points should be of the same class  however in high dimensions even the nearest neighbors that we have to a random test point will tend to be far away so this principle is no longer useful.  
    

7. Improving k NN
    1 Obtain More Training Data  
    More training data allows us to counter act the sparsity in high dimensional space.  

    2 Dimensionality Reduction  Feature Selection and Feature Projection  
    Reduce the dimensionality of the features and/or pick better features. The best way to counteract the curse of dimensionality.  

    3 Different Choices of Metrics/Distance Functions  
    We can modify the distance function. E.g.  
    
     The family of Minkowski Distances that are induced by the $$L^p$$ norms  
        $$D{p}\mathbf{x} \mathbf{z}=\left\sum{i=1}^{d}\left|x{i} z{i}\right|^{p}\right^{\frac{1}{p}}$$  
        Without preprocessing the data $$1 \mathrm{NN}$$ with the $$L^{3}$$ distance outperforms $$1 \mathrm{NN}$$ with $$L^{2}$$ on MNIST.  
     We can also use kernels to compute distances in a different feature space.  
        For example if $$k$$ is a kernel with associated feature map $$\Phi$$ and we want to compute the Euclidean distance from $$\Phix$$ to $$\Phiz$$ then we have  
        $$\begin{aligned}\|\Phi\mathbf{x} \Phi\mathbf{z}\| {2}^{2} &=\Phi\mathbf{x}^{\top} \Phi\mathbf{x} 2 \Phi\mathbf{x}^{\top} \Phi\mathbf{z}+\Phi\mathbf{z}^{\top} \Phi\mathbf{z}  &=k\mathbf{x} \mathbf{x} 2 k\mathbf{x} \mathbf{z}+k\mathbf{z} \mathbf{z} \end{aligned}$$  
        Thus if we define $$D\mathrm{x} \mathrm{z}=\sqrt{k\mathrm{x} \mathrm{x} 2 k\mathrm{x} \mathrm{z}+k\mathrm{z} \mathrm{z}}$$  then we can perform Euclidean nearest neighbors in $$\mathrm{\Phi}$$ space without explicitly representing $$\Phi$$ by using the kernelized distance function $$D$$.  

    


 Clustering






Clustering  Introduction

1. Clustering



8. Notes
    



    Clustering Types  
     Hard Clustering clusters do not overlap
         Elements either belong to a cluster or it doesn't
     Soft Clustering cluster may overlap  
         Computes a strength of association between clusters and instances  
        
    Mixture Models  
    
     probabilistically grounded way of doing soft clustering 
     each cluster a generative model Gaussian or multinomial 
     parameters e.g. mean/covariance are unknown 

    Mixture Models as Latent Variable Models  
    A mixture model can be described more simply by assuming that each observed data point has a corresponding unobserved data point or latent variable specifying the mixture component to which each data point belongs.  
    


    Expectation Maximization EM  
    
     Chicken and egg problem  
         need $$\left\mu{a} \sigma{a}^{2}\right\left\mu{b} \sigma{b}^{2}\right$$ to guess source of points 
         need to know source to estimate $$\left\mu{a} \sigma{a}^{2}\right\left\mu{b} \sigma{b}^{2}\right$$  
     EM algorithm 
         start with two randomly placed Gaussians $$\left\mu{a} \sigma{a}^{2}\right\left\mu{b} \sigma{b}^{2}\right$$
         for each point $$Pb \vert xi$$ does it look like it came from $$b$$?  
         adjust $$\left\mu{a} \sigma{a}^{2}\right\left\mu{b} \sigma{b}^{2}\right$$ to fit points assigned to them  





 Regression





 Least Squares Linear Regression  
    MLE + Noise Normally Distributed + Conditional Probability Normally Distributed  
 Logistic Regression  
    MLE + Noise $$\sim$$ Logistic Distribution latent + Conditional Probability $$\sim$$ Bernoulli Distributed  
 Ridge Regression 
    MAP + Noise Normally Distributed + Conditional Probability Normally Distributed + Weight Prior Normally Distributed  



Regression

Linear Regression


Assume that the target distribution is a sum of a deterministic function $$fx; \theta$$ and a normally distributed error $$\epsilon \sim \mathcal{N}\left0 \sigma^{2}\right$$  
$$y = fx; \theta + \epsilon$$  
Thus $$y \sim \mathcal{N}\leftfx; \theta \sigma^{2}\right$$ and we assume there is a distribution $$py\vert x$$ where $$y \sim \mathcal{N}\leftfx; \theta \sigma^{2}\right$$.  
  Notice that $$\epsilon = y  \hat{y} \implies $$  
$$\begin{align} 
    \epsilon &\sim \mathcal{N}\left0 \sigma^{2}\right 
            &\sim \frac{1}{\sigma \sqrt{2 \pi}} e^{ \frac{\left\epsilon\right^{2}}{2 \sigma^{2}}} 
            &\sim \frac{1}{\sigma \sqrt{2 \pi}} e^{ \frac{\lefty \hat{y}\right^{2}}{2 \sigma^{2}}}
    \end{align}$$  

In LR the equivalent is  
We assume that we are given data $$x{1} \ldots x{n}$$ and outputs $$y{1} \ldots y{n}$$ where $$x{i} \in \mathbb{R}^{d}$$ and $$y{i} \in \mathbb{R}$$ and that there is a distribution $$py \vert x$$ where $$y \sim \mathcal{N}\leftw^{\top} x \sigma^{2}\right$$.  
 In other words we assume that the conditional distribution of $$Yi \vert \theta$$ is a Gaussian Each individual term $$p\lefty{i} \vert \mathbf{x} {i} \boldsymbol{\theta}\right$$ comes from a Gaussian  
$$Y{i} \vert \boldsymbol{\theta} \sim \mathcal{N}\lefth{\boldsymbol{\theta}}\left\mathbf{x} {i}\right \sigma^{2}\right$$  
In other words we assume that there is a true linear model weighted by some true $$w$$ and the values generated are scattered around it with some error $$\epsilon \sim \mathcal{N}\left0 \sigma^{2}\right$$.  
Then we just want to obtain the max likelihood estimation  
$$\begin{aligned} pY \vert X w &=\prod{i=1}^{n} p\lefty{i} \vert x{i} w\right  \log p\cdot &=\sum{i} \log \left2 \pi \sigma^{2}\right \frac{1}{2 \sigma^{2}}\lefty{i} w^{\top} x{i}\right^{2} \end{aligned}$$  






Logistic Regression

The errors are not directly observable since we never observe the actual probabilities directly.  


Latent Variable Interpretation  
The logistic regression can be understood simply as finding the $$\beta$$ parameters that best fit  
$$y=\left\{\begin{array}{ll}{1} & {\beta{0}+\beta{1} x+\varepsilon>0}  {0} & {\text { else }}\end{array}\right.$$  
where $\varepsilon$ is an error distributed by the standard logistic distribution.  
The associated latent variable is $${\displaystyle y'=\beta {0}+\beta {1}x+\varepsilon }$$. The error term $$ \varepsilon $$ is not observed and so the $$y'$$ is also an unobservable hence termed "latent" the observed data are values of $$y$$ and $$ x$$. Unlike ordinary regression however the $$ \beta  $$ parameters cannot be expressed by any direct formula of the $$y$$ and $$ x$$ values in the observed data. Instead they are to be found by an iterative search process.  



Notes  

 Can be used with a polynomial kernel.
 Convex Cost Function
 No closed form solution



 Ensemble Learning  Aggregating










Ensemble Learning

1. Ensemble Learning
    In machine learning Ensemble Learning is a set of ensemble methods that use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.  
    

3. Ensemble Theory
    An ensemble is itself a supervised learning algorithm because it can be trained and then used to make predictions. The trained ensemble therefore represents a single hypothesis. This hypothesis however is not necessarily contained within the hypothesis space of the models from which it is built. Thus ensembles can be shown to have more flexibility in the functions they can represent. This flexibility can in theory enable them to over fit the training data more than a single model would but in practice some ensemble techniques especially bagging tend to reduce problems related to over fitting of the training data.  

    Empirically ensembles tend to yield better results when there is a significant diversity among the models. Many ensemble methods therefore seek to promote diversity among the models they combine.  
    Although perhaps non intuitive more random algorithms like random decision trees can be used to produce a stronger ensemble than very deliberate algorithms like entropy reducing decision trees.  
    Using a variety of strong learning algorithms however has been shown to be more effective than using techniques that attempt to dumb down the models in order to promote diversity.  
    

     In any network the bias can be reduced at the cost of increased variance
     In a group of networks the variance can be reduced at no cost to bias

4. Types of Ensembles
     Bayes optimal classifier Theoretical
     Bootstrap aggregating bagging
     Boosting
     Bayesian parameter averaging
     Bayesian model combination
     Bucket of models
     Stacking
    

5. Applications
     Remote sensing
         Land cover mapping
         Change detection
     Computer security
         Distributed denial of service
         Malware Detection
         Intrusion detection
     Face recognition
     Emotion recognition
     Fraud detection
     Financial decision making
     Medicine
    

7. Ensemble Size
    It is an important problem that hasn't been well studied/addressed.  


    

8. Notes
     Averaging models increases capacity.  


    Ensemble Averaging  
    Relies 


Bayes optimal classifier Theoretical

1. Bayes Optimal Classifier

    


Bootstrap Aggregating Bagging

1. Bootstrap Aggregating Bagging
    Bootstrap Aggregating Bagging is an ensemble meta algorithm designed to improve the stability and accuracy of ml algorithms. It is designed to reduce variance and help to avoid overfitting.  

    It is applicable to both classification and regression problems.  
    
    Although it is usually applied to decision tree methods it can be used with any type of method. Bagging is a special case of the model averaging approach.  
    

2. Bootstrapping
    Bootstrapping is a sampling technique. From a set $$D$$ of $$n$$ sample points it constructs $$m$$ subsets $$Di$$ each of size $$n'$$ by sampling from $$D$$ uniformly
     By sampling with replacement some observations may be repeated in each $${\displaystyle D{i}}$$.  
     If $$n'=n$$ then for large $$n$$ the set $$D{i}$$ is expected to have the fraction $$1  1/e$$ $$\approx 63.2\%$$ of the unique examples of $$D$$ the rest being duplicates.  

    The point of sampling with replacement is to make the re sampling truly random. If done without replacement the samples drawn will be dependent on the previous ones and thus not be random.  
    

3. Aggregating
    The predictions from the above models are aggregated to make a final combined prediction. This aggregation can be done on the basis of predictions made or the probability of the predictions made by the bootstrapped individual models.  
    


4. The Algorithm
    Bagging uses multiple weak models and aggregates the predictions from each of them to get the final prediction. The weak models should be such that each specialize in a particular part of the feature space thus enabling us to leverage predictions from each model to maximum use. As suggested by the name it consists of two parts bootstrapping and aggregation.  

     Given a set $$D$$ of $$n$$ sample points 
     Bootstrapping Construct $$m$$ bootstap samples subsets $$Di$$.  
     Fit $$m$$ models using the $$m$$ bootstrap samples
     Aggregating Combine the models by  
         Regression Averaging  
         Classification Voting  
    



5. Advantages
     Improves "unstable" procedures  
     Reduces variance $$\rightarrow$$ helps avoid overfitting  
     Ensemble models can be used to capture the linear as well as the non linear relationships in the data.This can be accomplished by using 2 different models and forming an ensemble of the two.  
    

6. Disadvantages
     On the other hand it can mildly degrade the performance of "stable" methods such as K NN  
     It causes a Reduction in the interpretability of the model  
     Prone to high bias if not modeled properly  
     Though improves accuracy it is computationally expensive and hard to design  
        It is not good for real time applications.    
    

7. Examples bagging algorithms
     Random Forests is a bagging algorithm that further reduces variance by selecting a subset of features   
        1. Suppose there are N observations and M features. A sample from observation is selected randomly with replacementBootstrapping.
        1. A subset of features are selected to create a model with sample of observations and subset of features.
        1. Feature from the subset is selected which gives the best split on the training data.Visit my blog on Decision Tree to know more of best split
        1. This is repeated to create many models and every model is trained in parallel
        Prediction is given based on the aggregation of predictions from all the models.
    


Boosting

1. Boosting
    Boosting is an ensemble meta algorithm for primarily reducing bias but also variance in supervised learning. It belongs to a family of machine learning algorithms that convert weak learners to strong ones.  

    It is an iterative technique which adjusts the weight of an observation based on the last classification. If an observation was classified incorrectly it tries to increase the weight of this observation and vice versa. Thus future weak learners focus more on the examples that previous weak learners misclassified.  

    Boosting in general decreases the bias error and builds strong predictive models. However they may sometimes over fit on the training data.  

    Boosting increases the capacity.  

    Summary
    Boosting create different hypothesis $$hi$$s sequentially + make each new hypothesis decorrelated with previous hypothesis.  
     Assumes that this will be combined/ensembled  
     Ensures that each new model/hypothesis will give a different/independent output  
    

2. Motivation  "The Hypothesis Boosting Problem"
    Boosting is based on a question posed by Kearns and Valiant 1988  
    "Can a set of weak learners create a single strong learner?"  

    This question was formalized as a hypothesis called "The Hypothesis Boosting Problem".  

    The Hypothesis Boosting Problem  
    Informally the hypothesis boosting problem asks whether an efficient learning algorithm … that outputs a hypothesis whose performance is only slightly better than random guessing i.e. a weak learner implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy i.e. a strong learner.  


     A weak learner is defined to be a classifier that is only slightly correlated with the true classification it can label examples better than random guessing.  
     A strong learner is a classifier that is arbitrarily well correlated with the true classification.  
    

    Countering BAgging Limitations  
    Bagging suffered from some limitations; namely that the models can be dependent/correlated which cause the voting to be trapped in the wrong hypothesis of the weak learners. This  motivated the intuition behind Boosting  
     Instead of training parallel models one needs to train models sequentially &
     Each model should focus on where the previous classifier performed poorly  
    

3. Boosting Theory and Convexity
    Only algorithms that are provable boosting algorithms in the probably approximately correct PAC learning formulation can accurately be called boosting algorithms. Other algorithms that are similar in spirit to boosting algorithms are sometimes called "leveraging algorithms" although they are also sometimes incorrectly called boosting algorithms.  
    

    Convexity  
    Boosting algorithms can be based on convex or non convex optimization algorithms  
     Convex Algorithms  
        such as AdaBoost and LogitBoost can be "defeated" by random noise such that they can't learn basic and learnable combinations of weak hypotheses.  
        This limitation was pointed out by Long & Servedio in 2008.   
     Non Convex Algorithms  
        such as BrownBoost was shown to be able to learn from noisy datasets and can specifically learn the underlying classifier of the "Long Servedio dataset".  
    

33.The Boosting MetaAlgorithm
     Finding defining Weak Learners  
        The algorithm defines weak learners as those that have weak rules rules that are not powerful enough for accurate classification    
     Identifying Weak Rules  
         To find weak rule we apply base learning ML algorithms with a different distribution. Each time base learning algorithm is applied it generates a new weak prediction rule. This is an iterative process. After many iterations the boosting algorithm combines these weak rules into a single strong prediction rule.
     Choosing different distribution for each round  
        1. The base learner takes all the distributions and assign equal weight or attention to each observation.
        2. If there is any prediction error caused by  base learning algorithm then we pay higher attention to observations having prediction error. Then we apply the next base learning algorithm.
        3. Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.
     Aggregating Outputs  
        Finally it combines the outputs from weak learner and creates a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis classiﬁed or have higher errors by preceding weak rules.  
    


44.Boosting Algorithms
     AdaBoost Adaptive Boosting
     Gradient Tree Boosting
     XGBoost
    

4. The AdaBoost Algorithm  Adaptive Boosting
    AdaBoost It works on similar method as discussed above. It fits a sequence of weak learners on different weighted training data. It starts by predicting original data set and gives equal weight to each observation. If prediction is incorrect using the  learner then it gives higher weight to observation which have been predicted incorrectly. Being an iterative process it continues to add learners until a limit is reached in the number of models or accuracy.

    Mostly we use decision stamps with AdaBoost. But we can use any machine learning algorithms as base learner if it accepts weight on training data set. We can use AdaBoost algorithms for both classification and regression problem.  





    

    Notes  
     order of trees matter in AdaBoost  
    
            

5. Advantages
     Decreases the Bias  
     Better accuracy over Bagging e.g. Random Forest  
     Boosting can lead to learning complex non linear decision boundaries   

    

6. Disadvantages
     Reduced interpretability  
     Harder to tune than other models because you have so many hyperparameters and you can easily overfit  
     Computationally expensive for training sequential and inference   
    

8. Bagging VS Boosting



Stacking


 





 K Means









# K Means

K Means    
It is a method for cluster analysis. It aims to partition $$n$$ observations into $$k$$ clusters in which each observation belongs to the cluster with the nearest mean. It results in a partitioning of the data space into Voronoi Cells.  


IDEA  
 Minimizes the aggregate Intra Cluster distance
 Equivalent to minimizing the Variance
 Thus it finds k clusters with minimum aggregate Variance.  


Formal Description    
Given a set of observations $$\left\mathbf{x}{1} \mathbf{x} {2} \ldots \mathbf{x}{n}\right$$ $$\mathbf{x} i \in \mathbb{R}^d$$ the algorithm aims to partition the $$n$$ observations into $$k$$ sets $$\mathbf{S}=\left\{S{1} S{2} \ldots S{k}\right\}$$ so as to minimize the intra cluster Sum of Squares i.e. variance.  

The Objective  
$$\underset{\mathbf{S}}{\arg \min } \sum{i=1}^{k} \sum{\mathbf{x} \in S{i}}\left\|\mathbf{x} \boldsymbol{\mu}{i}\right\|^{2}=\underset{\mathbf{S}}{\arg \min } \sum{i=1}^{k}\left|S{i}\right| \operatorname{Var} S{i}$$  
where $$\boldsymbol{\mu}i$$ is the mean of points in $$Si$$. 



Algorithm  
 Choose two random points call them "Centroids"  
 Assign the closest $$N/2$$ points Euclidean wise to each of the Centroids  
 Compute the mean of each "group"/class of points  
 Re Assign the centroids to the newly computed Means ↑
 REPEAT!

The "assignment" step is referred to as the "expectation step" while the "update step" is a maximization step making this algorithm a variant of the generalized expectation maximization algorithm.


Complexity    
The original formulation of the problem is NP Hard; however EM algorithms specifically Coordinate Descent can be used as efficient heuristic algorithms that converge quickly to good local minima.  
Lloyds algorithm and variants have $${\displaystyle \mathcal{O}nkdi}$$ runtime.   


Convergence    
Guaranteed to converge after a finite number of iterations  
 Proof  
    The Algorithm Minimizes a monotonically decreasing Non Negative Energy function on a finite Domain  
    By Monotone Convergence Theorem the objective Value Converges.  

    

  


Optimality    
 Locally optimal due to convergence property  
 Non Globally optimal  
     The objective function is non convex  
     Moreover coordinate Descent doesn't converge to global minimum on non convex functions.  


Objective Function    
$$JS \mu= \sum{i=1}^{k} \sum{\mathbf{x} \in S{i}} \| \mathbf{x}  \mui \|^{2}$$  


Optimization Objective  
$$\min {\mu} \min {S} \sum{i=1}^{k} \sum{\mathbf{x} \in S{i}}\left\|\mathbf{x}  \mu{i}\right\|^{2}$$  


Coordinate Descent  
 Fix $$S = \hat{S}$$ optimize $$\mu$$  
    $$\begin{aligned} & \min {\mu} \sum{i=1}^{k} \sum{\mathbf{x} \in \hat{S}{i}}\left\|\mu{i} x{j}\right\|^{2}
        =&  \sum{i=1}^{k} \min {\mui} \sum{\mathbf{x} \in \hat{S}{i}}\left\|\mathbf{x}  \mu{i}\right\|^{2}
    \end{aligned}$$  
     MLE  
        $$\min {\mui} \sum{\mathbf{x} \in \hat{S}{i}}\left\|\mathbf{x}  \mu{i}\right\|^{2}$$  
        $$ \implies $$  
        $${\displaystyle \hat{\mui} = \dfrac{\sum{\mathbf{x} \in \hat{S} {i}} \mathbf{x}}{\vert\hat{S} i\vert}}$$  
        
    
 Fix $$\mui = \hat{\mui} \forall i$$ optimize $$S$$^1  
    $$\arg \min {S} \sum{i=1}^{k} \sum{\mathbf{x} \in S{i}}\left\|\mathbf{x}  \hat{\mu{i}}\right\|^{2}$$  
    $$\implies$$  
    $$S{i}^{t}=\left\{x{p} \left\|x{p} m{i}^{t}\right\|^{2} \leq\left\|x{p} m{j}^{t}\right\|^{2} \forall j 1 \leq j \leq k\right\}$$  
     MLE  
        
    







^1 The optimization here is an $$\arg \min$$ not a $$\min$$ since we are optimizing for $$i$$ over $$Si$$.  


 Decision Trees



Decision Trees


     onclick="showTextwithParentPopHideevent;"}






1. Decision Trees
    A decision tree is a decision support tool that uses a tree like model of decisions and their possible consequences including chance event outcomes resource costs and utility. It is one way to display an algorithm that only contains conditional control statements.  

    The trees have two types of Nodes  
    
    1. Internal Nodes test feature values usually just $$1$$ and branch accordingly  
    2. Leaf Nodes specify class $$h\mathbf{x}$$  


    

11.CART Classification And Regression Trees Learning
    Decision tree learning uses a decision tree as a predictive model to go from observations about an item represented in the branches to conclusions about the item's target value represented in the leaves.  

    "Nonlinear method for classification and regression."  Schewchuk  

    Classification Trees  
    Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures  
    
     Leaves represent class labels and 
     Branches represent conjunctions of features that lead to those class labels.  


    Regression Trees  
    Decision trees where the target variable can take continuous values typically real numbers are called regression trees.  

    

2. Properties
     Non Linear
     Used for Both Classification and Regressions
     Cuts $$X$$ space into rectangular cells  
     Works well with both quantitative and categorical features
     Interpretable results inference
     Decision boundary can be arbitrarily complicated increase number of nodes to split on  
         Linear Classifiers VS Decision Trees $$x$$ axis \| Linear VS Non Linear data $$y$$ axis  

    

3. Classification Learning Algorithm
    Greedy top down learning heuristic.  
    Let $$S \subseteq\{12 \ldots n\}$$ be set of sample point indices.  
    Top level call $$S=\{12 \ldots n\} .$$  
    Algorithm  



     How to choose best split?  
    
    1. Try all splits
    2. For a set $$S$$ let $$JS$$ be the cost of $$S$$  
    3. Choose the split that minimizes $$JSl + JSr$$; or   
         The split that minimizes the weighted average  
            $$\dfrac{\left\vert S{l}\right\vert  J\leftS{l}\right+\left\vert S{r}\right\vert  J\leftS{r}\right}{\left\vert S{l}\right\vert +\left\vert S{r}\right\vert }$$  



     Choosing the Split  Further Discussion  
         For binary feature $$x{i} $$ children are $$x{i}=0 \\ \& \\ x{i}=1$$
         If $$x{i}$$ has $$3+$$ discrete values split depends on application.
             Sometimes it makes sense to use multiway splits; sometimes binary splits.  
         If $$x{i}$$ is quantitative sort $$x{i}$$ values in $$S$$ ; try splitting between each pair of unequal consecutive values.
             We can radix sort the points in linear time and if $$n$$ is huge we should.  
         Efficient Splitting clever bit As you scan sorted list from left to right you can update entropy in $$\mathcal{O}1$$ time per point!  
             This is important for obtaining a fast tree building time.  
        




    How to choose the cost $$JS$$  
    We can accomplish this by Measuring the Entropy from information theory  
    Let $$Y$$ be a random class variable and suppose $$PY=C = pC$$.  
     The Self Information "surprise"  of $$Y$$ being class $$C$$ non negative is  
        $$ \log2 pC$$   
         Event w/ probability $$= 1$$  gives us zero surprise  
         Event w/ probability $$= 0$$  gives us infinite surprise  
     The Entropy "average surprisal" of an index set $$S$$  
        $$HS= \sum{\mathbf{C}} p{\mathbf{C}} \log {2} p{\mathbf{C}} \quad \text { where } p{\mathbf{C}}=\dfrac{\left|\left\{i \in S  y{i}=\mathbf{C}\right\}\right|}{|S|}$$  
        The proportion of points in $$S$$ that are in class $$C$$.  
         If all points in $S$ belong to same class? $$HS= 1 \log{2} 1=0$$  
         Half class $$C$$ half class $$D ? HS= 0.5 \log{2} 0.5 0.5 \log{2} 0.5=1$$  
         $$n$$ points all different classes? $$HS= \log{2} \dfrac{1}{n}=\log{2} n$$  
     Weighted avg entropy after split is  
        $$H{\text {after }}=\dfrac{\left|S{l}\right| H\leftS{l}\right+\left|S{r}\right| H\leftS{r}\right}{\left|S{l}\right|+\left|S{r}\right|}$$  
     Choose the split that Maximizes Information Gain  
        $$\text{Info Gain} = HS  H{\text{after}}$$  
        $$\iff$$  
        Minimizing $$H{\text{after}}$$.  

         Info gain always positive except when one child is empty or  
            $$\forall \mathrm{C} P\lefty{i}=\mathrm{C} | i \in S{l}\right=P\lefty{i}=\mathrm{C} | i \in S{r}\right$$  






    


4. Algorithms and their Computational Complexity


    Test Point  
     Algorithm Walk down tree until leaf. Return its label.  
     Run Time  
         Worst case Time is $$\mathcal{O}\text{tree depth}$$.  
             For binary features that’s $$\leq d$$.  
             For Quantitative features they may go deeper.  
             Usually not always $$\leq \mathcal{O}\log n$$   
    
    Training  
     Algorithm  
         For Binary Features try $$\mathcal{O}d$$ splits at each node.  
         For Quantitative Features try $$\mathcal{O}n'd$$; where $$n' = \#$$ of points in node  
     Run Time  
         Splits/Per Node Time is $$\mathcal{O}d$$ for both binary and quantitative.  
            Quantitative features are asymptotically just as fast as binary features because of our clever way of computing the entropy for each split.  
         Points/Per Node Amount number is $$\mathcal{O}n$$ points per node   
         Nodes/per point Amount number is $$\mathcal{O}\text{depth}$$ nodes per point i.e. each point participates in $$\mathcal{O}\text{depth}$$ nodes   
        $$\implies$$  
         Worst case Time  
            $$\mathcal{O}d \times \mathcal{O}n \times \mathcal{O}\text{depth} \leq \mathcal{O}nd  \text{ depth} $$  
      

5. Multivariate Splits

    

6. Regression Tree Learning

    

7. Early Stopping

    

8. Pruning

    


Notes  

 Number of splits in a Decision Tree $$= dn$$  
 Complexity of finding the split  
    1. Naive $$\mathcal{O}dn^2$$  
    2. Fast sort $$\mathcal{O}dn \ log n$$  


Random Forests

1. Ensemble Learning

    

2. Bagging  Bootstrap AGGregating

    

3. Random Forests




 The Perceptron


Introduction

1. The Perceptron
       The Perceptron is an algorithm for supervised learning of binary classifiers.

2. Type
       It is a type of linear classifiers.

3. The Problem Setup
       Consider $$n$$ sample points $$X1 X2 ... Xn$$.
       For each sample point let $${\displaystyle yi = {\begin{cases} \\ 1&{\text{if }}\ Xi \in C  1&{\text{if}}\ Xi \notin C\end{cases}}}$$
    where 'C' is a given class of interest.


The Perceptron Method

0. Goal
       Find weights '$$w$$' such that $${\displaystyle {\begin{cases}Xi \cdot w \geq 0&{\text{if }}\ yi = 1Xi \cdot w \leq 0&{\text{if }}\ yi =  1\end{cases}}}$$
    Where $$Xi \cdot w$$ is the signed distance.
       Equivalently  
       $$yiXi \cdot w \geq 0$$
    Where $$yiXi \cdot w \geq 0$$ is a constraint on the problem.

1. Procedure
       Compute the point of greatest descent until you find a local minima and update the weights using "Gradient Descent".

2. Decision Function
       $${\displaystyle fx={\begin{cases}1&{\text{if }}\ w\cdot Xi+\alpha>00&{\text{otherwise}}\end{cases}}}$$
       where $$\alpha$$ is added by The fictitious Diminsion Trick

3. Loss Function
       $${\displaystyle Lz yi = {\begin{cases}0&{\text{if }}\ yi\cdot zi \geq 0 yi z&{\text{otherwise}}\end{cases}}}$$

4. Objective cost Function
       $$Rw = \sum{i=1}^n LXi \cdot w yi = \sum{i \in V}  yiXi \cdot w$$
    where $$V$$ is the set of indices $$i$$ for which $$yiXi \cdot w < 0$$.  

     Risk func is Convex but Non Smooth?  

5. Constraints
       $$yiXi \cdot w \geq 0$$

00.The Optimization Problem
       Find weights $$w$$ that minimizes $$Rw$$.

6. Optimization Methods
       The Perceptron algorithm uses a numerical optimization method.
       Gradient Descent is the most commonly used method.
       Newtons Method can also be used to optimize the objective. 

77.The Gradient Descent Step
       $$\begin{align}
            \nablaw Rw & \ = 
            & \ = \nablaw \sum{i=1}^n LXi \cdot w yi 
            & \ = \nablaw \sum{i \in V}  yiXi \cdot w 
            & \ = \sum{i \in V}  yiXi
            \end{align}$$

7. The Algorithm Frank Rosenblatt 1957
       1 Choose the weights $$\vec{w}$$ arbitrarily.  
       2 While $$R\vec{w} > 0$$  
        $$\\\\\\\$$ 3 $$V \leftarrow$$ set of indices such that $$ yiXi \cdot w < 0$$  
        $$\\\\\\\$$ 4 $$ w \leftarrow w + \epsilon \cdot \sum{i \in V} yiXi \;\;\;\;\;\; $$ GD  
        $$\\\\\\\$$ 4 $$ w \leftarrow w + \epsilon \cdot yiXi \;\;\;\;\;\; $$ SGD  
       5 Recurse

8. Avoiding the constriction of the separating hyperplane to passing through the origin
       In the procedure we have just described the separating hyperplane that this algorithm will produce will be forced to pass through the origin since the resulting hyperplane is not translated from the origin.
       We can get around that by moving our problem to a higher diminsion.  
        We achieve that by adding a "fictitious" diminsion as follows
       We re write  
       $$\vec{w}\cdot Xi \rightarrow \left\begin{array}{c} w1  & w2  & \cdots & wd & \alpha  \end{array} \right \cdot \left\begin{array}{ccccc} x1   x2  \vdots  xd 1 \end{array} \right$$
       Now we run the perceptron algorithm in d + 1 dimensional space.

12.The Boundary
       The boundary is a hyperplane  
       $$\{x \in \mathbf{R}^d  fx = 0\}$$  
    where $$fx = wXi + \alpha$$.



Convergence and Complexity

1. The Perceptron Convergence Theorem I
       If the data is linearly separable the perceptron algorithm will always find  a linear classifier that classifies all the data points correctly.

2. The Perceptron Convergence Theorem II
       If the perceptron is guranteed to converge on a data set then it will converge in at most $$\mathcal{O}\dfrac{R^2}{\gamma}$$ iterations.
    where $$R = \maxi \|Xi\|$$ called the radius of the data and $$\gamma = $$ max margin possible.

3. Complexity Runtime
       $$\mathcal{O}nd$$
       Can be made faster with 'SGD'.
       Although the step size/learning rate doesn’t appear in that big O expression it does have an effect on the
    running time but the effect is hard to characterize.  
        The algorithm gets slower if $$\epsilon$$ is too small because it has to take lots of steps to get down the hill. But it also gets slower if $$\epsilon$$ is too big for a different reason it jumps
        right over the region with zero risk and oscillates back and forth for a long time.


Further Analysis

1. Properties
    1. It is an Online Algorithm.
    2. The algorithm is quite slow.
    3. There is no way to reliably choose the learning rate.
    4. It is currently obsolete.
    5. It will not converge nor approach any approximate solutions for non linearly separable data.  



 Maximum Margin Classifiers


Introduction and Set up

1. The Margin
    The margin of a linear classifier is the distance from the decision boundary to the nearest sample point.  
    

2. The current Problem
       All the classifiers discussed thus far i.e. Centroid Perceptron will converge to a correct classifier on linearly seprable data; however the classifier they converge to is not unique nor the best.
    But what does it mean to be the "best" classifier?
       We assume that if we can maximize the distance between the data points to be classified and the hyperplane that classifies them then we have reached a boundary that allows for the "best fit" i.e. allows for the most room for error.

3. The Solution
       We enforce a constraint that achieves a classifier that has a maximum margin.

4. The Signed Distance
    The signed distance is the minimum distance from a point to a hyperplane.
    We solve for the signed distance to achieve the following formula for it
    $$d = \dfrac{\| w \cdot x0 + b \|}{\|w\|}$$  
    where we have an n dimensional hyperplane $$w \cdot x + b = 0$$ and a point $$\mathbf{x} n$$.  
     Proof.  
         Suppose we have an affine hyperplane defined by $$w \cdot x + b$$ and a point $$\mathbf{x} n$$.
         Suppose that $$\mathbf{x} \in \mathbf{R}^n$$ is a point satisfying $$w \cdot \mathbf{x} + b = 0$$ i.e. it is a point on the plane.
         We construct the vector $$\mathbf{x} n−\mathbf{x}$$ which points from $$\mathbf{x}$$ to $$\mathbf{x} n$$ and then project scalar projection==signed distance it onto the unique vector perpendicular to the plane i.e. $$w$$  

            $$d=| \text{comp}{w} \mathbf{x} n \mathbf{x}| = \left| \frac{\mathbf{x} n \mathbf{x}\cdot w}{\|w\|} \right| = \frac{|\mathbf{x} n \cdot w  \mathbf{x} \cdot w|}{\|w\|}.$$

         Since $$\mathbf{x}$$  is a vector on the plane it must satisfy $$w\cdot \mathbf{x}= b$$ so we get  

            $$d=| \text{comp}{w} \mathbf{x} n \mathbf{x}| = \frac{|\mathbf{x} n \cdot w +b|}{\|w\|}$$  

    Thus we conclude that if $$\|w\| = 1$$ then the signed distance from a datapoint $$Xi$$ to the hyperplane is $$\|wXi + b\|$$.

    Caltech  
    So now we can characterize the margin with its size as the distance $$\frac{1}{\|\mathbf{w}\|}$$ between the hyperplane/boundary and the closest point to the plane $$\mathbf{x} n$$ in both directions multiply by 2 $$= \frac{2}{\|\mathbf{w}\|}$$ ; given the condition we specified earlier $$\left|\mathbf{w}^{\top} \mathbf{x} {n} + b\right|=1$$ for the closest point $$\mathbf{x} n$$.  

    Thus we formulate the optimization problem of maximizing the margin by maximizing the distance subject to the condition on how we derived the distance  
    $$\max{\mathbf{w}} \dfrac{2}{\|\mathbf{w}\|} \\\  \\ \min {n=12 \ldots N}\left|\mathbf{w}^{\top} \mathbf{x}{n}+b\right|=1$$  
    
    $$\minw \dfrac{1}{2} \mathbf{w}^T\mathbf{w} \\\  \\ y{n}\left\mathbf{w}^{\top} \mathbf{x} {n}+b\right \geq 1 \\ \forall i \in 1N$$  
    Now when we solve the "friendly" equation above we will get the separating plane with the best possible margin best=biggest.  

    To solve the above problem we need something that deals with inequality constraints; thus we use the KKT method for solving a Lagrnagian under inequality constraints.  
    The Lagrange Formulation  
     Formulate the Lagrangian  
        1. Take each inequality constraint and put them in the zero form equality with Zero  
        2. Multiply each inequality by a Lagrange Multiplier $$\alphan$$
        3. Add them to the objective function $$\minw \dfrac{1}{2} \mathbf{w}^T\mathbf{w}$$  
            The sign will be $$ $$ negative simply because the inequality is $$\geq 0$$  
        $$\min{w b} \max{\alphan} \mathcal{L}\mathbf{w} b \boldsymbol{\alpha} = \dfrac{1}{2} \mathbf{w}^T\mathbf{w}  \sum{n=1}^{N} \alpha{n}\lefty{n}\left\mathbf{w}^{\top} \mathbf{x} {n}+b\right 1\right \\\  \\ \alphan \geq 0$$  
     Optimize the objective independently for each of the unconstrained variables  
        1. Gradient w.r.t. $$\mathbf{w}$$   
            $$\nabla{\mathrm{w}} \mathcal{L}=\mathrm{w} \sum{n=1}^{N} \alpha{n} y{n} \mathrm{x} {n}=0  \implies  \mathbf{w}=\sum{n=1}^{N} \alpha{n} y{n} \mathbf{x} {n}$$  
        2. Derivative w.r.t. $$b$$  
            $$\frac{\partial \mathcal{L}}{\partial b}= \sum{n=1}^{N} \alpha{n} y{n}=0  \implies  \sum{n=1}^{N} \alpha{n} y{n}=0$$  
     Get the Dual Formulation w.r.t. the tricky constrained variable $$\alphan$$  
         Substitute with the above conditions in the original lagrangian such that the optimization w.r.t. $$\alphan$$ will become free of $$\mathbf{w}$$ and $$b$$   
            $$\mathcal{L}\boldsymbol{\alpha}=\sum{n=1}^{N} \alpha{n} \frac{1}{2} \sum{n=1}^{N} \sum{m=1}^{N} y{n} y{m} \alpha{n} \alpha{m} \mathbf{x}{n}^{\mathrm{T}} \mathbf{x}{m}$$  
         Notice that the  constraint $$\mathbf{w}=\sum{n=1}^{N} \alpha{n} y{n} \mathbf{x} {n}$$ has no effect/doesn't constraint $$\alphan$$ so it's a vacuous constraint. However not the  constraint $$\sum{n=1}^{N} \alpha{n} y{n}=0$$.   
         Set the optimization objective and the constraints a quadratic function in $$\alphan$$  
        $$\max{\alpha} \mathcal{L}\boldsymbol{\alpha}=\sum{n=1}^{N} \alpha{n} \frac{1}{2} \sum{n=1}^{N} \sum{m=1}^{N} y{n} y{m} \alpha{n} \alpha{m} \mathbf{x}{n}^{\mathrm{T}} \mathbf{x}{m}  \\\\\\\\\\  \\ \alphan \geq 0 \\ \forall \ n= 1 \ldots N \\ \wedge \\ \sum{n=1}^{N} \alpha{n} y{n}=0$$  
     Set the problem as a Quadratic Programming problem  
         Change the maximization to minimization by flipping the signs  
            $$\min {\alpha} \frac{1}{2} \sum{n=1}^{N} \sum{m=1}^{N} y{n} y{m} \alpha{n} \alpha{m} \mathbf{x}{0}^{\mathrm{T}} \mathbf{x}{m} \sum{n=1}^{N} \alpha{n}$$  
         Isolate the Coefficients from the $$\alphan$$s and set in matrix form  
            $$\min {\alpha} \frac{1}{2} \alpha^{\top} 
                \underbrace{\begin{bmatrix}
                    y{1} y{1} \mathbf{x}{1}^{\top} \mathbf{x}{1} & y{1} y{2} \mathbf{x}{1}^{\top} \mathbf{x}{2} & \ldots & y{1} y{N} \mathbf{x}{1}^{\top} \mathbf{x}{N}  
                    y{2} y{1} \mathbf{x}{2}^{\top} \mathbf{x}{1} & y{2} y{2} \mathbf{x}{2}^{\top} \mathbf{x}{2} & \ldots & y{2} y{N} \mathbf{x}{2}^{\top} \mathbf{x}{N} 
                    \ldots & \ldots & \ldots & \ldots 
                    y{N} y{1} \mathbf{x}{N}^{\top} \mathbf{x}{1} & y{N} y{2} \mathbf{x}{N}^{\top} \mathbf{x}{2} & \ldots & y{N} y{N} \mathbf{x}{N}^{\top} \mathbf{x}{N} 
                \end{bmatrix}}{\text{quadratic coefficients}}
            \alpha+\underbrace{\left 1^{\top}\right} {\text { linear }} \alpha  
        \\\\\\\\\\  \\ \underbrace{\mathbf{y}^{\top} \boldsymbol{\alpha}=0}{\text { linear constraint }} \\ \wedge \\ \underbrace{0}{\text { lower bounds }} \leq \alpha \leq \underbrace{\infty}{\text { upper bounds }}  $$  
            The Quadratic Programming Package asks you for the Quadratic Term Matrix and the Linear Term and for the Linear Constraint and the Range of $$\alphan$$s; and then gives you back an $$\mathbf{\alpha}$$.     

        Equivalently  
        $$\min {\alpha} \frac{1}{2} \boldsymbol{\alpha}^{\mathrm{T}} \mathrm{Q} \boldsymbol{\alpha} \mathbf{1}^{\mathrm{T}} \boldsymbol{\alpha} \quad \text { subject to } \quad \mathbf{y}^{\mathrm{T}} \boldsymbol{\alpha}=0 ; \quad \boldsymbol{\alpha} \geq \mathbf{0}$$  
                
            
6. Geometric Analysis
     we notice that for any given plane $$w^Tx = 0$$ the equations $$\gamma  w^Tx = 0$$ where $$\gamma \in \mathbf{R}$$ is a scalar basically characterize the same plane and not many planes.  
    This is because $$w^Tx = 0 \iff \gamma  w^Tx = \gamma  0 \iff \gamma  w^Tx = 0$$.  
    The above implies that any model that takes input $$w$$ and produces a margin will have to be Scale Invariant.  
    To get around this and simplify the analysis I am going to consider all the representations of the same plane and I am going to pick one where we normalize re scale the weight $$w$$ such that the signed distance distance to the point closest to the margin is equal to one  
    $$|w^Txn| > 0 \rightarrow |w^Txn| = 1$$  
     where $$xn$$ is the point closest to the plane.  
    We constraint the hyperplane by normalizing $$w$$ to this equation $$|w^Txi| = 1$$ or with added bias $$|w^Txi + b| = 1$$.  
    This implies that there exists a "slab" of width $$\dfrac{2}{\|w\|}$$.  


5. The Margin mathematically
    Now we can mathematically characterize the margin.  
    By substituting the constraints $$\ yiw^TXi+ b \geq 1 \forall i \in 1n$$ and the signed distance  
    $$\mini \dfrac{1}{\|w\|} \|w^TXi + b\| \geq \dfrac{1}{w}$$  

9. The distance of the point closest to the hyperplane
       We find the distance of the point closest to the hyperplane.
       Let $$Xn$$ be the point that is closest to the plane and let $$\hat{w} = \dfrac{w}{\|w\|}$$.  
        Take any point $$X$$ on the plane and let $$\vec{v}$$ be the vector $$\vec{v} = Xn  X$$.  
       Now the distance d is equal to 
       $$\begin{align}
            d & \ = \|\hat{w}\vec{v}\| 
            & \ = \|\hat{w}Xn  X\| 
            & \ = \|\hat{w}Xn  \hat{w}X\| 
            & \ = \dfrac{1}{\|w\|}\|wXn + b  wX  b\|  & \text{we add and subtract the bias } b
            & \ = \dfrac{1}{\|w\|}\|wXn + b  wX + b\| 
            & \ = \dfrac{1}{\|w\|}\|wXn + b  0\|  & \text{from the eq. of the plane on a point on the plane} 
            & \ = \dfrac{1}{\|w\|}\|1  0\|  & \text{from the constraint on the distance of the closest point} 
            & \ = \dfrac{1}{\|w\|}
            \end{align}
        $$

7. Slab Existence
       The analysis done above allows us to conclusively prove that there exists a slab of width $$\dfrac{2}{\|w\|}$$ containing no sample points where the hyperplane runs through bisects its center.

8. Maximizing the Margin
       To maximize the margin we need to maximize the width of the slab i.e. maximize $$\dfrac{2}{\|w\|}$$   
    or equivalently 
       $$\maxw \dfrac{2}{\|w\|} = \minw \dfrac{\|w\|}{2} = \minw \dfrac{1}{2}\|w\| \minw \dfrac{1}{2}\|w\|^2$$
       subject to the constraint mentioned earlier $$\mini \|wX + b\| = 1 \forall i \in 1n$$ or equivalently
       $$yiwXi + b \geq 1 \forall i \in 1n$$
       since the equation $$yiwXi + b$$ enforces the absolute value condition as was our analysis for regular linear classifiers.

11.The Optimization Problem for Maximum Margin Classifiers
       $$\minw \dfrac{1}{2}w^Tw \\\  \\ yiwXi + b \geq 1 \forall i \in 1n$$
    The above problem is a Quadratic Program in $$d + 1$$ diminsions and $$n$$ constraints in standard form.
    Notice that we use the quadratic $$w^Tw$$ instead of the linear $$w$$ as the objective because the quadratic function is smooth at zero as opposed to the linear objective which hinders the optimization.


12.Notes
     The weight vector $$\mathbf{w}$$ is orthogonal to the separating plane/decision boundary defined by $$\mathbf{w}^T\mathbf{x} + b = 0$$ in the $$\mathcal{X}$$ space; Reason  
        Since if you take any two points $$\mathbf{x}^\prime$$ and $$\mathbf{x}^{\prime \prime}$$ on the plane and create the vector $$\left\mathbf{x}^{\prime} \mathbf{x}^{\prime \prime}\right$$  parallel to the plane by subtracting the two points then the following equations must hold  
        $$\mathbf{w}^{\top} \mathbf{x}^{\prime}+b=0 \wedge \mathbf{w}^{\top} \mathbf{x}^{\prime \prime}+b=0 \implies \mathbf{w}^{\top}\left\mathbf{x}^{\prime} \mathbf{x}^{\prime \prime}\right=0$$  
     In a problem of minimizing a function  
         Unconstrained problem  
            You set the gradient of the function to Zero and solve.  
         Constrained regularization?  
            The gradient becomes related to the constraint; the gradient $$\nabla E{\mathrm{in}}$$ is normal to the constraint.  
     Conceptual Dichotomy between Regularization and SVM  
        img
                
            


 Hard Margin Support Vector Machines  SVM


Introduction  Support Vector Machines

1. Support Vector Machines
    Support Vector Machines SVMs are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.  
    The SVM is a Maximum Margin Classifier/workfiles/research/ml/13 that aims to find the "maximum margin hyperplane" that divides the group of points $${\displaystyle {\vec {x}}{i}} {\vec {x}}{i}$$ for which $${\displaystyle y{i}=1}$$ from the group of points for which $${\displaystyle y{i}= 1}$$.  

3. Support Vectors
    Support Vectors are the data points that lie exactly on the margin i.e. on the boundary of the slab.  
    They satisfy $$\|w^TX' + b\| = 1 \forall $$ support vectors $$X'$$  

2. The Hard Margin SVM
    The Hard Margin SVM is just a maximum margin classifier with features and kernels discussed later.  


The Hard Margin SVM

0. Goal
       Find weights '$$w$$' and scalar '$$b$$' that correctly classifies the data points and moreover does so in the "best" possible way.
    Where we had defined best as the classifier that admits the maximum

1. Procedure
    1 Use a linear classifier
    2 But Maximize the Margin
    3 Do so by Minimizing $$\|w\|$$  

2. Decision Function
       $${\displaystyle fx={\begin{cases}1&{\text{if }}\ w\cdot Xi+\alpha>00&{\text{otherwise}}\end{cases}}}$$

5. Constraints
       $$yiwXi + b \geq 1 \forall i \in 1n$$

7. The Optimization Problem
    Find weights '$$w$$' and scalar '$$b$$' that minimize  
    $$ \dfrac{1}{2} w^Tw$$  
    Subject to  
    $$yiwXi + b \geq 1 \forall i \in 1n$$  
    Formally  
    $$\minw \dfrac{1}{2}w^Tw \\\  \\ yiwXi + b \geq 1 \forall i \in 1n$$  

6. Optimization Methods
       The SVM optimization problem reduces to a Quadratic Programworkfiles/research/convopt/33.

Further Analysis

1. Generalization
       We notice that geometrically the hyperplane the maximum margin classifier is completely characterized by the support vectors the vectors that lie on the margin.  
       A very important conclusion arises.  
        The maximum margin classifier SVM depends only on the number of support vectors and not on the diminsion of the problem.  
        This implies that the computation doesn't scale up with the diminsion and also implies that the kernel trick works very well.

2. Properties
       1. In addition to performing linear classification SVMs can efficiently perform a non linear classification using what is called the kernel trick implicitly mapping their inputs into high dimensional feature spaces.
        2. The hyperplane is determined solely by its support vectors.
        3. The SVM always converges on linearly seprable data.
        4. The Hard Margin SVM fails if the data is not linearly separable. 
        4. The Hard Margin SVM is quite sensetive to outliers


 Soft Margin Support Vector Machines  The SVM


Introduction

1. Why another SVM? i.e. The Problem
       The Hard Margin SVM faces a few issues  
        1. The Hard Margin SVM fails if the data is not linearly separable. 
        4. The Hard Margin SVM is quite sensetive to outliers
       The Soft Margin SVM aims to fix/reconcile these problems.

3. The solution
       Allow some points to violate the margin by introducing slack variables.


The Soft Margin SVM

1. Procedure
    1 Use a linear classifier  
    2 But Maximize the Margin  
    3 Do so by Minimizing $$\|w\|$$  
    4 But allow some points to penetrate the margin

5. Modified Constraints
    $$yiwXi + b \geq 1  \zetai \forall i \in 1n$$  
    where the $$\zetai$$s are slack variables.  
    We also enforce the non negativity constraint on the slack variables  
    $$\zetai \geq 0 \\\ \forall i \in 1 n$$  

    The non negativity constraint forces the slack variables to be zero for all points that do not violate the original constraint  
      > i.e. are not inside the slab.

4. Modified Objective cost Function
       $$ Rw = \dfrac{1}{2} w^Tw + C \sum{i=1}^n \zetai$$

7. The Optimization Problem
    Find weights '$$w$$' scalar '$$b$$' and $$\zetai$$s that minimize  
    $$ \dfrac{1}{2} w^Tw + C \sum{i=1}^n \zetai$$  
    Subject to  
    $$yiwXi + b \geq 1  \zetai \zetai \geq 0 \forall i \in 1n$$  
    Formally  
    $$\minw \dfrac{1}{2}w^Tw \\\  \\ yiwXi + b \geq 1  \zetai \zetai \geq 0 \forall i \in 1n$$  

6. Optimization Methods
       The SVM optimization problem reduces to a Quadratic Programworkfiles/research/convopt/33 in $$d + n + 1$$ dimensions and $$2n$$ constraints.

7. Effects of the Regularization Hyperparameter $$C$$
       | |Small C|Large C  
    | Desire|Maximizing Margin = $$\dfrac{1}{\|w\|}$$|keep most slack variables zero or small  
    | Danger|underfitting High Misclassification|overfitting awesome training awful test  
    | outliers|less sensitive|very sensitive  
    | boundary|more "flat"|more sinuous  
    The last row only applies to nonlinear decision boundaries.
        We choose '$$C$$' with cross validation.

An Equivalent Formulation

1. Introduction
       In the current SVM model we are optimizing the objective $$Rw = \dfrac{1}{2} w^Tw + C \sum{i=1}^n \zetai$$ which looks like an $$l2$$ regularization on the weights and an $$l1$$ regularization on the slack variables.
       However usually in function estimation we prefer the standard form objective  to minimize and trade off; the loss + penalty form.

2. Modified Loss Function
       We introduce a loss function to moderate the use of the slack variables i.e. to avoid abusing the slack variables.
       But  we motivate it by comparing it to the traditional $$0 1$$ Loss function.  
        Notice that the $$0 1$$ loss is actually non convex. It has an infinite slope at $$0$$.  
        On the other hand the hinge loss is actually convex.
       The hinge loss
       $${\displaystyle \max \left0 1 y{i}{\vec {w}}\cdot {\vec {x}}{i} b\right.}$$
       This function is zero if the constraint $$y{i}{\vec {w}}\cdot {\vec {x}}{i} b\geq 1$$ is satisfied in other words if $${\displaystyle {\vec {x}}{i}} {\vec {x}}  {i}$$ lies on the correct side of the margin.  
        For data on the wrong side of the margin the function's value is proportional to the distance from the margin.
       All of the above suggests that this loss function is ideal for binary classification as it doesn't penalize correct classification at all.  
        which is something we are seeking for classification as opposed to regression.

3. Modified Objective Function
       $$ Rw = \dfrac{\lambda}{2} w^Tw +  \sum{i=1}^n {\displaystyle \max \left0 1 y  {i}{\vec {w}}\cdot {\vec {x}}  {i} b\right}$$

4. Proof of Equivalence
       Here we show that the two objectives to optimize for the SVM are actually equivalent.
       $$\begin{align}
            yif\leftxi\right & \ \geq 1 \zetai & \text{from 1st constraint } 
            \implies \zetai & \ \geq 1 yif\leftxi\right 
            \zetai & \ \geq 1 yif\leftxi\right \geq 0 & \text{from 2nd positivity constraint on} \zetai 
            \iff \zetai & \ \geq \max \{0 1 yif\leftxi\right\} 
            \zetai & \ = \max \{0 1 yif\leftxi\right\} & \text{minimizing means } \zetai \text{reach lower bound}
            \implies Rw & \ = \dfrac{\lambda}{2} w^Tw +  \sum{i=1}^n {\displaystyle \max \left0 1 y  {i}{\vec {w}}\cdot {\vec {x}}  {i} b\right} & \text{plugging in and multplying } \lambda = \dfrac{1}{C}
            \end{align}$$

5. The Optimization Problem
       Find weights '$$w$$' and scalar '$$b$$' that minimize
       $$ \dfrac{\lambda}{2} w^Tw +  \sum{i=1}^n {\displaystyle \max \left0 1 y  {i}{\vec {w}}\cdot {\vec {x}}  {i} b\right}$$
       Subject to
       $$yiwXi + b \geq 1  \zetai \zetai \geq 0 \forall i \in 1n$$
       Formally
       $$\min{w b}\dfrac{\lambda}{2} w^Tw +  \sum{i=1}^n {\displaystyle \max \left0 1 y  {i}{\vec {w}}\cdot {\vec {x}}  {i} b\right} \\ \forall i \in 1n$$

Further Analysis

1. Generalization
       We notice that geometrically the hyperplane the maximum margin classifier is completely characterized by the support vectors the vectors that lie on the margin.  
       A very important conclusion arises.  
        The maximum margin classifier SVM depends only on the number of support vectors and not on the dimension of the problem.  
        This implies that the computation doesn't scale up with the dimension and also implies that the kernel trick works very well.

2. Properties
       1. In addition to performing linear classification SVMs can efficiently perform a non linear classification using what is called the kernel trick implicitly mapping their inputs into high dimensional feature spaces.
        2. The hyperplane is determined solely by its support vectors.
        3. The SVM always converges on linearly separable data.
        4. The Soft Margin SVM will converge on non linearly separable data.


   
   
   

Extra info to be added  
Requiring the Margin to be bigger puts a restriction on the growth function i.e. the number of possible dichotomies. So fewer VC dimension.




Why $$w$$ is Orthogonal to the plane   
    img



The number of non zero paramters in a model correspond to the VC dimension



Normalizing $$w$$  

Observations  
1. Normalize $$w$$  
    The Hyperplane $$\mathbf{w}^{\top} \mathbf{x}=0$$ which is incidentally the signal defined by $$w$$ is scale invariant to $$w$$; since you can multiply $$w$$ by any scalar and the equation of the plane will still hold. Thus you can normalize it by dividing by a scalar.  
    So we choose $$w$$ by normalizing/scaling it such that the following equation holds for the variable $$\mathbf{x} n$$  
    $$\left|\mathbf{w}^{\top} \mathbf{x} {n}\right|=1$$   



Effect of the gamma hyperparam in RBF kernel
    The gamma parameter in SVM tuning signifies the influence of points either near or far away from the hyperplane.
    For a low gamma the model will be too constrained and include all points of the training dataset without really capturing the shape.
    For a higher gamma the model will capture the shape of the dataset well.


Bias = Underfit = low complexity model
Variance = Overfit = high complexity

Regularization reduce overfitting = reduce variance = simplify model = increase bias

Increase C hparam in SVM = reduce underfitting


 The Centroid Method


The Centroid Method

1. The Centroid
       In mathematics and physics the centroid or geometric center of a plane figure is the arithmetic mean "average" position of all the points in the shape. 
       The definition extends to any object in n dimensional space its centroid is the mean position of all the points in all of the coordinate directions.

2. Procedure
       Compute the mean $$\muc$$ of all the vectors in class C and the mean $$\mux$$ of all the vectors not in C.

3. Decision Function
       $$fx = \muc  \mux \cdot \vec{x}  \muc  \mux \cdot \dfrac{\muc + \mux}{2}$$

4. Decision Boundary
       The decision boundary is a Hyperplane that bisects the line segment with endpoints $$<\muc \mux>$$.





 Introduction to  NLP


Introduction

1. Problems in NLP
        Question Answering QA 
         Information Extraction IE    
         Sentiment Analysis  
         Machine Translation MT  
         Spam Detection  
         Parts of Speech POS Tagging  
         Named Entity Recognition NER
         Conference Resolution  
         Word Sense Disambugation WSD  
         Parsing  
         Paraphrasing  
         Summarization  
         Dialog  

2. mostly Solved Problems in NLP
        Spam Detection  
         Parts of Speech POS Tagging  
         Named Entity Recognition NER  


3. Within Reach Problems
        Sentiment Analysis  
         Conference Resolution    
         Word Sense Disambugation WSD  
         Parsing  
         Machine Translation MT  
         Information Extraction IE    


4. Open Problems in NLP
        Question Answering QA   
         Paraphrasing  
         Summarization  
         Dialog  

5. Issues in NLP why nlp is hard?
        Non Standard English "Great Job @ahmedbadary! I luv u 2!! were SOO PROUD of dis."  
         Segmentation Issues "New York New Haven" vs "New York New Haven"  
         Idioms "dark horse" "getting cold feet" "losing face"  
         Neologisms "unfriend" "retweet" "google" "bromance"  
         World Knowledge "Ahmed and Zach are brothers" "Ahmed and Zach are fathers"    
         Tricky Entity Names "Where is Life of Pie playing tonight?" "Let it be was a hit song!"  

6. Tools we need for NLP
        Knowledge about Language.  
         Knowledge about the World.   
         A way to combine knowledge sources.  

7. Methods
       In general we need to construct Probabilistic Models built from language data.    
       We do so by using rough text features.  
        All the names models methods and tools mentioned above will be introduced later as you progress in the text.  



Definitions

1. K Nearest Neighbors
       

       Complexity  
            Training $$\\\\\mathcal{O}1$$   
             Predict $$\\\\\mathcal{O}N$$ 









Metrics

1. L1 Distance
       $$d1I1 I2 = \sump{\|I1^p  I2^p\|}$$  
       Pixel wise absolute value differences.  

2. L2 Distance
       $$d2 I1 I2 = \sqrt{\sum{p} \left I^p1  I^p2 \right^2}$$
       

3. L1 vs. L2
       The L2 distance penalizes errors pixel differences much more than the L1 metric does.  
    The L2 distnace will be small iff there are man small differences in the two vectors but will explode if there is even one big difference between them.  
       Another difference we highlight is that the L1 distance is dependent on the corrdinate system frame while the L2 distance is coordinate invariant.








 Information Extraction <br \> Named Entity Recognition


Introduction to Information Extraction

1. Information Extraction IE
       is the task of automatically extracting structured information from a non/semi structured piece of text.  

2. Structured Representations of Inforamtion
       Usually the extracted information is represented as  
         Relations in the DataBase sense  
         A Knowledge Base

3. Goals
       1. Organize information in a way that is useful to humans. 
        2. Put information in a semantically precise form that allows further inference to be made by other computer algorithms. 

4. Common Applications
        Gathering information earning profits HQs etc. from reports  
         Learning drug gene product interactions from medical research literature  
         Low Level Information Extraction  
             Information about possible dates schedules activites gathered by companys e.g. google facebook  

5. Tasks and Sub tasks
        Named entity extraction   
             Named Entity Recognition recognition of known entity names for people and organizations place names temporal expressions and certain types of numerical expressions.     
             Coreference Resolution  detection of coreference and anaphoric links between text entities.  
             Relationship Extraction identification of relations between entities.  
         Semi structured information extraction
             Table Extraction finding and extracting tables from documents.  
             Comments extraction extracting comments from actual content of article in order to restore the link between author of each sentence.
         Language and Vocabulary Analysis   
             Terminology extraction finding the relevant terms for a given corpus.          

6. Methods
       There are three standard approaches for tackling the problem of IE  
         Hand written Regular Expressions usually stacked.  
         Classifiers   
             Generative   
                 Naive Bayes Classifier
             Discriminative   
                 MaxEnt Models Multinomial Logistic Regr.
         Sequence Models   
             Hidden Markov Models
             Conditional Markov model CMM / Maximum entropy Markov model MEMM
             Conditional random fields CRF


Named Entity Recognition NER

1. Named Entity Recognition
       is the recognition of known entity names for people and organizations place names temporal expressions and certain types of numerical expressions;  
    this is usually done by employing existing knowledge of the domain or information extracted from other sentences.    

2. Applications
        Named entities can be indexed linked off etc.
         Sentiment can be attributed to companies or products  
         They define a lot of the IE relations as associations between the named entities
         In QA answers are often named entities  


3. Evaluation of NER Tasks
       Evaluation is usually done at the level of Entities and not of Tokens.  
       One common issue with the metrics defined for text classification namely Precision/Recall/F1 is that they penalize the system based on a binary evaluation on how the system did; however let's demonstrate why that would be problamitic.  
        Consider the following text  
            "The  Bank of Chicago announced earnings..."  
            Let the italic part of the text be the enitity we want to recognize and let the bolded part of the text be the entitiy that our model identified.  
            The Precision/Recall/F1 metrics would penalize the model twice once as a false positive for having picked an incorrect entitiy name and again as a false negative for not having picked the actual entitiy name.   
            However we notice that our system actually picked $$3/4$$ths of the actual entity name to be recognized.  
         This leads us seeking an evaluation metric that awards partial credit for this task.  
        The MUC Scorer is one such metric for giving partial credit.  
       Albeit such complications and issues with the metrics described above the field has unfortunately continued using the F1 Score as a metric for NER systems due to the complexity of formulating a metric that  gives partial credit.  


Sequence Models for Named Entity Recognition

1. Approach
        Training   
            1. Collect a set of representative training documents
            2. Label each token for its entity class or other O
            3. Design feature extractors appropriate to the text and classes
            4. Train a sequence classifier to predict the labels from the data
        Testing  
            1. Receive a set of testing documents
            2. Run sequence model inference to label each token  
            3. Appropriately output the recognized entities   

2. Encoding Classes for Sequence Labeling
       There are two common ways to encode the classes  
         IO Encoding this encoding will only encode only the entitiy of the token disregarding its order/position in the text PER.  
             For $$C$$ classes IO produces $$C+1$$  labels.  
         IOB Encoding this encoding is similar to IO encoding however it also keeps track to whether the token is the beginning of an entitiy name B PER or a continuation of such an entity name I PER.  
             For $$C$$ classes IO produces $$2C+1$$  labels.
       The IO encoding thus is much lighter and thus allows the algorithm to run much faster than the IOB encoding.  
        Moreover in practice the issue presented for IO encoding rarely occurs and is only limited to instances where the entities that occur next to each other are the same entity.   
        IOB encoded systems also tend to not learn quite as well due to the huge number of labels and are usually still prone to the same issues
       Thus due to the reasons mentioned above the IO Encoding scheme is the one most commonly used.            

3. Features for Sequence Labeling
        Words   
             Current word like a learned dictionary
             Previous/Next word context
         Other kinds of Inferred Linguistic Classification    
             Part of Speech Tags POS Tags   
         Label Context   
             Previous and perhaps next label
            This is usually what allows for sequence modeling 
       Other useful features  
         Word Substrings usually there are substrings in words that are categorical in nature  
             Examples  
                "oxa"  > Drug  
                "noun noun"  > Movie  
                "noun field"  > usually place  
         Word Shapes the idea is to map words to simplified representations that encode attributes such as  
           length capitalization numerals Greek Letters internal punctuation etc.  
            Example  
                The representation below shows only the  two letters and the last two letters; for everything else it will add the capitalization and the special characters and for longer words it will represent them in set notation.   

                | Varicella zoster | Xx xxx  
                | mRNA | xXXX   
                | CPA1 | XXXd  



Maximum Entropy Sequence Models

1. Maximum Entropy Conditional Markov Model
       is a discriminative graphical model for sequence labeling that combines features of hidden Markov models HMMs and maximum entropy MaxEnt models.  
       It is a discriminative model that extends a standard maximum entropy classifier by assuming that the unknown values to be learned are connected in a Markov chain rather than being conditionally independent of each other.
       It makes a single decision at a time conditioned on evidence from observations and previous decisions.  
    A larger space of sequences is usually explored via search.  

2. Inference

3. Exploring the Sequence Space Search Methods
        Beam Inference  
             Algorithm  
                 At each position keep the top $$k$$ complete sequences.  
                 Extend each sequence in each local way.  
                 The extensions compete for the $$k$$ slots at the next position.  
             Advantages   
                 Fast. Beam sizes of 3 5 are almost as good as exact inference in many cases.  
                 Easy. Implementation does not require dynamic programming.  
             Disadvantages    
                 Inexact. The globally best sequence can fall off the beam.   
        Viterbi Inference  
             Algorithm  
                 Dynamic Programming or Memoization.  
                 Requires small window of state influence eg. past two states are relevant  
             Advantages  
                 Exact. the global best sequence is returned.  
             Disadvantages  
                 Hard. Harder to implement long distance state state interactions. 


4. Conditional Random Fields CRFs
       are a type of discriminative undirected probabilistic graphical model.   
       They can take context into account; and are commonly used to encode known relationships between observations and construct consistent interpretations.


 Discriminative Models in NLP <br \> Maxent Models and Discriminative Estimation


Generative vs Discriminative Models

Given some data $$\{dc\}$$ of paired observations $$d$$ and hidden classes $$c$$  

1. Generative Joint Models
       Generative Models are Joint Models.  
       Joint Models place probabilities $$\leftPcd\right$$ over both the observed data and the "target" hidden variables that can only be computed from those observed.  
       Generative models are typically probabilistic specifying a joint probability distribution $$Pdc$$ over observation and target label values  
    and tries to Maximize this joint Likelihood.  
        Choosing weights turn out to be trivial chosen as the relative frequencies.  
       Examples  
         n gram Models
         Naive Bayes Classifiers  
         Hidden Markov Models HMMs
         Probabilistic Context Free Grammars PCFGs
         IBM Machine Translation Alignment Models

2. Discriminative Conditional Models
       Discriminative Models are Conditional Models.  
       Conditional Models provide a model only for the "target" hidden variabless.  
        They take the data as given and put a probability $$\leftPc \| d\right$$ over the "target" hidden structures given the data.  
       Conditional Models seek to Maximize the Conditional Likelihood.  
        This maximization task is usually harder to do.  
       Examples  
         Logistic Regression
         Conditional LogLinear/Maximum Entropy Models  
         Condtional Random Fields  
         SVMs  
         Perceptrons  

3. Generative VS Discriminative Models
       Basically Discriminative Models infer outputs based on inputs  
        while Generative Models generate both inputs and outputs typically given some hidden paramters.  
       However notice that the two models are usually viewed as complementary procedures.  
        One does not necessarily outperform the other in either classificaiton or regression tasks.   


Feature Extraction for Discriminative Models in NLP

1. Features Intuitively
       Features $$f$$ are elementary pieces of evidence that link aspects od what we observe $$d$$ with a category $$c$$ that we want to predict.  

2. Features Mathematically
       A Feature $$f$$ is a function with a bounded real value.  
       $$f  \ C \times D \rightarrow \mathbf{R}$$


3. Models and Features
       Models will assign a weight to each Feature  
         A Positive Weight votes that this configuration is likely Correct.  
         A Negative Weight votes that this configuration is likely InCorrect. 

4. Feature Expectations
        Empirical  Expectation count  
       $$E{\text{emp}}fi = \sum{cd\in\text{observed}CD} ficd$$    
        Model Expectation  
       $$Efi = \sum{cd\inCD} Pcdficd$$
    The two Expectations represent the Actual and the Predicted Counts of a feature firing respectively.  

5. Features in NLP
       In NLP features have a particular form.  
        They consist of  
         Indicator Function a boolean matching function of properties of the input  
         A Particular Class specifies some class $$cj$$  
     $$ficd \cong \Phid \wedge c=cj = \{0 \vee 1\}$$  
       where $$\Phid$$ is a given predicate on the data $$d$$ and $$cj$$ is a particular class.  
    Basically each feature picks out a data subset and suggests a label for it.  


Feature Based Linear Classifiers  

1. Linear Classifiers Classification
        We have a Linear Function from the feature sets $$\{fi\}$$ to the classes $$\{c\}$$  
         Assign Weights $$\lambdai$$ to each feature $$fi$$  
         Consider each class for an observed datum $$d$$  
         Features Vote with their weights    
           $$\text{vote}c = \sum \lambdai ficd$$  
         Classification  
            choose the class $$c$$ which Maximizes the vote $$\sum \lambdai ficd$$  


2. Exponential Models
       Exponential Models make a probabilistic model from the linear combination $$\sum\lambdaificd$$
        Making the Value Positive    
       $$\sum\lambdaificd \rightarrow e^{\sum\lambdaificd}$$   
        Normalizing the Value Making a Probability  
      $$e^{\sum\lambdaificd} \rightarrow \dfrac{e^{\sum\lambdaificd}}{\sum{c \in C} e^{\sum\lambdaificd}}$$ 
       $$\implies$$
       $$Pc \| d \vec{\lambda} = \dfrac{e^{\sum\lambdaificd}}{\sum{c \in C} e^{\sum\lambdaificd}}$$
       The function $$Pc \| d\vec{\lambda}$$ is referred to as the Soft Max function.  
       Here the Weights are the Paramters of the probability model combined via a Soft Max function.
       Learning  
         Given this model form we want to choose paramters $$\{\lambdai\}$$ that Maximize the Conditional Likelihood of the data according to this model i.e. the soft max func..    
       Exponential Models construct not onlt classifications but also Probability Distributions over the classifications.
       Examples  
         Log Linear Model
         Max Entropy MaxEnt Model
         Logistic Regression  
         Gibbs Model 

3. Exponential Models Training | Maximizing the Likelihood
       The Likelihood Value  
         The log conditional likelihood of a maxend model is a function of the iid data $$CD$$ and the parameters $$\lambda$$  
       $$log PC \| D\lambda = log \prod{cd \in CD} Pc \| d\lambda = \sum{cd \in CD} log Pc \| d\lambda$$
        If there aren't many values of $$c$$ it's easy to calculate  
       $$log Pc \| d\lambda = \sum{cd \in CD} log \dfrac{e^{\sumi \lambdaificd}}{\sumc e^{\sumi \lambdaificd}}$$
        We can separate this into two components  
       $$log Pc \| d\lambda = \sum{cd \in CD} log e^{\sumi \lambdaificd}  \sum{cd \in CD} log \sumc' e^{\sumi \lambdaific'd}$$
       $$\implies$$
       $$log PC \| D \lambda = N\lambda  M\lambda$$
        The Derivative of the Numerator is easy to calculate  
       $$\dfrac{\partial N\lambda}{\partial \lambdai} = \dfrac{\partial \sum{cd \in CD} log e^{\sumi \lambdaificd}}{\partial \lambdai}
    = \dfrac{\partial \sum{cd \in CD} \sumi \lambdaificd}{\partial \lambdai} 
    = \sum{cd \in CD} \dfrac{\partial \sumi \lambdaificd}{\partial \lambdai} 
    = \sum{cd \in CD} ficd$$
       The derivative of the Numerator is the Empirical Expectation $$E{\text{emp}}fi$$
        The Derivative of the Denominator  
       $$\dfrac{\partial M\lambda}{\partial \lambdai}
    = \dfrac{\partial \sum{cd \in CD} log \sumc' e^{\sumi \lambdaific'd}}{\partial \lambdai}
    = \sum{cd \in CD} \sumc' Pc' \| d \lambdafic' d$$
       The derivative of the Denominator is equal to the Predicted Expectation count $$Efi \lambda$$
     Thus the derivative of the log likelihood is  
       $$\dfrac{\partial log PC \| D \vec{\lambda}}{\partial \lambdai} = \text{Actual Count}fi C  \text{Predicted Count}fi \vec{\lambda}$$
        Thus the optimum parameters are those for which rach feature's predicted expectation equals its empirical expectation.  
        The Optimum Distribution is always  
             Unique parameters need not be unique
             Exists if feature counts are from actual data
        These models are called Maximum Entropy Maxent Models because we find the model having the maximum entropy and satisfying the constraints  
       $$Epfj = E\hat{p}fj \\\ \forall j$$
        Finally to find the optimal parameters $$\lambda1 \dots \lambdad$$ one needs to optimize maximize the log liklehood or equivalently minimize the  ve loglik.  
            One can do that in variety of was using optimization methods.  
        Common Optimization Methods  
             Stochastic Gradient Descent
             Iterative Proportional Fitting Methods  
                 Generalized Iterative Scaling GIS
                 Improved Iterative Scaling IIS
             Conjugate Gradient CG + Preconditioning
             Quasi Newton Methods   Limited Memory Variable Metric LMVM  
                 L BFGS
                This one is the most commonly used.  



 Text Classification


Introduction and Definitions

1. Text Classification
        The task of assigning a piece of text to one or more classes or categories.  

2. Applications
     Spam Filtering discerning spam emails form legitimate emails.  
     Email Routing sending an email sento to a genral address to a specfic affress based on the topic.  
     Language Identification automatiacally determining the genre of a piece of text.  
     Readibility Assessment determining the degree of readability of a piece of text.  
     Sentiment Analysis determining the general emotion/feeling/attitude of the author of a piece of text.  
     Authorship Attribution determining which author wrote which piece of text.  
     Age/Gender Identification determining the age and/or gender of the author of a piece of text.      

3. Classification Methods
        Hand CodedRules Based Algorithms use rules based on combinations of words or other features.   
             Can have high accuracy if the rules are carefully refined and maintained by experts.  
             However building and maintaining these rules is very hard.  
         Supervised Machine Learning using an ML algorithm that trains on a training set of document class elements to train a classifier.  
             Types of Classifiers  
                 Naive Bayes  
                 Logistic Regression
                 SVMs
                 K NNs  
   

The Naive Bayes Classifier

1. Naive Bayes Classifiers
    
       The Probabilistic Model  
        Abstractly naive Bayes is a conditional probability model given a problem instance to be classified represented by a vector $${\displaystyle \mathbf{x} =x{1}\dots x{n}}=x{1}\dots x{n}$$ representing some n features independent variables it assigns to this instance probabilities  
       $${\displaystyle pC{k}\mid x{1}\dots x{n}\}$$
       for each of the $$k$$ possible outcome or classes $$Ck$$.  
       Now using Bayes' Theorem we decompose the conditional probability as  
       $${\displaystyle pC{k}\mid \mathbf {x} ={\frac {pC{k}\ p\mathbf {x} \mid C{k}}{p\mathbf {x} }}\}$$
       Or equivalenty and more intuitively  
       $${\displaystyle {\mbox{posterior}}={\dfrac{\text{prior} \times \text{likelihood}}{\text{evidence}}}\}$$  
       We can disregard the Denomenator since it does not depend on the classes $$C$$ making it a constant.  
       Now using the Chain Rule for repeated application of the conditional probability   the joint probability model can be rewritten as  
       $$pC{k}x{1}\dots x{n}\ = px{1}\mid x{2}\dots x{n}C{k}px{2}\mid x{3}\dots x{n}C{k}\dots px{n 1}\mid x{n}C{k}px{n}\mid C{k}pC{k}$$  
       Applying the naive conditional independence assumptions  
          i.e. assume that each feature $${\displaystyle x{i}}$$ is conditionally independent of every other feature $${\displaystyle x{j}} $$ for $${\displaystyle j\neq i}$$ given the category $${\displaystyle C}$$  
       $$\implies  
    {\displaystyle px{i}\mid x{i+1}\dots x{n}C{k}=px{i}\mid C{k}\}.$$
       Thus we can write the join probability model as  
       $${\displaystyle pC{k}\mid x{1}\dots x{n}={\frac {1}{Z}}pC{k}\prod {i=1}^{n}px{i}\mid C{k}}$$
       Where $${\displaystyle Z=p\mathbf {x} =\sum {k}pC{k}\ p\mathbf {x} \mid C{k}}$$ is a constant scaling factor a function of the known feature variables.  


       Thus the classifier becomes  
       $${\displaystyle {\hat {y}}={\underset {k\in \{1\dots K\}}{\operatorname {argmax} }}\ pC{k}\displaystyle \prod {i=1}^{n}px{i}\mid C{k}.}$$
       A function that assigns a class label $${\displaystyle {\hat {y}}=C{k}}$$ for some $$k$$.

2. Multinomial Naive Bayes
       With a multinomial event model samples feature vectors represent the frequencies with which certain events have been generated by a multinomial $${\displaystyle p{1}\dots p{n}}$$ where $${\displaystyle p{i}}$$ is the probability that event $$i$$ occurs.  
       The likelihood of observing a feature vector histogram $$\mathbf{x}$$ is given by  
       $${\displaystyle p\mathbf {x} \mid C{k}={\frac {\sum {i}x{i}!}{\prod {i}x{i}!}}\prod {i}{p{ki}}^{x{i}}}$$  
       The multinomial naive Bayes classifier becomes a linear classifier when expressed in log space  
       $${\displaystyle {\begin{aligned}\log pC{k}\mid \mathbf {x} &\varpropto \log \leftpC{k}\prod {i=1}^{n}{p{ki}}^{x{i}}\right&=\log pC{k}+\sum {i=1}^{n}x{i}\cdot \log p{ki}&=b+\mathbf {w} {k}^{\top }\mathbf {x} \end{aligned}}}$$  
       where $${\displaystyle b=\log pC{k}}$$ and $${\displaystyle w{ki}=\log p{ki}}$$.  

3. Bag of Words
       The bag of words model or vector space model is a simplifying representation of text/documents.  
       A text is represented as the bag Multi Set of its words with multiplicity disregarding any grammatrical rules and word orderings.

4. The Simplifying Assumptions Used
        Bag of Words we assume that the position of the words does not matter.  
         Naive Independence the feature probabilities are indpendenet given a class $$c$$.   

5. Learning the Multi Nomial Naive Bayes Model
        The Maximum Likelihood Estimate we simply use the frequencies in the date.
             $$\hat{P}cj = \dfrac{\text{doc count}C=cj}{N\text{doc}}$$  
            The Prior Probability of a document being in class $$cj$$ is the fraction of the documents in the training data that are in class $$cj$$.  
             $$\hat{P}wi | ci = \dfrac{\text{count}wicj}{\sum{w \in V} \text{count}w cj}$$  
            The likelihood of the word $$wi$$ given a class $$cj$$ is the fraction of the occurunces of the word $$wi$$ in class $$cj$$ over all words in the class.    
        The Problem with Maximum Likelihood  
            If a certain word occurs in the test set but not in the training set the likelihood of that word given the equation above will be set to $$0$$.  
            Now since we are multiplying all the likelihood terms together the MAP estimate will be set to $$0$$ as well regardless of the other values.  

6. Solutions to the MLE Problem
       Usually the problem of reducing the estimate to zero is solved by adding a regularization technique known as smoothing. 

7. Lidstone Smoothing additive smoothing
       is a technique used to smooth categorical data as the following  
       Given an observation vector $$x = x1 \ldots xd$$ from a multinomial distribution with $$N$$ trials a smoothed version of the data produces the estimators  
       $${\hat {\theta }}{i}={\frac {x{i}+\alpha }{N+\alpha d}}\qquad i=1\ldots d$$


8. Laplace Smoothing
       is a special case of additive smoothing Lidstone Smoothing with $$\alpha = 1$$  
       $${\hat {\theta }}{i}={\frac {x{i}+1 }{N+ d}}\qquad i=1\ldots d$$   
9. The Algorithm
        Extract the Vocabulary from the trianing data  
         Calculate $$Pcj$$ terms  
             For each $$cj \in C$$ do  
                 $$\text{docs}j \leftarrow$$ all docs with class $$=cj$$  
                 $$Pcj \leftarrow \dfrac{\|\text{docs}j\|}{\|\text{total # docs}\|}$$
         Calculate $$Pwk \| cj$$ terms  
             $$\text{Text}j \leftarrow$$ single doc containing all $$\text{docs}j$$  
             For each word $$wk \in$$ Vocab.  
                 $$nk \leftarrow$$ # of occurunces of $$wk \in \text{Text}j$$  
                 $$Pwk \| cj \leftarrow \dfrac{nk + \alpha}{n + \alpha \|Vocab.\|}$$  
10.Summary
        Very fast  
         Low storage requirements
         Robust to Irrelevant Features  
             Irrelevant features cancel each other out.  
         Works well in domains with many equally important features  
             Decision Trees suffer from fragmentation in such cases  especially if there is little data.  
         It is Optimal if the independence conditions hold.  


Evaluation of Text Classification  

1. The $$2x2$$ Contingency Table
       | | correct Spam | not correct not Spam    
        selected Spam | tp | fp  
        not selected not Spam | fn | tn  

2. Accuracy
       $$ \text{Acc} = \dfrac{\text{tp} + \text{tn}}{\text{tp} + \text{fp} + \text{fn} + \text{tn}}$$
       The Problem  
        Accuracy can be easily fooled i.e. produce a very high number in a scenario where the number of occurrences of a class we desire is much less than the data we are searching.  

3. Precision positive predictive value PPV
       is the fraction of relevant instances among the retrieved instances.  
       Equivalently  
        the % of selected items that are correct.    

       $${\displaystyle {\text{Precision}}={\frac {tp}{tp+fp}}\}$$     

4. Recall True Positive Rate Sensitivity
       Also referred to as the true positive rate or sensitivity. 
        is the fraction of relevant instances that have been retrieved over the total amount of relevant instances.  
       Equivalently  
        the % of correct items that are selected.  
       $${\displaystyle {\text{Recall}}={\frac {tp}{tp+fn}}\}$$

5. The Trade Off
       Usually the two measures discusses above have an inverse relation between them due to the quantities they measure.  

6. The F measure
       is a measure that combines precision and recall.  
       It is the harmonic mean of precision and recall  
       $${\displaystyle F=2\cdot {\frac {\mathrm {precision} \cdot \mathrm {recall} }{\mathrm {precision} +\mathrm {recall} }}}$$ 
   

General Discussion of Issues in Text Classification

1. Very Little Data
        Use Naive Bayes  
             Naive Bayes is a "high bias" algorithm; it tends to not overfit the data.  
         Use Semi Supervised Learning  
             Try Bootstrapping or EM over unlabeled documents  

2. Reasonable Amount of Data
        Use  
             SVM  
             Regularized Logistic Regression  
             try Decision Trees  

3. Huge Amount of Data
       Be careful of the run time  
         SVM slow train time  
         KNN slow test time  
         Reg. Log. Regr. somewhat faster  
         Naive Bayes might be good to be used.

4. Underflow Prevention
       Problem Due to the "multiplicative" nature of the algorithms we are using we might run into a floating point underflow problem.  
       Solution transfer the calculations to the log space where all the multiplications are transformed into additions.  

5. Tweaking the Performance of the Algorithms
        Utilize Domain Specific features and weights  
         Upweighting counting a word as if it occurred multiple times.  
   


 Sentiment Analysis


Introduction

1. Sentiment Analysis
       is the automated identification and quantification of affective states and subjective information in textual data.

2. Applications
       

3. Formulating the Problem
       Tasks to Extract  
         Holder source of the attitude.  
         Target aspect of the attitude.  
         Type of the attitude.  
       Input  
         Text Contains the attitude  
             Sentence Analysis  
             Entire Document Analysis  
         main    


Algorithms

1. Binarized Boolean Feature Multinomial Naive Bayes

       However the features Tokens used in this algorithm are counted based on occurrence rather than frequency  
        i.e. if a certain word occurs in the text then its count is always one regardless of the number of occurrences of the word in the text.  
       Justification The reason behind the binarized version is evident intuitively in the nature of the problem.  
        The sentiment behind a certain piece of text is usually represented in just one occurrence of a word that represents that sentiment e.g. "Fantastic" rather than how many times did that word actually appear in the sentence.  
2. Better Algorithms
        Max Entropy  
         SVMs



Sentiment Lexicons

1. Sentiment Lexicons
       Specific key words that are related to specific polarities.  
        They are much more useful to be used instead of analyzing all of the words tokens in a piece of text. 



 Introduction to  NLP


Introduction

1. Problems in NLP
        Question Answering QA 
         Information Extraction IE    
         Sentiment Analysis  
         Machine Translation MT  
         Spam Detection  
         Parts of Speech POS Tagging  
         Named Entity Recognition NER
         Conference Resolution  
         Word Sense Disambiguation WSD  
         Parsing  
         Paraphrasing  
         Summarization  
         Dialog  

2. mostly Solved Problems in NLP
        Spam Detection  
         Parts of Speech POS Tagging  
         Named Entity Recognition NER  


3. Within Reach Problems
        Sentiment Analysis  
         Conference Resolution    
         Word Sense Disambiguation WSD  
         Parsing  
         Machine Translation MT  
         Information Extraction IE    


4. Open Problems in NLP
        Question Answering QA   
         Paraphrasing  
         Summarization  
         Dialog  

5. Issues in NLP why nlp is hard?
        Non Standard English "Great Job @ahmedbadary! I luv u 2!! were SOO PROUD of dis."  
         Segmentation Issues "New York New Haven" vs "New York New Haven"  
         Idioms "dark horse" "getting cold feet" "losing face"  
         Neologisms "unfriend" "retweet" "google" "bromance"  
         World Knowledge "Ahmed and Zach are brothers" "Ahmed and Zach are fathers"    
         Tricky Entity Names "Where is Life of Pie playing tonight?" "Let it be was a hit song!"  

6. Tools we need for NLP
        Knowledge about Language.  
         Knowledge about the World.   
         A way to combine knowledge sources.  

7. Methods
       In general we need to construct Probabilistic Models built from language data.    
       We do so by using rough text features.  
        All the names models methods and tools mentioned above will be introduced later as you progress in the text.  



 Text Processing


Introduction and Definitions

1. Text Normalization
       Every NLP process starts with a task called Text Normalization.  
       Text Normaliization is the process of transforming text into a single canonical form that it might not have had before.  
       Importance Normalizing text before storing or processing it allows for separation of concerns since input is guaranteed to be consistent before operations are performed on it.  
       Steps  
        1. Segmenting/Tokenizing words in running text.  
        2. Normalizing word formats.  
        3. Segmenting sentences in running text.  

0. Methods for Normalization
        Case Folding reducing all letters to lower case.  
            Possibly with the exception of capital letters mid sentence.  
         Lemmatization reducing inflections or variant forms to base form.  
            Basically finding the correct dictionary headword form.  

9. Morphology
       The study of words how they are formed and their relationship to other words in the same language.  
        Morphemes the small meaningfuk units that make up words.  
         Stems the core meaning bearing units of words.  
         Affixes the bits and pieces that adhere to stems often with grammatical functions.     


2. Word Equivalence in NLP
       Two words have the same  
         Lemma if they have the same  
             Stem  
             POS  
             Rough Word Sense  
            cat & cats  > same Lemma  
         Wordform if they have the same  
             full inflected surface form  
            cat & cats  > different wordforms   

3. Types and Tokens
        Type an element of the vocabulary.  
            It is the class of all tokens containing the same character sequence.  
         Token an instance of that type in running text.  
            It is an instance of a sequence of characters that are grouped together.  

4. Notation
        N = Number of Tokens.  
         V = Vocabulary = set of Types.     
         $$\|V\|$$ = size/cardinality of the vocabulary.  

5. Growth of the Vocabulary
       Church and Gale 1990 suggested that the size of the vocabulary grows larger than the square root of the number of tokens in a piece of text  
       $$\|V\| > \mathcal{O}N^{1/2}$$  
  


Tokenization

1. Tokenization
       It is the task of chopping up a character sequence and a defined document unit into pieces called tokens.  
        It may involve throwing away certain characters such as punctuation.  

2. Methods for Tokenization
        Regular Expressions  
         A Flag Specific squences of characters.  
         Delimiters pecific separating characters.  
         Dictionary exlicit definitions by a dictionary.     


3. Categorization
       Tokens are categorized by  
         Character Content  
         Context  
    within a data stream.  
       Categories  
         Identifiers names the programmer chooses  
         keywords names already in the programming language.  
         Operators symbols that operate on arguments and produce results.    
         Grouping Symbols 
         Data Types
       Categories are used for post processing of the tokens either by the parser or by other functions in the program.  
   


Word Normalization Stemming

1. Stemming
       is the process of reducing inflected or sometimes derived words to their word stem base or root form.  
       The stem need not map to a valid root in the language.  
    Basically Stemming is a crude chopping of affixes
    Example "automate" "automatic" "automation"  > "automat".  

2. Porter's Algorithm
       The most common English stemmer.  
       It is an iterated series of simple replace rules.  


3. Algorithms
        The Production Technique we produce the lookup table that is used by a naive stemmer semi automaically.  
         Suffix Stripping Algorithms those algorithms avoid using lookup tables; instead they use a small list of rules to navigate through the text and find theroot forms from word forms.  
         Lemmatisation Algorithms the lemmatization process starts determining the part of speech of a word and then applying normalization rules to for each part of speech.   
         Stochastic Algorithms those algorithms are trained on a table of root form to inflected form relations to develop a probablistic model.  
           The model looks like a set of rules similar to the suffic stripping list of rules.      


Sentence Segmentation

1. Sentence Segmentation
       It is the problem of diving a piece of text into its component sentences.  

2. Identifiers
       Identifiers such as "!" "?" are unambiguous; they usually signify the end of a sentence.  
       The period "." is quite ambiguous since it can be used in other ways such as in abbreviations and in decimal number notation.  

3. Dealing with Ambiguous Identifiers
       One way of dealing with ambiguous identifiers is by building a Binary Classifier.  
        On a given occurrence of a period the classifier has to decide between one of "Yes this is the end of a sentence" or "No this is not the end of a sentence".  
       Types of Classifiers  
         Decision Trees  
         Logistic Regression  
         SVM  
         Neural Net  
       Decision Trees are a common classifier used for this problems.   



 Reinforcement Learning


Deep Learning Generalization





Concepts Notes and Observations  

 Three Schools of Learning 1 Bayesians   2 Kernel People    3 Frequentists 
 Bayesians' INCORRECTLY claimed that  
     Highly over parametrised models fitted via maximum likelihood can't possibly work they will overfit won't generalise etc.  
     Any model with infinite parameters should be strictly better than any large but finite parametric model.  
        e.g. nonparametric models like kernel machines are a principled way to build models with effectively infinite number of parameters  
 


1. Casting ML Algorithms as Bayesian Approximations
     Classical ML  
         L1 regularisation is just MAP estimation with sparsity inducing priors  
         SVMS support vector machines are just the wrong way to train Gaussian processes  

     DL  

        
         


     Deep Nets memorize 

2. Why do Deep Nets Generalize?
    One possibility is "because they are really just an approximation to Bayesian machine learning."  Ferenc  

     SGD  
        SGD could be responsible for the good generalization capabilities of Deep Nets.  
         SGD finds Flat Minima.  
             A Flat Minima is a minima where the Hessian  and consequently the inverse Fisher information matrix  has small eigenvalues.  
             Flat might be better than sharp minima  
                If you are in a flat minimum there is a relatively large region of parameter space where many parameters are almost equivalent inasmuch as they result in almost equally low error. Therefore given an error tolerance level one can describe the parameters at the flat minimum with limited precision using fewer bits while keeping the error within tolerance. In a sharp minimum you have to describe the location of your minimum very precisely otherwise your error may increase by a lot.  

             And it may be because SGD biases learning towards flat minima rather than sharp minima.  



         One conclusion is The reason deep networks work so well and generalize at all is not just because they are some brilliant model but because of the specific details of how we optimize them.  
            Stochastic gradient descent does more than just converge to a local optimum it is biased to favour local optima with certain desirable properties resulting in better generalization.  
         Is SGD Bayesian?  
             Some work 

             Flat Minima is Bayesian  




                

    
    
    Indeed the empirical evidence about generalization in deep neural networks with a huge number of weights is that they generalize better than the theory would predict. This is not just in terms of the loose bounds that the theory provides. The performance is better than even the "tight" rules of thumb that were based on the theory and worked in practice.

    This is not the  time this happens in ML. When boosting was the method of choice generalization was better than it should be. Specifically there was no overfitting in cases where the model complexity was going up and overfitting would be expected. In that case a theoretical approach to explain the phenomenon based on a cost function other than 𝐸𝑖𝑛 in sample error was advanced. It made sense but it didn't stand up to scrutiny as minimizing that cost function directly instead of letting it be minimized through the specific structure of the AdaBoost algorithm for instance suffered from the usual overfitting. There was no conclusive verdict about how AdaBoost avoids overfitting. There were bits and pieces of intuition but it is difficult to tell whether that was explanation or rationalization.

    In the case of neural networks there have also been efforts to explain why the performance is better. There are other approaches to generalization e.g. based on "stability" of learning that were invoked. However the theoretical work on stability was based on perturbation of the training set that does not lead to a significant change in the final hypothesis. The way stability is discussed in the results I have seen in neural networks is based on perturbation of the weights that does not lead to a significant change. It thus uses the concept of stability rather than the established theoretical results to explain why generalization is good. In fact there are algorithms that deliberately look for a solution that has this type of stability as a way to get good generalization a regularization of sorts.

    It is conceivable that the structure of deep neural networks similar to the case of AdaBoost tends to result in better generalization than the general theory would indicate. To establish that we need to identify what is it about the structure that makes this happen. In comparison if you study SVM as a model without getting into the notion of support vectors you will encounter "inexplicable" good generalization. Once you know about how the number of support vectors affects generalization the mystery is gone.

    Let me conclude by emphasizing that the VC theory is not violated in any of these instances since the theory only provides an upper bound. Those cases show a much better performance for particular models but the performance is still within the theoretical bound. What would be a breakthrough is another better bound that is applicable to an important class of models. For example the number of parameters in deep neural networks is far bigger than previous models. If better generalization bounds can be proven for models with huge number of parameters for instance that would be quite a coup.  
    



    Notes  
    































 Representation Learning












Representation Learning

1. Representation Learning
    Representation Learning Feature Learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.  
    This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task.  


    Hypothesis  Main Idea  
    The core hypothesis for representation learning is that the unlabeled data can be used to learn a good representation.  
    
    
    Types  
    Representation learning can be either supervised or unsupervised.  


    Representation Learning Approaches  
    There are various ways of learning different representations  
    
     Probabilistic Models the goal is to learn a representation that captures the probability distribution of the underlying explanatory features for the observed input. Such a learnt representation can then be used for prediction.  
     Deep Learning the representations are formed by composition of multiple non linear transformations of the input data with the goal of yielding abstract and useful representations for tasks like classification prediction etc.  


    Representation Learning Tradeoff  
    Most representation learning problems face a tradeoff between preserving as much information about the input as possible and attaining nice properties such as independence.  


    The Problem of Data Semi Supervised Learning\  
    We often have very large amounts of unlabeled training data and relatively little labeled training data. Training with supervised learning techniques on the labeled subset often results in severe overfitting. Semi supervised learning offers the chance to resolve this overfitting problem by also learning from the unlabeled data. Specifically we can learn good representations for the unlabeled data and then use these representations to solve the supervised learning task.  


    Learning from Limited Data  
    Humans and animals are able to learn from very few labeled examples.   
    Many factors could explain improved human performance — for example the brain may use very large ensembles of classifiers or Bayesian inference techniques.  
    One popular hypothesis is that the brain is able to leverage unsupervised or semi supervised learning.  

    
    Motivation/Applications  
    
    1. ML tasks such as classification often require input that is mathematically and computationally convenient to process.  
        However real world data such as images video and sensor data has not yielded to attempts to algorithmically define specific features.  
    2. Learning good representations enables us to perform certain specific tasks in a more optimal manner.  
         E.g. linked lists $$\implies$$ $$\mathcal{O}n$$ insertion \| red black tree $$\implies$$ $$\mathcal{O}\log n$$ insertion.  
         
             Goal Learn Portuguese
             For 1 month you listen to Portuguese on the radio this is unlabeled data
             You develop an intuition for the language phrases and grammar a model in your head
             It is easier to learn now from a tutor because you have a better higher representation of the data/language    

    3. Representation Learning is particularly interesting because it provides one way to perform unsupervised and semi supervised learning.  
    4. Feature Engineering is hard. Representation Learning allows us to avoid having to engineer features manually.  
    5. In general representation learning can allow us to achieve multi task learning transfer learning and domain adaptation through shared representations.  


    The Quality of Representations  
    Generally speaking a good representation is one that makes a subsequent learning task easier.  
    The choice of representation will usually depend on the choice of the subsequent learning task.  


    Success of Representation Learning  
    The success of representation learning can be attributed to many factors including  
    
     Theoretical advantages of distributed representations Hinton et al. 1986  
     Theoretical advantages of deep representations Hinton et al. 1986   
     The Causal Factors Hypothesis a general idea of underlying assumptions about the data generating process in particular about underlying causes of the observed data.  



    Representation Learning Domain Applications  
    
     Computer Vision CNNs.  
     Natural Language Processing Word Embeddings.  
     Speech Recognition Speech Embeddings.  



    
     “What is a good representation?”   
         Generally speaking a good representation is one that makes a subsequent learning task easier.  
            The choice of representation will usually depend on the choice of the subsequent learning task.  

     “What makes one representation better than another?”   
         Causal Factors Hypothesis  
            An ideal representation is one in which the features within the representation correspond to the underlying causes of the observed data with separate features or directions in feature space corresponding to different causes so that the representation disentangles the causes from one another.  
             Why  
                 Ease of Modeling A representation that cleanly separates the underlying causal factors is also one that is easy to model.  
                     For many AI tasks the two properties coincide once we are able to obtain the underlying explanations for the observations it generally becomes easy to isolate individual attributes from the others.  
                     Specifically if a representation $$\boldsymbol{h}$$ represents many of the underlying causes of the observed $$\boldsymbol{x}$$ and the outputs $$\boldsymbol{y}$$ are among the most salient causes then it is easy to predict $$\boldsymbol{y}$$ from $$\boldsymbol{h}$$.  
         Summary of the Causal Factors Hypothesis  
            An ideal representation is one in which the features within the representation correspond to the underlying causes of the observed data with separate features or directions in feature space corresponding to different causes so that the representation disentangles the causes from one another especially those factors that are relevant to our applications.  

     “What is a "salient factor"?”   
         A "salient factor" is a causal factor latent variable that explains well the observed variations in $$X$$.  
             

             What makes a feature "salient" for humans?  
                It could be something really simple like correlation or predictive power.  
                Ears are a salient feature of Humans because in a majority of cases presence of one implies presence of another.  
             Discriminative features as salient features  
                Note that in object detection case the predictive power is only measured in  
                ear $$\rightarrow$$ person direction not person $$\rightarrow$$ ear direction.  
                E.g. if your task was to discriminate between males and females presence of ears would not be a useful feature even though all humans have ears. Compare this to the pimples case in human vs dog classification pimples are a really good predictor of 'human' even though they are not a salient feature of Humans.  
                Basically I think discriminative =/= salient  



    Notes  
    
     Representation Learning can be done with both generative and discriminative models.  
     In DL representation learning uses a composition of transformations of the input data features to create learned features.  
    

    

7. Distributed Representation
    Distributed Representations of concepts are representations composed of many elements that can be set separately from each other.  
    
    Distributed representations of concepts are one of the most important tools for representation learning  
    
     Distributed representations are powerful because they can use $$n$$ features with $$k$$ values to describe $$k^{n}$$ different concepts.  
     Both neural networks with multiple hidden units and probabilistic models with multiple latent variables make use of the strategy of distributed representation.  
     Motivation for using Distributed Representations   
        Many deep learning algorithms are motivated by the assumption that the hidden units can learn to represent the underlying causal factors that explain the data.  
        Distributed representations are natural for this approach because each direction in representation space can correspond to the value of a different underlying configuration variable.  
     Distributed vs Symbolic Representations  
         Number of "Representable" Configurations  by example  
             An example of a distributed representation is a vector of $$n$$ binary features.  
                It can take $$2^{n}$$ configurations each potentially corresponding to a different region in input space.  
                

             An example of a symbolic representation is one hot representation^6 where the input is associated with a single symbol or category.  
                If there are $$n$$ symbols in the dictionary one can imagine $$n$$ feature detectors each corresponding to the detection of the presence of the associated category.  
                In that case only $$n$$ different configurations of the representation space are possible carving $$n$$ different regions in input space.  
                

                A symbolic representation is a specific example of the broader class of non distributed representations which are representations that may contain many entries but without significant meaningful separate control over each entry.  
         Generalization  
            An important related concept that distinguishes a distributed representation from a symbolic one is that generalization arises due to shared attributes between different concepts.  
             
                 As pure symbols “cat” and “dog” are as far from each other as any other two symbols.  
                    However if one associates them with a meaningful distributed representation then many of the things that can be said about cats can generalize to dogs and vice versa.  
                     For example our distributed representation may contain entries such as “hasfur” or “numberoflegs” that have the same value for the embedding of both “cat ” and “dog.”  
                        Neural language models that operate on distributed representations of words generalize much better than other models that operate directly on one hot representations of words section 12.4.  
                        Distributed representations induce a rich similarity space in which semantically close concepts or inputs are close in distance a property that is absent from purely symbolic representations.    

            Distributed representations induce a rich similarity space in which semantically close concepts or inputs are close in distance a property that is absent from purely symbolic representations.  
            \Analysis Generalization of Distributed Representations\
     
         Clustering methods including the $$k$$ means algorithm each input point is assigned to exactly one cluster.
         k nearest neighbors algorithms one or a few templates or prototype examples are associated with a given input. In the case of $$k>1$$ there are multiple values describing each input but they can not be controlled separately from each other so this does not qualify as a true distributed representation.  
         Decision trees only one leaf and the nodes on the path from root to leaf is activated when an input is given.
         Gaussian mixtures and mixtures of experts the templates cluster centers or experts are now associated with a degree of activation. As with the k nearest neighbors algorithm each input is represented with multiple values but those values cannot readily be controlled separately from each other.  
         Kernel machines with a Gaussian kernel or other similarly local kernel although the degree of activation of each “support vector” or template example is now continuous valued the same issue arises as with Gaussian mixtures.  
         Language or translation models based on n grams The set of contexts sequences of symbols is partitioned according to a tree structure of suffixes. A leaf may correspond to the last two words being w1 and w2 for example. Separate parameters are estimated for each leaf of the tree with some sharing being possible.  

    For some of these non distributed algorithms the output is not constant by parts but instead interpolates between neighboring regions. The relationship between the number of parameters or examples and the number of regions they can define remains linear.  
    

    Generalization of Distributed Representations{ }  
    We know that for distributed representations Generalization arises due to shared attributes between different concepts.  

    But an important question is  
    "When and why can there be a statistical advantage from using a distributed representation as part of a learning algorithm?"   
    
     Distributed representations can have a statistical advantage when an apparently complicated structure can be compactly represented using a small number of parameters.  
     Some traditional nondistributed learning algorithms generalize only due to the smoothness assumption which states that if $$u \approx v$$ then the target function $$f$$ to be learned has the property that $$fu \approx fv$$ in general.  
        There are many ways of formalizing such an assumption but the end result is that if we have an example $$x y$$ for which we know that $$fx \approx y$$ then we choose an estimator $$\hat{f}$$ that approximately satisfies these constraints while changing as little as possible when we move to a nearby input $$x+\epsilon$$.  
         This assumption is clearly very useful but it suffers from the curse of dimensionality in order to learn a target function that increases and decreases many times in many different regions1 we may need a number of examples that is at least as large as the number of distinguishable regions.  
            One can think of each of these regions as a category or symbol by having a separate degree of freedom for each symbol or region we can learn an arbitrary decoder mapping from symbol to value.  
            However this does not allow us to generalize to new symbols for new regions.  
     If we are lucky there may be some regularity in the target function besides being smooth.  
        For example a convolutional network with max pooling can recognize an object regardless of its location in the image even though spatial translation of the object may not correspond to smooth transformations in the input space.  

    Justifying Generalization in distributed representations  
    
     Geometric justification by analyzing binary linear feature extractors units  
        Let us examine a special case of a distributed representation learning algorithm that extracts binary features by thresholding linear functions of the input   
        
         Each binary feature in this representation divides $$\mathbb{R}^{d}$$ into a pair of half spaces.  
         The exponentially large number of intersections of $$n$$ of the corresponding half spaces determines the number of regions this distributed representation learner can distinguish.  
         The number of regions generated by an arrangement of $$n$$ hyperplanes in $$\mathbb{R}^{d}$$  
            By applying a general result concerning the intersection of hyperplanes Zaslavsky } 1975 one can show Pascanu et al 2014b that the number of regions this binary feature representation can distinguish is  
            $$\sum{j=0}^{d}\left\begin{array}{l}{n}  {j}\end{array}\right=O\leftn^{d}\right$$  
         Therefore we see a growth that is exponential in the input size and polynomial in the number of hidden units.  
         This provides a geometric argument to explain the generalization power of distributed representation  
            with $$\mathcal{O}n d$$ parameters for $$n$$ linear threshold features in $$\mathbb{R}^{d}$$ we can distinctly represent $$\mathcal{O}\leftn^{d}\right$$ regions in input space.  
             If instead we made no assumption at all about the data and used a representation with unique symbol for each region and separate parameters for each symbol to recognize its corresponding portion of $$\mathbb{R}^{d}$$ then  
                specifying $$\mathcal{O}\leftn^{d}\right$$ regions would require $$\mathcal{O}\leftn^{d}\right$$ examples.  
         More generally the argument in favor of the distributed representation could be extended to the case where instead of using linear threshold units we use nonlinear possibly continuous feature extractors for each of the attributes in the distributed representation.  
            The argument in this case is that if a parametric transformation with $$k$$ parameters can learn about $$r$$ regions in input space with $$k \ll r$$ and if obtaining such a representation was useful to the task of interest then we could potentially generalize much better in this way than in a non distributed setting where we would need $$\mathcal{O}r$$ examples to obtain the same features and associated partitioning of the input space into $$r$$ regions.  
            Using fewer parameters to represent the model means that we have fewer parameters to fit and thus require far fewer training examples to generalize well.  

     VC Theory justification  Fixed Capacity  
        
        
        The capacity remains limited despite being able to distinctly encode so many different regions.  
        For example the VC dimension of a neural network of linear threshold units is only $$\mathcal{O}w \log w$$ where $$w$$ is the number of weights Sontag 1998.  

        This limitation arises because while we can assign very many unique codes to representation space we cannot  
        
         Use absolutely all of the code space
         Learn arbitrary functions mapping from the representation space $$h$$ to the output $$y$$ using a linear classifier.  

        The use of a distributed representation combined with a linear classifier thus expresses a prior belief that the classes to be recognized are linearly separable as a function of the underlying causal factors captured by $$h$$.  

        We will typically want to learn categories such as the set of all images of all green objects or the set of all images of cars but not categories that require nonlinear $$\mathrm{XOR}$$ logic. For example we typically do not want to partition the data into the set of all red cars and green trucks as one class and the set of all green cars and red trucks as another class.  
        
     Experimental justification  
        
        Though the above ideas have been abstract they may be experimentally validated  
         Zhou et al. 2015 find that hidden units in a deep convolutional network trained on the ImageNet and Places benchmark datasets learn features that are very often interpretable corresponding to a label that humans would naturally assign.  
            In practice it is certainly not always the case that hidden units learn something that has a simple linguistic name but it is interesting to see this emerge near the top levels of the best computer vision deep networks. What such features have in common is that one could imagine learning about each of them without having to see all the configurations of all the others.  
         Radford et al. 2015 demonstrated that a generative model can learn a representation of images of faces with separate directions in representation space capturing different underlying factors of variation.  
            The following illustration demonstrates that one direction in representation space corresponds to whether the person is male or female while another corresponds to whether the person is wearing glasses.  
            

            These features were discovered automatically not fixed a priori.  
            There is no need to have labels for the hidden unit classifiers gradient descent on an objective function of interest naturally learns semantically interesting features so long as the task requires such features.  
            We can learn about the distinction between male and female or about the presence or absence of glasses without having to characterize all of the configurations of the $$n − 1$$ other features by examples covering all of these combinations of values.  
            This form of statistical separability is what allows one to generalize to new configurations of a person’s features that have never been seen during training.  



    Notes  
    
    
        
     Distributed representations based on latent variables can obtain all of the advantages of representation learning that we have seen with deep feedforward and recurrent networks.  
     Food for Thought F2T  
        "since feature engineering was made obsolete by deep learning algorithm engineering will be made obsolete by meta learning"  Sohl Dickstein  
    

8. Deep Representations  Exponential Gain from Depth

    Exponential Gain in MLPs  
    We have seen in section 6.4.1 that multilayer perceptrons are universal approximators and that some functions can be represented by exponentially smaller deep networks compared to shallow networks.  
    This decrease in model size leads to improved statistical efficiency.  

    Similar results apply more generally to other kinds of models with distributed hidden representations.  

    Justification/Motivation  
    In this and other AI tasks the factors that can be chosen almost independently from each other yet still correspond to meaningful inputs are more likely to be very high level and related in highly nonlinear ways to the input.  
    Goodfellow et al. argue that this demands deep distributed representations where the higher level features seen as functions of the input or factors seen as generative causes are obtained through the composition of many nonlinearities.  
    E.g. the example of a generative model that learned about the explanatory factors underlying images of faces including the person’s gender and whether they are wearing glasses.  
        It would not be reasonable to expect a shallow network such as a linear network to learn the complicated relationship between these abstract explanatory factors and the pixels in the image.  

    Universal Approximation property in Models from Depth  
    
     It has been proven in many different settings that organizing computation through the composition of many nonlinearities and a hierarchy of reused features can give an exponential boost to statistical efficiency on top of the exponential boost given by using a distributed representation.  
     Many kinds of networks e.g. with saturating nonlinearities Boolean gates sum/products or RBF units with a single hidden layer can be shown to be universal approximators.  
        A model family that is a universal approximator can approximate a large class of functions including all continuous functions up to any non zero tolerance level given enough hidden units.  
        However the required number of hidden units may be very large.  
     Theoretical results concerning the expressive power of deep architectures state that there are families of functions that can be represented efficiently by an architecture of depth $$k$$ but would require an exponential number of hidden units wrt. input size with insufficient depth depth $$2$$ or depth $$k − 1$$.  
    
    Exponential Gains in Structured Probabilistic Models  
    
     PGMs as Universal Approximators  
         Just like deterministic feedforward networks are universal approximators of functions
            Many structured probabilistic models with a single hidden layer of latent variables including restricted Boltzmann machines and deep belief networks are universal approximators of probability distributions
     Exponential Gain from Depth in PGMs 
         Just like a sufficiently deep feedforward network can have an exponential advantage over a network that is too shallow.    
            Such results can also be obtained for other models such as probabilistic models.  
             E.g. The sum product network SPN Poon and Domingos 2011.  
                These models use polynomial circuits to compute the probability distribution over a set of random variables.  
                 Delalleau and Bengio 2011 showed that there exist probability distributions for which a minimum depth of SPN is required to avoid needing an exponentially large model.  
                 Later Martens and Medabalimi 2014 showed that there are significant differences between every two finite depths of SPN and that some of the constraints used to make SPNs tractable may limit their representational power.  

    Expressiveness of Convolutional Networks  
    Another interesting development is a set of theoretical results for the expressive power of families of deep circuits related to convolutional nets  
    They highlight an exponential advantage for the deep circuit even when the shallow circuit is allowed to only approximate the function computed by the deep circuit  Cohen et al. 2015.  
    By comparison previous theoretical work made claims regarding only the case where the shallow circuit must exactly replicate particular functions.  
    


Unsupervised Representation Learning

11.Unsupervised Representation Learning
    In Unsupervised feature learning features are learned with unlabeled data.  

    The Goal of unsupervised feature learning is often to discover low dimensional features that captures some structure underlying the high dimensional input data.  

    
    
     Unsupervised Dictionary Learning  
     ICA/PCA  
     AutoEncoders 
     Matrix Factorization  
     Clustering Algorithms
    

    Learning  
    Unsupervised deep learning algorithms have a main training objective but also learn a representation as a side effect.  

    Unsupervised Learning for Semisupervised Learning  xw
    When the feature learning is performed in an unsupervised way it enables a form of semisupervised learning where features learned from an unlabeled dataset are then employed to improve performance in a supervised setting with labeled data.  
    

1. Greedy Layer Wise Unsupervised Pretraining
    Greedy Layer Wise Unsupervised Pretraining 

    
     Greedy it is a greedy algorithm.  
        It optimizes each piece of the solution independently one piece at a time rather than jointly optimizing all pieces.  
     Layer Wise the independent pieces are the layers of the network^1.  
     Unsupervised each layer is trained with an unsupervised representation learning algorithm.  
     Pretraining^2 it is supposed to be only a  step before a joint training algorithm is applied to fine tune all the layers together.  

    This procedure is a canonical example of how a representation learned for one task unsupervised learning trying to capture the shape of the input distribution can sometimes be useful for another task supervised learning with the same input domain.  


    Algorithm/Procedure  
    

    
     Supervised Learning Phase  
        It may involve  
        1. Training a simple classifier on top of the features learned in the pretraining phase.  
        2. Supervised fine tuning of the entire network learned in the pretraining phase.  


    Interpretation in Supervised Settings  
    In the context of a supervised learning task the procedure can be viewed as  
    
     A Regularizer.  
        In some experiments pretraining decreases test error without decreasing training error.  
     A form of Parameter Initialization.  


    Applications  
    
     Training Deep Models  
        Greedy layer wise training procedures based on unsupervised criteria have long been used to sidestep the difficulty of jointly training the layers of a deep neural net for a supervised task.  
        The deep learning renaissance of 2006 began with the discovery that this greedy learning procedure could be used to find a good initialization for a joint learning procedure over all the layers and that this approach could be used to successfully train even fully connected architectures.  
        Prior to this discovery only convolutional deep networks or networks whose depth resulted from recurrence were regarded as feasible to train.  
     Parameter Initialization  
        THey can also be used as initialization for other unsupervised learning algorithms such as  
         Deep Autoencoders Hinton and Salakhutdinov 2006  
         Probabilistic mModels with many layers of latent variables  
            E.g. deep belief networks DBNs Hinton et al. 2006 and deep Boltzmann machines DBMs Salakhutdinov and Hinton 2009a.  

    

2. Clustering \| K Means
3. Local Linear Embeddings
4. Principal Components Analysis PCA
5. Independent Components Analysis ICA
6. Unsupervised Dictionary Learning



Supervised Representation Learning

11.Supervised Representation Learning
    In Supervised feature learning features are learned using labeled data.  

    Learning  
    The data label allows the system to compute an error term the degree to which the system fails to produce the label which can then be used as feedback to correct the learning process reduce/minimize the error.  

    Examples  
    
     Supervised Neural Networks  
     Supervised Dictionary Learning  

    FFNs as Representation Learning Algorithms  
    
     We can think of Feed Forward Neural Networks trained by supervised learning as performing a kind of representation learning.  
     All the layers except the last layer usually a linear classifier are basically producing representations featurizing of the input.  
     Training with a supervised criterion naturally leads to the representation at every hidden layer but more so near the top hidden layer taking on properties that make the classification task easier  
        E.g. Making classes linearly separable in the latent space.  
     The features in the penultimate layer should learn different properties depending on the type of the last layer.  
     Supervised training of feedforward networks does not involve explicitly imposing any condition on the learned intermediate features.  
     We can however explicitly impose certain desirable conditions.  
        
        
        Suppose we want to learn a representation that makes density estimation easier. Distributions with more independences are easier to model so we could design an objective function that encourages the elements of the representation vector $$\boldsymbol{h}$$ to be independent.  
          


1. Greedy Layer Wise Supervised Pretraining
    As discussed in section 8.7.4 it is also possible to have greedy layer wise supervised pretraining.  
    This builds on the premise that training a shallow network is easier than training a deep one which seems to have been validated in several contexts Erhan et al. 2010.

2. Neural Networks
3. Supervised Dictionary Learning
  


Transfer Learning and Domain Adaptation














1. Introduction  Transfer Learning and Domain Adaptation
    Transfer Learning and Domain Adaptation refer to the situation where what has been learned in one setting i.e. distribution $$P{1}$$ is exploited to improve generalization in another setting say distribution $$P{2}$$.  

    This is a generalization of unsupervised pretraining where we transferred representations between an unsupervised learning task and a supervised learning task.  

    In Supervised Learning transfer learning domain adaptation and concept drift can be viewed as particular forms of Multi Task Learning.  
    However Transfer Learning is a more general term that applies to both Supervised and Unsupervised Learning as well as Reinforcement Learning.  

    Goal/Objective and Relation to Representation Learning  
    In the cases of Transfer Learning Multi Task Learning and Domain Adaptation 
    The Objective/Goal is to take advantage of data from the  setting to extract information that may be useful when learning or even when directly making predictions in the  setting.  

    The core idea of Representation Learning is that the same representation may be useful in both settings.  

    Thus we can use shared representations to accomplish Transfer Learning etc.  
    Shared Representations are useful to handle multiple modalities or domains or to transfer learned knowledge to tasks for which few or no examples are given but a task representation exists.  

    

    


2. Transfer Learning
    Transfer Learning in ML is the problem of storing knowledge gained while solving one problem and applying it to a different but related problem.  

    Definition  
    Formally the definition of transfer learning is given in terms of  
    
     A Domain $$\mathcal{D}=\{\mathcal{X} PX\}$$ $$\\$$ consisting of  
         Feature Space $$\mathcal{X}$$  
         Marginal Probability Distribution $$PX$$  
            where $$X=\left\{x{1} \ldots x{n}\right\} \in \mathcal{X}$$.  
     A Task $$\mathcal{T}=\{\mathcal{Y} f\cdot\}$$  
        given a specific domain $$\mathcal{D}=\{\mathcal{X} PX\}$$ consisting of  
         A label space $$\mathcal{Y}$$   
         An objective predictive function $$f\cdot$$  
            It is learned from the training data which consist of pairs $$\left\{x {i} y{i}\right\}$$ where $$x{i} \in X$$ and $$y{i} \in \mathcal{Y}$$.  
            It can be used to predict the corresponding label $$fx$$ of a new instance $$x$$.  

    Given a source domain $$\mathcal{D} {S}$$ and learning task $$\mathcal{T} {S}$$ a target domain $$\mathcal{D} {T}$$ and learning task $$\mathcal{T} {T}$$ transfer learning aims to help improve the learning of the target predictive function $$f {T}\cdot$$ in $$\mathcal{D} {T}$$ using the knowledge in $$\mathcal{D} {S}$$ and $$\mathcal{T} {S}$$ where $$\mathcal{D} {S} \neq \mathcal{D} {T}$$ or $$\mathcal{T} {S} \neq \mathcal{T} {T}$$.  
    


    In Transfer Learning the learner must perform two or more different tasks but we assume that many of the factors that explain the variations in $$P1$$ are relevant to the variations that need to be captured for learning $$P2$$. This is typically understood in a supervised learning context where the input is the same but the target may be of a different nature.  
    
    
    We may learn about one set of visual categories such as cats and dogs in the  setting then learn about a different set of visual categories such as ants and wasps in the  setting.  
    If there is significantly more data in the  setting sampled from $$P1$$ then that may help to learn representations that are useful to quickly generalize from only very few examples drawn from $$P2$$.  
    Many visual categories share low level notions of edges and visual shapes the effects of geometric changes changes in lighting etc.  
    


    Types of Transfer Learning  
    
     Inductive Transfer Learning  
        $$\mathcal{D} {S} = \mathcal{D} {T} \\\ \text{  and  }\\\ \mathcal{T} {S} \neq \mathcal{T} {T}$$  
        e.g. $$\left\mathcal{D} {S} = \text{ Wikipedia } = \mathcal{D} {T}\right \\ \text{  and  } \\ \left\mathcal{T} {S} = \text{ Skip Gram }\right \neq \left\mathcal{T} {T} = \text{ Classification }\right$$  
     Transductive Transfer Learning Domain Adaptation  
        $$\mathcal{D} {S} \neq \mathcal{D} {T} \\\ \text{  and  }\\\ \mathcal{T} {S} = \mathcal{T} {T}$$  
        e.g. $$\left\mathcal{D} {S} = \text{ Reviews }\right \neq \left\mathcal{D} {T} = \text{ Tweets }\right \\ \text{  and  } \\ \left\mathcal{T} {S} = \text{ Sentiment Analysis } = \mathcal{T} {T}\right$$  
     Unsupervised Transfer Learning  
        $$\mathcal{D} {S} \neq \mathcal{D} {T} \\\ \text{  and  }\\\ \mathcal{T} {S} \neq \mathcal{T} {T}$$  
        e.g. $$\left\mathcal{D} {S} = \text{ Animals}\right \neq \left\mathcal{D} {T} = \text{ Cars}\right \ \text{  and  } \ \left\mathcal{T} {S} = \text{ Recog.}\right \neq \left\mathcal{T} {T} = \text{ Detection}\right$$  
     


    
    Concept Drift  
    Concept Drift is a phenomena where the statistical properties of the target variable which the model is trying to predict change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.  

    It can be viewed as a form of transfer learning due to gradual changes in the data distribution over time.  

    
    
    Another example is in reinforcement learning. Since the agent's policy affects the environment the agent learning and updating its policy directly results in a changing environment with shifting data distribution.  
    

    
    Unsupervised Deep Learning for Transfer Learning  
    
    
    Unsupervised Deep Learning for Transfer Learning has seen success in some machine learning competitions Mesnil et al. 2011; Goodfellow et al. 2011.  
    In the  of these competitions the experimental setup is the following  
    
     Each participant is  given a dataset from the  setting from distribution $$P1$$ illustrating examples of some set of categories.  
     The participants must use this to learn a good feature space mapping the raw input to some representation such that when we apply this learned transformation to inputs from the transfer setting distribution $$P2$$  a linear classifier can be trained and generalize well from very few labeled examples.  

    One of the most striking results found in this competition is that as an architecture makes use of deeper and deeper representations learned in a purely unsupervised way from data collected in the  setting $$P1$$ the learning curve on the new categories of the  transfer setting $$P2$$ becomes much better.  
    For deep representations fewer labeled examples of the transfer tasks are necessary to achieve the apparently asymptotic generalization performance.  
    

3. Domain Adaptation
    Domain Adaptation is a form of transfer learning where we aim at learning from a source data distribution a well performing model on a different but related target data distribution.  
    
    It is a sequential process.  
    
    In domain adaptation the task and the optimal input to output mapping remains the same between each setting but the input distribution is slightly different.  
    
    
    Consider the task of sentiment analysis which consists of determining whether a comment expresses positive or negative sentiment. Comments posted on the web come from many categories. A domain adaptation scenario can arise when a sentiment predictor trained on customer reviews of media content such as books videos and music is later used to analyze comments about consumer electronics such as televisions or smartphones.   
    One can imagine that there is an underlying function that tells whether any statement is positive neutral or negative but of course the vocabulary and style may vary from one domain to another making it more difficult to generalize across domains.  
    Simple unsupervised pretraining with denoising autoencoders has been found to be very successful for sentiment analysis with domain adaptation Glorot et al. 2011b.  
    


4. Multitask Learning
    Multitask Learning is a transfer learning where multiple learning tasks are solved at the same time while exploiting commonalities and differences across tasks.  

    In particular it is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.  

    It is a parallel process.  

    Multitask vs Transfer Learning  

    1. Multi Task Learning general term for training on multiple tasks  
        1. Joint Learning by choosing mini batches from two different tasks simultaneously/alternately
        1. Pre Training  train on one task then train on another  
            widely used for word embeddings.  
    1. Transfer Learning  
        a type of multi task learning where we are focused on one task; by learning on another task then applying those models to our main task  
    


5. Representation Learning for the Transfer of Knowledge
    We can use Representation Learning to achieve Multi Task Learning Transfer Learning and Domain Adaptation.  

    In general Representation Learning can be used to achieve Multi Task Learning Transfer Learning and Domain Adaptation when there exist features that are useful for the different settings or tasks corresponding to underlying factors that appear in more than one setting.    
    This applies in two cases  
    
     Shared Input Semantics   
        
        
        We may learn about one set of visual categories such as cats and dogs in the  setting then learn about a different set of visual categories such as ants and wasps in the  setting.  
        If there is significantly more data in the  setting sampled from $$P1$$ then that may help to learn representations that are useful to quickly generalize from only very few examples drawn from $$P2$$.  
        Many visual categories share low level notions of edges and visual shapes the effects of geometric changes changes in lighting etc.  
        
        In this case we share the lower layers and have a task dependent upper layers.  
        

     Shared Output Semantics  
        
        
        A speech recognition system needs to produce valid sentences at the output layer but the earlier layers near the input may need to recognize very different versions of the same phonemes or sub phonemic vocalizations depending on which person is speaking.  
        
        In cases like these it makes more sense to share the upper layers near the output of the neural network and have a task specific preprocessing.  
        

    

6. K Shot Learning
    K Shot Few Shot Learning is a supervised learning setting problem where the goal is to learn from an extremely small number $$k$$ of labeled examples called shots.  

    General Setting  
    We  train a model on a large dataset $$\mathcal{D}=\left\{\widetilde{\mathbf{x}} {i} \widetilde{\gamma} {i}\right\} {i=1}^{N}$$ of inputs $$\widetilde{\mathbf{x}} {i}$$ and labels $$\widetilde{y} {i} \in\{1 \ldots \widetilde{C}\}$$ that indicate which of the $$\widetilde{C}$$ classes each input belongs to.  
    Then using knowledge from the model trained on the large dataset we perform $$\mathrm{k}$$ shot learning with a small dataset $$\mathcal{D}=\left\{\mathbf{x} {i} y {i}\right\} {i=1}^{N}$$ with $$C$$ new classes labels $$y {i} \in\{\widetilde{C}+1 \widetilde{C}+C\}$$ and $$k$$ examples inputs from each new class.  
    During test time we classify unseen examples inputs $$\mathbf{x}^{ }$$ from the new classes $$C$$ and evaluate the predictions against ground truth labels $$y^{ }$$.  


    Comparison to alternative Learning Paradigms  



    As Transfer Learning  
    Two extreme forms of transfer learning are One Shot Learning and Zero Shot Learning; they provide only one and zero labeled examples of the transfer task respectively.  


    One Shot Learning  
    One Shot Learning Fei Fei et al. 2006 is a form of k shot learning where $$k=1$$.  
    
    It is possible because the representation learns to cleanly separate the underlying classes during the  stage.  
    During the transfer learning stage only one labeled example is needed to infer the label of many possible test examples that all cluster around the same point in representation space.  
    This works to the extent that the factors of variation corresponding to these invariances have been cleanly separated from the other factors in the learned representation space and we have somehow learned which factors do and do not matter when discriminating objects of certain categories.  


    Zero Shot Learning  
    Zero Shot Learning Palatucci et al. 2009; Socher et al. 2013b or Zero data learning Larochelle et al. 2008 is a form of k shot learning where $$k=0$$.  

    Example Zero Shot Learning Setting  
    Consider the problem of having a learner read a large collection of text and then solve object recognition problems.  
    It may be possible to recognize a specific object class even without having seen an image of that object if the text describes the object well enough.  
    For example having read that a cat has four legs and pointy ears the learner might be able to guess that an image is a cat without having seen a cat before.  


    Justification and Interpretation  
    Zero Shot Learning is only possible because additional information has been exploited during training.  

    We can think of think of the zero data learning scenario as including three random variables  
    
    1. Traditional Inputs $$x$$  
    2. Traditional Outputs or Targets $$\boldsymbol{y}$$
    3. Additional Random Variable describing the task $$T$$  

    The model is trained to estimate the conditional distribution $$p\boldsymbol{y} \vert \boldsymbol{x} T$$.  
    
     
        
        In the example of recognizing cats after having read about cats the output is a binary variable $$y$$ with $$y=1$$ indicating "yes" and $$y=0$$ indicating "no".  
        The task variable $$T$$ then represents questions to be answered such as "Is there a cat in this image?".  
        If we have a training set containing unsupervised examples of objects that live in the Same space as $$T$$ we may be able to infer the meaning of unseen instances of $$T$$.  
        In our example of recognizing cats without having seen an image of the cat it is important that we have had unlabeled text data containing sentences such as "cats have four legs" or "cats have pointy ears".  
        

    Representing the task $$T$$  
    Zero shot learning requires $$T$$ to be represented in a way that allows some sort of generalization.  
    For example $$T$$ cannot be just a one hot code indicating an object category.  
    Socher et al. 2013 b provide instead a distributed representation of object categories by using a learned word embedding for the word associated with each category.  

    
    Representation Learning for Zero Shot Learning  
    The principle underlying zero shot learning as a form of transfer learning capturing a representation in one modality a representation in another modality and the relationship in general a joint distribution between pairs $$\boldsymbol{x} \boldsymbol{y}$$ consisting of one observation $$\boldsymbol{x}$$ in one modality and another observation $$\boldsymbol{y}$$ in the other modality Srivastava and Salakhutdinov 2012.   
    By learning all three sets of parameters from $$\boldsymbol{x}$$ to its representation from $$\boldsymbol{y}$$ to its representation and the relationship between the two representations concepts in one representation are anchored in the other and vice versa allowing one to meaningfully generalize to new pairs.  

    In particular Transfer learning between two domains $$x$$ and $$y$$ enables zero shot learning.  

    



    Zero Shot Learning in Machine Translation  
    
    
    A similar phenomenon happens in machine translation Klementiev et al. 2012; Mikolov et al. 2013b; Gouws et al. 2014  
    we have words in one language and the relationships between words can be learned from unilingual corpora; on the other hand we have translated sentences which relate words in one language with words in the other. Even though we may not have labeled examples translating word $$A$$ in language $$X$$ to word $$B$$ in language $$Y$$ we can generalize and guess a translation for word $$A$$ because we have learned a distributed representation for words in language $$X$$ a distributed representation for words in language $$Y$$ and created a link possibly two way relating the two spaces via training examples consisting of matched pairs of sentences in both languages.  
    This transfer will be most successful if all three ingredients the two representations and the relations between them are learned jointly.  
    

    Relation to Multi modal Learning  
    Zero Shot Learning can be performed using Multi model Learning and vice versa.  
    The same principle of transfer learning with representation learning explain how one can perform either tasks.  



    Notes  
    


     Zero Shot Learning is a form of extending supervised learning to a setting of solving for example a classification problem when not enough labeled examples are available for all classes.   
        "Zero shot learning is being able to solve a task despite not having received any training examples of that task."  Goodfellow  
     Detecting Gravitational Waves is a form of Zero Shot Learning   
     Few shot one shot or zero shot learning are encompassed in a recently emerging field known as meta learning.  

        Works in this area seems to be primarily motivated by the notion of human level AI since humans appear to be able to require far fewer training data than most deep learning models.  
    


7. Multi Modal Learning
    Multi Modal Learning   


    Representation Learning for Multi modal Learning  
    The same principle underlying zero shot learning as a form of transfer learning explains how one can perform multi modal learning; capturing a representation in one modality a representation in the other and the relationship in general a joint distribution between pairs $$\boldsymbol{x} \boldsymbol{y}$$ consisting of one observation $$\boldsymbol{x}$$ in one modality and another observation $$\boldsymbol{y}$$ in the other modality Srivastava and Salakhutdinov 2012.   
    By learning all three sets of parameters from $$\boldsymbol{x}$$ to its representation from $$\boldsymbol{y}$$ to its representation and the relationship between the two representations concepts in one representation are anchored in the other and vice versa allowing one to meaningfully generalize to new pairs.  





Causal Factor Learning


3. Semi Supervised Disentangling of Causal Factors
    
    Quality of Representations  
      An important question in Representation Learning is  
    “what makes one representation better than another?”   
    
    1. One answer to that is the Causal Factors Hypothesis  
        An ideal representation is one in which the features within the representation correspond to the underlying causes of the observed data with separate features or directions in feature space corresponding to different causes so that the representation disentangles the causes from one another.  
         This hypothesis motivates approaches in which we  seek a good representation for $$p\boldsymbol{x}$$.  
            This representation may also be a good representation for computing $$p\boldsymbol{y} \vert \boldsymbol{x}$$ if $$\boldsymbol{y}$$ is among the most salient causes of $$\boldsymbol{x}$$^3 ^4.  
    2. Ease of Modeling  
        In many approaches to representation learning we are often concerned with a representation that is easy to model e.g. sparse entries independent entries etc..  
        It is not directly observed however that a representation that cleanly separates the underlying causal factors is also one that is easy to model.  
        The answer to that is an extension of the Causal Factor Hypothesis  
        For many AI tasks the two properties coincide once we are able to obtain the underlying explanations for the observations it generally becomes easy to isolate individual attributes from the others.  
        Specifically if a representation $$\boldsymbol{h}$$ represents many of the underlying causes of the observed $$\boldsymbol{x}$$ and the outputs $$\boldsymbol{y}$$ are among the most salient causes then it is easy to predict $$\boldsymbol{y}$$ from $$\boldsymbol{h}$$.  

    
    
     The complete Causal Factors Hypothesis motivates Semi Supervised Learning via Unsupervised Representation Learning.
    


    Analysis  When does Semi Supervised Learning Work  
    
    
     When does Semi Supervised Disentangling of Causal Factors Work?  
        Let's start by considering two scenarios where Semi Supervised Learning via Unsupervised Representation Learning Fails and Succeeds  
         
            
            Let us see how semi supervised learning can fail because unsupervised learning of $$p\mathbf{x}$$ is of no help to learn $$p\mathbf{y} \vert \mathbf{x}$$.  
            Consider the case where $$p\mathbf{x}$$ is uniformly distributed and we want to learn $$f\boldsymbol{x}=\mathbb{E}\mathbf{y} \vert \boldsymbol{x}$$.  
            Clearly observing a training set of $$\boldsymbol{x}$$ values alone gives us no information about $$p\mathbf{y} \vert \mathbf{x}$$.  
            
         
            
            Consider the case where $$x$$ arises from a mixture with one mixture component per value of $$y$$.  
            If the mixture components are well separated then modeling $$px$$ reveals precisely where each component is and a single labeled example of each class will then be enough to perfectly learn $$p\mathbf{y} \vert \mathbf{x}$$.    
            

            

        
        Thus we conclude that semi supervised learning works when $$p\mathbf{y} \vert \mathbf{x}$$ and $$p\mathbf{x}$$ are tied together.  
        
     When are $$p\mathbf{y} \vert \mathbf{x}$$ and $$p\mathbf{x}$$ tied?  
        This happens when $$\mathbf{y}$$ is closely associated with one of the causal factors of $$\mathbf{x}$$ then $$p\mathbf{x}$$ and $$p\mathbf{y} \vert \mathbf{x}$$ will be strongly tied.  
         Thus unsupervised representation learning that tries to disentangle the underlying factors of variation is likely to be useful as a semi supervised learning strategy.  

    Now Consider the assumption that $$\mathbf{y}$$ is one of the causal factors of $$\mathbf{x}$$ and let $$\mathbf{h}$$ represent all those factors  
    
     The "true" generative process can be conceived as structured according to this directed graphical model with $$\mathbf{h}$$ as the parent of $$\mathbf{x}$$  
        $$p\mathbf{h} \mathbf{x}=p\mathbf{x} \vert \mathbf{h} p\mathbf{h}$$  
         As a consequence the data has marginal probability  
            $$p\boldsymbol{x}=\mathbb{E} {\mathbf{h}} p\boldsymbol{x} \vert \boldsymbol{h}$$  

        From this straightforward observation we conclude that  
        
        The best possible model of $$\mathbf{x}$$ wrt. generalization is the one that uncovers the above "true" structure with $$\boldsymbol{h}$$ as a latent variable that explains the observed variations in $$\boldsymbol{x}$$.  
          
        I.E. The "ideal" representation learning discussed above should thus recover these latent factors.  
        If $$\mathbf{y}$$ is one of these or closely related to one of them then it will be very easy to learn to predict $$\mathbf{y}$$ from such a representation.  
     We also see that the conditional distribution of $$\mathbf{y}$$ given $$\mathbf{x}$$ is tied by Bayes' rule to the components in the above equation  
        $$p\mathbf{y} \vert \mathbf{x}=\frac{p\mathbf{x} \vert \mathbf{y} p\mathbf{y}}{p\mathbf{x}}$$  

        
        Thus the marginal $$p\mathbf{x}$$ is intimately tied to the conditional $$p\mathbf{y} \vert \mathbf{x}$$ and knowledge of the structure of the former should be helpful to learn the latter.  
        
    
    
    Therefore in situations respecting these assumptions semi supervised learning should improve performance.  
      
    

    Justifying the setting where Semi Supervised Learning Works  
    
     Semi Supervised Learning^5 Works when $$p\mathbf{y} \vert \mathbf{x}$$ and $$p\mathbf{x}$$ are tied together.  
     $$p\mathbf{y} \vert \mathbf{x}$$ and $$p\mathbf{x}$$ are Tied when $$\mathbf{y}$$ is closely associated with one of the causal factors of $$\mathbf{x}$$ or it is a causal factor itself.  
         Let $$\mathbf{h}$$ represent all the causal factors of $$\mathbf{x}$$ and let $$\mathbf{y} \in \mathbf{h}$$ be a causal factor of $$\mathbf{x}$$ then  
            The "true" generative process can be conceived as structured according to this directed graphical model with $$\mathbf{h}$$ as the parent of $$\mathbf{x}$$  
            $$p\mathbf{h} \mathbf{x}=p\mathbf{x} \vert \mathbf{h} p\mathbf{h}$$  
             Thus the Marginal Probability of the Data $$p\mathbf{x}$$ is  
                1. Tied to the conditional $$p\mathbf{x} \vert \mathbf{h}$$ as  
                    $$p\boldsymbol{x}=\mathbb{E} {\mathbf{h}} p\boldsymbol{x} \vert \boldsymbol{h}$$  
                    $$\implies$$  
                     The best possible model of $$\mathbf{x}$$ wrt. generalization is the one that uncovers the above "true" structure with $$\boldsymbol{h}$$ as a latent variable that explains the observed variations in $$\boldsymbol{x}$$.  
                        I.E. The “ideal” representation learning discussed above should thus recover these latent factors.  
                2. intimately Tied to the conditional $$p\mathbf{y} \vert \mathbf{x}$$ by Bayes' rule as  
                    $$p\mathbf{y} \vert \mathbf{x}=\frac{p\mathbf{x} \vert \mathbf{y} p\mathbf{y}}{p\mathbf{x}}$$  

    
    Therefore in situations respecting these assumptions semi supervised learning should improve performance.  
    


    Encoding/Learning Causal Factors  
    
     Problem  Number of Causal Factors
        An important research problem regards the fact that most observations are formed by an extremely large number of underlying causes.  
         Suppose $$\mathbf{y}=\mathrm{h} {i}$$ but the unsupervised learner does not know which $$\mathrm{h} {i}$$  
             The brute force solution is for an unsupervised learner to learn a representation that captures all the reasonably salient generative factors $$\mathrm{h} {j}$$ and disentangles them from each other thus making it easy to predict $$\mathbf{y}$$ from $$\mathbf{h}$$ regardless of which $$\mathrm{h} {i}$$ is associated with $$\mathbf{y}$$.  
                 In practice the brute force solution is not feasible because it is not possible to capture all or most of the factors of variation that influence an observation.  
                    For example in a visual scene should the representation always encode all of the smallest objects in the background?  
                    It is a well documented psychological phenomenon that human beings fail to perceive changes in their environment that are not immediately relevant to the task they are performing Simons and Levin 1998.  
     Solution  Determining which causal factor to encode/learn
        An important research frontier in semi supervised learning is determining "what to encode in each situation".  
         Currently there are two main strategies for dealing with a large number of underlying causes  
            1. Use a supervised learning signal at the same time as the "plus" unsupervised learning signal  
                so that the model will choose to capture the most relevant factors of variation. 
            2. Use much larger representations if using purely unsupervised learning.  
         New Emerging Strategy for unsupervised learning  
            Redefining the definition of "salient" factors.  

    

    The definition of "Salient"  
    
     The current definition of "salient" factors  
        In practice we encode the definition of "salient" by using the objective criterion e.g. MSE.  
        Historically autoencoders and generative models have been trained to optimize a fixed criterion often similar to MSE.  

         Problem with current definition  
            Since these fixed criteria determine which causes are considered salient they will be emphasizing different factors depending on their e.g. effects on the error    
             E.g. MSE applied to the pixels of an image implicitly specifies that an underlying cause is only salient if it significantly changes the brightness of a large number of pixels.  
                This can be problematic if the task we wish to solve involves interacting with small objects.  
                

     Learned pattern based "Saliency"  
        Certain factors could be considered "salient" if they follow a highly recognizable pattern.  
        E.g. if a group of pixels follow a highly recognizable pattern even if that pattern does not involve extreme brightness or darkness then that pattern could be considered extremely salient.  

         This definition is implemented by Generative Adversarial Networks GANs.  
            In this approach a generative model is trained to fool a feedforward classifier. The feedforward classifier attempts to recognize all samples from the generative model as being fake and all samples from the training set as being real.  
            In this framework any structured pattern that the feedforward network can recognize is highly salient.  
            They learn how to determine what is salient.  
            
            
            Lotter et al. 2015 showed that models trained to generate images of human heads will often neglect to generate the ears when trained with mean squared error but will successfully generate the ears when trained with the adversarial framework.  
            Because the ears are not extremely bright or dark compared to the surrounding skin they are not especially salient according to mean squared error loss but their highly recognizable shape and consistent position means that a feedforward network can easily learn to detect them making them highly salient under the generative adversarial framework.    
            

            
    
    Generative adversarial networks are only one step toward determining which factors should be represented.  
    We expect that future research

    Robustness to Change  Causal Invariance  
    A benefit of learning the underlying causal factors Schölkopf et al. 2012 is that  
    if the true generative process has $$\mathbf{x}$$ as an effect and $$\mathbf{y}$$ as a cause then modeling $$p\mathbf{x} \vert \mathbf{y}$$ is robust to changes in $$p\mathbf{y}$$.  
    If the cause effect relationship was reversed this would not be true since by Bayes' rule $$p\mathbf{x} \vert \mathbf{y}$$ would be sensitive to changes in $$p\mathbf{y}$$.  

    Very often when we consider changes in distribution due to different domains temporal non stationarity or changes in the nature of the task the causal mechanisms remain invariant
    Hence better generalization and robustness to all kinds of changes can be expected via learning a generative model that attempts to recover the causal factors
    

4. Providing Clues to Discover Underlying Causes
    Quality of Representations  
    The answer to the following question  
    “what makes one representation better than another?”   
    was the Causal Factors Hypothesis  
    An ideal representation is one in which the features within the representation correspond to the underlying causes of the observed data with separate features or directions in feature space corresponding to different causes so that the representation disentangles the causes from one another especially those factors that are relevant to our applications.  

    Clues for Finding the Causal Factors of Variation  
    Most strategies for representation learning are based on  
    Introducing clues that help the learning to find these underlying factors of variations.  
    The clues can help the learner separate these observed factors from the others.  
    
    Supervised learning provides a very strong clue a label $$\boldsymbol{y}$$ presented with each $$\boldsymbol{x}$$ that usually specifies the value of at least one of the factors of variation directly.  

    More generally to make use of abundant unlabeled data representation learning makes use of other less direct hints about the underlying factors.  
    These hints take the form of implicit prior beliefs that we the designers of the learning algorithm impose in order to guide the learner.  

    Clues in the form of Regularization  
    Results such as the no free lunch theorem show that regularization strategies are necessary to obtain good generalization.  
    While it is impossible to find a universally superior regularization strategy one goal of deep learning is to find a set of fairly generic regularization strategies that are applicable to a wide variety of AI tasks similar to the tasks that people and animals are able to solve.  

    We can use generic regularization strategies to encourage learning algorithms to discover features that correspond to underlying factors E.G. Bengio et al. 2013d  
    
     Smoothness This is the assumption that $$f\boldsymbol{x}+\epsilon \boldsymbol{d} \approx f\boldsymbol{x}$$ for unit $$\boldsymbol{d}$$ and small $$\epsilon$$. This assumption allows the learner to generalize from training examples to nearby points in input space. Many machine learning algorithms leverage this idea but it is insufficient to overcome the curse of dimensionality.  
     Linearity Many learning algorithms assume that relationships between some variables are linear. This allows the algorithm to make predictions even very far from the observed data but can sometimes lead to overly extreme predictions. Most simple machine learning algorithms that do not make the smoothness assumption instead make the linearity assumption. These are in fact different assumptions—linear functions with large weights applied to high dimensional spaces may not be very smooth^7.
     Multiple explanatory factors Many representation learning algorithms are motivated by the assumption that the data is generated by multiple underlying explanatory factors and that most tasks can be solved easily given the state of each of these factors. Section 15.3 describes how this view motivates semisupervised learning via representation learning. Learning the structure of $$p\boldsymbol{x}$$ requires learning some of the same features that are useful for modeling $$p\boldsymbol{y} \vert \boldsymbol{x}$$ because both refer to the same underlying explanatory factors. Section 15.4 describes how this view motivates the use of distributed representations with separate directions in representation space corresponding to separate factors of variation. 
     Causal factors the model is constructed in such a way that it treats the factors of variation described by the learned representation $$\boldsymbol{h}$$ as the causes of the observed data $$\boldsymbol{x}$$ and not vice versa. As discussed in section 15.3 this is advantageous for semi supervised learning and makes the learned model more robust when the distribution over the underlying causes changes or when we use the model for a new task. 
     Depth or a hierarchical organization of explanatory factors High level abstract concepts can be defined in terms of simple concepts forming a hierarchy. From another point of view the use of a deep architecture expresses our belief that the task should be accomplished via a multi step program with each step referring back to the output of the processing accomplished via previous steps.  
     Shared factors across tasks In the context where we have many tasks corresponding to different $$y{i}$$ variables sharing the same input $$\mathbf{x}$$ or where each task is associated with a subset or a function $$f^{i}\mathbf{x}$$ of a global input $$\mathbf{x}$$ the assumption is that each $$\mathbf{y} {i}$$ is associated with a different subset from a common pool of relevant factors $$\mathbf{h}$$. Because these subsets overlap learning all the $$P\lefty{i} \vert \mathbf{x}\right$$ via a shared intermediate representation $$P\mathbf{h} \vert \mathbf{x}$$ allows sharing of statistical strength between the tasks.  
     Manifolds Probability mass concentrates and the regions in which it concentrates are locally connected and occupy a tiny volume. In the continuous case these regions can be approximated by low dimensional manifolds with a much smaller dimensionality than the original space where the data lives. Many machine learning algorithms behave sensibly only on this manifold Goodfellow et al. 2014b. Some machine learning algorithms especially autoencoders attempt to explicitly learn the structure of the manifold. 
     Natural clustering Many machine learning algorithms assume that each connected manifold in the input space may be assigned to a single class. The data may lie on many disconnected manifolds but the class remains constant within each one of these. This assumption motivates a variety of learning algorithms including tangent propagation double backprop the manifold tangent classifier and adversarial training. 
     Temporal and spatial coherence Slow feature analysis and related algorithms make the assumption that the most important explanatory factors change slowly over time or at least that it is easier to predict the true underlying explanatory factors than to predict raw observations such as pixel values. See section 13.3 for further description of this approach. 
     Sparsity Most features should presumably not be relevant to describing most inputs—there is no need to use a feature that detects elephant trunks when representing an image of a cat. It is therefore reasonable to impose a prior that any feature that can be interpreted as “present” or “absent” should be absent most of the time. 
     Simplicity of Factor Dependencies In good high level representations the factors are related to each other through simple dependencies. The simplest possible is marginal independence $$P\mathbf{h}=\prod{i} P\left\mathbf{h} {i}\right$$ but linear dependencies or those captured by a shallow autoencoder are also reasonable assumptions. This can be seen in many laws of physics and is assumed when plugging a linear predictor or a factorized prior on top of a learned representation.  
         Consciousness Prior    
             
                 Key Ideas  
                    1 Seek Objective Functions defined purely in abstract space no decoders   
                    2 "Conscious" thoughts are low dimensional.  
                     Conscious thoughts are very low dimensional objects compared to the full state of the unconscious brain  
                     Yet they have unexpected predictive value or usefulness  
                        $$\rightarrow$$ strong constraint or prior on the underlying representation  
                        e.g. we can plan our lives by only thinking of simple/short sentences at a time that can be expressed with few variables words; short term memory is only 7 words underutilization? no rather prior.  

                         Thought composition of few selected factors / concepts key/value at the highest level of abstraction of our brain.  
                         Richer than but closely associated with short verbal expression such as a sentence or phrase a rule or fact link to classical symbolic Al & knowledge representation  
                     Thus true statements about the very complex world could be conveyed with very low dimensional representations.  
                 How to select a few relevant abstract concepts making a thought  
                    Content based Attention.  
                     Thus Abstraction is related to Attention  
                        

                 Two Levels of Representations  
                     High dimensional abstract representation space all known concepts and factors $$h$$  
                     Low dimensional conscious thought $$c$$ extracted from $$h$$  
                         $$c$$ includes names keys and values of factors  

                     The Goal of using attention on the unconscious states  
                        is to put pressure constraint on the mapping between input and representations Encoder and the unconscious states representations $$h$$ such that the Encoder is encouraged to learn representations that have the property that that if I pick just a few elements of it I can make a true statement or very highly probable statement about the world e.g. a highly probable prediction.  


     Causal/Mechanism Independence  
         Controllable Factors.  

    
    The concept of representation learning ties together all of the many forms of deep learning.  
    Feedforward and recurrent networks autoencoders and deep probabilistic models all learn and exploit representations. Learning the best possible representation remains an exciting avenue of research.  
    





^1 It proceeds one layer at a time training the k  th layer while keeping the previous ones fixed. In particular the lower layers which are trained  are not adapted after the upper layers are introduced.  
^2 Commonly “pretraining” to refer not only to the pretraining stage itself but to the entire two phase protocol that combines the pretraining phase and a supervised learning phase.  
^3 This idea has guided a large amount of deep learning research since at least the 1990s Becker and Hinton 1992; Hinton and Sejnowski 1999 in more detail.  
^4 For other arguments about when semi supervised learning can outperform pure supervised learning we refer the reader to section 1.2 of Chapelle et al. 2006.
^5 Using unsupervised representation learning that tries to disentangle the underlying factors of variation.  
^6 It is also called a one hot representation since it can be captured by a binary vector with $$n$$ bits that are mutually exclusive only one of them can be active.  
^7 See Goodfellow et al. 2014b for a further discussion of the limitations of the linearity assumption.  


 Manifold Learning








1. The Manifold Hypothesis
    The Manifold Hypothesis states that real world high dimensional data such as images lie on low dimensional manifolds embedded in the high dimensional space.  

    Intuitively the existence of a low dimensional representation makes sense because if we had to learn the entirety of the space of input images $$256\times256\times3$$ in a capacity limited NN we will fail; but in practice we very easily can.  
    Basically learning an arbitrary $$256\times256\times3$$ function would be intractable.  

     
    
  

     Any shape 









   Theoretical Motivation A lot of the natural transformations you might want to perform on an image like translating or scaling an object in it or changing the lighting would form continuous curves in image space if you performed them continuously.  
  








 Probability Theory  Mathematics of Deep Learning














Motivation

1. Uncertainty in General Systems and the need for a Probabilistic Framework
    1. Inherent stochasticity in the system being modeled  
        Take Quantum Mechanics most interpretations of quantum mechanics describe the dynamics of sub atomic particles as being probabilistic.  
    2. Incomplete observability  
        Deterministic systems can appear stochastic when we cannot observe all the variables that drive the behavior of the system.  
        i.e. Point of View determinism Monty Hall  
    3. Incomplete modeling  
        Building a system that makes strong assumptions about the problem and discards observed information result in uncertainty in the predictions.    
    

2. Bayesian Probabilities and Frequentist Probabilities
    Frequentist Probabilities describe the predicted number of times that a repeatable process will result in a given output in an absolute scale.  

    Bayesian Probabilities describe the degree of belief that a certain non repeatable event is going to result in a given output in an absolute scale.      
    
    We assume that Bayesian Probabilities behaves in exactly the same way as Frequentist Probabilities.  
    

3. Probability as an extension of Logic
    "Probability can be seen as the extension of logic to deal with uncertainty. Logic provides a set of formal rules for determining what propositions are implied to be true or false given the assumption that some other set of propositions is true or false. Probability theory provides a set of formal rules for determining the likelihood of a proposition being true given the likelihood of other propositions."  deeplearningbook p.54



Basics

0. Elements of Probability
     Sample Space $$\Omega$$ The set of all the outcomes of a stochastic experiment; where each outcome is a complete description of the state of the real world at the end of the experiment.  
     Event Space $${\mathcal {F}}$$ A set of events; where each event $$A \in \mathcal{F}$$ is a subset of the sample space $$\Omega$$  it is a collection of possible outcomes of an experiment.  
     Probability Measure $$\operatorname {P}$$ A function $$\operatorname {P} \mathcal{F} \rightarrow \mathbb{R}$$ that satisfies the following properties  
         $$\operatorname {P}A \geq 0 \ \forall A \in \mathcal{f}$$ 
         $$\operatorname {P}\Omega = 1$$ $$\operatorname {P}\emptyset = 0$$^1  
         $${\displaystyle \operatorname {P}\bigcupi Ai = \sumi \operatorname {P}Ai }$$ where $$A1 A2 ...$$ are disjoint events  

    Properties  
     $${\text { If } A \subseteq B \Longrightarrow PA \leq PB}$$   
     $${PA \cap B \leq \min PA PB} $$  
     Union Bound $${PA \cup B \leq PA+PB}$$  
     $${P\Omega \backslash A=1 PA}$$.  
     Law of Total Probability LOTB $$\text { If } A{1} \ldots A{k} \text { are a set of disjoint events such that } \cup{i=1}^{k} A{i}=\Omega \text { then } \sum{i=1}^{k} P\leftA{k}\right=1$$  
     Inclusion Exclusion Principle  
        $$\mathbb{P}\left\bigcup{i=1}^{n} A{i}\right=\sum{i=1}^{n} \mathbb{P}\leftA{i}\right \sum{i< j} \mathbb{P}\leftA{i} \cap A{j}\right+\sum{i< j < k} \mathbb{P}\leftA{i} \cap A{j} \cap A{k}\right \cdots+ 1^{n 1} \sum{i< \ldots< n} \mathbb{P}\left\bigcap{i=1}^{n} A{i}\right$$  


              
              



          

^1 Corresponds to "wanting" the probability of events that are certain to have p=1 and events that are impossible to have p=0  
                

1. Random Variables
    A Random Variable is a variable that can take on different values randomly.  
    Formally a random variable $$X$$ is a function that maps outcomes to numerical quantities labels typically real numbers
    $${\displaystyle X\colon \Omega \to \mathbb{R}}$$  

    Think of a R.V. as a numerical "summary" of an aspect of the experiment.  

    Types
     Discrete is a variable that has a finite or countably infinite number of states  
     Continuous is a variable that is a real value  

    Examples  
     Bernoulli A r.v. $$X$$ is said to have a Bernoulli distribution if $$X$$ has only $$2$$ possible values $$0$$ and $$1$$ and $$PX=1 = p PX=0 = 1 p$$; denoted $$\text{Bern}p$$.    
     Binomial The distr. of #successes in $$n$$ independent $$\text{Bern}p$$ trials and its distribution is $$PX=k = \left\begin{array}{l}{n}  {k}\end{array}\right p^k 1 p^{n k}$$; denoted $$\text{Bin}n p$$.          
    

2. Probability Distributions
    A Probability Distribution is a function that describes the likelihood that a random variable or a set of r.v. will take on each of its possible states.  
    Probability Distributions are defined in terms of the Sample Space.  
     Classes  
         Discrete Probability Distribution is encoded by a discrete list of the probabilities of the outcomes known as a Probability Mass Function PMF.  
         Continuous Probability Distribution is described by a Probability Density Function PDF.  
     Types  
         Univariate Distributions are those whose sample space is $$\mathbb{R}$$.  
        They give the probabilities of a single random variable taking on various alternative values 
         Multivariate Distributions also known as Joint Probability distributions  are those whose sample space is a vector space.   
        They give the probabilities of a random vector taking on various combinations of values.  


    A Cumulative Distribution Function CDF is a general functional form to describe a probability distribution  
    $${\displaystyle Fx=\operatorname {P} X\leq x\qquad {\text{ for all }}x\in \mathbb {R} .}$$  
    Because a probability distribution P on the real line is determined by the probability of a scalar random variable X being in a half open interval $$−\infty x$$ the probability distribution is completely characterized by its cumulative distribution function i.e. one can calculate the probability of any event in the event space  
    
 

3. Probability Mass Function
    A Probability Mass Function PMF is a function probability distribution that gives the probability that a discrete random variable is exactly equal to some value.  
    Mathematical Definition  
    Suppose that $$X S \rightarrow A \\\ A {\displaystyle \subseteq }  \mathbb{R}$$ is a discrete random variable defined on a sample space $$S$$. Then the probability mass function $$fX A \rightarrow 0 1$$ for $$X$$ is defined as   
    $$p{X}x=PX=x=P\{s\in SXs=x\}$$  
    The total probability for all hypothetical outcomes $$x$$ is always conserved  
    $$\sum {x\in A}p{X}x=1$$
    Joint Probability Distribution is a PMF over many variables denoted $$P\mathrm{x} = x \mathrm{y} = y$$ or $$Px y$$.  

    A PMF must satisfy these properties  
     The domain of $$P$$ must be the set of all possible states of $$\mathrm{x}$$.  
     $$\forall x \in \mathrm{x} \ 0 \leq Px \leq 1$$. Impossible events has probability $$0$$. Guaranteed events have probability $$1$$.  
     $${\displaystyle \sum{x \in \mathrm{x}} Px = 1}$$ i.e. the PMF must be normalized.  
    
            
4. Probability Density Function
    A Probability Density Function PDF is a function probability distribution whose value at any given sample or point in the sample space can be interpreted as providing a relative likelihood that the value of the random variable would equal that sample.  
    The PDF is defined as the derivative of the CDF  
    $$f{X}x = \dfrac{dF{X}x}{dx}$$  
    A Probability Density Function $$px$$ does not give the probability of a specific state directly; instead the probability of landing inside an infinitesimal region with volume $$\delta x$$ is given by $$px\delta x$$.  
    We can integrate the density function to find the actual probability mass of a set of points. Specifically the probability that $$x$$ lies in some set $$S$$ is given by the integral of $$px$$ over that set.  
    In the Univariate example the probability that $$x$$ lies in the interval $$a b$$ is given by $$\int{a b} pxdx$$  


    A PDF must satisfy these properties  
     The domain of $$P$$ must be the set of all possible states of $$x$$.  
     $$\forall x \in \mathrm{x} \ 0 \leq Px \leq 1$$. Impossible events has probability $$0$$. Guaranteed events have probability $$1$$.  
     $$\int pxdx = 1$$ i.e. the integral of the PDF must be normalized.  
    


44.Cumulative Distribution Function
    A Cumulative Distribution Function CDF is a function probability distribution of a real valued random variable $$X$$ or just distribution function of $$X$$ evaluated at $$x$$ is the probability that $$X$$ will take a value less than or equal to $$x$$.    
    $$F{X}x=\operatorname {P} X\leq x$$   
    The probability that $$X$$ lies in the semi closed interval $$a b$$ where $$a  <  b$$ is therefore  
    $${\displaystyle \operatorname {P} a<X\leq b=F{X}b F{X}a.}$$  
    
    Properties    
     $$0 \leq Fx \leq 1$$ 
     $$\lim{x \rightarrow  \infty} Fx = 0$$ 
     $$\lim{x \rightarrow \infty} Fx = 1$$ 
     $$x \leq y \implies Fx \leq Fy$$.  
    

5. Marginal Probability
    The Marginal Distribution of a subset of a collection of random variables is the probability distribution of the variables contained in the subset.  
    Two variable Case  
    Given two random variables $$X$$ and $$Y$$ whose joint distribution is known the marginal distribution of $$X$$ is simply the probability distribution of $$X$$ averaging over information about $$Y$$.
     Discrete    
        $${\displaystyle \PrX=x=\sum {y}\PrX=xY=y=\sum {y}\PrX=x\mid Y=y\PrY=y}$$    
     Continuous    
        $${\displaystyle p{X}x=\int {y}p{XY}xy\\mathrm {d} y=\int {y}p{X\mid Y}x\mid y\p{Y}y\\mathrm {d} y}$$  
     Marginal Probability as Expectation    
    $${\displaystyle p{X}x=\int {y}p{X\mid Y}x\mid y\p{Y}y\\mathrm {d} y=\mathbb {E} {Y}p{X\mid Y}x\mid y}$$  
    
    

    Marginalization the process of forming the marginal distribution with respect to one variable by summing out the other variable  

    Notes  
    
     Marginal Distribution of a variable is just the prior distr of the variable  
     Marginal Likelihood also known as the evidence or model evidence is the denominator of the Bayes equation. Its only role is to guarantee that the posterior is a valid probability by making its area sum to 1.  

     both terms above are the same  
     Marginal Distr VS Prior  

         Summary  
            Basically it's a conceptual difference.  
            The prior denoted $$p\theta$$ denotes the probability of some event 𝜔 even before any data has been taken.  
            A marginal distribution is rather different. You hold a variable value and integrate over the unknown values.  
            But in some contexts they are the same.  

            


            


6. Conditional Probability
    Conditional Probability is a measure of the probability of an event given that another event has occurred.  
    Conditional Probability is only defined when $$Px > 0$$  We cannot compute the conditional probability conditioned on an event that never happens.   
    Definition  
    $$PA|B={\frac {PA\cap B}{PB}} = {\frac {PA B}{PB}}$$  

    Intuitively it is a way of updating your beliefs/probabilities given new evidence. It's inherently a sequential process.  



7. The Chain Rule of Conditional Probability
    Any joint probability distribution over many random variables may be decomposed into conditional distributions over only one variable.  
    The chain rule permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities    
    $$\mathrm {P} \left\bigcap {k=1}^{n}A{k}\right=\prod {k=1}^{n}\mathrm {P} \leftA{k}\{\Bigg |}\\bigcap {j=1}^{k 1}A{j}\right$$  

8. Independence and Conditional Independence
    Two random variables $$x$$ and $$y$$ or events  are independent if their probability distribution can be expressed as a product of two factors one involving only $$x$$ and one involving only $$y$$  
    $$\mathrm{P}A \cap B = \mathrm{P}A\mathrm{P}B$$  

    Two random variables $$A$$ and $$B$$ are conditionally independent given a random variable $$Y$$ if the conditional probability distribution over $$A$$ and $$B$$ factorizes in this way for every value of $$Y$$  
    $$\PrA\cap B\mid Y=\PrA\mid Y\PrB\mid Y$$  
    or equivalently  
    $$\PrA\mid B\cap Y=\PrA\mid Y$$  
    In other words $$A$$ and $$B$$ are conditionally independent given $$Y$$ if and only if given knowledge that $$Y$$ occurs knowledge of whether $$A$$ occurs provides no information on the likelihood of $$B$$ occurring and knowledge of whether $$B$$ occurs provides no information on the likelihood of $$A$$ occurring.  


    Pairwise VS Mutual Independence  
     Pairwise  
        $$\mathrm{P}\leftA{m} \cap A{k}\right=\mathrm{P}\leftA{m}\right \mathrm{P}\leftA{k}\right$$  
     Mutual Independence
        $$\mathrm{P}\left\bigcap{i=1}^{k} B{i}\right=\prod{i=1}^{k} \mathrm{P}\leftB{i}\right$$  
        for all subsets of size $$k \leq n$$  

    Pairwise independence does not imply mutual independence but the other way around is TRUE by definition.  



    Notation  
     $$A$$ is Independent from $$B$$  $$A{\perp}B$$
     $$A$$ and $$B$$ are conditionally Independent given $$Y$$  $$A{\perp}B \\vert Y$$  

    Notes  
    
     Unconditional Independence is very rare there is usually some hidden factor influencing the interaction between the two events/variables  
     Conditional Independence is the most basic and robust form of knowledge about uncertain environments  
            
    
                
9. Expectation
    The expectation or expected value of some function $$fx$$ with respect to a probability distribution $$Px$$ is the "theoretical" average or mean value that $$f$$ takes on when $$x$$ is drawn from $$P$$.  
    The Expectation of a R.V. is a weighted average of the values $$x$$ that the R.V. can take   $$\operatorname {E}X = \sum{x \in X} x \cdot px$$  
     Discrete case  
        $${\displaystyle \operatorname {E}{x \sim P} fX=fx{1}px{1}+fx{2}px{2}+\cdots +fx{k}px{k}} = \sumx Pxfx$$             
     Continuous case  
    $${\displaystyle \operatorname {E} {x \sim P} fX = \int pxfxdx}$$   
    Linearity of Expectation  
    $${\displaystyle {\begin{aligned}\operatorname {E} X+Y&=\operatorname {E} X+\operatorname {E} Y6pt\operatorname {E} aX&=a\operatorname {E} X\end{aligned}}}$$   
    Independence   
    If $$X$$ and $$Y$$ are independent $$\implies \operatorname {E} XY = \operatorname {E} X \operatorname {E} Y$$  
    

10.Variance
    Variance is the expectation of the squared deviation of a random variable from its mean.  
    It gives a measure of how much the values of a function of a random variable $$x$$ vary as we sample different values of $$x$$ from its probability distribution  
    $$\operatorname {Var} fx=\operatorname {E} \leftfx \mu ^{2}\right = \sum{x \in X} x  \mu^2 \cdot px$$  
    Variance expanded  
    $${\displaystyle {\begin{aligned}\operatorname {Var} X&=\operatorname {E} \leftX \operatorname {E} X^{2}\right
        &=\operatorname {E} \leftX^{2} 2X\operatorname {E} X+\operatorname {E} X^{2}\right
        &=\operatorname {E} \leftX^{2}\right 2\operatorname {E} X\operatorname {E} X+\operatorname {E} X^{2}
        &=\operatorname {E} \leftX^{2}\right \operatorname {E} X^{2}\end{aligned}}}$$     
    Variance as Covariance 
    Variance can be expressed as the covariance of a random variable with itself 
    $$\operatorname {Var} X=\operatorname {Cov} XX$$   
    
    Properties  
     $$\operatorname {Var} a = 0 \forall a \in \mathbb{R}$$ constant $$a$$  
     $$\operatorname {Var} afX = a^2 \operatorname {Var} fX$$ constant $$a$$
     $$\operatorname {Var} X + Y = a^2 \operatorname {Var} X + \operatorname {Var} Y + 2 \operatorname {Cov} X Y$$.  
    

11.Standard Deviation
    The Standard Deviation is a measure that is used to quantify the amount of variation or dispersion of a set of data values.  
    It is defined as the square root of the variance  
    $${\displaystyle {\begin{aligned}\sigma &={\sqrt {\operatorname {E} X \mu ^{2}}}&={\sqrt {\operatorname {E} X^{2}+\operatorname {E}  2\mu X+\operatorname {E} \mu ^{2}}}&={\sqrt {\operatorname {E} X^{2} 2\mu \operatorname {E} X+\mu ^{2}}}&={\sqrt {\operatorname {E} X^{2} 2\mu ^{2}+\mu ^{2}}}&={\sqrt {\operatorname {E} X^{2} \mu ^{2}}}&={\sqrt {\operatorname {E} X^{2} \operatorname {E} X^{2}}}\end{aligned}}}$$  
    
    Properties  
     68% of the data points lie within $$1 \cdot \sigma$$s from the mean
     95% of the data points lie within $$2 \cdot \sigma$$s from the mean
     99% of the data points lie within $$3 \cdot \sigma$$s from the mean
    

12.Covariance
    Covariance is a measure of the joint variability of two random variables.  
    It gives some sense of how much two values are linearly related to each other as well as the scale of these variables  
    $$\operatorname {cov} XY=\operatorname {E} { {\big }X \operatorname {E} XY \operatorname {E} Y{ \big } }$$   
    Covariance expanded  
    $${\displaystyle {\begin{aligned}\operatorname {cov} XY&=\operatorname {E} \left\leftX \operatorname {E} \leftX\right\right\leftY \operatorname {E} \leftY\right\right\right&=\operatorname {E} \leftXY X\operatorname {E} \leftY\right \operatorname {E} \leftX\rightY+\operatorname {E} \leftX\right\operatorname {E} \leftY\right\right&=\operatorname {E} \leftXY\right \operatorname {E} \leftX\right\operatorname {E} \leftY\right \operatorname {E} \leftX\right\operatorname {E} \leftY\right+\operatorname {E} \leftX\right\operatorname {E} \leftY\right&=\operatorname {E} \leftXY\right \operatorname {E} \leftX\right\operatorname {E} \leftY\right.\end{aligned}}}$$   
    when $${\displaystyle \operatorname {E} XY\approx \operatorname {E} X\operatorname {E} Y} $$ this last equation is prone to catastrophic cancellation when computed with floating point arithmetic and thus should be avoided in computer programs when the data has not been centered before.  

    Covariance of Random Vectors  
    $${\begin{aligned}\operatorname {cov} \mathbf {X} \mathbf {Y} &=\operatorname {E} \left\mathbf {X}  \operatorname {E} \mathbf {X} \mathbf {Y}  \operatorname {E} \mathbf {Y} ^{\mathrm {T} }\right&=\operatorname {E} \left\mathbf {X} \mathbf {Y} ^{\mathrm {T} }\right \operatorname {E} \mathbf {X} \operatorname {E} \mathbf {Y} ^{\mathrm {T} }\end{aligned}}$$   

    The Covariance Matrix of a random vector $$x \in \mathbb{R}^n$$ is an $$n \times n$$ matrix such that    
    $$ \operatorname {cov} X {ij} = \operatorname {cov}xi xj 
        \operatorname {cov}xi xj = \operatorname {Var} xi$$   
    Interpretations  
     High absolute values of the covariance mean that the values change very much and are both far from their respective means at the same time.
     The sign of the covariance   
        The sign of the covariance shows the tendency in the linear relationship between the variables  
         Positive  
            the variables tend to show similar behavior
         Negative  
            the variables tend to show opposite behavior  
         Reason  
        If the greater values of one variable mainly correspond with the greater values of the other variable and the same holds for the lesser values i.e. the variables tend to show similar behavior the covariance is positive. In the opposite case when the greater values of one variable mainly correspond to the lesser values of the other i.e. the variables tend to show opposite behavior the covariance is negative.  

    Covariance and Variance  
    $$\operatorname{Var}X+Y=\operatorname{Var}X+\operatorname{Var}Y+2 \operatorname{Cov}X Y$$  

    Covariance and Independence  
    If $$X$$ and $$Y$$ are independent $$\implies \operatorname{cov}X Y=\mathrm{E}X Y \mathrm{E}X \mathrm{E}Y = 0$$.  
     Independence $$\Rightarrow$$ Zero Covariance  
     Zero Covariance $$\nRightarrow$$ Independence

    Covariance and Correlation  
    If $$\operatorname{Cov}X Y=0 \implies $$ $$X$$ and $$Y$$ are Uncorrelated.  



          


          


          

      

13.Mixtures of Distributions
    It is also common to define probability distributions by combining other simpler probability distributions. One common way of combining distributions is to construct a mixture distribution.    
    A Mixture Distribution is the probability distribution of a random variable that is derived from a collection of other random variables as follows  a random variable is selected by chance from the collection according to given probabilities of selection and then the value of the selected random variable is realized.    
    On each trial the choice of which component distribution should generate the sample is determined by sampling a component identity from a multinoulli distribution  
    $$Px = \sumi Px=iPx \vert c=i$$    
    where $$Pc$$ is the multinoulli distribution over component identities.    

14.Bayes' Rule
    Bayes' Rule describes the probability of an event based on prior knowledge of conditions that might be related to the event.    
    $${\displaystyle PA\mid B={\frac {PB\mid A\PA}{PB}}}$$  
    where   
    $$PB =\sumA PB \vert A PA$$  


15.Common Random Variables
    Discrete RVs    
    
     Bernoulli  
     Binomial  
     Geometric  
     Poisson  

    Continuous RVs  
    
     Uniform  
     Exponential  
     Normal/Gaussian  
            
            
16.Summary of Distributions


17.Formulas
     $$\overline{X} = \hat{\mu}$$  
     $$\operatorname {E}\overline{X}=\operatorname {E}\left\frac{X{1}+\cdots+X{n}}{n}\right = \mu$$  
     $$\operatorname{Var}\overline{X}=\operatorname{Var}\left\frac{X{1}+\cdots+X{n}}{n}\right = \dfrac{\sigma^2}{n}$$    
     $$\operatorname {E}\leftX{i}^{2}\right=\operatorname {Var} X+\operatorname {E} X^{2} = \sigma^{2}+\mu^{2}$$  
     $$\operatorname {E}\left\overline{X}^{2}\right=\operatorname {E}\left\hat{\mu}^{2}\right=\frac{\sigma^{2}}{n}+\mu^{2}\$$ ^2  
    


18.Correlation
    In the broadest sense correlation is any statistical association though it commonly refers to the degree to which a pair of variables are linearly related.  

    There are several correlation coefficients often denoted $${\displaystyle \rho }$$ or $$r$$ measuring the degree of correlation  


    It is a measure of the linear correlation between two variables $$X$$ and $$Y$$.  
    $$\rho{X Y}=\frac{\operatorname{cov}X Y}{\sigma{X} \sigma{Y}}$$  
    where $${\displaystyle \sigma{X}}$$ is the standard deviation of $${\displaystyle X}$$ and $${\displaystyle \sigma{Y}}$$  is the standard deviation of $${\displaystyle Y}$$ and $$\rho \in  1 1$$.   



    Correlation and Independence  
    1. Uncorrelated $$\nRightarrow$$ Independent  
    2. Independent $$\implies$$ Uncorrelated  

    Zero correlation will indicate no linear dependency however won't capture non linearity. Typical example is uniform random variable $$x$$ and $$x^2$$ over $$ 11$$ with zero mean. Correlation is zero but clearly not independent.  
     

19.Probabilistic Inference
    Probabilistic Inference compute a desired probability from other known probabilities e.g. conditional from joint.  

    We generally compute Conditional Probabilities  
    
     $$p\text{sun} \vert T=\text{12 pm} = 0.99$$  
     These represent the agents beliefs given the evidence  

    Probabilities change with new evidence  
     
     $$p\text{sun} \vert T=\text{12 pm} C=\text{Stockholm} = 0.85$$  
    $$\longrightarrow$$  
     $$p\text{sun} \vert T=\text{12 pm} C=\text{Stockholm} M=\text{Jan} = 0.40$$  
     Observing new evidence causes beliefs to be updated

    Inference by Enumeration  
    


          

    Problems  
     Worst case time complexity $$\mathrm{O}\left\mathrm{d}^{n}\right$$
     Space complexity $$\mathrm{O}\left\mathrm{d}^{n}\right$$ to store the joint distribution  

    Inference with Bayes Theorem  
     Diagnostic Probability from Causal Probability  
        $$P\text { cause } | \text { effect }=\frac{P\text { effect } | \text { cause } P\text { cause }}{P\text { effect }}$$  




^2 Comes from $$\operatorname{Var}\overline{X}=\operatorname {E}\left\overline{X}^{2}\right \{\operatorname {E}\overline{X}\}^{2}$$  



Discrete Distributions



2. Bernoulli Distribution
       A distribution over a single binary random variable.  
        It is controlled by a single parameter $$\phi \in 0 1$$ which fives the probability of the r.v. being equal to $$1$$.  
        It models the probability of a single experiment with a boolean outcome e.g. coin flip $$\rightarrow$$ {heads 1 tails 0}  
       PMF  
       $${\displaystyle Px={\begin{cases}p&{\text{if }}p=1q=1 p&{\text{if }}p=0.\end{cases}}}$$  
       Properties  
        $$PX=1 = \phi$$
        $$PX=0 = 1  \phi$$
        $$PX=x = \phi^x 1  \phi^{1 x}$$
        $$\operatorname {E}X = \phi$$
        $$\operatorname {Var}X = \phi 1  \phi$$

3. Binomial Distribution
    $${\binom {n}{k}}={\frac {n!}{k!n k!}}$$ is the number of possible ways of getting $$x$$ successes and $$n x$$ failures

    

Notes Tips and Tricks

 It is more practical to use a simple but uncertain rule rather than a complex but certain one even if the true rule is deterministic and our modeling system has the fidelity to accommodate a complex rule.  
    For example the simple rule “Most birds fly” is cheap to develop and is broadly useful while a rule of the form “Birds fly except for very young birds that have not yet learned to fly sick or injured birds that have lost the ability to fly flightless species of birds including the cassowary ostrich and kiwi. . .” is expensive to develop maintain and communicate and after all this effort is still brittle and prone to failure.

 Disjoint Events Mutually Exclusive{ .bodyContents10 } are events that cannot occur together at the same time
    Mathematically  
     $$Ai \cap Aj = \varnothing$$ whenever $$i \neq j$$  
     $$pAi Aj = 0$$  

 Complexity of Describing a Probability Distribution  
    A description of a probability distribution is exponential in the number of variables it models.  
    The number of possibilities is exponential in the number of variables.  

 Probability VS Likelihood  
    Probabilities are the areas under a fixed distribution  
    $$pr$$data$$|$$distribution$$$$  
    i.e. probability of some data left hand side given a distribution described by the right hand side  
    Likelihoods are the y axis values for fixed data points with distributions that can be moved..  
    $$L$$distribution$$|$$observation/data$$$$  

    Likelihood is basically a specific probability that can only be calculated after the fact of observing some outcomes. It is not normalized to $$1$$ it is not a probability. It is just a way to quantify how likely a set of observation is to occur given some distribution with some parameters; then you can manipulate the parameters to make the realization of the data more "likely" it is precisely meant for that purpose of estimating the parameters; it is a function of the parameters.  
    Probability on the other hand is absolute for all possible outcomes. It is a function of the Data.  

 Maximum Likelihood Estimation  
    A method that tries to find the optimal value for the mean and/or stdev for a distribution given some observed measurements/data points.

 Variance  
    When $$\text{Var}X = 0 \implies X = EX = \mu$$. not interesting  

 Reason we sometimes prefer Biased Estimators  
        


 The Theory of Learning








Learning

1. Learning
    Learning is the process of acquiring new or modifying existing knowledge behaviors skills values or preferences.  
    




Types of Learning

1. Hebbian Associative Learning
    Hebbian/Associative Learning is the process by which a person or animal learns an association between two stimuli or events in which simultaneous activation of cells leads to pronounced increases in synaptic strength between those cells.  

    Hebbian Learning in Artificial Neural Networks  
    From the pov of ANNs Hebb's principle can be described as a method of determining how to alter the weights between model neurons.  
    
     The weight between two neurons  
         Increases if the two neurons activate simultaneously  
         Reduces if they activate separately.  
     Nodes that tend to be either both positive or both negative at the same time have strong positive weights 
        while those that tend to be opposite have strong negative weights.  

    Hebb's Rule  
    The change in the $i$ th synaptic weight $w{i}$ is equal to a learning rate $\eta$ times the $i$ th input $x{i}$ times the postsynapic response $y$  
    $$\Delta w{i}=\eta x{i} y$$  
    where in the case of a linear neuron  
    $$y=\sum{j} w{j} x{j}$$  


    Notes  
    
     It is regarded as the neuronal basis of unsupervised learning.  
    


2. Supervised Learning
    Supervised Learning the task of learning a function that maps an input to an output based on example input output pairs.  
    

3. Unsupervised Learning
    Unsupervised Learning the task of making inferences by learning a better representation from some datapoints that do not have any labels associated with them.  


    
     Clustering
         hierarchical clustering
         k means
         mixture models
         DBSCAN
     Anomaly Detection Local Outlier Factor
     Neural Networks
         Autoencoders
         Deep Belief Nets
         Hebbian Learning
         Generative Adversarial Networks
         Self organizing map
     Approaches for learning latent variable models such as
         Expectation maximization algorithm EM
         Method of moments
         Blind signal separation techniques
             Principal component analysis
             Independent component analysis
             Non negative matrix factorization
             Singular value decomposition


    

4. Reinforcement Learning
    Reinforcement Leaning the task of learning how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward.  
    

5. Semi supervised Learning


6. Zero Shot Learning

7. Transfer Learning

8. Multitask Learning

9. Domain Adaptation


Theories of Learning



Reasoning and Inference

1. Logical Reasoning


     Inductive Learning is the process of using observations to draw conclusions  
         It is a method of reasoning in which the premises are viewed as supplying some evidence for the truth of the conclusion.  
         It goes from specific to general "bottom up logic".    
         The truth of the conclusion of an inductive argument may be probable
     Deductive Learning is the process of using conclusions to form observations.  
         It is the process of reasoning from one or more statements premises to reach a logically certain conclusion.  
         It goes from general to specific "top down logic".    
         The conclusions reached "observations" are necessarily True.  
     Abductive Learning is a form of inductive learning where we use observations to draw the simplest and most likely conclusions.  
        It can be understood as "inference to the best explanation".  
        It is used by Sherlock Holmes.  


    In Mathematical Modeling ML  
    In the context of Mathematical Modeling the three kinds of reasoning can be described as follows  
    
     The construction/creation of the structure of the model is abduction.  
     Assigning values or probability distributions to the parameters of the model is induction.  
     Executing/running the model is deduction.  

    


2. Inference
    Inference has two definitions  
    
    1. A conclusion reached on the basis of evidence and reasoning.  
    2. The process of reaching such a conclusion.  
    


3. Transductive Inference/Learning Transduction

    The Goal of Transductive Learning is to “simply” add labels to the unlabeled data by exploiting labelled samples.  
    While the goal of inductive learning is to infer the correct mapping from $$X$$ to $$Y$$.  

    Transductive VS Semi supervised Learning  
    Transductive Learning is only concerned with the unlabeled data.   

    Transductive Learning  

    Inductive Learning  


    Notes  
    

    






 Statistical Learning Theory













Fundamental Theorem of Statistical Learning binary classification  
Let $$\mathcal{H}$$ be a hypothesis class of functions from a domain $$X$$ to $$\{01\}$$ and let the loss function be the $$0 1$$ loss.  
The following are equivalent  
$$\begin{array}{l}{\text { 1. } \mathcal{H} \text { has uniform convergence. }}  {\text { 2. The ERM is a PAC learning algorithm for } \mathcal{H} \text { . }}  {\text { 3. } \mathcal{H} \text { is } PAC \text { learnable. }}  {\text { 4. } \mathcal{H} \text { has finite } VC \text { dimension. }}\end{array}$$  
This can be extended to regression and multiclass classification.   



 1 $$\Rightarrow 2$$ We have seen uniform convergence implies that $$\mathrm{ERM}$$ is $$\mathrm{PAC}$$ learnable
 2 $$\Rightarrow 3$$ Obvious.
 3 $$\Rightarrow 4$$ We just proved that PAC learnability implies finite $$\mathrm{VC}$$ dimension.
 4 $$\Rightarrow 1$$ We proved that finite $$\mathrm{VC}$$ dimension implies uniform convergence.


Notes  

 VC dimension fully determines learnability for binary classification.  
 The VC dimension doesn’t just determine learnability it also gives a bound on the sample complexity which can be shown to be tight

 



Statistical Learning Theory

1. Statistical Learning Theory
    Statistical Learning Theory is a framework for machine learning drawing from the fields of statistics and functional analysis. Under certain assumptions this framework allows us to study the question  
    How can we affect performance on the test set when we can only observe the training set?

    It is a statistical approach to Computational Learning Theory.  

2. Formal Definition
    Let  
     $$X$$ $$\$$ the vector space of all possible inputs  
     $$Y$$ $$\$$ the vector space of all possible outputs  
     $$Z = X \times Y$$ $$\$$ the product space of inputoutput pairs  
     $$n$$ $$\$$ the number of samples in the training set  
     $$S=\left\{\left\vec{x}{1} y{1}\right \ldots\left\vec{x}{n} y{n}\right\right\}=\left\{\vec{z}{1} \ldots \vec{z}{n}\right\}$$ $$\$$ the training set  
     $$\mathcal{H} = f  X \rightarrow Y$$ $$\$$ the hypothesis space of all functions  
     $$Vf\vec{x} y$$ $$\$$ an error/loss function  

    Assumptions  
    
     The training and test data are generated by an unknown joint probability distribution over datasets over the product space $$Z$$ denoted $$p{\text{data}}z=p\vec{x} y$$ called the data generating process.  
         $$p{\text{data}}$$ is a joint distribution so that it allows us to model uncertainty in predictions e.g. from noise in data because $$y$$ is not a deterministic function of $$\vec{x}$$ but rather a random variable with conditional distribution $$py \vert \vec{x}$$ for a fixed $$\vec{x}$$.  
     The i.i.d. assumptions  
         The examples in each dataset are independent from each other  
         The training set and test set are identically distributed drawn from the same probability distribution as each other  

        A collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent.  
        Informally it says that all the variables provide the same kind of information independently of each other.  



    The Inference Problem
    Finding a function $$f  X \rightarrow Y$$ such that $$f\vec{x} \sim y$$.  

    The Expected Risk  
    $$If=\mathbf{E}Vf\vec{x} y=\int{X \times Y} Vf\vec{x} y p\vec{x} y d \vec{x} d y$$  

    The Target Function  
    is the best possible function $$f$$ that can be chosen is given by  
    $$f=\inf{h \in \mathcal{H}} Ih$$  

    The Empirical Risk  
    Is a proxy measure for the expected risk based on the training set.  
    It is necessary because the probability distribution $$p\vec{x} y$$ is unknown.  
    $$I{S}f=\frac{1}{n} \sum{i=1}^{n} V\leftf\left\vec{x}{i}\right y{i}\right$$  

3. Empirical risk minimization
    Empirical Risk Minimization ERM is a principle in statistical learning theory that is based on approximating the Generalization Error True Risk by measuring the Training Error Empirical Risk i.e. the performance on training data.  

    A learning algorithm that chooses the function $$f{S}$$ that minimizes the empirical risk is called empirical risk minimization  
    $$R{\mathrm{emp}}h = I{S}f=\frac{1}{n} \sum{i=1}^{n} V\leftf\left\vec{x}{i}\right y{i}\right$$    
    $$f{S} = \hat{h} = \arg \min {h \in \mathcal{H}} R{\mathrm{emp}}h$$  

    Complexity  
    
     onclick="showTextwithParentPopHideevent;"}
     Empirical risk minimization for a classification problem with a 0 1 loss function is known to be an NP hard problem even for such a relatively simple class of functions as linear classifiers.  

     Though it can be solved efficiently when the minimal empirical risk is zero i.e. data is linearly separable.  
     Coping with Hardness  
         Employing a convex approximation to the 0 1 loss Hinge Loss SVM  
         Imposing Assumptions on the data generating distribution and thus stop being an agnostic learning algorithm.  
  


4. Definitions
    Generalization Error  
        AKA Expected Risk/Error Out of Sample Error^2 $$E{\text{out}}$$   
        It is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.  
    Generalization Gap  
        It is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.  

        Formally  
        The generalization gap is the difference between the expected and empirical error  
        $$G =I\leftf{n}\right I{S}\leftf{n}\right$$  

        An Algorithm is said to Generalize achieve Generalization if  
        $$\lim {n \rightarrow \infty} Gn = \lim {n \rightarrow \infty} I\leftf{n}\right I{S}\leftf{n}\right=0$$  
        Equivalently  
        $$E{\text { out }}g \approx E{\text { in }}g$$  
        or 
        $$I\leftf{n}\right \approx I{S}\leftf{n}\right$$  

        Computing the Generalization Gap  
        Since $$I\leftf{n}\right$$ cannot be computed for an unknown distribution the generalization gap cannot be computed either.  
        Instead the goal of statistical learning theory is to bound or characterize the generalization gap in probability  
        $$P{G}=P\leftI\leftf{n}\right I{S}\leftf{n}\right \leq \epsilon\right \geq 1 \delta{n}$$  
        That is the goal is to characterize the probability $${\displaystyle 1 \delta {n}}$$ that the generalization gap is less than some error bound $${\displaystyle \epsilon }$$ known as the learning rate and generally dependent on $${\displaystyle \delta }$$ and $${\displaystyle n}$$.  
    The Empirical Distribution  
        AKA Data Generating Distribution  
        is the discrete uniform distribution over the sample points.   
    The Approximation Generalization Tradeoff  
         Goal  
            Small $$E{\text{out}}$$ Good approximation of $$f$$ out of sample not in sample.  
         The tradeoff is characterized by the complexity of the hypothesis space $$\mathcal{H}$$  
             More Complex $$\mathcal{H}$$ Better chance of approximating $$f$$  
             Less Complex $$\mathcal{H}$$ Better chance of generalizing out of sample  


              

    Excess Risk Generalization Gap Decomposition \| Estimation Approximation Tradeoff  
        Excess Risk is defined as the difference between the expected risk/generalization error of any function $$\hat{f} = g^{\mathcal{D}}$$ that we learn from the data exactly just bias variance and the expected risk of the target function $$f$$ known as the bayes optimal predictor


              

    

8. Notes
     Choices of Loss Functions  
         Regression  
             MSE $$\ Vf\vec{x} y=y f\vec{x}^{2}$$ 
             MAE $$\ Vf\vec{x} y=\vert{y f\vec{x}}\vert$$  
         Classification  
             Binary $$\ Vf\vec{x} y=\theta y f\vec{x}$$  
                where $$\theta$$ is the Heaviside Step Function.  
     Training Data Errors and Risk  
         Training Error is the Empirical Risk  
             It is a proxy for the Generalization Error/Expected Risk  
             This is what we minimize
         Test Error is an approximation to the Generalization Error/Expected Risk 
             This is what we can compute to ensure that minimizing Training Err/Empirical Risk ERM also minimized the Generalization Err/Expected Risk which we can't compute directly  
     Why the goal is NOT to minimize $$E{\text{in}}$$ completely intuition  
        Basically if you have noise in the data; then fitting the finite training data completely; i.e. minimizing the in sample err completely will underestimate the out of sample err.  
        Since if noise existed AND you fit training data completely $$E{\text{in}} = 0$$ THEN you inherently have fitted the noise AND your performance on out sample will be lower.   



The Vapnik Chervonenkis VC Theory


The Bias Variance Decomposition Theory

11.The Bias Variance Decomposition Theory
    The Bias Variance Decomposition Theory is a way to quantify the Approximation Generalization Tradeoff.  

    Assumptions  
    
     The analysis is done over the entire data distribution  
     The target function $$f$$ is already known; and you're trying to answer the question  
        "How can $$\mathcal{H}$$ approximate $$f$$ over all? not just on your sample."  
     Applies to real valued targets can be extended  
     Use Square Error can be extended  
    

1. The Bias Variance Decomposition
    The Bias Variance Decomposition is a way of analyzing a learning algorithm's expected out of sample error^1 as a sum of three terms  
     Bias is an error from erroneous assumptions in the learning algorithm.  
     Variance is an error from sensitivity to small fluctuations in the training set.  
     Irreducible Error resulting from noise in the problem itself  

    Equivalently Bias and Variance measure two different sources of errors in an estimator   
     Bias measures the expected deviation from the true value of the function or parameter.  
        AKA Approximation Error^3  statistics  How well can $$\mathcal{H}$$ approximate the target function '$$f$$'  
     Variance measures the deviation from the expected estimator value that any particular sampling of the data is likely to cause.  
        AKA Estimation Generalization Error statistics How well we can zoom in on a good $$h \in \mathcal{H}$$  


    Bias Variance Decomposition Formula  
    For any function $$\hat{f} = g^{\mathcal{D}}$$ we select we can decompose its expected out of sample error on an unseen sample $$x$$ as  
    $$\mathrm{E}\lefty \hat{f}x^{2}\right=\operatorname{Bias}\hat{f}x^{2}+\operatorname{Var}\hat{f}x+\sigma^{2}$$  
    Where  
     Bias  
        $$\operatorname{Bias}\hat{f}x=\mathrm{E}\hat{f}x fx$$  
     Variance  
        $$\operatorname{Var}\hat{f}x=\mathrm{E}\left\hat{f}x^{2}\right \mathrm{E}\hat{f}x^{2}$$  
    and the expectation ranges over different realizations of the training set $$\mathcal{D}$$.  

2. The Bias Variance Tradeoff
    is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples and vice versa.  

    Effects of Bias
    
     High Bias simple models lead to underfitting
     Low Bias complex models lead to overfitting
    
    Effects of Variance
    
     High Variance complex models lead to overfitting
     Low Variance simple models lead to underfitting
    

3. Derivation
    $${\displaystyle {\begin{aligned}\mathbb{E}{\mathcal{D}} {\big }Ig^{\mathcal{D}}{\big }&=\mathbb{E}{\mathcal{D}} {\big }\mathbb{E}{x}{\big }g^{\mathcal{D}} y^{2}{\big }{\big }
    &=\mathbb{E}{x} {\big }\mathbb{E}{\mathcal{D}}{\big }g^{\mathcal{D}} y^{2}{\big }{\big }
    &=\mathbb{E}{x}{\big }\mathbb{E}{\mathcal{D}}{\big }g^{\mathcal{D}} f  \varepsilon^{2}{\big }{\big }
    &=\mathbb{E}{x}{\big }\mathbb{E}{\mathcal{D}} {\big }f+\varepsilon  g^{\mathcal{D}}+\bar{g} \bar{g}^{2}{\big }{\big }
    &=\mathbb{E}{x}{\big }\mathbb{E}{\mathcal{D}} {\big }\bar{g} f^{2}{\big }+\mathbb{E}{\mathcal{D}} \varepsilon ^{2}+\mathbb{E}{\mathcal{D}} {\big }g^{\mathcal{D}}  \bar{g}^{2}{\big }+2\ \mathbb{E}{\mathcal{D}} {\big }\bar{g} f\varepsilon {\big }+2\ \mathbb{E}{\mathcal{D}} {\big }\varepsilon g^{\mathcal{D}} \bar{g}{\big }+2\ \mathbb{E}{\mathcal{D}} {\big }g^{\mathcal{D}}  \bar{g}\bar{g} f{\big }{\big }
    &=\mathbb{E}{x}{\big }\bar{g} f^{2}+\mathbb{E}{\mathcal{D}} \varepsilon ^{2}+\mathbb{E}{\mathcal{D}} {\big }g^{\mathcal{D}}  \bar{g}^{2}{\big }+2\bar{g} f\mathbb{E}{\mathcal{D}} \varepsilon \ +2\ \mathbb{E}{\mathcal{D}} \varepsilon \ \mathbb{E}{\mathcal{D}} {\big }g^{\mathcal{D}} \bar{g}{\big }+2\ \mathbb{E}{\mathcal{D}} {\big }g^{\mathcal{D}} \bar{g}{\big }\bar{g} f{\big }
    &=\mathbb{E}{x}{\big }\bar{g} f^{2}+\mathbb{E}{\mathcal{D}} \varepsilon ^{2}+\mathbb{E}{\mathcal{D}} {\big }g^{\mathcal{D}}  \bar{g}^{2}{\big }{\big }
    &=\mathbb{E}{x}{\big }\bar{g} f^{2}+\operatorname {Var} y+\operatorname {Var} {\big }g^{\mathcal{D}}{\big }{\big }
    &=\mathbb{E}{x}{\big }\operatorname {Bias} g^{\mathcal{D}}^{2}+\operatorname {Var} y+\operatorname {Var} {\big }g^{\mathcal{D}}{\big }{\big }
    &=\mathbb{E}{x}{\big }\operatorname {Bias} g^{\mathcal{D}}^{2}+\sigma ^{2}+\operatorname {Var} {\big }g^{\mathcal{D}}{\big }{\big }
\end{aligned}}}$$  
    where  
    $$\overline{g}\mathbf{x}=\mathbb{E}{\mathcal{D}}\leftg^{\mathcal{D}}\mathbf{x}\right$$ is the average hypothesis over all realization of $$N$$ data points $$\mathcal{D} i$$ and $${\displaystyle \varepsilon }$$ and $${\displaystyle {\hat {f}}} = g^{\mathcal{D}}$$ are independent.    

    
     onclick="showTextwithParentPopHideevent;"}
    $${\displaystyle {\begin{aligned}\operatorname {E} {\mathcal{D}} {\big }y {\hat {f}}^{2}{\big }&=\operatorname {E} {\big }f+\varepsilon  {\hat {f}}^{2}{\big }&=\operatorname {E} {\big }f+\varepsilon  {\hat {f}}+\operatorname {E} {\hat {f}} \operatorname {E} {\hat {f}}^{2}{\big }&=\operatorname {E} {\big }f \operatorname {E} {\hat {f}}^{2}{\big }+\operatorname {E} \varepsilon ^{2}+\operatorname {E} {\big }\operatorname {E} {\hat {f}} {\hat {f}}^{2}{\big }+2\operatorname {E} {\big }f \operatorname {E} {\hat {f}}\varepsilon {\big }+2\operatorname {E} {\big }\varepsilon \operatorname {E} {\hat {f}} {\hat {f}}{\big }+2\operatorname {E} {\big }\operatorname {E} {\hat {f}} {\hat {f}}f \operatorname {E} {\hat {f}}{\big }&=f \operatorname {E} {\hat {f}}^{2}+\operatorname {E} \varepsilon ^{2}+\operatorname {E} {\big }\operatorname {E} {\hat {f}} {\hat {f}}^{2}{\big }+2f \operatorname {E} {\hat {f}}\operatorname {E} \varepsilon +2\operatorname {E} \varepsilon \operatorname {E} {\big }\operatorname {E} {\hat {f}} {\hat {f}}{\big }+2\operatorname {E} {\big }\operatorname {E} {\hat {f}} {\hat {f}}{\big }f \operatorname {E} {\hat {f}}&=f \operatorname {E} {\hat {f}}^{2}+\operatorname {E} \varepsilon ^{2}+\operatorname {E} {\big }\operatorname {E} {\hat {f}} {\hat {f}}^{2}{\big }&=f \operatorname {E} {\hat {f}}^{2}+\operatorname {Var} y+\operatorname {Var} {\big }{\hat {f}}{\big }&=\operatorname {Bias} {\hat {f}}^{2}+\operatorname {Var} y+\operatorname {Var} {\big }{\hat {f}}{\big }&=\operatorname {Bias} {\hat {f}}^{2}+\sigma ^{2}+\operatorname {Var} {\big }{\hat {f}}{\big }\end{aligned}}}$$


4. Results and Takeaways of the Decomposition
    Match the "Model Complexity" to the Data Resources NOT to the Target Complexity.  

    
    onclick="showTextwithParentPopHideevent;"}
    Pretty much like I'm sitting in my office and I want a document of some kind an old letter. Someone has asked me for a letter of recommendation and I don't want to rewrite it from scratch. So I want to take the older letter and just see what I wrote and then add the update to that.  
    Before everything was archived in the computers it used to be a piece of paper. So I know the letter of recommendation is somewhere. Now I face the question should I write the letter of recommendation from scratch? Or should I look for the letter of recommendation? The recommendation is there. It's much easier when I find it. However finding it is a big deal. So the question is not that the target function is there. The question is can I find it?  
    Therefore when I give you 100 examples you choose the hypothesis set to match the 100 examples. If the 100 examples are terribly noisy that's even worse. Because their information to guide you is worse.  
    The data resources you have is "what do you have in order to navigate the hypothesis set?". Let's pick a hypothesis set that we can afford to navigate. That is the game in learning. Done with the bias and variance.


5. Measuring the Bias and Variance
     Training Error reflects Bias NOT variance
     Test Error reflects Both


6. Reducing the Bias and Variance and Irreducible Err
     Adding Good Feature  
         Decrease Bias  
     Adding Bad Feature  
         Doesn't affect increase Bias much  
     Adding ANY Feature  
         Increases Variance  
     Adding more Data  
         Decreases Variance
         May Decreases Bias if $$h$$ can fit $$f$$ exactly.  
     Noise in Test Set  
         Affects ONLY Irreducible Err
     Noise in Training Set  
         Affects BOTH and ONLY Bias and Variance  
     Dimensionality Reduction  
         Decrease Variance by simplifying models  
     Feature Selection  
         Decrease Variance by simplifying models  
     Regularization  
         Increase Bias
         Decrease Variance
     Increasing # of Hidden Units in ANNs  
         Decrease Bias
         Increase Variance  
     Increasing # of Hidden Layers in ANNs  
         Decrease Bias  
         Increase Variance  
     Increasing $$k$$ in K NN  
         Increase Bias
         Decrease Variance  
     Increasing Depth in Decision Trees  
         Increase Variance  
     Boosting  
         Decreases Bias  
     Bagging  
         Reduces Variance              

     We Cannot Reduce the Irreducible Err  
            
            



7. Application of the Decomposition to Classification
    A similar decomposition exists for  
     Classification w/ $$0 1$$ loss  
     Probabilistic Classification w/ Squared Error  

8. Bias Variance Decomposition and Risk Excess Risk Decomposition
    The Bias Variance Decomposition analyzes the behavior of the Expected Risk/Generalization Error for any function $$\hat{f}$$  
    $$R\hat{f} = \mathrm{E}\lefty \hat{f}x^{2}\right=\operatorname{Bias}\hat{f}x^{2}+\operatorname{Var}\hat{f}x+\sigma^{2}$$  
    Assuming that $$y = fx + \epsilon$$.  

    The Bayes Optimal Predictor is $$fx = \mathrm{E}Y\vert X=x$$.  

    The Excess Risk is  
    $$\text{ExcessRisk}\hat{f} = R\hat{f}  Rf$$  

    Excess Risk Decomposition  
    We add and subtract the target function $$f{\text{target}}=\inf{h \in \mathcal{H}} Ih$$ that minimizes the true expected risk  
    $$\text{ExcessRisk}\hat{f} = \underbrace{\leftR\hat{f}  Rf{\text{target}}\right} {\text { estimation error }} + \underbrace{\leftRf{\text{target}}  Rf\right} {\text { approximation error }}$$  



    The Bias Variance Decomposition for Excess Risk  
     Re Writing Excess Risk  
        $$\text{ExcessRisk}\hat{f} = R\hat{f}  Rf = \mathrm{E}\lefty \hat{f}x^{2}\right  \mathrm{E}\lefty fx^{2}\right$$  
        which is equal to  
        $$R\hat{f}  Rf = \mathrm{E}\leftfx \hat{f}x^{2}\right$$  
    $$\mathrm{E}\leftfx \hat{f}x^{2}\right = \operatorname{Bias}\hat{f}x^{2}+\operatorname{Var}\hat{f}x$$  

     if you dont want to mess with stat jargon; lemme rephrase  
        is the minimizer $${\displaystyle f=\inf{h \in \mathcal{H}} Ih}$$ where $$Ih$$ is the expected risk/generalization error assume MSE;  
        is it $$\overline{f}\mathbf{x}=\mathbb{E} {\mathcal{D}}\leftf^{\mathcal{D}}\mathbf{x}\right$$ the average hypothesis over all realizations of $$N$$ data points $$\mathcal{D} i$$??  



Generalization Theory




1. Generalization Theory
    

    Approaches to Notions of Quantitative Description of Generalization Theory  
    
     VC Dimension
     Rademacher Complexity
     PAC Bayes Bound  



    Prescriptive vs Descriptive Theory  
    
     Prescriptive only attaches a label to the problem without giving any insight into how to solve the problem.  
     Descriptive describes the problem in detail e.g. by providing cause and allows you to solve the problem.  

    Generalization Theory Notions consist of attaching a descriptive label to the basic phenomenon of lack of generalization. They are hard to compute for today’s complicated ML models let alone to use as a guide in designing learning systems.  
    Generalization Bounds as Descriptive Labels  
    
     Rademacher Complexity  
        
         labels and loss are 01  
         the badly generalizing $$h$$ predicts perfectly on the training sample $$S$$ and  
            is completely wrong on the heldout set $$S2$$ meaning  
            $$\Delta{S}h \Delta{S{2}}h \approx 1$$  

    
        


4. Overfitting
    One way to summarize Overfitting is  
    discovering patterns in data that do not exist in the intended application.  
    The typical case of overfitting "decreasing loss only on the training set and increasing the loss on the validation set" is only one example of this.  

    The question then is how we prevent overfitting from occurring.  
    The problem is that we cannot know apriori what patterns will generalize from the dataset.  


    No Free Lunch NFL Theorem  
    
     Suppose you see someone toss a coin and get heads. What is the probability distribution over the next result of the coin toss?  
        Did you think heads 50% and tails 50%? If so you’re wrong the answer is that we don’t know. For all we know the coin could have heads on both sides. Or it might even obey some strange laws of physics and come out as tails every  toss. The point is there is no way we can extract only patterns that will generalize there will always be some scenarios where those patterns will not exist. This is the essence of what is called the No Free Lunch Theorem any model that you create will always “overfit” and be completely wrong in some scenarios.  
  


    



^1 with respect to a particular problem.  
^2 Note that Abu Mostafa defines out sample error $$E{\text{out}}$$ as the expected error/risk $$If$$; thus making $$G = E{\text{out}}  E{\text{in}}$$.  
^3 can be viewed as a measure of the average network approximation error over all possible training data sets $$\mathcal{D}$$   


 Regularization



Resources  

 Regularization in FFN/workfiles/research/dl/nlp/dlbookpt1  
 Regularization Concept/concepts  











Regularization Basics and Definitions

1. Regularization
    Regularization can be loosely defined as any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.  

    Formally it is a set of techniques that impose certain restrictions on the hypothesis space by adding information in order to solve an ill posed problem or to prevent overfitting.^1  
    


2. Theoretical Justification for Regularization
    A theoretical justification for regularization is that it attempts to impose Occam's razor on the solution.  
    From a Bayesian point of view many regularization techniques correspond to imposing certain prior distributions on model parameters.  

    Regularization from the NFL Theorems  
    
     The No Free Lunch Theorem  
        The following statement is FALSE  
        "Given a number of points and a confidence we can always achieve a prescribed error."  
     Interpretation inference from finite samples can be effectively performed if and only if the problem satisfies some a priori condition.  
     Implications  
         This implies that there is no silver bullet we shouldn't expect any single optimization method to be perfect for all problems. Rather we should try to design optimization methods that are tailored to the problem we're trying to solve.  
            e.g. if you want to use local search you'll probably need to define a neighborhood relation that is informed by the problem domain.  
         A practical implication is that machine learning won't work if there is no structure at all on the space of possible models/hypotheses.  
            Instead we need some kind of prior that makes some models more likely than others.  
     Implying Regularization  
        One of the most used priors is Occam's Razor
        This leads to use of regularization in machine learning as it effectively applies Occam's razor to candidate models.^6  
     Summary NFL Theorem shows that regularization strategies are necessary to obtain good generalization.  


    Notes  
    
    
    

    

3. Regularization in Deep Learning
    In the context of DL most regularization strategies are based on regularizing estimators which usually works by trading increased bias for reduced variance.  

    An effective regularizer is one that makes a profitable trade reducing variance significantly while not overly increasing the bias.
    

4. Regularization and Data Domains in DL  A Practical Motivation
    Most applications of DL are to domains where the true data generating process is almost certainly outside the model family hypothesis space. Deep learning algorithms are typically applied to extremely complicated domains such as images audio sequences and text for which the true generation process essentially involves simulating the entire universe.  

    Thus controlling the complexity of the mdoel is not a simple matter of finding the model of the right size with the right number of parameters; instead the best fitting model wrt. generalization error is a large model that has been regularized appropriately.  


^1 Where we Hadamard define Well Posed Problems as having the properties 1 A Solution Exists 2 It is Unique 3 It's behavior changes continuously with the initial conditions.  


Parameter Norm Penalties

1. Parameter Norms
    Many regularization approaches are based on limiting the capacity of models by adding a parameter norm penalty $$\Omega\boldsymbol{\theta}$$ to the objective function $$J$$. We denote the regularized objective function by $$\tilde{J}$$  
    $$\tilde{J}\boldsymbol{\theta} ; \boldsymbol{X} \boldsymbol{y}=J\boldsymbol{\theta} ; \boldsymbol{X} \boldsymbol{y}+\alpha \Omega\boldsymbol{\theta} \tag{7.1}$$  
    where $$\alpha \in0 \infty$$ is a HP that weights the relative contribution of the norm penalty term $$\Omega$$ relative to the standard objective function $$J$$.  
     Effects of $$\alpha$$  
         $$\alpha = 0$$ results in NO regularization
         Larger values of $$\alpha$$ correspond to MORE regularization

    The effect of minimizing the regularized objective function is that it will decrease both the original objective $$J$$ on the training data and some measure of the size of the parameters $$\boldsymbol{\theta}$$.  

    Different choices for the parameter norm $$\Omega$$ can result in different solutions being preferred.  
    


2. Parameter Penalties and the Bias parameter
    In NN we usually penalize only the weights of the affine transformation at each layer and we leave the biases unregularized.  
    Biases typically require less data than the weights to fit accurately. The reason is that each weight specifies how TWO variables interact so fitting the weights well requires observing both variable sin a variety of conditions. However each bias controls only a single variable thus we dont induce too much variance by leaving the biases unregularized. If anything regularizing the bias can introduce a significant amount of underfitting.  
    


3. Note on the $$\alpha$$ parameter for different hidden layers
    In the context of neural networks it is sometimes desirable to use a separate penalty with a different $$\alpha$$ coefficient for each layer of the network. Because it can be expensive to search for the correct value of multiple hyperparameters it is still reasonable to use the same weight decay at all layers just to reduce the size of search space.  
    


4. $$L^2$$ Parameter Regularization Weight Decay
    It is a regularization strategy that drives the weights closer to the origin^2 by adding a regularization term  
    $$\Omega\mathbf{\theta} = \frac{1}{2}\|\boldsymbol{w}\| {2}^{2}$$  
    to the objective function.  
    
    In statistics $$L^2$$ regularization is also known as Ridge Regression or Tikhonov Regularization.  

    Analyzing Weight Decay  
    
     What happens in a Single Step  
        We can gain some insight into the behavior of weight decay regularization by studying the gradient of the regularized objective function.  
        Take the models objective function  
        $$\tilde{J}\boldsymbol{w} ; \boldsymbol{X} \boldsymbol{y}=\frac{\alpha}{2} \boldsymbol{w}^{\top} \boldsymbol{w}+J\boldsymbol{w} ; \boldsymbol{X} \boldsymbol{y} \tag{7.2}$$  
        with the corresponding parameter gradient  
        $$\nabla{\boldsymbol{w}} \tilde{J}\boldsymbol{w} ; \boldsymbol{X} \boldsymbol{y}=\alpha \boldsymbol{w}+\nabla{\boldsymbol{w}} J\boldsymbol{w} ; \boldsymbol{X} \boldsymbol{y} \tag{7.3}$$  
        The gradient descent update  
        $$\boldsymbol{w} \leftarrow \boldsymbol{w} \epsilon\left\alpha \boldsymbol{w}+\nabla{\boldsymbol{w}} J\boldsymbol{w} ; \boldsymbol{X} \boldsymbol{y}\right \tag{7.4}$$  
        Equivalently  
        $$\boldsymbol{w} \leftarrow1 \epsilon \alpha \boldsymbol{w} \epsilon \nabla{\boldsymbol{w}} J\boldsymbol{w} ; \boldsymbol{X} \boldsymbol{y} \tag{7.5}$$    

        Observe that the addition of the weight decay term has modified the learning rule to multiplicatively shrink the weight vector by  a constant factor on each step just before performing the usual gradient update.  

     What happens over the Entire course of training  
        We simplify the analysis by making a quadratic 2nd order Taylor approximation to the objective function in the neighborhood of the optimal wight parameter of the unregularized objective $$\mathbf{w}^{\ast} = \arg \min{\boldsymbol{w}} J\boldsymbol{w}$$.^3  
        The approximation $$\hat{J}$$  
        $$\hat{J}\boldsymbol{\theta}=J\left\boldsymbol{w}^{\ast}\right+\frac{1}{2}\left\boldsymbol{w} \boldsymbol{w}^{\ast}\right^{\top} \boldsymbol{H}J\boldsymbol{w}^{\ast}\left\boldsymbol{w} \boldsymbol{w}^{\ast}\right  \tag{7.6}$$  
        where $$\boldsymbol{H}$$ is the Hessian matrix of $$J$$ with respect to $$\mathbf{w}$$ evaluated at $$\mathbf{w}^{\ast}$$.  

        Notice  
         There is no  order term in this quadratic approximation because $$\boldsymbol{w}^{\ast}$$  is defined to be a minimum where the gradient vanishes.  
         Because $$\boldsymbol{w}^{\ast}$$ is the location of a minimum of $$J$$ we can conclude that $$\boldsymbol{H}$$ is positive semidefinite.  

        The gradient of $$\hat{J} + \Omega\mathbf{\theta}$$  
        $$\nabla{\boldsymbol{w}} \hat{J}\boldsymbol{w}=\boldsymbol{H}J\boldsymbol{w}^{\ast}\left\tilde{\boldsymbol{w}} \boldsymbol{w}^{\ast}\right + \alpha \tilde{\boldsymbol{w}} \tag{7.7}$$  
        And the minimum is achieved at $$\nabla{\boldsymbol{w}} \hat{J}\boldsymbol{w} = 0$$  
        $$\tilde{\boldsymbol{w}}=\boldsymbol{H}+\alpha \boldsymbol{I}^{ 1} \boldsymbol{H} \boldsymbol{w}^{\ast} \tag{7.10}$$  

        Effects  
         As $$\alpha$$ approaches $$0$$ the regularized solution $$\tilde{\boldsymbol{w}}$$ approaches $$\boldsymbol{w}^{\ast}$$.  
         As $$\alpha$$ grows we apply spectral decomposition to the real and symmetric $$\boldsymbol{H} = \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{\top}$$  
            $$\begin{aligned} \tilde{\boldsymbol{w}} &=\left\boldsymbol{Q} \mathbf{\Lambda} \boldsymbol{Q}^{\top}+\alpha \boldsymbol{I}\right^{ 1} \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{\top} \boldsymbol{w}^{\ast}  &=\left\boldsymbol{Q}\boldsymbol{\Lambda}+\alpha \boldsymbol{I} \boldsymbol{Q}^{\top}\right^{ 1} \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{\top} \boldsymbol{w}^{\ast}  &=\boldsymbol{Q}\boldsymbol{\Lambda}+\alpha \boldsymbol{I}^{ 1} \boldsymbol{\Lambda} \boldsymbol{Q}^{\top} \boldsymbol{w}^{\ast} \end{aligned} \tag{7.13}$$  

        Thus we see that the effect of weight decay is to rescale $$\boldsymbol{w}^{\ast}$$ along the axes defined by the eigenvector of $$\boldsymbol{H}$$. Specifically the component of $$\boldsymbol{w}^{\ast}$$ that is aligned with the $$i$$ th eigenvector of $$\boldsymbol{H}$$  is rescaled by a factor of $$\frac{\lambda{i}}{\lambda{i}+\alpha}$$.  


        Summary  

        \vert Condition|Effect of Regularization \vert   
        \vert $$\lambda{i}>>\alpha$$ \vert Not much \vert  
        \vert $$\lambda{i}<<\alpha$$ \vert The weight value almost shrunk to $$0$$ \vert  

     Applying $$L^2$$ regularization to Linear Regression   
         

  
    

    $$L^2$$ Regularization Derivation  
    $$L^2$$ regularization is equivalent to MAP Bayesian inference with a Gaussian prior on the weights.  

    The MAP Estimate  
    
    $$\begin{aligned} \hat{\theta} {\mathrm{MAP}} &=\arg \max{\theta} P\theta \vert y  &=\arg \max{\theta} \frac{Py \vert \theta P\theta}{Py}  &=\arg \max{\theta} Py \vert \theta P\theta  &=\arg \max{\theta} \log Py \vert \theta P\theta  &=\arg \max{\theta} \log Py \vert \theta+\log P\theta \end{aligned}$$  
    
    We place a Gaussian Prior on the weights with zero mean and equal variance $$\tau^2$$  
    $$\begin{aligned} \hat{\theta} {\mathrm{MAP}} &=\arg \max{\theta} \log Py \vert \theta+\log P\theta  &=\arg \max {\boldsymbol{w}}\left\log \prod{i=1}^{n} \dfrac{1}{\sigma \sqrt{2 \pi}} e^{ \dfrac{\lefty{i} \boldsymbol{w}^{\top}\boldsymbol{x}i\right^{2}}{2 \sigma^{2}}}+\log \prod{j=0}^{p} \dfrac{1}{\tau \sqrt{2 \pi}} e^{ \dfrac{w{j}^{2}}{2 \tau^{2}}} \right  &=\arg \max {\boldsymbol{w}} \left \sum{i=1}^{n} \dfrac{\lefty{i} \boldsymbol{w}^{\top}\boldsymbol{x}i\right^{2}}{2 \sigma^{2}} \sum{j=0}^{p} \dfrac{w{j}^{2}}{2 \tau^{2}}\right  &=\arg \min{\boldsymbol{w}} \dfrac{1}{2 \sigma^{2}}\left\sum{i=1}^{n}\lefty{i} \boldsymbol{w}^{\top}\boldsymbol{x}i\right^{2}+\dfrac{\sigma^{2}}{\tau^{2}} \sum{j=0}^{p} w{j}^{2}\right  &=\arg \min{\boldsymbol{w}} \left\sum{i=1}^{n}\lefty{i} \boldsymbol{w}^{\top}\boldsymbol{x}i\right^{2}+\lambda \sum{j=0}^{p} w{j}^{2}\right  &= \arg \min{\boldsymbol{w}} \left \|XW  \boldsymbol{y}\|^2 + \lambda {\|\boldsymbol{w}\| 2}^2\right\end{aligned}$$  
    
    

    Properties  
    
    

     Adding L2 regularization to a convex function gives a strongly convex function. So L2 regularization can make gradient descent converge much faster.  ^ same ref      

    Notes  
    


    

5. $$L^1$$ Regularization
    $$L^1$$ Regularization is another way to regulate the model by penalizing the size of its parameters; the technique adds a regularization term  
    $$\Omega\boldsymbol{\theta}=\|\boldsymbol{w}\|{1}=\sum{i}\left|w{i}\right| \tag{7.18}$$  
    which is a sum of absolute values of the individual parameters.  

    The regularized objective function is given by  
    $$\tilde{J}\boldsymbol{w} ; \boldsymbol{X} \boldsymbol{y}=\alpha\|\boldsymbol{w}\| {1}+J\boldsymbol{w} ; \boldsymbol{X} \boldsymbol{y} \tag{7.19}$$  
    with the corresponding sub gradient  
    $$\nabla{\boldsymbol{w}} \tilde{J}\boldsymbol{w} ; \boldsymbol{X} \boldsymbol{y}=\alpha \operatorname{sign}\boldsymbol{w}+\nabla{\boldsymbol{w}} J\boldsymbol{X} \boldsymbol{y} ; \boldsymbol{w} \tag{7.20}$$  

    Notice that the regularization contribution to the gradient no longer scales linearly with each $$wi$$; instead it is a constant factor with a sign = $$\text{sign}wi$$.  

    \Analysis\  

    Sparsity of the $$L^1$$ regularization  
    In comparison to $$L^2$$ $$L^1$$ regularization results in a solution that is more sparse.  
    The sparsity property has been used extensively as a feature selection mechanism.  
     LASSO The Least Absolute Shrinkage and Selection Operator integrates an $$L^1$$ penalty with a linear model and a least squares cost function.  
        The $$L^1$$ penalty causes a subset of the weights to become zero suggesting that the corresponding features may safely be discarded.  

    $$L^1$$ Regularization Derivation  
    $$L^1$$ regularization is equivalent to the log prior term in MAP Bayesian inference with an isotropic Laplace distribution prior on the weights  
    $$\log p\boldsymbol{w}=\sum{i} \log \operatorname{Laplace}\leftw{i} ; 0 \frac{1}{\alpha}\right= \alpha\|\boldsymbol{w}\| {1}+n \log \alpha n \log 2 \tag{7.24}$$  
    note that we can ignore the terms $$\log \alpha \log 2$$ because they do not depend on $$\boldsymbol{w}$$.      
    
    $$\begin{aligned} \hat{\theta} {\mathrm{MAP}} &=\arg \max{\theta} \log Py \vert \theta+\log P\theta   &=\arg \max {\boldsymbol{w}}\left\log \prod{i=1}^{n} \dfrac{1}{\sigma \sqrt{2 \pi}} e^{ \dfrac{\lefty{i} \boldsymbol{w}^{\top}\boldsymbol{x}i\right^{2}}{2 \sigma^{2}}}+\log \prod{j=0}^{p} \dfrac{1}{2 b} e^{ \dfrac{\left|\theta{j}\right|}{2 b}} \right     &=\arg \max {\boldsymbol{w}} \left \sum{i=1}^{n} \dfrac{\lefty{i} \boldsymbol{w}^{\top}\boldsymbol{x}i\right^{2}}{2 \sigma^{2}} \sum{j=0}^{p} \dfrac{\left|w{j}\right|}{2 b}\right     &=\arg \min{\boldsymbol{w}} \dfrac{1}{2 \sigma^{2}}\left\sum{i=1}^{n}\lefty{i} \boldsymbol{w}^{\top}\boldsymbol{x}i\right^{2}+\dfrac{\sigma^{2}}{b} \sum{j=0}^{p}\left|w{j}\right|\right     &=\arg \min{\boldsymbol{w}} \left\sum{i=1}^{n}\lefty{i} \boldsymbol{w}^{\top}\boldsymbol{x}i\right^{2}+\lambda \sum{j=0}^{p}\left|w{j}\right|\right     &= \arg \min{\boldsymbol{w}} \left \|XW  \boldsymbol{y}\|^2 + \lambda \|\boldsymbol{w}\| 1\right\end{aligned}$$

    Properties  
    
     $$L^1$$ regularization can occasionally produce non unique solutions. A simple example is provided in the figure when the space of possible solutions lies on a 45 degree line. 
    


6. $$L^1$$ VS $$L^2$$ Regularization
    
    
     Feature Correlation and Sparsity  
         Identical features   
             $$L^1$$ regularization spreads weight arbitrarily all weights same sign 
             $$L^2$$ regularization spreads weight evenly 
         Linearly related features   
             $$L^1$$ regularization chooses variable with larger scale $$0$$ weight to others  
             $$L^2$$ prefers variables with larger scale — spreads weight proportional to scale  


    
    Interpreting Sparsity with an Example  
    Let's imagine we are estimating two coefficients in a regression. In $$L^2$$ regularization the solution $$\boldsymbol{w} =01$$ has the same weight as $$\boldsymbol{w}=\frac{1}{\sqrt{2}} \frac{1}{\sqrt{2}}$$  so they are both treated equally. In $$L^1$$ regularization the same two solutions favor the sparse one  
    $$\|10\|{1}=1<\left\|\left\frac{1}{\sqrt{2}} \frac{1}{\sqrt{2}}\right\right\|{1}=\sqrt{2}$$  
    So $$L^2$$ regularization doesn't have any specific built in mechanisms to favor zeroed out coefficients while $$L^1$$ regularization actually favors these sparser solutions.  


    


Notes  
 Elastic Net Regularization  
    $$\Omega = \lambda\left\alpha\|w\|{1}+1 \alpha\|w\|{2}^{2}\right \alpha \in01$$  
     Combines both $$L^1$$ and $$L^2$$  
     Used to produce sparse solutions but to avoid the problem of $$L^1$$ solutions being sometimes Non Unique  
         The problem mainly arises with correlated features  
     Elastic net regularization tends to have a grouping effect where correlated input features are assigned equal weights.  



^2 More generally we could regularize the parameters to be near any specific point in space and surprisingly still get a regularization effect but better results will be obtained for a value closer to the true one with zero being a default value that makes sense when we do not know if the correct value should be positive or negative.  

^3 The approximation is perfect if the objective function is truly quadratic as in the case of linear regression w/ MSE.  


Advanced Regularization Techniques

1. Regularization and Under Constrained Problems
    In some cases regularization is necessary for machine learning problems to be properly define.  

    Many linear models e.g. Linear Regression PCA depend on inverting $$\boldsymbol{X}^{\top}\boldsymbol{X}$$. This is not possible if $$\boldsymbol{X}^{\top}\boldsymbol{X}$$ is singular. In this case many forms of regularization correspond to solving inverting $$\boldsymbol{X}^{\top} \boldsymbol{X}+\alpha \boldsymbol{I}$$ instead. This regularized matrix is guaranteed to be invertible.  
     $$\boldsymbol{X}^{\top}\boldsymbol{X}$$ can be singular if  
         The data generating function truly has no variance in some direction.  
         No Variance is observed in some direction because there are fewer examples rows of $$\boldsymbol{X}$$ than input features columns.  

    Models with no closed form solution can also be underdetermined  
    Take logistic regression on a linearly separable dataset if a weight vector $$\boldsymbol{w}$$ is able to achieve perfect classification then so does $$2\boldsymbol{w}$$ but with even higher likelihood. Thus an iterative optimization procedure sgd will continually increase the magnitude of $$\boldsymbol{w}$$ and in theory will never halt.  
    We can use regularization to guarantee the convergence of iterative methods applied to underdetermined problems e.g. weight decay will cause gradient descent to quit increasing the magnitude of the weights when the slope of the likelihood is equal to the weight decay coefficient.  

    Linear Algebra Perspective  
    Given that the Moore Penrose pseudoinverse $$\boldsymbol{X}^{+}$$ of a matrix $$\boldsymbol{X}$$ can solve underdetermined linear equations  
    $$\boldsymbol{X}^{+}=\lim{\alpha \searrow 0}\left\boldsymbol{X}^{\top} \boldsymbol{X}+\alpha \boldsymbol{I}\right^{ 1} \boldsymbol{X}^{\top} \tag{7.29}$$  
    we can now recognize the equation as performing linear regression with weight decay.  
    Specifically $$7.29$$ is the limit of eq $$7.17$$ as the regularization coefficient shrinks to zero.  
    We can thus interpret the pseudoinverse as stabilizing underdetermined problems using regularization.  

    The Pseudoinverse  
    When applied to underdetermined systems w/ non unique solutions; It finds the minimum norm solution to a linear system.  
    This "OLS" solution implies that not all linear functions are the same for OLS. It restricts the space of all possible non unique linear functions that satisfy the equation to a subset of minimal norm.  
    From SLT perspective the pseudoinverse introduces bias towards certain solutions.  
    

2. Dataset Augmentation
    Having more data is the most desirable thing to improving a machine learning model’s performance. In many cases it is relatively easy to artificially generate data.  
     Applications for certain problems like classification this approach is readily usable. E.g. for a classification task we require the model to be invariant to certain types of transformations of which we can generate data by applying them on our current dataset.  
        The most successful application of data augmentation has been in object recognition.  
     Non Applicable this approach is not applicable to many problems especially those that require us to learn the true data distribution  E.g. Density Estimation.  

    Noise Injection as Data Augmentation  
    Injecting noise in the input to a NN Siestma and Dow 1991 can also be seen as a form of data augmentation.  
     Motivation  
         For many classification and some regression tasks the task should be possible to solve even if small random noise is added to the input Local Constancy/workfiles/research/dl/theory/dlbookpt1  
         Moreover NNs prove not to be very robust to noise.  

    Injecting Noise in the Hidden Units  
    It can be seen as doing data augmentation at multiple levels of abstraction. This approach can be highly effective provided that the magnitude of the noise is carefully tuned Poole et al. 2014.  
    Dropout can be seen as a process of constructing new inputs by multiplying by noise.  

    

3. Noise Robustness
    We can apply Noise Injection to different components of the model as a way to regularize the model  
    Injecting Noise in the Input Layer  
    
     Motivation  
        We have motivated the injection of noise to the inputs as a dataset augmentation strategy.        
     Interpretation  
        For some models the addition of noise with infinitesimal variance at the input of the model is equivalent to imposing a penalty on the norm of the weights Bishop 1995ab.  

    Injecting Noise in the Hidden Layers  
    
     Motivation  
        We can motivate it as a variation of data augmentation.  
     Interpretation  
        It can be seen as doing data augmentation at multiple levels of abstraction.  
     Applications  
        The most successful application of this type of noise injection is Dropout.  
        It can be seen as a process of constructing new inputs by multiplying by noise.  

    Injecting Noise in the Weight Matrices  
    
     Interpretation  
        1. It can be interpreted as a stochastic implementation of Bayesian inference over the weights.  
             The Bayesian View  
                The Bayesian treatment of learning would consider the model weights to be uncertain and representable via a probability distribution that reflects this uncertainty. Adding noise to the weights is a practical stochastic way to reflect this uncertainty.  
        2. It can also be interpreted as equivalent a more traditional form of regularization encouraging stability of the function to be learned.  
             
     Applications  
        This technique has been used primarily in the context of recurrent neural networks Jim et al. 1996; Graves 2011.  

    Injecting Noise in the Output Layer  
    
     Motivation  
         Most datasets have some number of mistakes in the $$y$$ labels. It can be harmful to maximize $$\log py \vert \boldsymbol{x}$$ when $$y$$ is a mistake. One way to prevent this is to explicitly model the noise on the labels.  
        One can assume that for some small constant $$\epsilon$$ the training set label $$y$$ is correct with probability $$1 \epsilon$$.  
            This assumption is easy to incorporate into the cost function analytically rather than by explicitly drawing noise samples e.g. label smoothing.  
         MLE with a softmax classifier and hard targets may never converge  the softmax can never predict a probability of exactly $$0$$ or $$1$$ so it will continue to learn larger and larger weights making more extreme predictions forever.{ }  
     Interpretation  
        For some models the addition of noise with infinitesimal variance at the input of the 
     Applications  
        Label Smoothing regularizes a model based on a softmax with $$k$$ output values by replacing the hard $$0$$ and $$1$$ classification targets with targets of $$\dfrac{\epsilon}{k 1}$$ and $$1 \epsilon$$ respectively.   
         Applied to MLE problem Label smoothing compared to weight decay has the advantage of preventing the pursuit of hard probabilities without discouraging correct classification.  
         Application in modern NN Szegedy et al. 2015 

    

4. Semi Supervised Learning
    Semi Supervised Learning is a class of ML tasks and techniques that makes use of both unlabeled examples from $$P\mathbf{x}$$ and labeled examples from $$P\mathbf{x} \mathbf{y}$$ to estimate $$P\mathbf{y} \vert \mathbf{x}$$ or predict $$\mathbf{y}$$ from $$\mathbf{x}$$.  

    In the context of Deep Learning Semi Supervised Learning usually refers to learning a representation $$\boldsymbol{h}=f\boldsymbol{x}$$; the goal being to learn a representation such that examples from the same class have similar representations.   
    Usually Unsupervised Learning provides us clues e.g. clustering that influence the representation of the data.  
    PCA as a preprocessing step before applying a classifier is a long standing variant of this approach.  

    Approach  
    Instead of separating the supervised and unsupervised criteria we can instead have a generative model of $$P\mathbf{x}$$ or $$P\mathbf{x} \mathbf{y}$$ which shares parameters with a discriminative model $$P\mathbf{y} \vert \mathbf{x}$$.  
    The idea is to share the unsupervised/generative criterion with the supervised criterion to express a prior belief that the structure of $$P\mathbf{x}$$ or $$P\mathbf{x} \mathbf{y}$$ is connected to the structure of $$P\mathbf{y} \vert \mathbf{x}$$ which is captured by the shared parameters.  
    By controlling how much of the generative criterion is included in the total criterion one can find a better trade off than with a purely generative or a purely discriminative training criterion Lasserre et al. 2006; Larochelle and Bengio 2008.  

        


5. Multitask Learning
    Multitask Learning is a way to improve generalization by pooling the examples which can be seen as soft constraints imposed on the parameters arising out of several tasks. In the same way that additional data put more pressure on the parameters of the model toward values that generalize well when part of a model is shared across tasks that part of the model is more constrained toward good values assuming the sharing is justified often yielding better generalization.  

    Improved generalization due to improved statistical strength of the shared parameters in proportion w/ increased \# of examples. This happens only is some assumptions about the statistical relationship of the different tasks are valid; i.e. they share something.  

    From the point of view of deep learning the underlying prior belief is the following  
    Among the factors that explain the variations observed in the data associated with the different tasks some are shared across two or more tasks.  


    Types  
    
     Task specific These parameters benefit only from that particular task for generalization.  
        These are the later layers in the NN.  
     Generic shared across all tasks benefit from the pooled data of all the tasks.  
        These are the earlier layers in the NN.  
    

6. Early Stopping
    Early Stopping is a regularization method that aims to obtain a model with better validation set error $$\implies$$ generalization by saving the model parameters at every epoch and returning to the parameter setting at the point in time with the lowest validation set error.  


    Premise of Early Stopping  
    For a model with high representational capacity after a certain point of time during training the training error continues to decrease but the validation error begins to increase overfitting. In such a scenario a better idea would be to return back to the point where the validation error was the least.  



    Algorithm  
    Early stopping requires the use of a validation set. Thus we are not using the entire dataset for training. Choosing the ideal number of steps before we stop training can be done with cross validation. Then to utilize the entire dataset a  phase of training can be done where the complete training set is used. There are two choices here  
    
    1. Train from scratch for the same number of steps as in the Early Stopping case.  
    2. Use the weights learned from the  phase of training and continue training using the complete data not as well behaved.  


    As Regularization  
    Early Stopping affects the optimization procedure by restricting it to a small volume of the parameter space in the neighbourhood of the initial parameter value $$\boldsymbol{\theta} {O}$$.  


    Let $$\tau$$ be the number of optimization steps taken $$\tau$$ training iterations with lr $$\epsilon$$. The product $$\epsilon\tau$$ can be seen as a measure of effective capacity
    The number of training iterations has a role inversely proportional to the weight decay coefficient  
    $$\tau \approx \dfrac{1}{\epsilon\lambda}$$    
     When the number of iterations $$\tau$$ is small $$\iff \lambda$$ is large Regularization is large and the capacity is small  
     When the number of iterations $$\tau$$ is large $$\iff \lambda$$ is small Regularization is small and the capacity is large  
    Parameter values corresponding to directions of significant curvature of the objective function are regularized less than directions of less curvature. Of course in the context of early stopping this really means that parameters that correspond to directions of significant curvature tend to learn early relative to parameters corresponding to directions of less curvature.  

    Equivalence to Weight Decay for Linear Models  
    To compare with classical $$L^{2}$$ regularization we examine a simple setting where the only parameters are linear weights $$\boldsymbol{\theta}=\boldsymbol{w}$$. We can model the cost function $$J$$ with a quadratic approximation in the neighborhood of the empirically optimal value of the weights $$\boldsymbol{w}^{  }$$  
    $$\hat{J}\boldsymbol{\theta}=J\left\boldsymbol{w}^{  }\right+\frac{1}{2}\left\boldsymbol{w} \boldsymbol{w}^{  }\right^{\top} \boldsymbol{H}\left\boldsymbol{w} \boldsymbol{w}^{  }\right$$  
    where $$\boldsymbol{H}$$ is the Hessian matrix of $$J$$ with respect to $$\boldsymbol{w}$$ evaluated at $$\boldsymbol{w}^{  } .$$ Given the assumption that $$\boldsymbol{w}^{  }$$ is a minimum of $$J\boldsymbol{w}$$ we know that $$\boldsymbol{H}$$ is positive semidefinite.  

    



  



    Properties  
    
     Early stopping is an unobtrusive form of regularization in that it requires almost no change in the underlying training procedure the objective function or the set of allowable parameter values  
         This means that it is easy to use early stopping without damaging the learning dynamics. This is in contrast to weight decay where one must be careful not to use too much weight decay and trap the network in a bad local minimum corresponding to a solution with pathologically small weights.  
     Early Stopping does not add any extra hyperparameters making it easy to incorporate without the need for extra tuning.  
     It reduces the computation cost of training by  
        1. Lowering the number of training steps  
        2. Regularizing the model without having to add additional penalty terms computation of gradients of additional terms   
     Early stopping may be used either alone or in conjunction with other regularization strategies. Even when using regularization strategies that modify the objective function to encourage better generalization it is rare for the best generalization to occur at a local minimum of the training objective.  
     It is a form of spectral regularization  




    Intuition  
    1 One way to think of early stopping is as a very efficient hyperparameter selection algorithm. In this view the number of training steps is just another hyperparameter. We can see that this hyperparameter has a $$U$$ shaped validation set performance curve. Most hyperparameters that control model capacity have such a $$U$$ shaped validation set performance curve. In the case of early stopping we are controlling the effective capacity of the model by determining how many steps it can take to fit the training set.   

    2 Early stopping can be viewed as regularization in time. Intuitively a training procedure like gradient descent will tend to learn more and more complex functions as the number of iterations increases. By regularizing on time the complexity of the model can be controlled improving generalization.  
    
    3 With a bounded step size the number of steps dictates the radius ball around your initial point of points that is reachable. By stopping early you limit this radius. "Weight decay is equivalent to a Lagrangian relaxation of a constraint on the weight norm. Early stopping by previous argument gives you the same constraint assuming you start from 0. If you don't start from 0 you might be able to still use triangle inequality to prove it."  
    

7. Parameter Tying and Parameter Sharing
    Unlike the the methods focused on bringing the weights to a fixed point e.g. 0 in the case of norm penalty there might be situations where we might have some prior knowledge on the kind of dependencies that the model should encode.   

    Parameter Tying  
    If two tasks are similar enough similar input/output distributions we might want to have the parameters of both models be close to each other in value. We do this with Parameter Tying to use regularization to have two sets of parameters close to each other.  
    One way to achieve that in an application is to regularize the parameters of one model $$L^2$$ norm trained as a classifier in a supervised paradigm to be close to the parameters of another model trained in an unsupervised paradigm to capture the distribution of the observed input data.^5  
    

    Parameter Sharing  
    The more popular way is Parameter Sharing to use constraints to force sets of parameters to be equal.  

    A significant advantage of parameter sharing over regularizing the parameters to be close via a norm penalty is that only a subset of the parameters the unique set needs to be stored in memory e.g. in CNNs this can lead to significant reduction in the memory footprint of the model.  

    Parameter sharing has enabled CNNs to dramatically lower the number of unique model parameters and to significantly increase network sizes without requiring a corresponding increase in training data.  

    

8. Sparse Representations
    Another strategy is to place a penalty on the activations of the units in a neural network encouraging their activations to be sparse. This indirectly imposes a complicated penalty on the model parameters.  
    Unlike Weight decay which acts by placing a penalty directly on the model parameters.  

    $$L^1$$ regularization induces sparse parametrization i.e. sparse weights.  
    Representational sparsity on the other hand induces sparse representations

    Norm penalty regularization of representations is performed by adding to the loss function $J$ a norm penalty on the representation. This penalty is denoted $$\Omega\boldsymbol{h} .$$ As before we denote the regularized loss function by $$\tilde{J}$$  
    $$\tilde{J}\boldsymbol{\theta} ; \boldsymbol{X} \boldsymbol{y}=J\boldsymbol{\theta} ; \boldsymbol{X} \boldsymbol{y}+\alpha \Omega\boldsymbol{h}$$  
    where $$\alpha \in0 \infty$$ weights the relative contribution of the norm penalty term with larger values of $$\alpha$$ corresponding to more regularization.  

    The regularizer $$\Omega$$  
    
     $$L^1$$ norm can be used  
     Penalty derived from a Student $t$ prior distribution on the representation  
     KL divergence penalties especially useful for representations with elements constrained to lie on the unit interval  
    Other approaches obtain representational sparsity with a hard constraint on the activation values. For example orthogonal matching pursuit OMP Pati et al 1993 encodes an input $$x$$ with the representation $$h$$ that solves the constrained optimization problem  
    $$\underset{\boldsymbol{h}\|\boldsymbol{h}\| {0}< k}{\arg \min }\|\boldsymbol{x} \boldsymbol{W} \boldsymbol{h}\|^{2}$$  
    where $$\|\boldsymbol{h}\| {0}$$ is the number of nonzero entries of $$\boldsymbol{h}$$ . This problem can be solved efficiently when $$\boldsymbol{W}$$ is constrained to be orthogonal. This method is often called OMP $$k$$ with the value of $$k$$ specified to indicate the number of nonzero features allowed. Coates and $$\mathrm{Ng}2011$$ demonstrated that $$\mathrm{OMP} 1$$ can be a very effective feature extractor for deep architectures.  
    

9. Ensemble Learning
    Ensemble Learning is a set of ensemble methods that use multiple learning algorithms models and a strategy called model averaging to combine the outcomes of those model to obtain better predictive performance.  

    Model averaging is an extremely powerful and reliable method for reducing generalization error.  

    Motivation  
    The reason that model averaging works is that different models will usually not make all the same errors on the test set.  
    Consider for example a set of $$k$$ regression models. Suppose that each model makes an error $$\epsilon{i}$$ on each example with the errors drawn from a zero mean multivariate normal distribution with variances $$\mathbb{E}\left\epsilon{i}^{2}\right=v$$ and covariances $$\mathbb{E}\left\epsilon{i} \epsilon{j}\right=c$$. Then the error made by the average prediction of all the ensemble models is $$\frac{1}{k} \sum{i} \epsilon{i}$$. The expected squared error of the ensemble predictor is  
    $$\begin{aligned} \mathbb{E}\left\left\frac{1}{k} \sum{i} \epsilon{i}\right^{2}\right &=\frac{1}{k^{2}} \mathbb{E}\left\sum{i}\left\epsilon{i}^{2}+\sum{j \neq i} \epsilon{i} \epsilon{j}\right\right  &=\frac{1}{k} v+\frac{k 1}{k} c \end{aligned}$$  
     When the errors are perfectly correlated $$c=v$$ the MSE reduces to $$v$$ so the model averaging does not help at all.  
     When the errors are perfectly uncorrelated $$c=0$$ the expected squared error of the ensemble is only $$\dfrac{1}{k} v$$.  
        Thus the expected squared error of the ensemble is inversely proportional to the ensemble size.  
    In other words on average the ensemble will perform at least as well as any of its members and if the members make independent errors the ensemble will perform significantly better than its members.  

    As Regularization  
    Regularization has two general definitions  
    1 Any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.  
    2 Regularization is a more general way of controlling a models capacity by allowing us to express preference for one function over another in the same hypothesis space; instead of including or excluding members from the hypothesis space completely.  

    From those perspectives the analysis motivation above shows that ensemble methods satisfy both criteria in gold.  


    Bagging  
    Bagging is an ensemble method that aims to reduce the generalization error.  
    It reduces variance which corresponds to generalization error thus decreasing capacity.  


    Boosting  
    Not all techniques for constructing ensembles are designed to make the ensemble more regularized than the individual models. For example a technique called boosting Freund and Schapire 1996ba constructs an ensemble with higher capacity than the individual models.   
    It reduces bias which corresponds to approximation error thus increasing capacity.  
    

10.Dropout
    Dropout Srivastava et al. 2014 provides a computationally inexpensive but powerful method of regularizing a broad family of models.  


    Dropout as an approximation to Bagging Ensemble Method for Neural Networks  
    To a  approximation dropout can be thought of as a method of making bagging practical for ensembles of very many large neural networks.  
    Bagging involves training multiple models and evaluating multiple models on each test example. This seems impractical when each model is a large neural network since training and evaluating such networks is costly in terms of runtime and memory. It is common to use ensembles of five to ten neural networks—Szegedy et al. 2014a used six to win the ILSVRC— but more than this rapidly becomes unwieldy.  
    Dropout provides an inexpensive approximation to training and evaluating a bagged ensemble of exponentially many neural networks. Specifically dropout trains the ensemble consisting of all subnetworks that can be formed by removing nonoutput units from an underlying base network. In modern NNs we can remove a unit by "multiplying" its output by zero.  

    Recall that to learn with bagging we define $$k$$ different models construct $$k$$ different datasets by sampling from the training set with replacement and then train model $$i$$ on dataset $$i$$. Dropout aims to approximate this process but with an exponentially large number of neural networks.  

    Dropout Algorithm  
    To train with dropout we  
    
     Use a minibatch based learning algorithm that makes small steps such as stochastic gradient descent.  
     Each time we load an example into a minibatch we randomly sample a different binary mask to apply to all the input and hidden units in the network.  
         The mask for each unit is sampled independently from all the others.  
         The probability of sampling a mask value of one causing a unit to be included is a hyperparameter fixed before training begins. It is not a function of the current value of the model parameters or the input example.  
     Typically an input unit is included with probability $$0.8$$ and a hidden unit is included with probability $$0.5$$.  
     We then run forward propagation back propagation and the learning update as usual.  

    Formally suppose that a mask vector $$\mu$$ specifies which units to include and $$J\boldsymbol{\theta} \boldsymbol{\mu}$$ defines the cost of the model defined by parameters $$\boldsymbol{\theta}$$ and mask $$\boldsymbol{\mu}$$ Then dropout training consists of minimizing $$\mathbb{E} {\boldsymbol{\mu}} J\boldsymbol{\theta} \boldsymbol{\mu}$$. The expectation contains exponentially many terms but we can obtain an unbiased estimate of its gradient by sampling values of $$\boldsymbol{\mu}$$.  

    Differences  
    
     Training compare each number  
         Bagging  
            1. The models are all independent.  
            2. Each model is trained to convergence on its respective training set.  
         Dropout  
            1. The models share parameters with each model inheriting a different subset of parameters from the parent neural network.  
                This parameter sharing makes it possible to represent an exponential number of models with a tractable amount of memory.  
            2. Most models are not explicitly trained at all—usually the model is large enough that it would be infeasible to sample all possible subnetworks within the lifetime of the universe. Instead a tiny fraction of the possible subnetworks are each trained for a single step and the parameter sharing causes the remaining subnetworks to arrive at good settings of the parameters.  

        Beyond these difference dropout follows the bagging algorithm; e.g. training set encountered by each subnetwork is indeed a subset of the original training set sampled with replacement.  

     Inference Prediction  
        So far our description of bagging and dropout has not required that the model be explicitly probabilistic. Now we assume that the model’s role is to output a probability distribution.  
         Bagging Each model $$i$$ produces a probability distribution $$p^{i}y \vert \boldsymbol{x}$$.  
            The prediction of the ensemble is given by the arithmetic mean of all these distributions  
            $$\frac{1}{k} \sum{i=1}^{k} p^{i}y \vert \boldsymbol{x}$$  
         Dropout Each submodel defined by mask vector $$\boldsymbol{\mu}$$ defines a probability distribution $$py \vert x \mu$$.  
            The arithmetic mean over all masks is given by  
            $$\sum{\mu} p\boldsymbol{\mu} py \vert \boldsymbol{x} \boldsymbol{\mu}$$  
            where $$p\boldsymbol{\mu}$$ is the probability distribution that was used to sample $$\boldsymbol{\mu}$$ at training time.  

             Issues with direct inference with the arithmetic mean  
                Because this sum includes an exponential number of terms it is intractable to evaluate except when the structure of the model permits some form of simplification.  
             Solution  Geometric Mean + Weight Scaling Inference  
                The following approach allows us to obtain a good approximation to the predictions of the entire ensemble at the cost of only one forward propagation.  
                The geometric mean of multiple probability distributions is not guaranteed to be a probability distribution. We guarantee it by imposing the requirement that none of the submodels assigns probability $$0$$ to any event and we renormalize the resulting distribution.  
                The Unnormalized distribution defined by the geometric mean  
                $$\tilde{p}{\text {ensemble }}y \vert \boldsymbol{x}=2 \sqrt2 d{\prod{\boldsymbol{\mu}} py \vert \boldsymbol{x} \boldsymbol{\mu}}$$  
                where $$d$$ is the number of units that may be dropped^4.  
                The Normalized distribution  
                $$p{\text {ensemble }}y \vert \boldsymbol{x}=\frac{\tilde{p}{\text {ensemble }}y \vert \boldsymbol{x}}{\sumy \tilde{p}{\text {ensemble }}\lefty^{\prime} \vert \boldsymbol{x}\right}$$  

                Weight Scaling Inference Rule that approximates $$p{\text {ensemble}}$$ by evaluating $$py \vert \boldsymbol{x}$$ in one model the model with all units but with the weights going out of unit $$i$$ multiplied by the probability of including unit $$i$$.  


                 Analysis and Justification  
                     Warde Farley et al. 2014 present arguments and empirical evidence that the geometric mean performs comparably to the arithmetic mean in this context.
                     A key insight Hinton et al. 2012c involved in dropout is that we can approximate $$p{\text {ensemble}}$$ by evaluating $$py \vert \boldsymbol{x}$$ in one model the model with all units but with the weights going out of unit $$i$$ multiplied by the probability of including unit $$i$$  The Weight Scaling Inference Rule. The motivation for this modification is to capture the right expected value of the output from that unit.   
                        There is not yet any theoretical argument for the accuracy of this approximate inference rule in deep nonlinear networks but empirically it performs very well.  
                        The goal is to make sure that the expected total input to a unit at test time is roughly the same as the expected total input to that unit at train time.  
                        is a method that aims to make sure that the expected total input to a unit at test time is roughly the same as the expected total input to that unit at train time.  
                        For many classes of models that do not have nonlinear hidden units the weight scaling inference rule is exact; e.g. softmax regression classifier proof page 260 dlbook.  
                        The weight scaling rule is also exact in other settings including regression networks with conditionally normal outputs as well as deep networks that have hidden layers without nonlinearities.  
                        However the weight scaling rule is only an approximation for deep models that have nonlinearities. Though the approximation has not been theoretically characterized it often works well empirically.  
                        Goodfellow et al. 2013a found experimentally that the weight scaling approximation can work better in terms of classification accuracy than Monte Carlo approximations to the ensemble predictor. This held true even when the Monte Carlo approximation was allowed to sample up to 1000 subnetworks. Gal and Ghahramani 2015 found that some models obtain better classification accuracy using twenty samples and the Monte Carlo approximation.  
                        It appears that the optimal choice of inference approximation is problem dependent.  
                         One other non efficient way to do inference is to approximate it with sampling by averaging together the output from many masks. Even 10 20 masks are often sufficient to obtain good performance.  
                            Weight Scaling Rule is far superior.  



    Properties and Advantages  
    
     Srivastava et al. 2014 showed that dropout is more effective than other standard computationally inexpensive regularizers such as weight decay filter norm constraints and sparse activity regularization.  
     Dropout may be combined with other forms of regularization to yield a further improvement.  
     Dropout is very computationally cheap.  
        Using it in training requires only $$\mathcal{O}n$$ computation per example per update to generate $$n$$ random binary numbers and multiply them by the state.  
     Dropout does not significantly limit the type of model or training procedure that can be used.  
        It works well with nearly any model that uses a distributed representation and can be trained with stochastic gradient descent; e.g. feedforward neural networks probabilistic models such as restricted Boltzmann machines and RNNs.  
     Stochasticity is Neither necessary Nor sufficient for the regularization effects of dropout  
         The stochasticity used while training with dropout is NOT necessary for the approaches success. It is just a means of approximating the sum over all submodels.   
            Fast Dropout is an analytical approximations to this marginalization.  
         Applying the same stochastic masks in a method analogous to boosting where the stochastically sampled ensemble members are not trained independently show almost no regularization effect compared to when the ensemble members are trained to perform well independently of each other.  
            Dropout boosting is one method that trains the entire ensemble to jointly maximize the log likelihood on the training set; and experiments have shown that it displays no regularization compared to training the entire network as a single model.   
     The sharing of the weights means that every model is very strongly regularized.  
        This regularization is much better than L2 or L1 penalties since instead of pulling the weights towards zero we are pulling the weights towards the correct value of the weights.  

     Running the stochastic model several times on the same input instead of all the weights halved gives an idea of the uncertainty in the answer.  
     We can use dropout in the input layer but with a higher probability of keeping an input unit  
        Denoising Autoencoders use this.    


    Practical Disadvantages/Issues  
    
     Typically the optimal validation set error is much lower when using dropout but this comes at the cost of a much larger model and many more iterations of the training algorithm. For very large datasets regularization confers little reduction in generalization error. In these cases the computational cost of using dropout and larger models may outweigh the benefit of regularization.  
     When extremely few labeled training examples are available dropout is less effective. Bayesian neural networks Neal 1996 outperform dropout on the Alternative Splicing Dataset Xiong et al. 2011 where fewer than 5000 examples are available Srivastava et al. 2014. When additional unlabeled data is available unsupervised feature learning can gain an advantage over dropout.  


    Dropout as Regularization  Effectiveness of Dropout  
    
     Dropout can be viewed as a means of performing efficient approximate bagging.  
     Dropout trains not just a bagged ensemble of models but an ensemble of models that share hidden units.  
        This means each hidden unit must be able to perform well regardless of which other hidden units are in the model. Hidden units must be prepared to be swapped and interchanged between models.  
        Dropout thus regularizes each hidden unit to be not merely a good feature but a feature that is good in many contexts.  
        This prevents Co Adaptation between hidden units on the Training Data.  
     A large portion of the power of dropout arises from the fact that the masking noise is applied to the hidden units. This can be seen as a form of highly intelligent adaptive destruction of the information content of the input rather than destruction of the raw values of the input.  
        This views dropout as noise injection in the hidden units. Which can be seen as doing data augmentation at multiple levels of abstraction.  
        
        For example if the model learns a hidden unit that detects a face by finding the $$hi$$ nose then dropping corresponds to erasing the information that there is a nose $$hi$$ in the image. The model must learn another $$hi$$ that either redundantly encodes the presence of a nose or detects the face by another feature such as the mouth. Traditional noise injection techniques that add unstructured noise at the input are not able to randomly erase the information about a nose from an image of a face unless the magnitude of the noise is so great that nearly all the information in the image is removed. Destroying extracted features rather than original values allows the destruction process to make use of all the knowledge about the input distribution that the model has acquired so far.  
     When viewed as noise injection an important aspect of dropouts success is that it uses multiplicative noise.  
        Multiplicative noise does not allow for pathological solutions to the noise robustness problem If the noise were additive with fixed scale then a rectified linear hidden unit $$h{i}$$ with added noise $$\epsilon$$ could simply learn to have $$h{i}$$ become very large in order to make the added noise $$\epsilon$$ insignificant by comparison.  


    Dropout and Batch Normalization  
    Another deep learning algorithm batch normalization reparametrizes the model in a way that introduces both additive and multiplicative noise on the hidden units at training time. The primary purpose of batch normalization is to improve optimization but the noise can have a regularizing effect and sometimes makes dropout unnecessary.  


    Modifying Dropout and source of Regularization  
    One of the key insights of dropout is that training a network with stochastic behavior and making predictions by averaging over multiple stochastic decisions implements a form of bagging with parameter sharing.  
    Earlier we described dropout as bagging an ensemble of models formed by including or excluding units. Yet this model averaging strategy does not need to be based on inclusion and exclusion. In principle any kind of random modification is admissible.  
    In practice we must choose modification families that  
    1 neural networks are able to learn to resist.  
    2 Ideally we should also use model families that allow a fast approximate inference rule.  
    We can think of any form of modification parametrized by a vector $$\mu$$ as training an ensemble consisting of $$py \vert \boldsymbol{x} \boldsymbol{\mu}$$ for all possible values of $$\boldsymbol{\mu}$$ There is no requirement that $$\boldsymbol{\mu}$$ have a finite number of values. For example $$\boldsymbol{\mu}$$ can be real valued. Srivastava et al. $$2014$$ showed that multiplying the weights by $$\boldsymbol{\mu} \sim \mathcal{N}\mathbf{1} I$$ can outperform dropout based on binary masks. Because $$\mathbb{E}\boldsymbol{\mu}=\mathbf{1}$$ the standard network automatically implements approximate inference in the ensemble without needing any weight scaling.   


    Dropout as Weight Decay in linear models  
    Wager et al. 2013 showed that when applied to linear regression dropout is equivalent to L2 weight decay with a different weight decay coefficient for each input feature. The magnitude of each feature’s weight decay coefficient is determined by its variance. Similar results hold for other linear models.  
    For deep models dropout is NOT equivalent to weight decay.  
    


11.Adversarial Training
    Adversarial training refers to training on examples that are adversarially generated and it has been shown to reduce the error rate.  

    Adversarial Examples are examples that are intentionally constructed by using an optimization procedure to search for an input $$\boldsymbol{x}^{\prime}$$ near a data point $$\boldsymbol{x}$$ such that the model output is very different at $$\boldsymbol{x}^{\prime} .$$ In many cases $$\boldsymbol{x}^{\prime}$$ can be so similar to $$\boldsymbol{x}$$ that a human observer cannot tell the difference between the original example and the adversarial example but the network can make highly different predictions.  
    Szegedy et al. 2014b found that even neural networks that perform at human level accuracy have a nearly $$100\%$$ error rate on adversarial examples.  



    Application in Regularization  
    Adversarial Examples are interesting in the context of regularization because one can reduce the error rate on the original test set via adversarial training—training on adversarially perturbed examples from the training set Szegedy et al. 2014b; Goodfellow et al. 2014b.  
    Goodfellow et al. 2014b showed that one of the primary causes of these adversarial examples is excessive linearity. Neural networks are built out of primarily linear building blocks. In some experiments the overall function they implement proves to be highly linear as a result. These linear functions are easy to optimize. Unfortunately the value of a linear function can change very rapidly if it has numerous inputs. If we change each input by $$\epsilon$$  then a linear function with weights $$w$$ can change by as much as $$\epsilon \|w\| 1$$ which can be a very large amount if $$w$$ is high dimensional.  
    Adversarial training discourages this highly sensitive locally linear behavior by encouraging the network to be locally constant in the neighborhood of the training data. This can be seen as a way of explicitly introducing a local constancy prior

    As Semi supervised Learning  
    Virtual Adversarial Examples are adversarial examples generated using not the true label but a label provided by a trained model Miyato et al. 2015.  
    Using virtual examples we can train a classifier to assign the same label to $$x$$ and $$x^{\prime}$$. This encourages the classifier to learn a function that is robust to small changes anywhere along the manifold where the unlabeled data lie. The assumption motivating this approach is that different classes usually lie on disconnected manifolds and a small perturbation should not be able to jump from one class manifold to another class manifold.  
    At a point $$\boldsymbol{x}$$ that is not associated with a label in the dataset the model itself assigns some label $$\hat{y}$$ . The model's label $$\hat{y}$$ may not be the true label but if the model is high quality then $$\hat{y}$$ has a high probability of providing the true label. We can seek an adversarial example $$\boldsymbol{x}^{\prime}$$ that causes the classifier to output a label $$y^{\prime}$$ with $$y^{\prime} \neq \hat{y}$$.  

    Motivation  
    The assumption motivating this approach is that different classes usually lie on disconnected manifolds and a small perturbation should not be able to jump from one class manifold to another class manifold.  
    


12.Tangent Distance Tangent Prop and Manifold Tangent Classifiers
    Tangent Distance Algorithm  
    Many ML models assume the data to lie on a low dimensional manifold to overcome the curse of dimensionality. The inherent assumption which follows is that small perturbations that cause the data to move along the manifold it originally belonged to shouldn't lead to different class predictions. The idea of the tangent distance algorithm to find the K nearest neighbors using the distance metric as the distance between manifolds. A manifold $$Mi$$ is approximated by the tangent plane at $$xi$$ hence this technique needs tangent vectors to be specified.  


    Tangent Prop Algorithm  
    The tangent prop algorithm proposed to learn a neural network based classifier $$fx$$ which is invariant to known transformations causing the input to move along its manifold. Local invariance would require that $$\bigtriangledownx fx$$ is perpendicular to the tangent vectors $$V^{i}$$. This can also be achieved by adding a penalty term that minimizes the directional directive of $$fx$$ along each of the $$Vi$$  
    $$\Omegaf=\sum{i}\left\left\nabla{\boldsymbol{x}} f\boldsymbol{x}\right^{\top} \boldsymbol{v}^{i}\right^{2}$$  
    Tangent Propagation is similar to data augmentation in that both of them use prior knowledge of the domain to specify various transformations that the model should be invariant to. However tangent prop only resists infinitesimal perturbations while data augmentation causes invariance to much larger perturbations.  

    Drawbacks  
    While this analytical approach is intellectually elegant it has two major drawbacks.  
    1. It only regularizes the model to resist infinitesimal perturbation.  
        Explicit dataset augmentation confers resistance to larger perturbations. 
    2. The infinitesimal approach poses difficulties for models based on rectified linear units.  
        These models can only shrink their derivatives by turning units off or shrinking their weights. They are not able to shrink their derivatives by saturating at a high value with large weights as sigmoid or tanh units can.  
        Dataset augmentation works well with rectified linear units because different subsets of rectified units can activate for different transformed versions of each original input.  


    Manifold Tangent Classifier  
    The manifold tangent classifier Rifai et al. 2011c eliminates the need to know the tangent vectors a priori. Autoencoders can estimate the manifold tangent vectors.  

    Manifold Tangent Classifier works in two parts  
    1. Use Autoencoders to learn the manifold structures using Unsupervised Learning.
    1. Use these learned manifolds with tangent prop.  


    



Notes  

 A practical rule for choosing a regularizer  
     Stochastic noise is "high frequency"  
     Deterministic noise is also non smooth  

    Thus we should constrain learning towards smoother hypotheses. I.E. fit the signal more than you fit the noise which is non smooth. We end up harming both but harming the irregular non smooth noise more.  
 Regularization does two things  reduce fit to noise AND reduce overfitting  


^5 The architectures were constructed such that many of the parameters in the classifier model could be paired to corresponding parameters in the unsupervised model.
^4 Here we use a uniform distribution over $$\mu$$ to simplify the presentation but nonuniform distributions are also possible.  
^6 So you can think of the NFL theorems as providing some kind of theoretical justification for regularization or theoretical understanding that helps us see what the role of regularization is and provides some partial explanation for the empirical observation that it seems to often be effective.  


 ML Models 






Statistical Models

22.Statistical Models
    A Statistical Model is a non deterministic mathematical model that embodies a set of statistical assumptions concerning the generation of sample data.  
    It is specified as a mathematical relationship between one or more random variables and other non random variables.  

    Formal Definition  
    A Statistical Model consists of a pair $$S \mathcal{P}$$ where $$S$$ is the set of possible observations the sample space and $$\mathcal{P}$$ is a set of probability distributions on $$S$$.  

    The set $$\mathcal{P}$$ can be and is usually parametrized  
    $$\mathcal{P}=\left\{P{\theta}  \theta \in \Theta\right\}$$  
    The set $$\Theta$$ defines the parameters of the model.  

    Notes  
    
     It is important that a statistical model consists of a set of probability distributions  
        while a probability model is just one known distribution.  
    

1. Parametric Model
    A parametric model is a set of probability distributions indexed by a parameter $$\theta \in \Theta$$. We denote this as  
    $$\{py ; \theta | \theta \in \Theta\}$$  
    where $$\theta$$ is the parameter and $$\Theta$$ is the Parameter Space.  
    

2. Non Parametric Model
    Non parametric models differ from parametric models in that the model structure is not specified a priori but is instead determined from data. The term non parametric is not meant to imply that such models completely lack parameters but that the number and nature of the parameters are flexible and not fixed in advance.  

    Examples  
    
     A histogram is a simple nonparametric estimate of a probability distribution.
     Kernel density estimation provides better estimates of the density than histograms.
     Nonparametric regression and semiparametric regression methods have been developed based on kernels splines and wavelets.  
     Data envelopment analysis provides efficiency coefficients similar to those obtained by multivariate analysis without any distributional assumption.
     KNNs classify the unseen instance based on the K points in the training set which are nearest to it.
     A support vector machine SVM with a Gaussian kernel is a nonparametric large margin classifier.  
     Method of moments statistics with polynomial probability distributions.  
    


3. Other classes of Statistical Models
    Given $$\mathcal{P}=\left\{P{\theta}  \theta \in \Theta\right\}$$ the set of probability distributions on $$S$$.  
     A model is "parametric" if all the parameters are in finite dimensional parameter spaces; i.e. $$\Theta$$ has finite dimension  
     A model is "non parametric" if all the parameters are in infinite dimensional parameter spaces  
     A "semi parametric" model contains finite dimensional parameters of interest and infinite dimensional nuisance parameters  
     A "semi nonparametric" model has both finite dimensional and infinite dimensional unknown parameters of interest  
    

4. Types of Statistical Models
     Linear Model
     GLM  General Linear Model
     GiLM  Generalized Linear Model
     Latent Variable Model
    


11.The Statistical Model for Linear Regression
    Given a random sample $$\leftY{i} X{i 1} \ldots X{i p}\right i=1 \ldots n$$ the relation between the observations $$Yi$$ and the independent variables $$X{ij}$$ is formulated as  
    $$Y{i}=\beta{0}+\beta{1} \phi{1}\leftX{i 1}\right+\cdots+\beta{p} \phi{p}\leftX{i p}\right+\varepsilon{i} \qquad i=1 \ldots n$$  
    where $${\displaystyle \phi{1}\ldots \phi{p}}$$ may be nonlinear functions. In the above the quantities $$\varepsiloni$$ are random variables representing errors in the relationship.   

    The Linearity of the Model  
    The "linear" part of the designation relates to the appearance of the regression coefficients $$\betaj$$ in a linear way in the above relationship.  
    Alternatively one may say that the predicted values corresponding to the above model namely  
    $$\hat{Y}{i}=\beta{0}+\beta{1} \phi{1}\leftX{i 1}\right+\cdots+\beta{p} \phi{p}\leftX{i p}\right \qquadi=1 \ldots n$$  
    are linear functions of the coefficients $$\betaj$$.  

    Estimating the Parameters $$\betaj$$  
    Assuming an estimation on the basis of a least squares analysis estimates of the unknown parameters $$\betaj$$ are determined by minimizing a sum of squares function  
    $$S=\sum{i=1}^{n}\leftY{i} \beta{0} \beta{1} \phi{1}\leftX{i 1}\right \cdots \beta{p} \phi{p}\leftX{i p}\right\right^{2}$$  

    Effects of Linearity  
    
     The function to be minimized is a quadratic function of the $$\betaj$$ for which minimization is a relatively simple problem  
     The derivatives of the function are linear functions of the $$\betaj$$ making it easy to find the minimizing values  
     The minimizing values $$\betaj$$ are linear functions of the observations $$Yi$$  
     The minimizing values $$\betaj$$ are linear functions of the random errors $$\varepsiloni$$  which makes it relatively easy to determine the statistical properties of the estimated values of $$\betaj$$.  
    


5. Latent Variable Models
    Latent Variable Models are statistical models that relate a set of observable variables so called manifest variables to a set of latent variables.  

    Core Assumption  Local Independence  
    Local Independence  
    The observed items are conditionally independent of each other given an individual score on the latent variables. This means that the latent variable explains why the observed items are related to another.  

    In other words the targets/labels on the observations are the result of an individual's position on the latent variables and that the observations have nothing in common after controlling for the latent variable.  

    $$pAB\vert z = pA\vert z \times B\vert z$$  


    



    Methods for inferring Latent Variables  
    
     Hidden Markov models HMMs
     Factor analysis
     Principal component analysis PCA
     Partial least squares regression
     Latent semantic analysis and probabilistic latent semantic analysis
     EM algorithms
     Pseudo Marginal Metropolis Hastings algorithm
     Bayesian Methods LDA




    Notes    
    
     Latent Variables encode  information about the data  
        e.g. in compression a 1 bit latent variable can encode if a face is Male/Female.  
     Data Projection  
        You "hypothesis" how the data might have been generated by LVs.  
        Then the LVs generate the data/observations.  
        



          


          


8. Three ways to build classifiers
    1. Generative models e.g. LDA We’ll learn about LDA next lecture.
         Assume sample points come from probability distributions different for each class.  
         Guess form of distributions  
         For each class $$C$$ fit distribution parameters to class $$C$$ points giving $$PX\vert Y = C$$  
         For each $$C$$ estimate $$PY = C$$   
         Bayes’ Theorem gives $$PY\vert X$$  
         If $$0 1$$ loss pick class $$C$$ that maximizes $$PY = C\vert X = x$$ posterior probability equivalently maximizes $$PX = x\vert Y = C PY = C$$  
    2. Discriminative models e.g. logistic regression We’ll learn about logistic regression in a few weeks.  
         Model $$PY\vert X$$ directly  
    3. Find decision boundary e.g. SVM  
         Model $$rx$$ directly no posterior  

    Advantage of 1 & 2 $$PY\vert X$$ tells you probability your guess is wrong  
        This is something SVMs don’t do.  
    Advantage of 1 you can diagnose outliers $$PX$$ is very small  
    Disadvantages of 1 often hard to estimate distributions accurately;  
        real distributions rarely match standard ones.  



Regression Models

1. Linear Models
    A Linear Model takes an input $$x$$ and computes a signal $$s = \sum{i=0}^d wixi$$ that is a linear combination of the input with weights then apply a scoring function on the signal $$s$$.  
     Linear Classifier as a Parametric Model  
        Linear classifiers $$fx W=W x+b$$  are an example of a parametric model that sums up the knowledge of the training data in the parameter weight matrix $$W$$.  
     Scoring Function  
         Linear Classification  
            $$hx = signs$$  
         Linear Regression  
            $$hx = s$$  
         Logistic Regression  
            $$hx = \sigmas$$  




 Introduction and Basics of Deep Learning


Introduction Deep Feedforward Neural Networks

1. Deep FeedForward Neural Networks
    For a classifier; given $$y=f^{\ast}\boldsymbol{x}$$ maps an input $$\boldsymbol{x}$$ to a category $$y$$.  
    An FNN defines a mapping $$\boldsymbol{y}=f\boldsymbol{x} ; \boldsymbol{\theta}$$  and learns the value of the parameters $$\boldsymbol{\theta}$$  that result in the best function approximation.  

     FNNs are called networks because they are typically represented by composing together many different functions
     The model is associated with a DAG describing how the functions are composed together. 
     Functions connected in a chain structure are the most commonly used structure of neural networks.  
        E.g. we might have three functions $$f^{1} f^{2}$$ and $$f^{3}$$ connected in a chain to form $$f\boldsymbol{x}=f^{3}\leftf^{2}\leftf^{1}\boldsymbol{x}\right\right$$; being called the $$n$$ th Layer respectively.   
     The overall length of the chain is the depth of the model.  
     During training we drive $$f\boldsymbol{x}$$ to match $$f^{\ast}\boldsymbol{x}$$.   
        The training data provides us with noisy approximate examples of $$f^{\ast}\boldsymbol{x}$$ evaluated at different training points.  
    

2. FNNs from Linear Models
    Consider linear models biggest limitation model capacity is limited to linear functions.  
    To extend linear models to represent non linear functions of $$\boldsymbol{x}$$ we can  
    
     Apply the linear model not to $$\boldsymbol{x}$$ itself but to a transformed input $$\phi\boldsymbol{x}$$ where $$\phi$$ is a nonlinear transformation.  
     Equivalently apply the kernel trick to obtain nonlinear learning algorithm based on implicitly applying the $$\phi$$ mapping.  

    We can think of $$\phi$$ as providing a set of features describing $$\boldsymbol{x}$$ or as providing a new representation for $$\boldsymbol{x}$$.  
    Choosing the mapping $$\phi$$  
    
    1. Use a very generic $$\phi$$ s.a. infinite dimensional RBF kernel.  
        If $$\phi\boldsymbol{x}$$ is of high enough dimension we can always have enough capacity to fit the training set but generalization to the test set often remains poor.  
        Very generic feature mappings are usually based only on the principle of local smoothness and do not encode enough prior information to solve advanced problems.  
    2. Manually Engineer $$\phi$$.  
        Requires decades of human effort and the results are usually poor and non scalable.  
    3. The strategy of deep learning is to learn $$\phi$$.  
         We have a model  
            $$y=f\boldsymbol{x} ; \boldsymbol{\theta} \boldsymbol{w}=\phi\boldsymbol{x} ; \boldsymbol{\theta}^{\top} \boldsymbol{w}$$  
            We now have parameters $$\theta$$ that we use to learn $$\phi$$ from a broad class of functions and parameters $$\boldsymbol{w}$$ that map from $$\phi\boldsymbol{x}$$ to the desired output.  
            This is an example of a deep FNN with $$\phi$$ defining a hidden layer.   
         This approach is the only one of the three that gives up on the convexity of the training problem but the benefits outweigh the harms.  
         In this approach we parametrize the representation as $$\phi\boldsymbol{x}; \theta$$ and use the optimization algorithm to find the $$\theta$$ that corresponds to a good representation.  
         Advantages  
             Capturing the benefit of the  approach  
                by being highly generic — we do so by using a very broad family $$\phi\boldsymbol{x};\theta$$.  
             Capturing the benefit of the  approach  
                Human practitioners can encode their knowledge to help generalization by designing families $$\phi\boldsymbol{x}; \theta$$ that they expect will perform well.  
                The advantage is that the human designer only needs to find the right general function family rather than finding precisely the right function.  

    Thus we can motivate Deep NNs as a way to do automatic non linear feature extraction from the inputs.  

    This general principle of improving models by learning features extends beyond the feedforward networks to all models in deep learning.  
    FFNs are the application of this principle to learning deterministic mappings from $$\boldsymbol{x}$$ to $$\boldsymbol{y}$$ that lack feedback connections.  
    Other models apply these principles to learning stochastic mappings functions with feedback and probability distributions over a single vector.  


    Advantage and Comparison of Deep NNs  
    
     Linear classifier  
         Negative Limited representational power
         Positive Simple
     Shallow Neural network Exactly one hidden layer  
         Positive Unlimited representational power
         Negative Sometimes prohibitively wide 
     Deep Neural network  
         Positive Unlimited representational power
         Positive Relatively small number of hidden units needed
    

3. Interpretation of Neural Networks
    It is best to think of feedforward networks as function approximation machines that are designed to achieve statistical generalization occasionally drawing some insights from what we know about the brain rather than as models of brain function.  





Gradient Based Learning

1. Stochastic Gradient Descent and FNNs
    Stochastic Gradient Descent applied to nonconvex loss functions has no convergence guarantees and is sensitive to the values of the initial parameters.  

    Thus for FNNs since they have nonconvex loss functions  
    
     Initialize all weights to small random values.  
     The biases may be initialized to zero or to small positive values.  
    

2. Learning Conditional Distributions with Maximum Likelihood
    When Training using Maximum Likelihood  
    The cost function is simply the negative log likelihood.  
    Equivalently the cross entropy between the training data and the model distribution.  

    $$J\boldsymbol{\theta}= \mathbb{E}{\mathbf{x} \mathbf{y} \sim \hat{p}{\text {data }}} \log p{\text {model }}\boldsymbol{y} | \boldsymbol{x}  \tag{6.12}$$  

     The specific form of the cost function changes from model to model depending on the specific form of $$\log p{\text {model}}$$.  
     The expansion of the above equation typically yields some terms that do not depend on the model parameters and may be discarded.  

    Maximum Likelihood and MSE  
    The equivalence between maximum likelihood estimation with an output distribution and minimization of mean squared error holds not just for a linear model but in fact the equivalence holds regardless of the $$f\boldsymbol{x} ; \boldsymbol{\theta}$$ used to predict the mean of the Gaussian.  
    
  

    If $$p{\text {model }}\boldsymbol{y} | \boldsymbol{x}=\mathcal{N}\boldsymbol{y} ; f\boldsymbol{x} ; \boldsymbol{\theta} \boldsymbol{I}$$  then we recover the mean squared error cost  
    $$J\theta=\frac{1}{2} \mathbb{E}{\mathbf{x} \mathbf{y} \sim \hat{p}{\text {data }}}\|\boldsymbol{y} f\boldsymbol{x} ; \boldsymbol{\theta}\|^{2}+\mathrm{const} \tag{6.13}$$   
    up to a scaling factor of $$1/2$$ and a term that does not depend on $$\theta$$.  
    The discarded constant is based on the variance of the Gaussian distribution which in this case we chose not to parametrize.

    Why derive the cost function from Maximum Likelihood?  
    It removes the burden of designing cost functions for each model.  
    Specifying a model $$p\boldsymbol{y} | \boldsymbol{x}$$ automatically determines a cost function $$\log p\boldsymbol{y} | \boldsymbol{x}$$.  

    Cost Function Design  Desirable Properties  
     The gradient of the cost function must be large and predictable enough to serve as a good guide.  
        Functions that saturate become very flat undermine this objective because they make the gradient become very small. In many cases this happens because the activation functions used to produce the output of the hidden units or the output units saturate.  
        The negative log likelihood helps to avoid this problem for many models. Several output units involve an exp function that can saturate when its argument is very negative. The log function in the negative log likelihood cost function undoes the exp of some output units.  

    
  


3. Learning Conditional Statistics
    Instead of learning a full probability distribution $$p\boldsymbol{y} | \boldsymbol{x} ; \boldsymbol{\theta}$$ we often want to learn just one conditional statistic of $$\boldsymbol{y}$$ given $$\boldsymbol{x}$$.  
    For example we may have a predictor $$f\boldsymbol{x} ; \boldsymbol{\theta}$$  that we wish to employ to predict the mean of $$\boldsymbol{y}$$.  

    The Cost Function as a Functional  
    If we use a sufficiently powerful neural network we can think of the neural network as being able to represent any function $$f$$ from a wide class of functions with this class being limited only by features such as continuity and boundedness rather than by having a specific parametric form. From this point of view we can view the cost function as being a functional rather than just a function.  
    A Functional is a mapping from functions to real numbers.  
    We can thus think of learning as choosing a function rather than merely choosing a set of parameters.  
    We can design our cost functional to have its minimum occur at some specific function we desire.  
    For example we can design the cost functional to have its minimum lie on the function that maps $$\boldsymbol{x}$$  to the expected value of $$\boldsymbol{y}$$ given $$\boldsymbol{x}$$.  

    Solving an optimization problem with respect to a function requires a mathematical tool called calculus of variations described in section 19.4.2.  

4. Important Results in Optimization
    The calculus of variations can be used to derive the following two important results in Optimization  
    1. Solving the optimization problem  
        $${\displaystyle f^{\ast}=\underset{f}{\arg \min } \ \mathbb{E}{\mathbf{x} \mathbf{y} \sim p{\text {data }}}\|\boldsymbol{y} f\boldsymbol{x}\|^{2}} \tag{6.14}$$   
        yields  
        $${\displaystyle f^{\ast}\boldsymbol{x}=\mathbb{E}{\mathbf{y} \sim p{\text {data }}\boldsymbol{y} | \boldsymbol{x}}\boldsymbol{y} \tag{6.15}}$$  
        so long as this function lies within the class we optimize over.  
        In words if we could train on infinitely many samples from the true data distribution minimizing the MSE cost function would give a function that predicts the mean of $$\boldsymbol{y}$$ for each value of $$\boldsymbol{x}$$.  
        Different cost functions give different statistics.  
    2. Solving the optimization problem commonly known as Mean Absolute Error  
        $$f^{\ast}=\underset{f}{\arg \min } \ \underset{\mathbf{x} \mathbf{y} \sim p{\mathrm{data}}}{\mathbb{E}}\|\boldsymbol{y} f\boldsymbol{x}\| {1} \tag{6.16}$$  
        yields a function that predicts the median value of $$\boldsymbol{y}$$ for each $$\boldsymbol{x}$$ as long as such a function may be described by the family of functions we optimize over.   




    Drawbacks of MSE and MAE mean absolute error  
    They often lead to poor results when used with gradient based optimization.  
    Some output units that saturate produce very small gradients when combined with these cost functions.  
    This is one reason that the cross entropy cost function is more popular than MSE or MAE even when it is not necessary to estimate an entire distribution $$p\boldsymbol{y} | \boldsymbol{x}$$.  




Output Units

1. Introduction
    The choice of cost function is tightly coupled with the choice of output unit. Most of the time we simply use the cross entropy between the data distribution and the model distribution.  
    Thus the choice of how to represent the output then determines the form of the cross entropy function.  

    Throughout this analysis we suppose that  
    The FNN provides a set of hidden features defined by $$\boldsymbol{h}=f\boldsymbol{x} ; \boldsymbol{\theta}$$.  
    The role of the output layer thus is to provide some additional transformation from the features to complete the task the FNN is tasked with.  


2. Linear Units
    Linear Units are a simple kind of output units based on an affine transformation with no non linearity.  
    Mathematically given features $$\boldsymbol{h}$$ a layer of linear output units produces a vector $$\hat{\boldsymbol{y}}=\boldsymbol{W}^{\top} \boldsymbol{h}+\boldsymbol{b}$$.   
    
    Application used for Gaussian Output Distributions.  
    Linear output layers are often used to produce the mean of a conditional Gaussian Distributions  
    $$p\boldsymbol{y} | \boldsymbol{x}=\mathcal{N}\boldsymbol{y} ; \hat{\boldsymbol{y}} \boldsymbol{I} \tag{6.17}$$   
    In this case maximizing the log likelihood is equivalent to minimizing the MSE.  

    Learning the Covariance of the Gaussian  

    The MLE framework makes it straightforward to  
    
     Learn the covariance of the Gaussian too
     Make the covariance of the Gaussian be a function of the input  
    However the covariance must be constrained to be a positive definite matrix for all inputs.  
    It is difficult to satisfy such constraints with a linear output layer so typically other output units are used to parametrize the covariance.  
    > Approaches to modeling the covariance are described shortly in section 6.2.2.4.  


    Saturation  
    Because linear units do not saturate they pose little difficulty for gradient based optimization algorithms and may be used with a wide variety of optimization algorithms.  



3. Sigmoid Units
    Sigmoid Units 

    Binary Classification is a classification problem over two classes. It requires predicting the value of a binary variable $$y$$. It is one of many tasks requiring that.  
    The MLE approach is to define a Bernoulli distribution over $$y$$ conditioned on $$\boldsymbol{x}$$.  
    A Bernoulli distribution is defined by just a single number.  
    The Neural Network needs to predict only $$Py=1 \vert \boldsymbol{x}$$.  





 Model Compression










1. Knowledge Distillation
    

    Distilling the Knowledge in a Neural Network Hinton et al.  
    


    


    







 Information Theory








Information Theory

1. Information Theory
    Information theory is a branch of applied mathematics that revolves around quantifying how much information is present in a signal.    
    In the context of machine learning we can also apply information theory to continuous variables where some of these message length interpretations do not apply instead we mostly use a few key ideas from information theory to characterize probability distributions or to quantify similarity between probability distributions.  
    

2. Motivation and Intuition
    The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. A message saying “the sun rose this morning” is so uninformative as to be unnecessary to send but a message saying “there was a solar eclipse this morning” is very informative.  
    Thus information theory quantifies information in a way that formalizes this intuition    
     Likely events should have low information content  in the extreme case guaranteed events have no information at all  
     Less likely events should have higher information content
     Independent events should have additive information. For example finding out that a tossed coin has come up as heads twice should convey twice as much information as finding out that a tossed coin has come up as heads once.  
    

33.Measuring Information
    In Shannons Theory to transmit $$1$$ bit of information means to divide the recipients Uncertainty by a factor of $$2$$.  

    Thus the amount of information transmitted is the logarithm base $$2$$ of the uncertainty reduction factor.   

    The uncertainty reduction factor is just the inverse of the probability of the event being communicated.  

    Thus the amount of information in an event $$\mathbf{x} = x$$ called the Self Information  is  
    $$Ix = \log 1/px =  \logpx$$  

    Shannons Entropy  
    It is the expected amount of information of an uncertain/stochastic source. It acts as a measure of the amount of uncertainty of the events.  
    Equivalently the amount of information that you get from one sample drawn from a given probability distribution $$p$$.  
    

3. Self Information
    The Self Information or surprisal is a synonym for the surprise when a random variable is sampled.  
    The Self Information of an event $$\mathrm{x} = x$$    
    $$Ix =  \log Px$$  
    Self information deals only with a single outcome.
    

4. Shannon Entropy
    To quantify the amount of uncertainty in an entire probability distribution we use Shannon Entropy.    
    Shannon Entropy is defined as the average amount of information produced by a stochastic source of data.  
    $$Hx = {\displaystyle \operatorname {E}{x \sim P} Ix} =  {\displaystyle \operatorname {E}{x \sim P} \log PX =  \sum{i=1}^{n} p\leftx{i}\right \log p\leftx{i}\right}$$  
    Differential Entropy is Shannons entropy of a continuous random variable $$x$$
    

5. Distributions and Entropy
    Distributions that are nearly deterministic where the outcome is nearly certain have low entropy; distributions that are closer to uniform have high entropy.
    

6. Relative Entropy \| KL Divergence
    The Kullback Leibler divergence Relative Entropy is a measure of how one probability distribution diverges from a  expected probability distribution.    
    Mathematically    
    $${\displaystyle D{\text{KL}}P\parallel Q=\operatorname{E}{x \sim P} \left\log \dfrac{Px}{Qx}\right=\operatorname{E}{x \sim P} \left\log Px  \log Qx\right}$$  
     Discrete    
    $${\displaystyle D{\text{KL}}P\parallel Q=\sum{i}Pi\log \left{\frac {Pi}{Qi}}\right}$$    
     Continuous    
    $${\displaystyle D{\text{KL}}P\parallel Q=\int{ \infty }^{\infty }px\log \left{\frac {px}{qx}}\right\dx}$$   

    Interpretation    
     Discrete variables  
        it is the extra amount of information needed to send a message containing symbols drawn from probability distribution $$P$$ when we use a code that was designed to minimize the length of messages drawn from probability distribution $$Q$$.  
     Continuous variables  
            
    Properties   
     Non Negativity  
            $${\displaystyle D{\mathrm {KL} }P\|Q \geq 0}$$  
     $${\displaystyle D{\mathrm {KL} }P\|Q = 0 \iff}$$ $$P$$ and $$Q$$ are
         Discrete Variables  
                the same distribution 
         Continuous Variables  
                equal "almost everywhere"  
     Additivity of Independent Distributions  
            $${\displaystyle D{\text{KL}}P\parallel Q=D{\text{KL}}P{1}\parallel Q{1}+D{\text{KL}}P{2}\parallel Q{2}.}$$  
     $${\displaystyle D{\mathrm {KL} }P\|Q \neq D{\mathrm {KL} }Q\|P}$$  
        This asymmetry means that there are important consequences to the choice of the ordering   
     Convexity in the pair of PMFs $$p q$$ i.e. $${\displaystyle p{1}q{1}}$$ and  $${\displaystyle p{2}q{2}}$$ are two pairs of PMFs  
            $${\displaystyle D{\text{KL}}\lambda p{1}+1 \lambda p{2}\parallel \lambda q{1}+1 \lambda q{2}\leq \lambda D{\text{KL}}p{1}\parallel q{1}+1 \lambda D{\text{KL}}p{2}\parallel q{2}{\text{ for }}0\leq \lambda \leq 1.}$$  

    KL Div as a Distance     
    Because the KL divergence is non negative and measures the difference between two distributions it is often conceptualized as measuring some sort of distance between these distributions.  
    However it is not a true distance measure because it is not symmetric.  
    KL div is however a Quasi Metric since it satisfies all the properties of a distance metric except symmetry  

    Applications      
    Characterizing  
     Relative Shannon entropy in information systems
     Randomness in continuous time series 
     It is a measure of Information Gain; used when comparing statistical models of inference  
    
    Example Application and Direction of Minimization    
    Suppose we have a distribution $$px$$ and we wish to approximate it with another distribution $$qx$$.  
    We have a choice of minimizing either  
    1. $${\displaystyle D{\text{KL}}p\|q} \implies q^\ast = \operatorname {arg\min}q {\displaystyle D{\text{KL}}p\|q}$$  
        Produces an approximation that usually places high probability anywhere that the true distribution places high probability.  
    2. $${\displaystyle D{\text{KL}}q\|p} \implies q^\ast \operatorname {arg\min}q {\displaystyle D{\text{KL}}q\|p}$$  
        Produces an approximation that rarely places high probability anywhere that the true distribution places low probability.  
        which are different due to the asymmetry of the KL divergence  

    
    
    

7. Cross Entropy
    The Cross Entropy between two probability distributions $${\displaystyle p}$$ and $${\displaystyle q}$$ over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme is used that is optimized for an "unnatural" probability distribution $${\displaystyle q}$$ rather than the "true" distribution $${\displaystyle p}$$.    
    $$Hpq = \operatorname{E}{p} \log q= Hp + D{\mathrm{KL}}p\|q = \sum{x }px\\log qx$$  
    
    It is similar to KL Div but with an additional quantity  the entropy of $$p$$.    
    
    Minimizing the cross entropy with respect to $$Q$$ is equivalent to minimizing the KL divergence because $$Q$$ does not participate in the omitted term.  
    
    We treat $$0 \log 0$$ as $$\lim{x \to 0} x \log x = 0$$.    
    

8. Mutual Information
    The Mutual Information MI of two random variables is a measure of the mutual dependence between the two variables. More specifically it quantifies the "amount of information" in bits obtained about one random variable through observing the other random variable.  
    It Can be seen as a way of measuring the reduction in uncertainty information content of measuring a part of the system after observing the outcome of another parts of the system; given two R.Vs knowing the value of one of the R.Vs in the system gives a corresponding reduction in the uncertainty information content of measuring the other one.  

    As KL Divergence  
    Let $$X Y$$ be a pair of random variables with values over the space $$\mathcal{X} \times \mathcal{Y}$$ . If their joint distribution is $$P{X Y}$$ and the marginal distributions are $$P{X}$$ and $$P{Y}$$ the mutual information is defined as  
    $$IX ; Y=D{\mathrm{KL}}\leftP{X Y} \| P{X} \otimes P{Y}\right$$  

    In terms of PMFs for discrete distributions  
    The mutual information of two jointly discrete random variables $$X$$ and $$Y$$ is calculated as a double sum  
    $$\mathrm{I}X ; Y=\sum{y \in \mathcal{Y}} \sum{x \in \mathcal{X}} p{X Y}x y \log \left\frac{p{X Y}x y}{p{X}x p{Y}y}\right$$  
    where $${\displaystyle p{XY}}$$ is the joint probability mass function of $${\displaystyle X}$$ X and $${\displaystyle Y}$$ and $${\displaystyle p{X}}$$ and $${\displaystyle p{Y}}$$ are the marginal probability mass functions of $${\displaystyle X}$$ and $${\displaystyle Y}$$ respectively.  

    In terms of PDFs for continuous distributions  
    In the case of jointly continuous random variables the double sum is replaced by a double integral  
    $$\mathrm{I}X ; Y=\int{\mathcal{Y}} \int{\mathcal{X}} p{X Y}x y \log \left\frac{p{X Y}x y}{p{X}x p{Y}y}\right d x d y$$  
    where $$p{X Y}$$ is now the joint probability density function of $$X$$ and $$Y$$ and $$p{X}$$ and $$p{Y}$$ are the marginal probability density functions of $$X$$ and $$Y$$ respectively.  


    Intuitive Definitions  
    
     Measures the information that $$X$$ and $$Y$$ share  
        It measures how much knowing one of these variables reduces uncertainty about the other.  
         $$X Y$$ Independent  $$\implies IX; Y = 0$$ their MI is zero  
         $$X$$ deterministic function of $$Y$$ and vice versa $$\implies IX; Y = HX = HY$$ their MI is equal to entropy of each variable   
     It's a Measure of the inherent dependence expressed in the joint distribution of  $$X$$ and  $$Y$$ relative to the joint distribution of $$X$$ and $$Y$$ under the assumption of independence.  
        i.e. The price for encoding $${\displaystyle XY}$$ as a pair of independent random variables when in reality they are not.  

    Properties  
    
     The KL divergence shows that $$IX; Y$$ is equal to zero precisely when the joint distribution conicides with the product of the marginals i.e. when  $$X$$ and $$Y$$ are independent
     The MI is non negative $$IX; Y \geq 0$$  
         It is a measure of the price for encoding $${\displaystyle XY}$$ as a pair of independent random variables when in reality they are not.  
     It is symmetric $$IX; Y = IY; X$$  
     Related to conditional and joint entropies  
        $${\displaystyle {\begin{aligned}\operatorname {I} X;Y&{}\equiv \mathrm {H} X \mathrm {H} X|Y&{}\equiv \mathrm {H} Y \mathrm {H} Y|X&{}\equiv \mathrm {H} X+\mathrm {H} Y \mathrm {H} XY&{}\equiv \mathrm {H} XY \mathrm {H} X|Y \mathrm {H} Y|X\end{aligned}}}$$  
        where $$\mathrm{H}X$$ and $$\mathrm{H}Y$$ are the marginal entropies $$\mathrm{H}X | Y$$ and $$\mathrm{H}Y | X$$ are the conditional entopries and $$\mathrm{H}X Y$$ is the joint entropy of $$X$$ and $$Y$$.  
         Note the analogy to the union difference and intersection of two sets  

     Related to KL div of conditional distribution  
        $$\mathrm{I}X ; Y=\mathbb{E}{Y}\leftD{\mathrm{KL}}\leftp{X | Y} \| p{X}\right\right$$  




    Applications  
    
     In search engine technology mutual information between phrases and contexts is used as a feature for k means clustering to discover semantic clusters concepts  
     Discriminative training procedures for hidden Markov models have been proposed based on the maximum mutual information MMI criterion.
     Mutual information has been used as a criterion for feature selection and feature transformations in machine learning. It can be used to characterize both the relevance and redundancy of variables such as the minimum redundancy feature selection.
     Mutual information is used in determining the similarity of two different clusterings of a dataset. As such it provides some advantages over the traditional Rand index.
     Mutual information of words is often used as a significance function for the computation of collocations in corpus linguistics.  
     Detection of phase synchronization in time series analysis
     The mutual information is used to learn the structure of Bayesian networks/dynamic Bayesian networks which is thought to explain the causal relationship between random variables  
     Popular cost function in decision tree learning.
     In the infomax method for neural net and other machine learning including the infomax based Independent component analysis algorithm



    Independence assumptions and low rank matrix approximation alternative definition  
    
    



    As a Metric relation to Jaccard distance  
    
    

    


9. Pointwise Mutual Information PMI
    The PMI of a pair of outcomes $$x$$ and $$y$$ belonging to discrete random variables $$X$$ and $$Y$$ quantifies the discrepancy between the probability of their coincidence given their joint distribution and their individual distributions assuming independence. Mathematically  
    $$\operatorname{pmi}x ; y \equiv \log \frac{px y}{px py}=\log \frac{px | y}{px}=\log \frac{py | x}{py}$$  
    In contrast to mutual information MI which builds upon PMI it refers to single events whereas MI refers to the average of all possible events.  
    The mutual information MI of the random variables $$X$$ and $$Y$$ is the expected value of the PMI over all possible outcomes.  



 Elements of Machine Learning


Machine Learning Basics

1. Introduction and Definitions
     Two Approaches to Statistics  
         Frequentest Estimators  
         Bayesian Inference  
     The Design Matrix  
        A common way for describing a dataset where it is a matrix containing a different example in each row. Each column of the matrix corresponds to a different feature.  
            

2. Learning Algorithms
     Learning  
        A computer program is said to learn from experience $$E$$ with respect to some class of tasks $$T$$ and performance measure $$P$$ if its performance at tasks in $$T$$ as measured by $$P$$ improves with experience $$E$$.  
     The Task $$T$$ 
         Classification  
            A task where the computer program is asked to specify which of $$k$$ categories some input belongs to.  
            To solve this task the learning algorithm is usually asked to produce a function $$f\mathbb{R}^n \rightarrow {1 . . .  k}$$.  
            When $$y=fx$$ the model assigns an input described by vector $$x$$ to a category identified by numeric code $$y$$.  
            e.g. Object Recognition
         Classification with Missing Inputs  
            Classification becomes more challenging if the computer program is not guaranteed that every measurement in its input vector will always be provided.  
            To solve this task rather than providing a single classification function as in the normal classification case the learning algorithm must learn a set of functions each corresponding to classifying $$x$$ with a different subset of its inputs missing.  

            One way to efficiently define such a large set of functions is to learn a probability distribution over all the relevant variables then solve the classification task by marginalizing out the missing variables.  
            With $$n$$ input variables we can now obtain all $$2^n$$ different classification functions needed for each possible set of missing inputs but the computer program needs to learn only a single function describing the joint probability distribution.  
            e.g. Medical Diagnosis where some tests weren't conducted for any reason  
         Regression  
            A computer is asked to predict a numerical value given some input.  
            To solve this task the learning algorithm is asked to output a function $$f\mathbb{R}^n \rightarrow R$$  
            e.g. Object Localization
         Transcription  
            In this type of task the machine learning system is asked to observe a relatively unstructured representation of some kind of data and transcribe the information into discrete textual form.  
            e.g. OCR
         Machine Translation  
            In a machine translation task the input already consists of a sequence of symbols in some language and the computer program must convert this into a sequence of symbols in another language.  
            e.g. Google Translate  
         Structured Output  
            Structured output tasks involve any task where the output is a vector or other data structure containing multiple values with important relationships between the different elements.  
            This is a broad category and subsumes the transcription and translation tasks described above as well as many other tasks.  
            These tasks are called structured output tasks because the program must output several values that are all tightly interrelated. For example the words produced by an image captioning program must form a valid sentence.  
            e.g. Syntax Parsing Image Segmentation  
         Anomaly Detection  
            In this type of task the computer program sifts through a set of events or objects and ﬂags some of them as being unusual or atypical.  
            e.g. Insider Trading Detection
         Synthesis and Sampling  
            In this type of task the machine learning algorithm is asked to generate new examples that are similar to those in the training data.  
            This is a kind of structured output task but with the added qualification that there is no single correct output for each input and we explicitly desire a large amount of variation in the output in order for the output to seem more natural and realistic.  
            e.g. Image Synthesis Speech Synthesis
         Imputation  
            In this type of task the machine learning algorithm is given a new example $$x \in \mathbb{R}^n$$ but with some entries $$xi$$ of $$x$$ missing. The algorithm must provide a prediction of the values of the missing entries.  
         Denoising  
            In this type of task the machine learning algorithm is given as input a corrupted example $$\tilde{x} \in \mathbb{R}^n$$ obtained by an unknown corruption process from a clean example $$x \in \mathbb{R}^n$$. The learner must predict the clean example $$x$$ from its corrupted version $$\tilde{x}$$ or more generally predict the conditional probability distribution $$px |\tilde{x}$$.  
            e.g. Signal Reconstruction Image Artifact Removal  
         Density Probability Mass Function Estimation  
            In the density estimation problem the machine learning algorithm is asked to learn a function $$p\text{model} \mathbb{R}^n \rightarrow R$$ where $$p\text{model}x$$ can be interpreted as a probability density function if $$x$$ is continuous or a probability mass function if $$x$$ is discrete on the space that the examples were drawn from.  
            To do such a task well the algorithm needs to learn the structure of the data it has seen. It must know where examples cluster tightly and where they are unlikely to occur.  
            Most of the tasks described above require the learning algorithm to at least implicitly capture the structure of the probability distribution i.e. it can be computed but we don't have an equation for it. Density estimation enables us to explicitly capture that distribution.  
            In principlewe can then perform computations on that distribution to solve the other tasks as well.  
            For example if we have performed density estimation to obtain a probability distribution px we can use that distribution to solve the missing value imputation task. Equivalently if a value $$xi$$ is missing and all the other values denoted $$x{−i}$$ are given then we know the distribution over it is given by $$pxi| x{−i}$$.  
                In practice density estimation does not always enable us to solve all these related tasks because in many cases the required operations on px are computationally intractable.  
            e.g. Language Modeling  
         A Lot More  
            Their are many more tasks that could be defined for and solved by Machine Learning. However this is a list of the most common problems which have a well known set of methods for handling them.  

     The Performance Measure $$P$$  
        A quantitative measure of the performance of a machine learning algorithm.  
        We often use accuracy or error rate.

        
11.Learning vs Optimization
    Generalization is the ability to perform well on previously unobserved inputs.  
    Generalization Test Error is defined as the expected value of the error on a new input.  

    Learning vs Optimization  
    
     The problem of Reducing the training error on the training set is one of optimization.  
     The problem of Reducing the training error as well as the generalization test error is one of learning.  
    

22.Statistical Learning Theory
    It is a framework that under certain assumptions allows us to study the question of "
    How can we affect performance on the test set when we can observe only the training set?"  

    Assumptions  
    
     The training and test data are generated by a probability distribution over datasets called the data generating process.  
     The i.i.d. assumptions  
         The examples in each dataset are independent from each other  
         The training set and test set are identically distributed drawn from the same probability distribution as each other  

        This assumption enables us to describe the data generating process with a probability distribution over a single example. The same distribution is then used to generate every train example and every test example.  
     We call that shared underlying distribution the data generating distribution denoted $$p{\text {data }}$$  

    This probabilistic framework and the i.i.d. assumptions enable us to mathematically study the relationship between training error and test error.  

33.Capacity Overfitting and Underfitting
    The ML process  
    We sample the training set then use it to choose the parameters to reduce training set error then sample the test set.  
    Under this process the expected test error is greater than or equal to the expected value of training error.  

    The factors determining how well a machine learning algorithm will perform are its ability to  
    
    1. Make the training error small
    2. Make the gap between training and test error small  
    
    These two factors correspond to the two central challenges in machine learning underfitting and overfitting.  
    Underfitting  occurs when the model is not able to obtain a sufficiently low error value on the training set.  
    Overfitting occurs when the gap between the training error and test error is too large.  

    We can control whether a model is more likely to overfit or underfit by altering its capacity.  
    Capacity a models capacity is its ability to fit a wide variety of functions.  
    
     Models with low capacity may struggle to fit the training set. 
     Models with high capacity can overfit by memorizing properties of the training set that do not serve them well on the test set. 

    One way to control the capacity of a learning algorithm is by choosing its hypothesis space.  
    Hypothesis Space the set of functions that the learning algorithm is allowed to select as being the solution.  

    Statistical learning theory provides various means of quantifying model capacity.Among these the most well known is the Vapnik Chervonenkis VC dimension.  
    The VC Dimension is defined as being the largest possible value of $$m$$ for which there exists a training set of $$m$$ different $$\mathbf{x}$$ points that the classifier can label arbitrarily.  
    It measure the capacity of a binary classifier.  

    Quantifying the capacity of the model enables statistical learning theory to make quantitative predictions. The most important results in statistical learning theory show that the discrepancy between training error and generalization error is bounded from above by a quantity that grows as the model capacity grows but shrinks as the number of training examples increases.  




    Effective capacity and Representational Capacity...  


33.Regularization
    Regularization is a more general way of controlling a models capacity by allowing us to express preference for one function over another in the same hypothesis space; instead of including or excluding members from the hypothesis space completely.  
    We can think of excluding a function from a hypothesis space as expressing an infinitely strong preference against that function.  

    Regularization can be defined as any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.  

    Example Weight Decay  
    It is a regularization form that adds the $$L^2$$ norm of the weights to the cost function; allowing us to express preference for smaller weights. It is controlled by a hyperparameter $$\lambda$$.  
    $$J\boldsymbol{w}=\mathrm{MSE} {\mathrm{train}}+\lambda \boldsymbol{w}^{\top} \boldsymbol{w}$$   
    This gives us solutions that have a smaller slope or that put weight on fewer of the features.  

    More generally the regularizer penalty of weight decay is  
    $$\Omega\boldsymbol{w}=\boldsymbol{w}^{\top} \boldsymbol{w}$$   

    

3. Estimators
    A Point Estimator or statistic is any function of the data  
    $$\hat{\boldsymbol{\theta}}{m}=g\left\boldsymbol{x}^{1} \ldots \boldsymbol{x}^{m}\right$$  
    such that a good estimator is a function whose output is close to the true underlying $$ \theta $$ that generated the training data.  
    We assume that the true $$\boldsymbol{\theta}$$ is fixed and that $$\hat{\boldsymbol{\theta}}$$ is a function of the data which is drawn from a random process making $$\hat{\boldsymbol{\theta}}$$ a random variable.  


    Function Estimation/Approximation refers to estimation of the relationship between input and target data.  
    I.E. We are trying to predict a variable $$y$$ given an input vector $$x$$ and we assume that there is a function $$fx$$ that describes the approximate relationship between $$y$$ and $$x$$.  
    If we assume that $$y = fx + \epsilon$$ where $$\epsilon$$ is the part of $$y$$ that is not predictable from $$x$$; then we are interested in approximating $$f$$ with a model or estimate $$ \hat{f} $$.  
    Function estimation is really just the same as estimating a parameter $$\boldsymbol{\theta}$$; the function estimator $$ \hat{f} $$ is simply a point estimator in function space.  



          

    


4. Properties of Estimators  Bias and Variance
    The Bias of an estimator is  
    $$ \operatorname{bias}\left\hat{\boldsymbol{\theta}}{m}\right=\mathbb{E}\left\hat{\boldsymbol{\theta}}{m}\right \boldsymbol{\theta} $$  
    where the expectation is over the data seen as samples from a random variable and $$ \theta $$ is the true underlying value of $$ \theta $$ used to define the data generating distribution.  
     Unbiased Estimators An estimator $$\hat{\boldsymbol{\theta}}{m}$$ is said to be unbiased if $$\operatorname{bias}\left\hat{\boldsymbol{\theta}}{m}\right=\mathbf{0}$$ which implies that $$ \mathbb{E}\left\hat{\boldsymbol{\theta}}{m}\right=\boldsymbol{\theta} $$.  
     Asymptotically Unbiased Estimators An estimator is said to be asymptotically unbiased if $$ \lim {m \rightarrow \infty} \operatorname{bias}\left\hat{\boldsymbol{\theta}}{m}\right=\mathbf{0}$$ which implies that $$\lim {m \rightarrow \infty} \mathbb{E}\left\hat{\boldsymbol{\theta}}{m}\right=\boldsymbol{\theta} $$  

    The Variance of an estimator is a way to measure how much we expect the estimator to vary as a function of the data sample defined simply as the variance over the training set random variable $$\hat{\theta}$$  
    $$ \operatorname{Var}\hat{\theta} $$  

    The Standard Error $$\operatorname{SE}\hat{\theta}$$ of an estimator is the square root of the variance.  
     E.g. The Standard Error of the Sample Mean  
        $$\operatorname{SE}\left\hat{\mu}{m}\right=\sqrt{\operatorname{Var}\left\frac{1}{m} \sum{i=1}^{m} x^{i}\right}=\frac{\sigma}{\sqrt{m}}$$  
        Where $$\sigma^2$$ is the true variance of the samples $$x^i$$.  

        
        onclick="showTextwithParentPopHideevent;"}
        $$\begin{aligned} \operatorname{Var}\left\overline{X}{n}\right &=\operatorname{Var}\left\frac{1}{n} \sum{i=1}^{n} X{i}\right  &=\frac{1}{n^{2}} \operatorname{Var}\left\sum{i=1}^{n} X{i}\right  &=\frac{1}{n^{2}} \sum{i=1}^{n} \operatorname{Var}\leftX{i}\right  &=\frac{1}{n^{2}} \sum{i=1}^{n} \sigma^{2}  &=\frac{1}{n^{2}} n \sigma^{2}=\frac{\sigma^{2}}{n} \end{aligned}$$  

    Unfortunately neither the square root of the sample variance nor the square root of the unbiased estimator of the variance provide an unbiased estimate of the standard deviation.  


    

5. Generalization Error from Standard Error of the mean
    We often estimate the generalization error by computing the sample mean of the error on the test set.  
    Taking advantage of the central limit theorem which tells us that the mean will be approximately distributed with a normal distribution we can use the standard error to compute the probability that the true expectation falls in any chosen interval.  
    For example the 95 percent confidence interval centered on the mean $$\hat{\mu} {m}$$ is  
    $$\left\hat{\mu}{m} 1.96 \mathrm{SE}\left\hat{\mu}{m}\right \hat{\mu}{m}+1.96 \mathrm{SE}\left\hat{\mu}{m}\right\right$$  
    under the normal distribution with mean $$\hat{\mu}{m}$$ and variance $$\mathrm{SE}\left\hat{\mu}{m}\right^{2}$$.  
    We say that algorithm $$\boldsymbol{A}$$ is better than algorithm $$\boldsymbol{B}$$ if the upper bound of the $$95$$ percent confidence interval for the error of algorithm $$\boldsymbol{A}$$ is less than the lower bound of the $$95$$ percent confidence interval for the error of algorithm $$\boldsymbol{B}$$.  

6. The Bias Variance Trade off
    Bias and variance measure two different sources of error in an estimator  
     Bias measures the expected deviation from the true value of the function or parameter  
     Variance provides a measure of the deviation from the expected estimator value that any particular sampling of the data is likely to cause  

    Evaluating Models  Trading off Bias and Variance  
     The most common way to negotiate this trade off is to use cross validation
     Alternatively we can also compare the mean squared error MSE of the estimates  
        $$\begin{aligned} \mathrm{MSE} &=\mathbb{E}\left\left\hat{\theta}{m} \theta\right^{2}\right  &=\operatorname{Bias}\left\hat{\theta}{m}\right^{2}+\operatorname{Var}\left\hat{\theta}{m}\right \end{aligned}$$  
        The MSE measures the overall expected deviation — in a squared error sense — between the estimator and the true value of the parameter $$\theta$$.  

    
    
  


7. Properties of Estimators  Consistency
    Consistency is a property implying that as the number of data points $$m$$ in our dataset increases our point estimates converge to the true value of the corresponding parameters. Formally  
    $$\mathrm{plim}{m \rightarrow \infty} \hat{\theta}{m}=\theta$$  
    Where    
    $${\text { The symbol plim indicates convergence in probability meaning that for any } \epsilon>0}$$ 
    $${P\left\vert\hat{\theta}{m} \theta \vert>\epsilon\right \rightarrow 0 \text { as } m \rightarrow \infty}$$  
    Sometimes referred to as Weak Consistency  

    Strong Consistency applies to almost sure convergence of $$\hat{\theta}$$ to $$\theta$$.  

    Consistency and Asymptotic Bias  
     Consistency ensures that the bias induced by the estimator diminishes as the number of data examples grows.  
     However asymptotic unbiasedness does not imply consistency

    

8. Maximum Likelihood Estimation MLE
    MLE is a method/principle from which we can derive specific functions that are good estimators for different models.  

    Let $$\mathbb{X}=\left\{\boldsymbol{x}^{1} \ldots \boldsymbol{x}^{m}\right\}$$ be a set of $$m$$ examples drawn independently from the true but unknown data generating distribution $$p{\text { data }}\mathbf{x}$$ and let $$p{\text { model }}\mathbf{x} ; \boldsymbol{\theta}$$ be a parametric family of probability distributions over the same space indexed by $$\boldsymbol{\theta}$$^4  
    The Maximum Likelihood Estimator for $$\boldsymbol{\theta}$$ is  
    $$\begin{aligned} \boldsymbol{\theta}{\mathrm{ML}} &=\underset{\boldsymbol{\theta}}{\arg \max } p{\text { model }}\mathbb{X} ; \boldsymbol{\theta}  &=\underset{\boldsymbol{\theta}}{\arg \max } \prod{i=1}^{m} p{\text { model }}\left\boldsymbol{x}^{i} ; \boldsymbol{\theta}\right \end{aligned}$$  
    We take the $$log$$ for numerical stability  
    $$\boldsymbol{\theta}{\mathrm{ML}}=\underset{\boldsymbol{\theta}}{\arg \max } \sum{i=1}^{m} \log p{\text { model }}\left\boldsymbol{x}^{i} ; \boldsymbol{\theta}\right \tag{5.58}$$  
    Because the $$\text { arg max }$$ does not change when we rescale the cost function we can divide by $$m$$ to obtain a version of the criterion that is expressed as an expectation with respect to the empirical distribution $$\hat{p} {\text { data }}$$  defined by the training data  
    $$\boldsymbol{\theta}{\mathrm{ML}}=\underset{\boldsymbol{\theta}}{\arg \max } \mathbb{E}{\mathbf{x} \sim \hat{p} \text { data }} \log p{\text { model }}\boldsymbol{x} ; \boldsymbol{\theta} \tag{5.59}$$  

    MLE as Minimizing KL Divergence between the Empirical dist. and the model dist.  
    We can interpret maximum likelihood estimation as minimizing the dissimilarity between the empirical distribution $$\hat{p} {\text { data }}$$ defined by the training set and the model distribution with the degree of dissimilarity between the two measured by the KL divergence.  
     The KL divergence is given by  
        $$D{\mathrm{KL}}\left\hat{p}{\text { data }} \| p{\text { model }}\right=\mathbb{E}{\mathbf{x} \sim \hat{p}{\text { data }}}\left\log \hat{p}{\text { data }}\boldsymbol{x} \log p{\text { model }}\boldsymbol{x}\right \tag{5.60}$$  
    The term on the left is a function only of the data generating process not the model. This means when we train the model to minimize the KL divergence we need only minimize  
    $$ \mathbb{E}{\mathbf{x} \sim \hat{p}{\text { data }}}\left\log p{\text { model }}\boldsymbol{x}\right \tag{5.61}$$  
    which is of course the same as the maximization in equation $$5.59$$.  

    Minimizing this KL divergence corresponds exactly to minimizing the cross entropy between the distributions.  
    Any loss consisting of a negative log likelihood is a cross entropy between the empirical distribution defined by the training set and theprobability distribution defined by model.  
    E.g. MSE is the cross entropy between the empirical distribution and a Gaussian model.  

    We can thus see maximum likelihood as an attempt to make the model distribution match the empirical distribution $$\hat{p}  {\text { data }}$$^5.  

    Maximum likelihood thus becomes minimization of the negative log likelihoodNLL or equivalently minimization of the cross entropy^6.  




    Conditional Log Likelihood MLE for Supervised Learning  
    The maximum likelihood estimator can readily be generalized to estimate a conditional probability $$P\mathbf{y} | \mathbf{x} ; \boldsymbol{\theta}$$ in order to predict $$\mathbf{y}$$  given $$\mathbf{x}$$. If $$X$$ represents all our inputs and $$Y$$ all our observed targets then the conditional maximum likelihood estimator is  
    $$\boldsymbol{\theta} {\mathrm{ML}}=\underset{\boldsymbol{\theta}}{\arg \max } P\boldsymbol{Y} | \boldsymbol{X} ; \boldsymbol{\theta} \tag{5.62}$$  
    and the log likelihood estimator is  
    $$\boldsymbol{\theta}{\mathrm{ML}}=\underset{\boldsymbol{\theta}}{\arg \max } \sum{i=1}^{m} \log P\left\boldsymbol{y}^{i} | \boldsymbol{x}^{i} ; \boldsymbol{\theta}\right \tag{5.63}$$  


    Properties of Maximum Likelihood Estimator  
    The main appeal of the maximum likelihood estimator is that it can be shown to be the best estimator asymptotically as the number of examples $$m \rightarrow \infty$$ in terms of its rate of convergence as $$m$$ increases.  
     Consistency as the number of training examples approaches infinity the maximum likelihood estimate of a parameter converges to the true value of the parameter under the following conditions  
         The true distribution $$p{\text { data }}$$ must lie within the model family $$p{\text { model }}\cdot ; \boldsymbol{\theta}$$. Otherwise no estimator can recover $$p{\text { data }}$$.  
         The true distribution $$p{\text { data }}$$ must correspond to exactly one value of $$\boldsymbol{\theta}$$. Otherwise maximum likelihood can recover the correct $$p{\text { data }}$$ but will not be able to determine which value of $$\boldsymbol{\theta}$$ was used by the data generating process.   
     Statistical Efficiency meaning that one consistent estimator may obtain lower generalization error for a fixed number of samples $$m$$ or equivalently may require fewer examples to obtain a fixed level of generalization error.^7  
        The Cramér Rao lower bound shows that no consistent estimator has a lower MSE than the maximum likelihood estimator.  
    

9. Maximum A Posteriori MAP Estimation
    The MAP estimate chooses the point of maximal posterior probability by allowing the prior to influence the choice of the point estimate  
    $$\boldsymbol{\theta} {\mathrm{MAP}}=\underset{\boldsymbol{\theta}}{\arg \max } p\boldsymbol{\theta} | \boldsymbol{x}=\underset{\boldsymbol{\theta}}{\arg \max } \log p\boldsymbol{x} | \boldsymbol{\theta}+\log p\boldsymbol{\theta} \tag{5.79}$$  

    Many regularized estimation strategies such as maximum likelihood learning regularized with weight decay can be interpreted as making the MAP approximation to Bayesian inference.  
    E.g. MAP Bayesian inference with a Gaussian prior on the weights corresponds to weight decay Regularization  
        consider a linear regression model with a Gaussian prior on the weights $$\mathbf{w}$$. If this prior is given by $$\mathcal{N}\left\boldsymbol{w} ; \mathbf{0} \frac{1}{\lambda} \boldsymbol{I}^{2}\right$$ then the log prior term in equation $$5.79$$ is proportional to the familiar $$\lambda w^{T} w$$ weight decay penalty plus a constant.    

    This view applies when the regularization consists of adding an extra term to the objective function that corresponds to $$\log p\boldsymbol{\theta}$$ i.e. logarithm of a probability distribution.  
        


^4 In other words $$p{\text { model }}x ; \boldsymbol{\theta}$$ maps any configuration $$x$$ to a real number estimating the true probability $$p{\text { data }}x$$.  
^5 Ideally we would like to match the true data generating distribution $$p{\text{ data }}$$ but we have no direct access to this distribution.  
^6 The perspective of maximum likelihood as minimum KL divergence becomes helpful in this case because the KL divergence has a known minimum value of zero. The negative log likelihood can actually become negative when $$x$$ is real valued.  
^7 Statistical efficiency measured by the MSE between the estimated and true parameter is typically studied in the parametric case as in linear regression where our goal is to estimate the value of a parameter assuming it is possible to identify the true parameter not the value of a function.  



The Mathematics of Neural Networks

0. Derivative
    The derivative of a function is the amount that the value of a function changes when the input changes by an $$\epsilon$$ amount  
    $$f'a=\lim{h\to 0}{\frac {fa+h fa}{h}}. 
    \text{i.e. } fx + \epsilon\approx fx+\epsilon f'x
    $$  
    
    The Chain Rule is a way to compute the derivative of composite functions.  
    If $$y = fx$$ and $$z = gy$$  
    $$\dfrac{\partial z}{\partial x} = \dfrac{\partial z}{\partial y} \dfrac{\partial y}{\partial x}$$       

1. Gradient Vector in Scalar out
    Gradients generalize derivatives to scalar functions of several variables  

    Property the gradient of a function $$\nabla fx$$ points in the direction of steepest ascent from $$x$$.  
    

2. The Jacobian Vector in Vector out
   The Jacobian of $$f \mathbb{R}^n \rightarrow \mathbb{R}^m $$ is a matrix of  order partial derivatives of a vector valued function  
   The Chain Rule  
    Let $$f  \mathbb{R}^N \rightarrow \mathbb{R}^M$$ and $$g  \mathbb{R}^M \rightarrow \mathbb{R}^ K$$; and let  $$x \in \mathbb{R}^N y \in \mathbb{R}^M$$ and $$z \in \mathbb{R}^K$$ with $$y = fx$$ and $$z = gy$$  
  $$\dfrac{\partial z}{\partial x} = \dfrac{\partial z}{\partial y} \dfrac{\partial y}{\partial x}$$    
   where $$\dfrac{\partial z}{\partial y} \in \mathbb{R}^{K \times M}$$ matrix $$\dfrac{\partial y}{\partial x} \in \mathbb{R}^{M \times N}$$ matrix and $$\dfrac{\partial z}{\partial x} \in \mathbb{R}^{K \times N}$$  matrix;  
    the multiplication of $$\dfrac{\partial z}{\partial y}$$  and $$\dfrac{\partial y}{\partial x}$$ is a matrix multiplication.  

2. The Generalized Jacobian Tensor in Tensor out
   A Tensor is a D dimensional grid of number.  
   Suppose that $$f \mathbb{R}^{N1 \times \cdots \times N{Dx}} \rightarrow \mathbb{R}^{M1 \times \cdots \times M{Dy}} $$.  
    If $$y = fx$$ then the derivative $$\dfrac{\partial y}{\partial x}$$ is a generalized Jacobian  an object with shape  
   $$M1 \times \cdots \times M{Dy} \times N1 \times \cdots \times N{Dx}$$ 
   >  we can think of the generalized Jacobian as generalization of a matrix where each “row” has the same shape as $$y$$  and each “column” has the same shape as $$x$$.  
    Just like the standard Jacobian the generalized Jacobian tells us the relative rates of change between all elements of $$x$$  and all elements of $$y$$  
   $$\dfrac{\partial y}{\partial x}{ij} = \dfrac{\partial yi}{\partial xj} \in \mathbb{R}$$
   Just as the derivative the generalized Jacobian gives us the relative change in $$y$$ given a small change in $$x$$  
   $$fx + \delta x\approx fx+ f'x \delta x = y + \dfrac{\partial y}{\partial x}\delta x$$  
   where now $$\delta x$$ is a tensor in $$\mathbb{R}{N1 \cdots N{dx}}$$ and $$\dfrac{\partial y}{\partial x}$$ is a generalized matrix in $$\mathbb{R}^{M1 \times \cdots \times M{Dy} \times N1 \times \cdots \times N{Dx}} $$.  
    The product $$\dfrac{\partial yi}{\partial xj} \delta x$$ is therefore a generalized matrix vector multiply which results in a tensor in $$\mathbb{R}^{M1 \times \cdots \times M{Dy}}$$.  
   The generalized matrix vector multiply follows the same algebraic rules as a traditional matrix vector multiply  
   The Chain Rule  


3. The Hessian
   The Hessian Matrix of a scalar function $$f \mathbb{R}^d \rightarrow \mathbb{R} $$ is a matric of  order partial derivatives
   Properties 
         The Hessian matrix is symmetric  since we usually work with smooth/differentiable functions  due to Clairauts Theorem.  
        Clairauts Theorem if the partial derivatives are continuous the order of differentiation can be interchanged  
         The Hessian is used in some optimization algorithms such as Newton’s method  
         It is expensive to calculate but can drastically reduce the number of iterations needed to converge to a local minimum by providing information about the curvature of $$f$$


4. Matrix Calculus
   Important Identities  
   $${\frac  {\partial {\mathbf  {a}}^{\top }{\mathbf  {x}}}{\partial {\mathbf  {x}}}}={\frac  {\partial {\mathbf  {x}}^{\top }{\mathbf  {a}}}{\partial {\mathbf  {x}}}}= a  
    {\frac  {\partial {\mathbf  {x}}^{\top }{\mathbf  {A}}{\mathbf  {x}}}{\partial {\mathbf  {x}}}}=  {\mathbf  {A}}+{\mathbf  {A}}^{\top }{\mathbf  {x}}  
    {\frac  {\partial {\mathbf  {x}}^{\top }{\mathbf  {A}}{\mathbf  {x}}}{\partial {\mathbf  {x}}}}=  2{\mathbf  {A}}{\mathbf  {x}} \\\\\ \text{Symmetric } A\text{}$$

   The Product Rule  
   $${\displaystyle {\begin{aligned}\nabla \mathbf {A} \cdot \mathbf {B} &=\mathbf {A} \cdot \nabla \mathbf {B} +\mathbf {B} \cdot \nabla \mathbf {A} +\mathbf {A} \times \nabla \times \mathbf {B} +\mathbf {B} \times \nabla \times \mathbf {A} &=\mathbf {J} {\mathbf {A} }^{\mathrm {T} }\mathbf {B} +\mathbf {J}{\mathbf {B} }^{\mathrm {T} }\mathbf {A} &=\nabla \mathbf {A} \cdot \mathbf {B} +\nabla \mathbf {B} \cdot \mathbf {A} \ \end{aligned}}} 
    \implies  
    \nabla fg = f'^T g + g'^T f$$ 
   Thus we set our function $$hx = \langle fx gx \rangle = fx^T gx$$; then  
   $$\nabla hx = f'x^T gx + g'x^T fx.$$







Challenges in Machine Learning

1. The Curse of Dimensionality
    It is a phenomena where many machine learning problems become exceedingly difficult when the number of dimensions in the data is high.  

     The number of possible distinct configurations of a set of variables increases exponentially as the number of variables increases  
        
  
         Statistical Challenge the number of possible configurations of $$x$$ is much larger than the number of training examples  

2. Local Constancy and Smoothness Regularization
    Prior believes about the particular data set/learning problem can be incorporated as  
     Beliefs about the distribution of parameters 
     Beliefs about the properties of the estimating function  
        expressed implicitly by choosing algorithms that are biased toward choosing some class of functions over another even though these biases may not be expressed or even be possible to express in terms of a probability distribution representing our degree of belief in various functions.  

    The Local Constancy Smoothness Prior states that the function we learn should not change very much within a small region.  
    Mathematically traditional ML methods are designed to encourage the learning process to learn a function $$f^\ast$$ that satisfies the condition  
    $$\\\\\\\$$ $$\\\\\\\$$ $$f^{}\boldsymbol{x} \approx f^{}\boldsymbol{x}+\epsilon$$  
    for most configurations $$x$$ and small change $$\epsilon$$.  
     
  


    A Local Kernel/concepts can be thought of as a similarity function that performs template matching by measuring how closely a test example $$x$$ resembles each training example $$x^{i}$$.  
    Much of the modern motivation for Deep Learning is derived from studying the limitations of local template matching and how deep models are able to succeed in cases where local template matching fails Bengio et al. 2006b.  

    
    Decision trees also suffer from the limitations of exclusively smoothness based learning because they break the input space into as many regions as there are leaves and use a separate parameter or sometimes many parameters for extensions of decision trees in each region. If the target function requires a tree with at least $$n$$ leaves to be represented accurately then at least $$n$$ training examples are required to fit the tree. A multiple of $$n$$ is needed to achieve some level of statistical confidence in the predicted output.


    In general to distinguish $$\mathcal{O}k$$ regions in input space all these methods require $$\mathcal{O}k$$ examples. Typically there are $$\mathcal{O}k$$ parameters with $$\mathcal{O}1$$ parameters associated with each of the $$\mathcal{O}k$$ regions.  

    
    Key Takeaways   
     Is there a way to represent a complex function that has many more regions to be distinguished than the number of training examples?  
        Clearly assuming only smoothness of the underlying function will not allow a learner to do that.  
        The smoothness assumption and the associated nonparametric learning algorithms work extremely well as long as there are enough examples for the learning algorithm to observe high points on most peaks and low points on most valleys of the true underlying function to be learned.  
     Is it possible to represent a complicated function efficiently? and if it is complicated Is it possible for the estimated function to generalize well to new inputs?  
        Yes.  
        The key insight is that a very large number of regions such as $$\mathcal{O}2^k$$ can be defined with $$\mathcal{O}k$$ examples so long as we introduce some dependencies between the regions through additional assumptions about the underlying data generating distribution.
        In this way we can actually generalize non locally Bengio and Monperrus 2005; Bengio et al. 2006c.  
     Deep Learning VS Machine Learning  
        The core idea in deep learning is that we assume that the data was generated by the composition of factors or features potentially at multiple levels in a hierarchy.  
        These apparently mild assumptions allow an exponential gain in the relationship between the number of examples and the number of regions that can be distinguished.  
        The exponential advantages conferred by the use of deep distributed representations counter the exponential challenges posed by the curse of dimensionality.  
        Further Reading on the exponential gain Sections 6.4.1 15.4 and 15.5.  
    

3. Manifold Learning
    A Manifold  a connected region  is a set of points associated with a neighborhood around each point. From any given point the manifold locally appears to be a Euclidean space.  

    Manifolds in ML  
    In ML the term is used loosely to designate a connected set of points that can be approximated well by considering only a small number of degrees of freedom or dimensions embedded in a higher dimensional space. Each dimension corresponds to a local direction of variation.  
    In the context of machine learning we allow the dimensionality of the manifold to vary from one point to another. This often happens when a manifold intersects itself. For example a figure eight is a manifold that has a single dimension in most places but two dimensions at the intersection at the center.  
    Manifold Assumptions  
    
    Many machine learning problems seem hopeless if we expect the machine learning algorithm to learn functions with interesting variations across all of $$\mathbb{R}^n$$. Manifold learning algorithms surmount this obstacle by assuming that most of $$\mathbb{R}^n$$ consists of invalid inputs and that interesting inputs occur only a long a collection of manifolds containing a small subset of points with interesting variations in the output of the learned function occurring only along directions that lie on the manifold or with interesting variations happening only when we move from one manifold to another. Manifold learning was introduced in the case of continuous valued data and in the unsupervised learning setting although this probability concentration idea can be generalized to both discrete data and the supervised learning setting the key assumption remains that probability mass is highly concentrated.
  
    We assume that the data lies along a low dimensional manifold  
     May not always be correct or useful  
     In the context of AI tasks e.g. processing images sounds or text At least approximately correct.  
        To show that is true we need to argue two points  
         The probability distribution over images text strings and sounds that occur in real life is highly concentrated.  
            
   
            Uniform noise essentially never resembles structured inputs from these domains.   
         We must also establish that the examples we encounter are connected to each other by other examples with each example surrounded by other highly similar examples that can be reached by applying transformations to traverse the manifold  
            Informally we can imagine such neighborhoods and transformations  
            In the case of images we can think of many possible transformations that allow us to trace out a manifold in image space we can gradually dim or brighten the lights gradually move or rotate objects in the image gradually alter the colors on the surfaces of objects and so forth.  
            Multiple manifolds are likely involved in most applications. For examplethe manifold of human face images may not be connected to the manifold of cat face images.  
            Rigorous Results Cayton 2005; Narayanan and Mitter2010; Schölkopf et al. 1998; Roweis and Saul 2000; Tenenbaum et al. 2000; Brand2003; Belkin and Niyogi 2003; Donoho and Grimes 2003; Weinberger and Saul2004  

    Benefits  
    When the data lies on a low dimensional manifold it can be most natural for machine learning algorithms to represent the data in terms of coordinates on the manifold rather than in terms of coordinates in $$\mathbb{R}^n$$.  
    E.g. In everyday life we can think of roads as 1 D manifolds embedded in 3 D space. We give directions to specific addresses in terms of address numbers along these 1 D roads not in terms of coordinates in 3 D space.  
    Learning Manifold Structure figure 20.6  




 The Theory of Learning







The Learning Problem

1. When to use ML
    When  
    1. A pattern Exists
    2. We cannot pin the pattern down mathematically 
    3. We have Data  

    We usually can do without the  two. But the  condition we CANNOT do without.  
    The Theory of Learning only depends on the data.  

    "We have to have data. We are learning from data. So if someone knocks on my door with an interesting machine learning application and they tell me how exciting it is and how great the application would be and how much money they would make the  question I ask 'what data do you have?'. If you have data we are in business. If you don't you are out of luck."  Prof. Ng


2. The ML Approach to Problem Solving
    Consider the Netflix problem Predicting how a viewer will rate a movie.   
     Direct Approach  
         Ask each user to give a rank/rate for the different "factors/features" E.g. Action Comedy etc.  
         Watch each movie and assign a rank/rate for the same factors  
         Match the factors and produce a rating  
     ML Approach  
        Essentially it is a Reversed approach  
         Start with the Ratings dataset that the users assigned to each movie  
         Then deduce the "factors/features" that are consistent with those Ratings  
            Note we usually start with random initial numbers for the factors  
    

3. Components of Learning
     Input $$\vec{x}$$  
     Output $$y$$ 
     Data  $${\vec{x} 1 y 1 \vec{x} 2 y 2 ... \vec{x} N y N}$$ 
     Target Function $$f  \mathcal{X} \rightarrow \mathcal{Y}$$  Unknown/Unobserved  
     Hypothesis $$g  \mathcal{X} \rightarrow \mathcal{Y}$$  
    

5. Components of the Solution
     The Learning Model  
         The Hypothesis Set  $$\mathcal{H}=\{h\}  g \in \mathcal{H}$$  
            E.g. Perceptron SVM FNNs etc.  
         The Learning Algorithm picks $$g \approx f$$ from a hypothesis set $$\mathcal{H}$$  
            E.g. Backprop Quadratic Programming etc.  

    Motivating the inclusion of a Hypothesis Set  
     No Downsides There is no loss of generality by including a hypothesis set since any restrictions on the elements of the set have no effect on what the learning algorithms  
        Basically there is no downside because from a practical POV thats what you do; by choosing an initial approach e.g. SVM Linear Regression Neural Network etc. we are already dictating a hypothesis set. If we don't choose one then the hypothesis set has no restrictions and is the set of all possible hypothesis without loss of generalization.  
     Upside The hypothesis set plays a pivotal role in the theory of learning by dictating whether we can learn or not.  
    

6. The Basic Premise/Goal of Learning
    "Using a set of observations to uncover an underlying process"  
    Rephrased mathematically the Goal of Learning is   
    Use the Data to find a hypothesis $$g \in \mathcal{H}$$ from the hypothesis set $$\mathcal{H}=\{h\}$$ that approximates $$f$$ well.  
    

7. Types of Learning
     Supervised Learning the task of learning a function that maps an input to an output based on example input output pairs.  
     Unsupervised Learning the task of making inferences by learning a better representation from some datapoints that do not have any labels associated with them.  

     Reinforcement Leaning the task of learning how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward.  
    

8. The Learning Diagram


The Feasibility of Learning

The Goal of this Section is to answer the question Can we make any statements/inferences outside of the sample data that we have?  

1. The Problem of Learning
    Learning a truly Unknown function is Impossible since outside of the observed values the function could assume any value it wants.  

2. The Bin Analogy  A Related Experiment

    $$\mu$$ is a constant that describes the actual/real probability of picking the red marble.  
    $$\nu$$ however is random and depends on the frequency of red marbles in the particular sample that you have collected.    

    Does $$\nu$$ approximate $$\mu$$?  
     The short answer is NO  
        The Sample an be mostly green while bin is mostly red.  
     The Long answer is YES  
        The Sample frequency $$\nu$$ is likely/probably close to bin frequency $$\mu$$.  
        Think of a presidential poll of 3000 people that can predict how the larger $$10^8$$ mil. people will vote  

    The Main distinction between the two answers is in the difference between Possible VS Probable. 

    What does $$\nu$$ say about $$\mu$$?  
    In a big sample Large $$N$$ $$\nu$$ is probably close to $$\mu$$ within $$\epsilon$$.   
    Formally we the Hoeffding's Inequality  
    $$\mathbb{P}|\nu \mu|>\epsilon \leq 2 e^{ 2 \epsilon^{2} N}$$  
    In other words the probability that $$\nu$$ does not approximate $$\mu$$ well they are not within an $$\epsilon$$ of each other is bounded by a negative exponential that dampens fast but depends directly on the tolerance $$\epsilon$$.  
    This reduces to the statement that "$$\mu = \nu$$" is PAC PAC Probably Approximately Correct.    

    Properties  
     It is valid for $$N$$ and $$\epsilon$$. 
     The bound does not depend on the value of $$\mu$$.  
     There is a Trade off between the number of samples $$N$$ and the tolerance $$\epsilon$$.  
     Saying that $$\nu \approx \mu \implies \mu \approx \nu$$ i.e. saying $$\nu$$ is approximately the same as $$\mu$$ implies that $$\mu$$ is approximately the same as $$\nu$$ yes tautology.   
        The logic here is subtle  
         Logically the inequality is making a statement on $$\nu$$ the random variable it is saying that $$\nu$$ tends to be close to $$\mu$$ the constant real probability.  
         However since the inequality is symmetric we are using the inequality to infer $$\mu$$ from $$\nu$$.  
            But that is not the cause and effect that actually takes place. $$\mu$$ actually affects $$\nu$$.  

    Translating to the Learning Problem  
    Notice how the meaning of the accordance between $$\mu$$ and $$\nu$$  is not accuracy of the model but rather accuracy of the TEST.  

    Back to the Learning Diagram  
    The marbles in the bin correspond to the input space datapoints. This adds a NEW COMPONENT to the Learning problem  the probability of generating the input datapoints up to this point we treated learning in an absolute sense based on some fixed datapoints.   
    To adjust the statement of the learning problem to accommodate the new component  
    we add a probability distribution $$P$$  over the input space $$\mathcal{X}$$. This however doesn't restrict the argument at all; we can invoke any probability on the space and the machinery still holds. We also do not even need to know what $$P$$ is even though $$P$$ affects $$\mu$$ since Hoeffding's Inequality allows us to bound the LHS with no dependence on $$\mu$$.  
    Thus now we assume that the input datapoints $$\vec{x}1 ... \vec{x}N$$ are assumed to be generated by $$P$$ independently.  
    So this is a very benign addition that would give us high dividends  The Feasibility of Learning.    

    However this is not learning; it is Verification. Learning involves using an algorithm to search a space $$\mathcal{H}$$  and try different functions $$h \in \mathcal{H}$$. Here we have already picked some specific function and are testing its performance on a sample using maths to guarantee the accuracy of the test within some threshold we are willing to tolerate.  


    Extending Hoeffding's Inequality to Multiple hypotheses $$hi$$  

    Putting the right notation  

    
  
    i.e. the 10 heads are not a good indication of the real probability  
    
    
  
    Equivalently in learning if the hypothesis set size is 1000 and there are 10 points we test against the probability that one of those hypothesis performing well on the 10 points but actually being a bad hypothesis is high and increases with the hypothesis set size.  
    Hoeffding's inequality has a guarantee for one experiment that gets terribly diluted as you increase the number of experiments.  

    Solution  
    We follow the very same reasoning we want to know the probability of at least one failing. This can be bounded by the union bound which intuitively says that the maximum probability of at least an event occurring in N is when all the events are independent in which case you just sum up the probabilities  
    $$\begin{aligned} \mathbb{P}\left | E{\text {in }}g E{\text {out }}g |>\epsilon\right \leq \mathbb{P} & | E{\text {in }}\lefth{1}\right E{\text {out }}\lefth{1}\right |>\epsilon  & \text {or } | E{\text {in }}\lefth{2}\right E{\text {out }}\lefth{2}\right |>\epsilon  & \cdots  & \text {or } | E{\text {in }}\lefth{M}\right E{\text {out }}\lefth{M}\right |>\epsilon   \leq & \sum{m=1}^{M} \mathbb{P}\left | E{\text {in }}\lefth{m}\right E{\text {out }}\lefth{m}\right |>\epsilon\right \end{aligned}$$  
    Which implies  
    $$\begin{aligned} \mathbb{P}\left | E{\text {in }}g E{\text {out }}g |>\epsilon\right & \leq \sum{m=1}^{M} \mathbb{P}\left | E{\text {in }}\lefth{m}\right E{\text {out }}\lefth{m}\right |>\epsilon\right  & \leq \sum{m=1}^{M} 2 e^{ 2 \epsilon^{2} N} \end{aligned}$$   
    Or  
    $$\mathbb{P}\left | E{\ln }g E{\text {out }}g |>\epsilon\right \leq 2 M e^{ 2 \epsilon^{2} N}$$  
    The more sophisticated the model you use the looser that in sample will track the out of sample. Because the probability of them deviating becomes bigger and bigger and bigger.  
    The conclusion may seem both awkward and obvious but the bigger the hypothesis set the higher the probability of at least one function being very bad. In the event that we have an infinite hypothesis set of course this bound goes to infinity and tells us nothing new.  




3. The Learning Analogy
    For an exam the practice problems are the training set. You're going to look at the question. You're going to answer. You're going to compare it with the real answer. And then you are going to adjust your hypothesis your understanding of the material in order to do it better and go through them and perhaps go through them again until you get them right or mostly right or figure out the material this makes you better at taking the exam.
    We don't give out the actual exams questions because acing the final is NOT the goal the goal is to learn the material have a small $$E{\text{out}}$$. The final exam is only a way of gauging how well you actually learned. And in order for it to gauge how well you actually learned I have to give you the final at the point you have already fixed your hypothesis. You prepared. You studied. You discussed with people. You now sit down to take the final exam. So you have one hypothesis. And you go through the exam. hopefully will reflect what your understanding will be outside.  
    The exam measures $$E{\text{in}}$$ and we know that it tracks $$E{\text{out}}$$ by Hoeffding so it tracks well how you understand the material proper.  



8. Notes
     Learning Feasibility  
        When learning we only deal with In Sample Errors $$E{\text{in}}\mathbf{w}$$; we never handle the out sample error explicitly; we take the theoretical guarantee that when you do well in sample $$\implies$$ you do well out sample Generalization.  


Error and Noise

The Current Learning Diagram  

1. Error Measures
    Error Measures aim to answer the question  
    "What does it mean for $$h$$ to approximate $$f$$ $$h \approx f$$?"  
    The Error Measure $$Eh f$$  
    It is almost always defined point wise $$\mathrm{e}h\mathbf{X} f\mathbf{X}$$.  
    Examples  
     Square Error  $$\\\\mathrm{e}h\mathbf{x} f\mathbf{x}=h\mathbf{x} f\mathbf{x}^{2}$$  
     Binary Error  $$\\\\mathrm{e}h\mathbf{x} f\mathbf{x}=h\mathbf{x} \neq f\mathbf{x}$$  1 if true else 0  

    The overall error $$Ehf = $$ average of pointwise errors $$\mathrm{e}h\mathbf{x} f\mathbf{x}$$  
     In Sample Error  
        $$E{\mathrm{in}}h=\frac{1}{N} \sum{n=1}^{N} \mathrm{e}\lefth\left\mathbf{x}{n}\right f\left\mathbf{x}{n}\right\right$$    
     Out Sample Error  
        $$E{\text {out }}h=\mathbb{E} {\mathbf{x}}\mathrm{e}h\mathbf{x} f\mathbf{x}$$  

2. The Learning Diagram  with pointwise error
    There are two additions to the diagram  
     The  is to realize that we are defining the error measure on a point. 
     Another is that in deciding whether $$g$$  is close to $$f$$  which is the goal of learning we test this with a point $$x$$. And the criterion for deciding whether $$gx$$ is approximately the same as $$fx$$ is our pointwise error measure.  

3. Defining the Error Measure

    Types  
     False Positive
     False Negative  

    There is no inherent merit to choosing one error function over another. It's not an analytic question. It's an application domain question.  
    
  
    
  

    The error measure should be specified by the user. Since that's not always possible the alternatives  
     Plausible Measures measures that have an analytic argument for their merit based on certain assumptions.  
        E.g. Squared Error comes from the Gaussian Noise Assumption.  
     Friendly Measures An easy to use error measure without much justification.  
        E.g. Linear Regression error leads to the easy closed form solution Convex Error measures are easy to optimize etc.  
                  

4. The Learning Diagram  with the Error Measure
    The Error Measure provides a quantitative assessment of the statement $$g\mathbf{x} \approx f\mathbf{x}$$.  


5. Noisy Targets
    The 'Target Function' is not always a function because two 'identical' input points can be mapped to two different outputs i.e. they have different labels.  

    The solution Replacing the target function with a Target Distribution.  
    Instead of $$y = fx$$ we use the  conditional target distribution $$Py | \mathbf{x}$$. What changes now  is that instead of $$y$$  being deterministic of $$\mathbf{x}$$ once you generate $$\mathbf{x}$$ $$y$$  is also probabilistic  generated by $$Py | \mathbf{x}$$.  
    $$\mathbf{x} y$$ is now generated by the joint distribution  
    $$P\mathbf{x} Py | \mathbf{x}$$  

    Equivalently we can define a Noisy Target as a deterministic target function $$\f\mathbf{x}=\mathbb{E}y | \mathbf{x}\$$  PLUS Noise $$\ y fx$$.     
    This can be done WLOG since a deterministic target is a special kind of a noisy target  
    Define $$Py \vert \mathbf{x}$$ to be identically Zero except for $$y = fx$$.   


6. The Learning Diagram  with the Noisy Target

    Now $$E{\text {out}}h = \mathbb{E} {x y}ehx y$$ instead of $$\mathbb{E} {\mathbf{x}}\mathrm{e}h\mathbf{x} f\mathbf{x}$$ and  
    $$\left\mathbf{x}{1} y{1}\right \cdots\left\mathbf{x}{N} y{N}\right$$ are generated independently of each each tuple.  


7. Distinction between $$Py | \mathbf{x}$$ and $$P\mathbf{x}$$
    The Target Distribution $$Py \vert \mathbf{x}$$ is what we are trying to learn.  
    The Input Distribution $$P\mathbf{x}$$ only quantifies relative importance  of $$\mathbf{x}$$; we are NOT trying to learn this distribution.  
    Rephrasing Supervised learning only learns $$Py \vert \mathbf{x}$$ and not $$P\mathbf{x}$$; $$P\mathbf{x} y$$ is NOT a target distribution for Supervised Learning.  

    Merging $$P\mathbf{x}Py \vert \mathbf{x}$$ as $$P\mathbf{x} y$$ although allows us to generate examples $$\mathbf{x} y$$ mixes the two concepts that are inherently different.  


8. Preamble to Learning Theory
    Generalization VS Learning  
    We know that Learning is Feasible.
     Generalization  
        It is likely that the following condition holds  
        $$\ E{\text {out }}g \approx E{\text {in }}g  \tag{3.1}$$  
        This is equivalent to "good" Generalization.  
     Learning  
        Learning corresponds to the condition that $$g \approx f$$ which in turn corresponds to the condition  
        $$E{\text {out }}g \approx 0  \tag{3.2}$$      


    How to achieve Learning    
    We achieve $$E{\text {out }}g \approx 0$$ through  
    
    1. $$E{\mathrm{out}}g \approx E{\mathrm{in}}g$$  
        A theoretical result achieved through Hoeffding PROBABILITY THEORY
    2. $$E{\mathrm{in}}g \approx 0$$  
        A Practical result of minimizing the In Sample Error Function ERM Optimization

    Learning is thus reduced to the 2 following questions  
    
    1. Can we make sure that $$E{\text {out }}g$$ is close enough to $$E{\text {in }}g$$? theoretical  
    2. Can we make $$E{\text {in}}g$$ small enough? practical  


    What the Learning Theory will achieve  
    
     Characterizing the feasibility of learning for infinite $$M$$ hypothesis.  
        We are going to measure the model not by the number of hypotheses but by a single parameter which tells us the sophistication of the model. And that sophistication will reflect the out of sample performance as it relates to the in sample performance through the Hoeffding then VC inequalities.   
     Characterizing the tradeoff  
        In words  
        We realized that we would like our model the hypothesis set to be elaborate in order to be able to fit the data. The more parameters you have the more likely you are going to fit the data and get here. So the $$E{\text{in}}$$ goes down if you use more complex models. However if you make the model more complex the discrepancy between $$E{\text{out}}$$ and $$E{\text{in}}$$ gets worse and worse. $$E{\text{in}}$$ tracks $$E{\text{out}}$$ much more loosely than it used to.  




Linear Models I








        


     In a linear model if the errors belong to a normal distribution the least squares estimators are also the maximum likelihood estimators.^1  
            


    




The Linear Model II

1. Linear Models  Logistic Regression
    The Logistic Regression applies a non linear transform  on the signal; it's a softer approximation to the hard threshold non linearity applied by Linear Classification.  


2. The Logistic Function $$\theta$$
    $$\thetas=\frac{e^{s}}{1+e^{s}}=\frac{1}{1+e^{ s}}$$  

     Soft Threshold corresponds to uncertainty; interpreted as probabilities.  
     Sigmoid looks like a flattened out 'S'.  
        

3. The Probability Interpretation
    $$h\mathbf{x}=\thetas$$ is interpreted as a probability.  
    It is in fact a Genuine Probability. The output of logistic regression is treated genuinely as a probability even during learning.  
    Justification  
    Data $$\mathbf{x} y$$ with binary $$y$$ we don't have direct access to probability but the binary $$y$$ is affected by the probability generated by a noisy target  
    $$Py | \mathbf{x}=\left\{\begin{array}{ll}{f\mathbf{x}} & {\text {for } y=+1}  {1 f\mathbf{x}} & {\text {for } y= 1}\end{array}\right.$$  
    The target $$f  \mathbb{R}^{d} \rightarrow01$$ is the probability.  
    We learn $$\\\\ g\mathbf{x}=\theta\left\mathbf{w}^{\top} \mathbf{x}\right \approx f\mathbf{x}$$.   
    In words So I'm going to call the probability the target function itself. The probability that someone gets heart attack is $$f\mathbf{x}$$. And I'm trying to learn $$f$$ notwithstanding the fact that the examples that I am getting are giving me just sample values of $$y$$ that happen to be generated by $$f$$.  

    
     Logistic Regression uses the sigmoid function to "squash" the output feature/signal into the $$0 1$$ space.  
        Although one could interpret the sigmoid classifier as just a function with $$01$$ range it is actually a Genuine Probability.  
     To see this  
         A labeled classification Data Set does NOT explicitly give you the probability that something is going to happen rather just the fact that an event either happened $$y=1$$ or that it did not $$y=0$$ without the actual probability of that event happening.  
         One can think of this data as being generated by a the following noisy target  
            $${\displaystyle Py \vert x ={\begin{cases}fx&{\text{for }}y = +11 fx&{\text{for }}y= 1.\end{cases}}}$$   
         They have the form that a certain probability that the event occurred and a certain probability that the event did NOT occur given their input data.  
         This is generated by the target we want to learn; thus the function $$fx$$ is the target function to approximate.  
     In Logistic Regression we are trying to learn $$fx$$ not withstanding the fact that the data points we are learning from are giving us just sample values of $$y$$ that happen to be generated by $$f$$.  
     Thus the Target $$f  \mathbb{R}^d \longrightarrow 01$$ is the probability.  
        The output of Logistic Regression is treated genuinely as a probability even during Learning.   


4. Deriving the Error Measure Cross Entropy from Likelihood
    The error measure for logistic regression is based on likelihood  it is both plausible and friendly/well behaved? for optimization.  
    For each $$\mathbf{x} y$$ $$y$$ is generated wit probability $$f\mathbf{x}$$.  

    Likelihood We are maximizing the likelihood of this hypothesis under the data set that we were given with respect to the weights. I.E. Given the data set how likely is this hypothesis? Which means what is the probability of that data set under the assumption that this hypothesis is indeed the target?  

     Deriving the Likelihood   
        1. We start with  
            $$Py | \mathbf{x}=\left\{\begin{array}{ll}{h\mathbf{x}} & {\text {for } y=+1}  {1 h\mathbf{x}} & {\text {for } y= 1}\end{array}\right.$$  
        2. Substitute $$h\mathbf{x}=\theta \left\mathbf{w}^{\top} \mathbf{x}\right$$  
            $$Py | \mathbf{x}=\left\{\begin{array}{ll}{\theta\mathbf{w}^T\mathbf{x}} & {\text {for } y=+1}  {1 \theta\mathbf{w}^T\mathbf{x}} & {\text {for } y= 1}\end{array}\right.$$  
        3. Since we know that $$\theta s=1 \thetas$$ we can simplify the piece wise function  
            $$Py | \mathbf{x}=\theta\lefty \mathbf{w}^{\top} \mathbf{x}\right$$  
        4. To get the likelihood of the dataset $$\mathcal{D}=\left\mathbf{x}{1} y{1}\right \ldots\left\mathbf{x}{N} y{N}\right$$  
            $$\prod{n=1}^{N} P\lefty{n} | \mathbf{x}{n}\right =\prod{n=1}^{N} \theta\lefty{n} \mathbf{w}^{\mathrm{T}} \mathbf{x} {n}\right$$  

     Maximizing the Likelihood Deriving the Cross Entropy Error  
        1. Maximize  
            $$\prod{n=1}^{N} \theta\lefty{n} \mathbf{w}^{\top} \mathbf{x} {n}\right$$  
        2. Take the natural log to avoid products  
            $$\ln \left\prod{n=1}^{N} \theta\lefty{n} \mathbf{w}^{\top} \mathbf{x} {n}\right\right$$  
            Motivation  
             The inner quantity is non negative and non zero.  
             The natural log is monotonically increasing its max is the max of its argument  
        3. Take the average still monotonic  
            $$\frac{1}{N} \ln \left\prod{n=1}^{N} \theta\lefty{n} \mathbf{w}^{\top} \mathbf{x} {n}\right\right$$  
        4. Take the negative and Minimize  
            $$ \frac{1}{N} \ln \left\prod{n=1}^{N} \theta\lefty{n} \mathbf{w}^{\top} \mathbf{x} {n}\right\right$$  
        5. Simplify  
            $$=\frac{1}{N} \sum{n=1}^{N} \ln \left\frac{1}{\theta\lefty{n} \mathbf{w}^{\tau} \mathbf{x} {n}\right}\right$$  
        6. Substitute $$\left\thetas=\frac{1}{1+e^{ s}}\right$$  
            $$\frac{1}{N} \sum{n=1}^{N} \underbrace{\ln \left1+e^{ y{n} \mathbf{w}^{\top} \mathbf{x}{n}}\right}{e\lefth\left\mathbf{x}{n}\right y{n}\right}$$  
        7. Use this as the Cross Entropy  Error Measure  
            $$E{\mathrm{in}}\mathrm{w}=\frac{1}{N} \sum{n=1}^{N} \underbrace{\ln \left1+e^{ y{n} \mathrm{w}^{\top} \mathbf{x}{n}}\right}{\mathrm{e}\lefth\left\mathrm{x}{n}\right y{n}\right}$$  


6. The Decision Boundary of Logistic Regression
    Decision Boundary It is the set of $$x$$ such that  
    $$\frac{1}{1+e^{ \theta \cdot x}}=0.5 \implies 0= \theta \cdot x= \sum{i=0}^{n} \theta{i} x{i}$$  


7. The Logistic Regression Algorithm

8. Summary of Linear Models

9. Nonlinear Transforms
    $$\mathbf{x}=\leftx{0} x{1} \cdots x{d}\right \stackrel{\Phi}{\longrightarrow} \mathbf{z}=\leftz{0} z{1} \cdots \cdots \cdots \cdots \cdots z{\tilde{d}}\right$$  
    $$\text {Each } z{i}=\phi{i}\mathbf{x} \\\\\ \mathbf{z}=\Phi\mathbf{x}$$  
    Example $$\mathbf{z}=\left1 x{1} x{2} x{1} x{2} x{1}^{2} x{2}^{2}\right$$  
    The Final Hypothesis $$g\mathbf{x}$$ in $$\mathcal{X}$$ space  
     Classification $$\operatorname{sign}\left\tilde{\mathbf{w}}^{\top} \Phi\mathbf{x}\right$$ 
     Regression $$\tilde{\mathbf{w}}^{\top} \Phi\mathbf{x}$$  

    Two Non Separable Cases  
     Almost separable with some outliers  
        1. Accept that $$E{\mathrm{in}}>0$$; use a linear model in $$\mathcal{X}$$.    
        2. Insist on $$E{\mathrm{in}}=0$$; go to a high dimensional $$\mathcal{Z}$$.  
            This has a worse chance for generalizing. 
     Completely Non Linear  
        Data snooping example it is hard to choose the right transformations; biggest flop is to look at the data to choose the right transformations; it invalidates the VC inequality guarantee.  
        Think of the VC inequality as providing you with a warranty.   



The Bias Variance Decomposition







 Interpretable Machine Learning Models








 Estimation





Estimation


Maximum Likelihood Estimation MLE

1. Maximum Likelihood Estimation
    Likelihood in Parametric Models  
    
    Suppose we have a parametric model $$\{py ; \theta \vert \theta \in \Theta\}$$ and a sample $$D=\left\{y{1} \ldots y{n}\right\}$$  
     The likelihood of parameter estimate $$\hat{\theta} \in \Theta$$ for sample $$\mathcal{D}$$ is  
        $$p\mathcal{D} ; \hat{\theta}=\prod{i=1}^{n} p\lefty{i} ; \hat{\theta}\right$$  
     In practice we prefer to work with the log likelihood.  Same maximum but  
        $$\log p\mathcal{D} ; \hat{\theta}=\sum{i=1}^{n} \log p\lefty{i} ; \theta\right$$  
        and sums are easier to work with than products.  

    MLE for Parametric Models  
    
    The maximum likelihood estimator MLE for $$\theta$$ in the parametric model $$\{py \theta \vert \theta \in \Theta\}$$ is  
    $$\begin{aligned} \hat{\theta} &=\underset{\theta \in \Theta}{\arg \max } \log p\mathcal{D} \hat{\theta}  &=\underset{\theta \in \Theta}{\arg \max } \sum{i=1}^{n} \log p\lefty{i} ; \theta\right \end{aligned}$$  

    You are finding the value of the parameter $$\theta$$ that if used in the model to generate the probability of the data would make the data most "likely" to occur.  

     MLE Intuition  
        If I choose a hypothesis $$h$$ underwhich the observed data is very plausible then the hypothesis is very likely.  


          
     Finding the MLE is an optimization problem.
     For some model families calculus gives a closed form for the MLE
     Can also use numerical methods we know e.g. SGD  


    Notes  
    
     Why maximize the natural log of the likelihood?  
        
           
            1. Numerical Stability change products to sums  
            2. The logarithm of a member of the family of exponential probability distributions which includes the ubiquitous normal is polynomial in the parameters i.e. max likelihood reduces to least squares for normal distributions  
            $$\log\left\exp\left \frac{1}{2}x^2\right\right =  \frac{1}{2}x^2$$   
            3. The latter form is both more numerically stable and symbolically easier to differentiate than the former. It increases the dynamic range of the optimization algorithm allowing it to work with extremely large or small values in the same way.  
            4. The logarithm is a monotonic transformation that preserves the locations of the extrema in particular the estimated parameters in max likelihood are identical for the original and the log transformed formulation  

            
                Justification the gradient of the original term will include a $$e^{\vec{x}}$$ multiplicative term that scales very quickly one way or another requiring the step size to equally scale/stretch in the opposite direction.  





Maximum A Posteriori MAP Estimation



 Learning for Machines Concepts










    

Learning

1. Learning
    Learning is the process of acquiring new or modifying existing knowledge behaviors skills values or preferences.  
    






1. Learning




Learning Concepts

1. Inductive Learning Bias
    





 Papers



1. 

GANs

0. Lists of Papers







1. Improving Training





2. GAN Architectures
       1. 


3. Improving GANs Theory



4. Theory
       

5. Code








RNNs


Maths


Statistics


Optimization


Machine Learning


Computer Vision


NLP
        Speech Recognition  
            
            1. Human parity in speech recognition xiong et al. 2016
            2. Deep Learning for Speech Deng et al.
        Timeline papers  
            1. Graves &Jaitley Tpwards End to End Speech Recog. with Neural Nets/
            2. Maas et al. Lexicon Free Conversational Speech Recog. with Neural Nets  
            3. Chan et al. LAS/


Physics


Medical NLP



Misc.


 Computational Learning Theory







1. Linear Models
































 Sampling and Monte Carlo Methods


Resources  













Sampling

1. Monte Carlo Sampling
    When a sum or an integral cannot be computed exactly we can approximate it using Monte Carlo sampling. 
    The idea is to view the sum or integral as if it were an expectation under some distribution and to approximate the expectation by a corresponding average   
      Sum  
    $$s=\sum{\boldsymbol{x}} p\boldsymbol{x} f\boldsymbol{x}=E{p}f\mathbf{x}$$  
      Integral  
    $$s=\int p\boldsymbol{x} f\boldsymbol{x} d \boldsymbol{x}=E{p}f\mathbf{x}$$  
    We can approximate $$s$$ by drawing $$n$$ samples $$\boldsymbol{x}^{1} \ldots \boldsymbol{x}^{n}$$ from $$p$$ and then forming the empirical average  
    $$\hat{s}{n}=\frac{1}{n} \sum{i=1}^{n} f\left\boldsymbol{x}^{i}\right$$

2. Importance Sampling
    There is no unique decomposition of the MC approximation because $$p\boldsymbol{x} f\boldsymbol{x}$$ can always be rewritten as  
    $$p\boldsymbol{x} f\boldsymbol{x}=q\boldsymbol{x} \frac{p\boldsymbol{x} f\boldsymbol{x}}{q\boldsymbol{x}} $$  
    where we now sample from $$q$$ and average $$\frac{p f}{q}$$.  

    Formally the expectation becomes  
    $$E{p}f\mathbf{x} = \sum{\boldsymbol{x}} p\boldsymbol{x} f\boldsymbol{x} = \sum{\boldsymbol{x}} q\boldsymbol{x} \dfrac{p\boldsymbol{x}}{q\boldsymbol{x}} f\boldsymbol{x} = Eq\left\dfrac{p\boldsymbol{x}}{q\boldsymbol{x}} f\boldsymbol{x}\right$$  


    Biased Importance Sampling  
    Another approach is to use biased importance sampling which has the advantage of not requiring normalized $$p$$ or $$q$$. In the case of discrete variables the biased importance sampling estimator is given by  
    $$\begin{aligned} \hat{s}{B I S} &=\frac{\sum{i=1}^{n} \frac{p\left\boldsymbol{x}^{i}\right}{q\left\boldsymbol{x}^{i}\right} f\left\boldsymbol{x}^{i}\right}{\sum{i=1}^{n} \frac{p\left\boldsymbol{x}^{i}\right}{q\left\boldsymbol{x}^{i}\right}}  &=\frac{\sum{i=1}^{n} \frac{p\left\boldsymbol{x}^{i}\right}{\tilde{q}\left\boldsymbol{x}^{i}\right} f\left\boldsymbol{x}^{i}\right}{\sum{i=1}^{n} \frac{pi}{\tilde{q}i}}  &=\frac{\sum{i=1}^{n} \frac{\tilde{p}\left\boldsymbol{x}^{i}\right}{\tilde{q}\left\boldsymbol{x}^{i}\right} f\left\boldsymbol{x}^{i}\right}{\sum{i=1}^{n} \frac{\tilde{p}\left\boldsymbol{x}^{i}\right}{\tilde{q}\left\boldsymbol{x}^{i}\right}} \end{aligned}$$  
    where $$\tilde{p}$$ and $$\tilde{q}$$ are the unnormalized forms of $$p$$ and $$q$$ and the $$\boldsymbol{x}^{i}$$ are the samples from $$q$$.  
    Bias  
    This estimator is biased because $$\mathbb{E}\hat{s} {BIS} \neq s$$ except asymptotically when $$n \rightarrow \infty$$ and the denominator of the  equation above converges to $$1$$. Hence this estimator is called asymptotically unbiased.  


    Statistical Efficiency  
    Although a good choice of $$q$$ can greatly improve the efficiency of Monte Carlo estimation a poor choice of $$q$$ can make the efficiency much worse.  
      If there are samples of $$q$$ for which $$\frac{p\boldsymbol{x}|f\boldsymbol{x}|}{q\boldsymbol{x}}$$ is large then the variance of the estimator can get very large.  
    This may happen when $$q\boldsymbol{x}$$ is tiny while neither $$p\boldsymbol{x}$$ nor $$f\boldsymbol{x}$$ are small enough to cancel it.  
    The $$q$$ distribution is usually chosen to be a simple distribution so that it is easy to sample from. When $$\boldsymbol{x}$$ is high dimensional this simplicity in $$q$$ causes it to match $$p$$ or $$p\vert f\vert $$ poorly.  
    1 When $$q\left\boldsymbol{x}^{i}\right \gg p\left\boldsymbol{x}^{i}\right\left|f\left\boldsymbol{x}^{i}\right\right|$$ importance sampling collects useless samples summing tiny numbers or zeros.  
    2 On the other hand when $$q\left\boldsymbol{x}^{i}\right \ll p\left\boldsymbol{x}^{i}\right\left|f\left\boldsymbol{x}^{i}\right\right|$$ which will happen more rarely the ratio can be huge.  
    Because these latter events are rare they may not show up in a typical sample yielding typical underestimation of $$s$$ compensated rarely by gross overestimation.  
    Such very large or very small numbers are typical when $$\boldsymbol{x}$$ is high dimensional because in high dimension the dynamic range of joint probabilities can be very large.  

     A good IS sampling distribution $$q$$ is a low variance distribution.{ .borderexample}  


    Applications  
    In spite of this danger importance sampling and its variants have been found very useful in many machine learning algorithms including deep learning algorithms. They have been used to  
    
     Accelerate training in neural language models with a large vocabulary  
     Accelerate other neural nets with a large number of outputs  
     Estimate a partition function the normalization constant of a probability distribution
     Estimate the log likelihood in deep directed models e.g. Variational Autoencoders  
     Improve the estimate of the gradient of the cost function used to train model parameters with stochastic gradient descent  
        Particularly for models such as classifiers in which most of the total value of the cost function comes from a small number of misclassified examples.  
        Sampling more difficult examples more frequently can reduce the variance of the gradient in such cases Hinton 2006.  


    Approximating Distributions  
    To approximate the expectation mean of a distribution $$p$$  
    $${\mathbb{E}} {p}x=\sum{x} x px$$  
    by sampling from a distribution $$q$$.  
    Notice that  
    1 $${\displaystyle {\mathbb{E}} {p}x=\sum{x} x px = \sum{x} x\frac{px}{qx} qx}$$  
    2 $${\displaystyle \sum{x} x\frac{px}{qx} qx=\mathbb{E} {q}\leftx \frac{px}{qx}\right}$$  
    We approximate the expectation over $$q$$ in 2 with the empirical distribution  
    $$\mathbb{E} {q}\leftx \frac{px}{qx}\right \approx \dfrac{1}{n} \sum{i=1}^n xi \dfrac{pxi}{qxi}$$  

    Approximating UnNormalized Distributions  Biased Importance Sampling  
    Let $$px=\frac{hx}{Z}$$ then  
    $$\begin{aligned}\mathbb{E}{p}x &= \sum{x} x \frac{hx}{Z}  &= \sum{x} x \frac{hx}{Z qx} qx  &\approx \frac{1}{Z} \frac{1}{n} \sum{i=1}^{n} x{i} \frac{h\leftx{i}\right}{q\leftx{i}\right} \end{aligned}$$  
    where the samples $$xi$$ are drawn from $$q$$.  
    To get rid of the $$\dfrac{1}{Z}$$ factor  
       we define the importance sample weight  
    $$wi = \frac{h\leftx{i}\right}{q\leftx{i}\right}$$  
      then the sample mean weight  
    $$\bar{w} = \dfrac{1}{n} \sum{i=1}^n wi = $$  
      Now we decompose $$Z$$ by noticing that  
    $$\mathbb{E} {p}1=1=\sum{x} \frac{hx}{Z}$$  
    $$\implies$$  
    $$Z = \sum{x} hx$$  
      we approximate the expectation again with IS  
    $$\begin{aligned} Z 
    &= \sum{x} hx  &= \sum{x} \frac{hx}{qx} qx  &\approx \dfrac{1}{n} \sum{i=1}^{n} \frac{h\leftx{i}\right}{q\leftx{i}\right}  &= \bar{w} \end{aligned}$$  
    Thus the sample normalizing constant $$\hat{Z}$$ is equal to the sample mean weight  
    $$Z = \bar{w}$$  

    Finally 
    $$\mathbb{E} {p}x \approx \frac{1}{\bar{w}} \frac{1}{n} \sum{i=1}^{n} x{i} wi  = \dfrac{\overline{xw}}{\bar{w}}$$  


    Curse of Dimensionality in IS  Variance of the Estimator  
    A big problem with Importance Sampling is that the variance of the IS estimator can be greatly sensitive to the choice of $$q$$.  
    The Variance is  
    $$\operatorname{Var}\left\hat{s} {q}\right=\operatorname{Var}\left\frac{p\mathbf{x} f\mathbf{x}}{q\mathbf{x}}\right / n$$  
    The Minimum Variance occurs when $$q$$ is  
    $$q^{ }\boldsymbol{x}=\frac{p\boldsymbol{x}|f\boldsymbol{x}|}{Z}$$  
    where $$Z$$ is the normalization constant chosen so that $$q^{ }\boldsymbol{x}$$ sums or integrates to $$1$$ as appropriate.  
      Any choice of sampling distribution $$q$$ is valid in the sense of yielding the correct expected value and 
      $$q^{  }$$ is the optimal one in the sense of yielding minimum variance.  
      Sampling from $$q^{  }$$ is usually infeasible but other choices of $$q$$ can be feasible while still reducing the variance somewhat.  


3. Markov Chain Monte Carlo MCMC Methods
    Motivation  
    In many cases we wish to use a Monte Carlo technique but there is no tractable method for drawing exact samples from the distribution $$p{\text {model}}\mathbf{x}$$ or from a good low variance importance sampling distribution $$q\mathbf{x}$$.  
    In the context of deep learning this most often happens when $$p{\text {model}}\mathbf{x}$$ is represented by an undirected model.  
    In these cases we introduce a mathematical tool called a Markov chain to approximately sample from $$p{\text {model}}\mathbf{x}$$. The family of algorithms that use Markov chains to perform Monte Carlo estimates is called Markov Chain Monte Carlo MCMC methods.  

    Idea of MCs  
      The core idea of a Markov chain is to have a state $$\boldsymbol{x}$$ that begins as an arbitrary value.  
      Over time we randomly update $$\boldsymbol{x}$$ repeatedly.  
      Eventually $$\boldsymbol{x}$$ becomes very nearly a fair sample from $$p\boldsymbol{x}$$.  

    Definition  
    Formally a Markov chain is defined by  
     A random state $$x$$ and  
     A transition distribution $$T\leftx^{\prime} \vert x\right$$  
        specifying the probability that a random update will go to state $$x^{\prime}$$ if it starts in state $$x$$.  
    Running the Markov chain means repeatedly updating the state $$x$$ to a value $$x^{\prime}$$ sampled from $$T\left\mathbf{x}^{\prime} \vert x\right$$.  


    Finite Countable States  
    We take the case where the random variable $$\mathbf{x}$$ has countably many states.  
    Representation  
    We represent the state as just a positive integer $$x$$.  
    Different integer values of $$x$$ map back to different states $$\boldsymbol{x}$$ in the original problem.  

    Consider what happens when we run infinitely many Markov chains in parallel.  
      All the states of the different Markov chains are drawn from some distribution $$q^{t}x$$ where $$t$$ indicates the number of time steps that have elapsed.  
      At the beginning $$q^{0}$$ is some distribution that we used to arbitrarily initialize $$x$$ for each Markov chain.  
      Later $$q^{t}$$ is influenced by all the Markov chain steps that have run so far.  
      Our goal is for $$q^{t}x$$ to converge to $$px$$.  

     Probability of transitioning to a new state  
        Let's update a single Markov chain's state $$x$$ to a new state $$x^{\prime}$$.  
        The probability of a single state landing in state $$x^{\prime}$$ is given by  
        $$q^{t+1}\leftx^{\prime}\right=\sum{x} q^{t}x T\leftx^{\prime} \vert x\right$$  
         Describing $$q$$  
            Because we have reparametrized the problem in terms of a positive integer $$x$$ we can describe the probability distribution $$q$$ using a vector $$\boldsymbol{v}$$ with  
            $$q\mathrm{x}=i=v{i}$$  
         The Transition Operator $$T$$ as a Matrix  
            Using our integer parametrization we can represent the effect of the transition operator $$T$$ using a matrix $$A$$.  
            We define $$A$$ so that  
            $$A{i j}=T\left\mathbf{x}^{\prime}=i \vert \mathbf{x}=j\right$$  

        Rather than writing it in terms of $$q$$ and $$T$$ to understand how a single state is updated we may now use $$v$$ and $$A$$ to describe how the entire distribution over all the different Markov chains running in parallel shifts as we apply an update.  
        Rewriting the probability of a single state landing in state $$x^{\prime} = i$$  
        $$\boldsymbol{v}^{t}=\boldsymbol{A} \boldsymbol{v}^{t 1}$$  
         Matrix Exponentiation  
            Applying the Markov chain update repeatedly corresponds to multiplying by the matrix $$A$$ repeatedly.  
            In other words we can think of the process as exponentiating the matrix $$\boldsymbol{A}$$.  

        Thus $$\boldsymbol{v}^{t}$$ can finally be rewritten as  
        $$\boldsymbol{v}^{t}=\boldsymbol{A}^{t} \boldsymbol{v}^{0}$$  
     Convergence  The Stationary Distribution  
        Let's  examine the matrix $$A$$.  
         Stochastic Matrices  
            Stochastic Matrices are ones where each of their columns represents a probability distribution.  
            The Matrix $$A$$ is a stochastic matrix.  
             Perron Frobenius Theorem  Largest Eigenvalue  
                If there is a nonzero probability of transitioning from any state $$x$$ to any other state $$x$$ for some power $$t$$ then the Perron Frobenius theorem guarantees that the largest eigenvalue is real and equal to $$1$$.  
             Unique Largest Eigenvalue  
                Under some additional mild conditions $$A$$ is guaranteed to have only one eigenvector with eigenvalue $$1$$.  
         Exponentiated Eigenvalues  
            Over time we can see that all the eigenvalues are exponentiated  
            $$\boldsymbol{v}^{t}=\left\boldsymbol{V} \operatorname{diag}\boldsymbol{\lambda} \boldsymbol{V}^{ 1}\right^{t} \boldsymbol{v}^{0}=\boldsymbol{V} \operatorname{diag}\boldsymbol{\lambda}^{t} \boldsymbol{V}^{ 1} \boldsymbol{v}^{0}$$  

            This process causes all the eigenvalues that are not equal to $$1$$ to decay to zero.  

        The process thus converges to a stationary distribution equilibrium distribution.  
         Convergence Condition  Eigenvector Equation  
            At convergence the following eigenvector equation holds  
            $$\boldsymbol{v}^{\prime}=\boldsymbol{A} \boldsymbol{v}=\boldsymbol{v}$$  
            and this same condition holds for every additional step.  
             Stationary Point Condition  
                Thus To be a stationary point $$\boldsymbol{v}$$ must be an eigenvector with corresponding eigenvalue $$1$$.  
                This condition guarantees that once we have reached the stationary distribution repeated applications of the transition sampling procedure do not change the distribution over the states of all the various Markov chains although the transition operator does change each individual state of course.  
         Convergence to $$p$$  
            If we have chosen $$T$$ correctly then the stationary distribution $$q$$ will be equal to the distribution $$p$$ we wish to sample from.  
            Gibbs Sampling is one way to choose $$T$$.  

    Continuous Variables  

    Convergence  
    In general a Markov chain with transition operator $T$ will converge under mild conditions to a fixed point described by the equation  
    $$q^{\prime}\left\mathbf{x}^{\prime}\right=\mathbb{E} {\mathbf{x} \sim q} T\left\mathbf{x}^{\prime} \vert \mathbf{x}\right$$  
    which is exactly what we had in the discrete case defined as a sum  
    $$q^{\prime}\leftx^{\prime}\right=\sum{x} q^{t}x T\leftx^{\prime} \vert x\right$$  
    and in the continuous case as an integral  
    $$q^{\prime}\leftx^{\prime}\right=\int{x} q^{\prime}x T\leftx^{\prime} \vert x\right$$  


    Using the Markov Chain  
    Regardless of whether the state is continuous or discrete all Markov chain methods consist of repeatedly applying stochastic updates until eventually the state begins to yield samples from the equilibrium distribution.  
     Training the Markov Chain  
    Running the Markov chain until it reaches its equilibrium distribution is called burning in the Markov chain.  

     Sampling from the Markov Chain  
    After the chain has reached equilibrium a sequence of infinitely many samples may be drawn from the equilibrium distribution.  
    There are difficulties/drawbacks with using Markov Chains for sampling  
    
     Representative Samples  Independence  
        The samples are identically distributed but any two successive samples will be highly correlated with each other.  
         Issue  
            A finite sequence of samples may thus not be very representative of the equilibrium distribution.   
         Solutions  
            1. One way to mitigate this problem is to return only every $$n$$ successive samples so that our estimate of the statistics of the equilibrium distribution is not as biased by the correlation between an MCMC sample and the next several samples.  
                Markov chains are thus expensive to use because of the time required to burn in to the equilibrium distribution and the time required to transition from one sample to another reasonably decorrelated sample after reaching equilibrium.   
            2. To get truly independent samples one can run multiple Markov chains in parallel.  
            This approach uses extra parallel computation to eliminate latency.  

          The strategy of using only a single Markov chain to generate all samples and the strategy of using one Markov chain for each desired sample are two extremes.  
          In deeplearning we usually use a number of chains that is similar to the number of examples in a minibatch and then draw as many samples as are needed from this fixed set of Markov chains.  
        A commonly used number of Markov chains is $$100$$.  
     Convergence to Equilibrium  Halting  
        The theory of Markov Chains allows us to guarantee convergence to equilibrium. However it does not specify anything about the convergence criterion  
        
         The theory does not allow us to know the Mixing Time in advance..   
            The Mixing Time is the number of steps the Markov chain must run before reaching its equilibrium distribution.  
         The theory also does not guide us on how to test/determine whether an MC has reached equilibrium..   

        Convergence Criterion Theoretical Analysis  
        If we analyze the Markov chain from the point of view of a matrix $$A$$ acting on a vector of probabilities $$v$$  then we know that the chain mixes when $$A^{t}$$ has effectively lost all the eigenvalues from $$A$$ besides the unique eigenvalue of 1.  
        This means that the magnitude of the  largest eigenvalue will determine the mixing time.  

        Convergence Criterion In Practice  
        In practice though we cannot actually represent our Markov chain in terms of a matrix.  
          The number of states that our probabilistic model can visit is exponentially large in the number of variables so it is infeasible to represent $$\boldsymbol{v}$$ $$A$$ or the eigenvalues of $$\boldsymbol{A}$$.  
        Because of these and other obstacles we usually do not know whether a Markov chain has mixed.  
        Instead we simply run the Markov chain for an amount of time that we roughly estimate to be sufficient and use heuristic methods to determine whether the chain has mixed.  
        These heuristic methods include manually inspecting samples or measuring correlations between successive samples.  
    
    
    This section described how to draw samples from a distribution $$qx$$ by repeatedly updating $$\boldsymbol{x} \leftarrow \boldsymbol{x}^{\prime} \sim T\left\boldsymbol{x}^{\prime} \vert \boldsymbol{x}\right$$. 
      


    Finding a useful $$qx$$  
    There are two basic approaches to ensure that $$qx$$ is a useful distribution  
    
    1 Derive $$T$$ from a given learned $$p{\text {model}}$$. E.g. Gibbs Sampling Metropolis Hastings etc.     
    2 Directly parameterize $$T$$ and learn it so that its stationary distribution implicitly defines the $$p{\text {model}}$$ of interest. E.g. Generative Stochastic Networks Diffusion Inversion Approximate Bayesian Computation.  
    

4. Gibbs Sampling
    Gibbs Sampling is an MCMC algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution when direct sampling is difficult.  

    It is a method for finding a useful distribution $$qx$$ by deriving $$T$$ from a given learned $$p{\text {model}}$$; in the case of sampling from EBMs.  

    It is a conceptually simple and effective approach to building a Markov Chain that samples from $$p{\text {model}}\boldsymbol{x}$$ in which sampling from $$T\left\mathbf{x}^{\prime} \vert \mathbf{x}\right$$ is accomplished by selecting one variable $$\mathbf{x} {i}$$ and sampling it from $$p{\text {model}}$$ conditioned on its neighbors in the undirected graph $$\mathcal{G}$$ defining the structure of the energy based model.  

    Block Gibbs Sampling  
    We can also sample several variables at the same time as long as they are conditionally independent given all their neighbors.  
    Block Gibbs Sampling is a Gibbs sampling approach that updates many variables simultaneously.  

    Application  RBMs  
    All the hidden units of an RBM may be sampled simultaneously because they are conditionally independent from each other given all the visible units.  
    Likewise all the visible units may be sampled simultaneously because they are conditionally independent from each other given all the hidden units.  


    In Deep Learning  
    In the context of the deep learning approach to undirected modeling it is rare to use any approach other than Gibbs sampling. Improved sampling techniques are one possible research frontier.  



    Summary  
    
     A method for sampling from probability distributions of $$\geq 2$$ dimensions.  
     It is an MCMC method; A dependent sampling algorithm.  
     It is a special case of the Metropolis Hastings Algorithm.  
         But accept all proposals i.e. no rejections.  
         It is slightly more efficient than MH because of no rejections.  
         It requires us to know the conditional probabilities $$pXi \vert X{0}^t \ldots X{i 1}^{t} X{i+1}^{t 1} \ldots X{n}^{t 1}$$ and be able to sample from them.  
         It is slow for correlated parameters; like MH.  
            Can be alleviated by doing block sampling blocks of correlated variables.  
            I.E. sample $$Xj Xk \sim pXj Xk \vert X{0}^t \ldots X{n}^{t 1}$$ at the same time.  
            It is more efficient than sampling from uni dimensional conditional distributions but generally harder.  
            

             Gibbs walks in a zig zag pattern.  
             MH walks in the diagonal direction but frequently goes off in the orthogonal direction which have to be rejected.  
             Hamiltonian MC best of both worlds walks in diagonal direction and accept a high proportion of steps.  
     Often used in Bayesian Inference.  
     Guaranteed to Asymptotically Converge to the true joint distribution.  
     It is an alternative to deterministic algorithms for inference like EM.  

    


5. The Challenge of Mixing between Separated Modes in MCMC Algorithms
    The primary difficulty involved with MCMC methods is that they have a tendency to mix poorly.  

    Slow Mixing/Failure to Mix  
    Ideally successive samples from a Markov chain designed to sample from $$p\boldsymbol{x}$$ would be completely independent from each other and would visit many different regions in $$\boldsymbol{x}$$ space proportional to their probability.  
    Instead especially in high dimensional cases MCMC samples become very correlated. We refer to such behavior as slow mixing or even failure to mix.  

    Intuition  Noisy Gradient Descent  
    MCMC methods with slow mixing can be seen as inadvertently performing something resembling noisy gradient descent on the energy function or equivalently noisy hill climbing on the probability with respect to the state of the chain the random variables being sampled.  
      The chain tends to take small steps in the space of the state of the Markov chain from a configuration $$\boldsymbol{x}^{t 1}$$ to a configuration $$\boldsymbol{x}^{t}$$ with the energy $$E\left\boldsymbol{x}^{t}\right$$ generally lower or approximately equal to the energy $$E\left\boldsymbol{x}^{t 1}\right$$ with a preference for moves that yield lower energy configurations.  
      When starting from a rather improbable configuration higher energy than the typical ones from $$p\mathbf{x}$$ the chain tends to gradually reduce the energy of the state and only occasionally move to another mode.  
      Once the chain has found a region of low energy for example if the variables are pixels in an image a region of low energy might be a connected manifold of images of the same object which we call a mode the chain will tend to walk around that mode following a kind of random walk.  
      Once in a while it will step out of that mode and generally return to it or if it finds an escape route move toward another mode.  
      The problem is that successful escape routes are rare for many interesting distributions so the Markov chain will continue to sample the same mode longer than it should.   

    In Gibbs Sampling  
    The problem is very clear when we consider the Gibbs Sampling algorithm.  
    The probability of going from one mode to a nearby mode within a given number of steps is determined by the shape of the “energy barrier” between these modes.  
      Transitions between two modes that are separated by a high energy barrier a region of low probability are exponentially less likely in terms of the height of the energy barrier.  
    

    The problem arises when there are multiple modes with high probability that are separated by regions of low probability especially when each Gibbs sampling step must update only a small subset of variables whose values are largely determined by the other variables.  

    Example and Analysis  
    


    Possible Solution  Block Gibbs Sampling  
    Sometimes this problem can be resolved by finding groups of highly dependent units and updating all of them simultaneously in a block. Unfortunately when the dependencies are complicated it can be computationally intractable to draw a sample from the group. After all the problem that the Markov chain was originally introduced to solve is this problem of sampling from a large group of variables.  


    In Generative Latent Variable Models  
    In the context of models with latent variables which define a joint distribution $$p{\text {model}}\boldsymbol{x} \boldsymbol{h}$$ we often draw samples of $$\boldsymbol{x}$$ by alternating between sampling from $$p{\text {model}}\boldsymbol{x} \vert \boldsymbol{h}$$ and sampling from $$p{\text {model}}\boldsymbol{h} \vert \boldsymbol{x}$$.  

    Learning Mixing Tradeoff  
      From the pov of mixing rapidly we would like $$p{\text {model}}\boldsymbol{h} \vert \boldsymbol{x}$$ to have high entropy.  
      From the pov of learning a useful representation of $$\boldsymbol{h}$$ we would like $$\boldsymbol{h}$$ to encode enough information about $$\boldsymbol{x}$$ to reconstruct it well which implies that $$\boldsymbol{h}$$ and $$\boldsymbol{x}$$ and $$\boldsymbol{x}$$ should have high mutual information.  
    These two goals are at odds with each other. We often learn generative models that very precisely encode $$\boldsymbol{x}$$ into $$\boldsymbol{h}$$ but are not able to mix very well.  

    In Boltzmann Machines  
    This situation arises frequently with Boltzmann machines the sharper the distribution a Boltzmann machine learns the harder it is for a Markov chain sampling from the model distribution to mix well.  
    



    Summary  Takeaways  
    All this could make MCMC methods less useful when the distribution of interest has a manifold structure with a separate manifold for each class the distribution is concentrated around many modes and these modes are separated by vast regions of high energy.  
    This type of distribution is what we expect in many classification problems and it would make MCMC methods converge very slowly because of poor mixing between modes.  
    


6. Solutions for the Slow Mixing Problem
    Since it is difficult to mix between the different modes of a distribution when the distribution has sharp peaks of high probability surrounded by regions of low probability  
    Several techniques for faster mixing are based on constructing alternative versions of the target distribution in which the peaks are not as high and the surrounding valleys are not as low.  
      A particularly simple way to do so is to use Energy based Models  
    $$p\boldsymbol{x} \propto \exp  E\boldsymbol{x}$$  
      Energy based models may be augmented with an extra parameter $$\beta$$ controlling how sharply peaked the distribution is  
    $$p{\beta}\boldsymbol{x} \propto \exp  \beta E\boldsymbol{x}$$  
      The $$\beta$$ parameter is often described as being the reciprocal of the temperature reflecting the origin of energy based models in statistical physics.  
        When the temperature falls to zero and $$\beta$$ rises to infinity the EBM becomes deterministic.  
        When the temperature rises to infinity and $$\beta$$ falls to zero the distribution for discrete $$\boldsymbol{x}$$ becomes uniform.  

    Typically a model is trained to be evaluated at $$\beta=1$$. However we can make use of other temperatures particularly those where $$\beta<1$$.  

    Tempering  
    Tempering is a general strategy of mixing between modes of $$p{1}$$ rapidly by drawing samples with $$\beta<1$$.  
    Markov chains based on tempered transitions Neal 1994 temporarily sample from higher temperature distributions to mix to different modes then resume sampling from the unit temperature distribution.  
    These techniques have been applied to models such as RBMs Salakhutdinov 2010.  

    Parallel Tempering  
    Another approach is to use parallel tempering Iba 2001 in which the Markov chain simulates many different states in parallel at different temperatures.  
      The highest temperature states mix slowly while the lowest temperature states at temperature $$1$$ provide accurate samples from the model.  
      The transition operator includes stochastically swapping states between two different temperature levels so that a sufficiently high probability sample from a high temperature slot can jump into a lower temperature slot. This approach has also been applied to RBMs Desjardins et al. 2010 ; Cho et al. 2010.  


    Results  In Practice  
    Although tempering is a promising approach at this point it has not allowed researchers to make a strong advance in solving the challenge of sampling from complex EBMs.  
    One possible reason is that there are critical temperatures around which the temperature transition must be very slow as the temperature is gradually reduced for tempering to be effective.  


    Depth for Mixing in Latent Variable Models  
    
     Problem  Mixing in Latent Variable Models  
        When drawing samples from a latent variable model $$p\boldsymbol{h} \boldsymbol{x}$$ we have seen that if $$p\boldsymbol{h} \vert \boldsymbol{x}$$ encodes $$\boldsymbol{x}$$ too well then sampling from $$p\boldsymbol{x} \vert \boldsymbol{h}$$ will not change $$\boldsymbol{x}$$ very much and mixing will be poor.  
         Example of the problem $$\alpha$$  
            Many representation learning algorithms such as Autoencoders and RBMs tend to yield a marginal distribution over $$\boldsymbol{h}$$ that is more uniform and more unimodal than the original data distribution over $$\boldsymbol{x}$$.  
         Reason for $$\alpha$$  
            It can be argued that this arises from trying to minimize reconstruction error while using all the available representation space because minimizing reconstruction error over the training examples will be better achieved when different training examples are easily distinguishable from each other in $$\boldsymbol{h}$$ space and thus well separated.  
     Solution  Deep Representations  
        One way to resolve this problem is to make $$\boldsymbol{h}$$ a deep representation encoding $$\boldsymbol{x}$$ into $$\boldsymbol{h}$$ in such a way that a Markov chain in the space of $$\boldsymbol{h}$$ can mix more easily.  
         Solution to the problem $$\alpha$$  
              Bengio et al. 2013 a observed that deeper stacks of regularized autoencoders or RBMs yield marginal distributions in the top level $$\boldsymbol{h}$$ space that appeared more spread out and more uniform with less of a gap between the regions corresponding to different modes categories in the experiments.  
              Training an RBM in that higher level space allowed Gibbs sampling to mix faster between modes.  
        
            It remains unclear however how to exploit this observation to help better train and sample from deep generative models.  

    
    Summary/Takeaway of MCMC methods In Practice DL  
    Despite the difficulty of mixing Monte Carlo techniques are useful and are often the best tool available.  
    Indeed they are the primary tool used to confront the intractable partition function of undirected models.  




 Inference and Approximate Inference


Resources  






Inference and Approximate Inference

1. Inference
    Inference usually refers to computing the probability distribution over one set of variables given another.  


    Goals  
    
     Computing the likelihood of observed data in models with latent variables.
     Computing the marginal distribution over a given subset of nodes in the model.
     Computing the conditional distribution over a subsets of nodes given a disjoint subset of nodes.
     Computing a mode of the density for the above distributions.

    Approaches  
    
     Exact inference algorithms  
         Brute force
         The elimination algorithm
         Message passing sum product algorithm belief propagation
         Junction tree algorithm  
     Approximate inference algorithms  
         Loopy belief propagation
         Variational Bayesian inference $$+$$ mean field approximations
         Stochastic simulation / sampling /  MCMC

    Inference in Deep Learning  Formulation  
    In the context of Deep Learning we usually have two sets of variables  
    1 Set of visible observed variables $$\ \boldsymbol{v}$$  
    2 Set of latent variables $$\ \boldsymbol{h}$$  

    Inference in DL corresponds to computing the likelihood of observed data $$p\boldsymbol{v}$$.  

    When training probabilistic models with latent variables we are usually interested in computing  
    $$p\boldsymbol{h} \vert \boldsymbol{v}$$   
    where $$\boldsymbol{h}$$ are the latent variables and $$\boldsymbol{v}$$ are the observed visible variables data.  
    


2. The Challenge of Inference
    Motivation  The Challenge of Inference  
    The challenge of inference usually refers to the difficult problem of computing $$p\boldsymbol{h} \vert \boldsymbol{v}$$ or taking expectations wrt it.  
    Such operations are often necessary for tasks like Maximum Likelihood Learning.  

    Intractable Inference  
    In DL intractable inference problems usually arise from interactions between latent variables in a structured graphical model.  
    These interactions are usually due to  
    
     Directed Models "explaining away" interactions between mutual ancestors of the same visible unit.  
     Undirected Models direct interactions between the latent variables.  

    In Models  
    
     Tractable Inference  
         Many simple graphical models with only one hidden layer have tractable inference.  
            E.g. RBMs PPCA.  
     Intractable Inference  
         Most graphical models with multiple hidden layers with hidden variables have intractable posterior distributions.  
            Exact inference requires an exponential time.  
            E.g. DBMs DBNs.  
         Even some models with only a single layer can be intractable.  
            E.g. Sparse Coding  

    


    Computing the Likelihood of Observed Data  
    We usually want to compute the likelihood of the observed data $$p\boldsymbol{v}$$ equivalently the log likelihood $$\log p\boldsymbol{v}$$.  
    This usually requires marginalizing out $$\boldsymbol{h}$$.  
    This problem is intractable difficult if it is costly to marginalize $$\boldsymbol{h}$$.  
    
     Data Likelihood  intractable  
        $$p{\theta}\boldsymbol{v}=\int\boldsymbol{h} p{\theta}h p{\theta}v \vert h dh$$  
     Marginal Likelihood evidence is the data likelihood $$p{\theta}\boldsymbol{v}$$ intractable    
        $$\int\boldsymbol{h} p{\theta}h p{\theta}v \vert h dh$$  
     Prior  
        $$p\boldsymbol{h}$$ 
     Conditional Likelihood  
        $$p{\theta}\boldsymbol{v} \vert h$$  
     Joint  
        $$p{\theta}\boldsymbol{v} \boldsymbol{h}$$  
     Posterior intractable    
        $$p{\theta}\boldsymbol{h} \vert \boldsymbol{v}=\frac{p{\theta}\boldsymbol{v} \boldsymbol{h}}{p{\theta}\boldsymbol{v}}=\frac{p{\theta}\boldsymbol{v} \vert h p{\theta}h}{\int{\boldsymbol{h}} p{\theta}h p{\theta}x \vert h d h}$$  

    


22.Approximate Inference
    Approximate Inference is an important and practical approach to confronting the challenge of intractable inference.  
    It poses exact inference as an optimization problem and aims to approximate the underlying optimization problem.  
      
    

3. Inference as Optimization
    Exact inference can be described as an optimization problem.  

     Inference Problem  
         Compute the log likelihood of the observed data $$\log p\boldsymbol{v} ; \boldsymbol{\theta}$$.  
            Can be intractable to marginalize $$\boldsymbol{h}$$.  
     Inference Problem as Optimization  Core Idea  
         Choose a family of distributions over the latent variables $$\boldsymbol{h}$$ with its own set of variational parameters $$\boldsymbol{v}$$ $$q\boldsymbol{h} \vert \boldsymbol{v}$$.  
         Find the setting of the parameters that makes our approximation closest to the posterior distribution over the latent variables $$p\boldsymbol{h} \vert \boldsymbol{v}$$.  
            I.E. Optimization  
         Use learned $$q$$ in place of the posterior as an approximation.  
     Optimization  Fitting $$q$$ to the posterior $$p$$  
         Optimize $$q$$ to approximate $$p\boldsymbol{h} \vert \boldsymbol{v}$$  
         Similarity Measure use the KL Divergence as a similarity measure between the two distributions  
            $$D{\mathrm{KL}}q \| p = \mathrm{E} {h \sim q}\left\log \frac{qh}{ph\vert {v}}\right =\int{h} qh \log \left\frac{qh}{ph\vert {v}}\right dh$$  
         Intractability minimizing the KL Divergence above is an intractable problem.  
            Because the expression contains the intractable term $$p\boldsymbol{h}\vert \boldsymbol{v}$$ which we were trying to avoid.  
     Evidence Lower Bound  
         We rewrite the KL Divergence expression in terms of log likelihood of the data  
            $$\begin{aligned} D{\mathrm{KL}}q \| p &=\int{\boldsymbol{h}} qh \log \frac{qh}{ph \vert v} dh  &=\int{\boldsymbol{h}} qh \log \frac{qh}{ph v} dh+\int{\boldsymbol{h}} qh \log pv dh  &=\int{\boldsymbol{h}} qh \log \frac{qh}{ph v} dh+\log p\boldsymbol{v} \end{aligned}$$  
            where we're using Bayes theorem on the  line and the RHS integral simplifies because it's simply integrating over the support of $$q$$ and $$p$$ is not a function of $$h$$.  
            Thus  
            $$\log p\boldsymbol{v} = D{\mathrm{KL}}q \| p  \int{\boldsymbol{h}} qh \log \frac{qh}{ph v} dh$$  
         Notice that since the KL Divergence is Non Negative  
            $$\begin{align}
                D{\mathrm{KL}}q \| p &\geq 0 
                D{\mathrm{KL}}q \| p  \int{\boldsymbol{h}} qh \log \frac{qh}{ph v} dh &\geq  \int{\boldsymbol{h}} qh \log \frac{qh}{ph v} dh 
                \log p\boldsymbol{v} &\geq  \int{\boldsymbol{h}} qh \log \frac{qh}{ph v} dh 
                \end{align}
                $$   
            Thus the term $$ \int{\boldsymbol{h}} qh \log \frac{qh}{ph v} dh$$ provides a lower bound
         We rewrite the term as  
            $$\mathcal{L}\boldsymbol{v} \boldsymbol{\theta} q =  \int{\boldsymbol{h}} qh \log \frac{qh}{ph v} dh$$  
            the Evidence Lower Bound ELBO
            Thus  
            $$\log p\boldsymbol{v} \geq \mathcal{L}\boldsymbol{v} \boldsymbol{\theta} q$$  
         The Evidence Lower Bound can also be defined as  
            $$\begin{align}
                \mathcal{L}\boldsymbol{v} \boldsymbol{\theta} q &=  \int{\boldsymbol{h}} qh \log \frac{qh}{ph v} dh 
                &= \log p\boldsymbol{v} ; \boldsymbol{\theta} D{\mathrm{KL}}q\boldsymbol{h} \vert \boldsymbol{v} \| p\boldsymbol{h} \vert \boldsymbol{v} ; \boldsymbol{\theta} 
                &= \mathbb{E} {\mathbf{h} \sim q}\log p\boldsymbol{h} \boldsymbol{v}+Hq  
                \end{align}
                $$   
            The latter being the canonical definition of the ELBO.  
     Inference with the Evidence Lower Bound  
         For an appropriate choice of $$q \mathcal{L}$$ is tractable to compute.  
         For any choice of $$q \mathcal{L}$$ provides a lower bound on the likelihood
         For $$q\boldsymbol{h} \vert \boldsymbol{v}$$ that are better approximations of $$p\boldsymbol{h} \vert \boldsymbol{v}$$ the lower bound $$\mathcal{L}$$ will be tighter  
            I.E. closer to $$\log p\boldsymbol{v}$$.  
         When $$q\boldsymbol{h} \vert \boldsymbol{v}=p\boldsymbol{h} \vert \boldsymbol{v}$$ the approximation is perfect and $$\mathcal{L}\boldsymbol{v} \boldsymbol{\theta} q=\log p\boldsymbol{v} ; \boldsymbol{\theta}$$.  
         Maximizing the ELBO minimizes the KL Divergence $$D{\mathrm{KL}}q \| p$$.  
     Inference  
        We can thus think of inference as the procedure for finding the $$q$$ that maximizes $$\mathcal{L}$$  
         Exact Inference maximizes $$\mathcal{L}$$ perfectly by searching over a family of functions $$q$$ that includes $$p\boldsymbol{h} \vert \boldsymbol{v}$$.  
         Approximate Inference approximate inference uses approximate optimization to find $$q$$.  
            We can make the optimization procedure less expensive but approximate by  
             Restricting the family of distributions $$q$$ that the optimization is allowed to search over  
             Using an imperfect optimization procedure that may not completely maximize $$\mathcal{L}$$ but may merely increase it by a significant amount.  
     Core Idea of Variational Inference  
        We don't need to explicitly compute the posterior or the marginal likelihood we can solve an optimization problem by finding the right distribution $$$$  that best fits the Evidence Lower Bound.  


    Learning and Inference wrt the ELBO  Summary  
    The ELBO $$\mathcal{L}\boldsymbol{v} \boldsymbol{\theta} q$$ is a lower bound on $$\log p\boldsymbol{v} ; \boldsymbol{\theta}$$  
    
     Inference can be viewed as maximizing $$\mathcal{L}$$ with respect to $$q$$.  
     Learning can be viewed as maximizing $$\mathcal{L}$$ with respect to $$\theta$$.  


    Notes  
    
     The difference between the ELBO and the KL divergence is the log normalizer i.e. the evidence which is the quantity that the ELBO bounds.  
     Maximizing the ELBO is equivalent to Minimizing the KL Divergence.  
    

4. Expectation Maximization
    The Expectation Maximization Algorithm is an iterative method to find maximum likelihood or maximum a posteriori MAP estimates of parameters in statistical models with unobserved latent variables.  

    It is based on maximizing a lower bound $$\mathcal{L}$$.  
    It is not an approach to approximate inference.  
    It is an approach to learning with an approximate posterior.  

    The EM Algorithm  
    The EM Algorithm consists of alternating between two steps until convergence  
    
     The Expectation step  
         Let $$\theta^{0}$$ denote the value of the parameters at the beginning of the step.  
         Set $$q\left\boldsymbol{h}^{i} \vert \boldsymbol{v}\right=p\left\boldsymbol{h}^{i} ; \boldsymbol{\theta}^{0}\right$$ for all indices $$i$$ of the training examples $$\boldsymbol{v}^{i}$$ we want to train on both batch and minibatch variants are valid.  
            By this we mean $$q$$ is defined in terms of the current parameter value of $$\boldsymbol{\theta}^{0}$$;  
            if we vary $$\boldsymbol{\theta}$$ then $$p\boldsymbol{h} \vert \boldsymbol{v} ; \boldsymbol{\theta}$$ will change but $$q\boldsymbol{h} \vert \boldsymbol{v}$$ will remain equal to $$p\left\boldsymbol{h} \vert \boldsymbol{v} ; \boldsymbol{\theta}^{0}\right$$.  
     The Maximization step  
         Completely or partially maximize  
            $$\sumi \mathcal{L}\left\boldsymbol{v}^{i} \boldsymbol{\theta} q\right$$  
            with respect to $$\boldsymbol{\theta}$$ using your optimization algorithm of choice.  

    Relation to Coordinate Ascent  
    The algorithm can be viewed as a Coordinate Ascent algorithm to maximize $$\mathcal{L}$$.  
    On one step we maximize $$\mathcal{L}$$ with respect to $$q$$ and on the other we maximize $$\mathcal{L}$$ with respect to $$\boldsymbol{\theta}$$.  
    Stochastic Gradient Ascent on latent variable models can be seen as a special case of the EM algorithm where the M step consists of taking a single gradient step.  
    Other variants of the EM algorithm can make much larger steps. For some model families the M step can even be performed analytically jumping all the way to the optimal solution for $$\theta$$ given the current $$q$$.  

    As Approximate Inference  Interpretation  
    Even though the E step involves exact inference the EM algorithm can be viewed as using approximate inference.  
    The M step assumes that the same value of $$q$$ can be used for all values of $$\theta$$.  
    This will introduce a gap between $$\mathcal{L}$$ and the true $$\log p\boldsymbol{v}$$ as the M step moves further and further away from the value $$\boldsymbol{\theta}^{0}$$ used in the E step.  
    Fortunately the E step reduces the gap to zero again as we enter the loop for the next time.  

    
    Insights/Takeaways  
    
    1. The Basic Structure of the Learning Process  
        We update the model parameters to improve the likelihood of a completed dataset where all missing variables have their values provided by an estimate of the posterior distribution.  
        This particular insight is not unique to the EM algorithm. For example using gradient descent to maximize the log likelihood also has this same property; the log likelihood gradient computations require taking expectations with respect to the posterior distribution over the hidden units.   
    2. Reusing $$q$$  
        We can continue to use one value of $$q$$ even after we have moved to a different value of $$\theta$$.  
        This particular insight is used throughout classical machine learning to derive large M step updates.  
        In the context of deep learning most models are too complex to admit a tractable solution for an optimal large M step update so this  insight which is more unique to the EM algorithm is rarely used.  



    Notes  
    

     The EM algorithm enables us to make large learning steps with a fixed $$q$$  
    


5. MAP Inference
    MAP Inference is an alternative form of inference where we are interested in computing the single most likely value of the missing variables rather than to infer the entire distribution over their possible values $$p\boldsymbol{h} \vert \boldsymbol{v}$$.  
    In the context of latent variable models we compute  
    $$\boldsymbol{h}^{ }=\underset{\boldsymbol{h}}{\arg \max } p\boldsymbol{h} \vert \boldsymbol{v}$$  

    As Approximate Inference  
    It is not usually thought of as approximate inference since it computes the exact most likely value of $$\boldsymbol{h}^{ }$$.  
    However to develop a learning process wrt maximizing the lower bound $$\mathcal{L}\boldsymbol{v} \boldsymbol{h} q$$ then it is helpful to think of MAP inference as a procedure that provides a value of $$q$$.  
    In this sense we can think of MAP inference as approximate inference because it does not provide the optimal $$q$$.  
    We can derive MAP Inference as a form of approximate inference by restricting the family of distributions $$q$$ may be drawn from.  
    Derivation  
    
     We require $$q$$ to take on a Dirac distribution  
        $$q\boldsymbol{h} \vert \boldsymbol{v}=\delta\boldsymbol{h} \boldsymbol{\mu}$$  
     This means that we can now control $$q$$ entirely via $$\boldsymbol{\mu}$$.  
     Dropping terms of $$\mathcal{L}$$ that do not vary with $$\boldsymbol{\mu}$$ we are left with the optimization problem  
        $$\boldsymbol{\mu}^{ }=\underset{\mu}{\arg \max } \log p\boldsymbol{h}=\boldsymbol{\mu} \boldsymbol{v}$$  
     which is equivalent to the MAP inference problem  
        $$\boldsymbol{h}^{ }=\underset{\boldsymbol{h}}{\arg \max } p\boldsymbol{h} \vert \boldsymbol{v}$$  

    The Learning Procedure with MAP Inference{ }  
    We can thus justify a learning procedure similar to EM where we alternate between  
    
     Performing MAP inference to infer $$\boldsymbol{h}^{ }$$ and  
     Updating update $$\boldsymbol{\theta}$$ to increase $$\log p\left\boldsymbol{h}^{ } \boldsymbol{v}\right$$.  

    As Coordinate Ascent  
    As with EM this is a form of coordinate ascent on $$\mathcal{L}$$ where we alternate between using inference to optimize $$\mathcal{L}$$ with respect to $$q$$ and using parameter updates to optimize $$\mathcal{L}$$ with respect to $$\boldsymbol{\theta}$$.  

    Lower Bound ELBO Justification  
    The procedure as a whole can be justified by the fact that $$\mathcal{L}$$ is a lower bound on $$\log p\boldsymbol{v}$$.  
    In the case of MAP inference this justification is rather vacuous because the bound is infinitely loose due to the Dirac distribution's differential entropy of negative infinity.  
    Adding noise to $$\mu$$ would make the bound meaningful again.   

    MAP Inference in Deep Learning  Applications  
    MAP Inference is commonly used in deep learning as both a feature extractor and a learning mechanism.  
    It is primarily used for sparse coding models.  

    MAP Inference in Sparse Coding Models{ }  
    


    Summary  
    Learning algorithms based on MAP inference enable us to learn using a point estimate of $$p\boldsymbol{h} \vert \boldsymbol{v}$$ rather than inferring the entire distribution.  
    


6. Variational Inference and Learning
    

    Main Idea  Restricting family of distributions $$q$$  
    The core idea behind variational learning is that we can maximize $$\mathcal{L}$$ over a restricted family of distributions $$q$$.  
    This family should be chosen so that it is easy to compute $$\mathbb{E} {q} \log p\boldsymbol{h} \boldsymbol{v}$$.  
    A typical way to do this is to introduce assumptions about how $$q$$ factorizes.  
    Mainly we make a Mean Field Approximation to $$q$$.  


    Mean Field Approximation  
    Mean Field Approximation is a type of Variational Bayesian Inference where we assume that the unknown variables can be partitioned so that each partition is independent of the others.  
    The Mean Field Approximation assumes the variational distribution over the latent variables factorizes as  
    $$q\boldsymbol{h} \vert \boldsymbol{v}=\prod{i} q\lefth{i} \vert \boldsymbol{v}\right$$  
    I.E. it imposes the restriction that $$q$$ is a factorial distribution.  

    More generally we can impose any graphical model structure we choose on $$q$$ to flexibly determine how many interactions we want our approximation to capture.  
    This fully general graphical model approach is called structured variational inference Saul and Jordan 1996.  

    The Optimal Probability Distribution $$q$$  
    The beauty of the variational approach is that we do not need to specify a specific parametric form for $$q$$.  
    We specify how it should factorize but then the optimization problem determines the optimal probability distribution within those factorization constraints.  
    The Inference Optimization Problem  
    
     For discrete latent variables we use traditional optimization techniques to optimize a finite number of variables describing the $$q$$ distribution.  
     For continuous latent variables we use calculus of variations to perform optimization over a space of functions and actually determine which function should be used to represent $$q$$.  
         Calculus of Variations removes much of the responsibility from the human designer of the model who now must specify only how $$q$$ factorizes rather than needing to guess how to design a specific $$q$$ that can accurately approximate the posterior.  

        Calculus of variations is the origin of the names "variational learning" and "variational inference" but the names apply in both discrete and continuous cases.    

    KL Divergence Optimization  
    
     The Inference Optimization Problem boils down to maximizing $$\mathcal{L}$$ with respect to $$q$$.  
     This is equivalent to minimizing $$D{\mathrm{KL}}q\boldsymbol{h} \vert \boldsymbol{v} \| p\boldsymbol{h} \vert \boldsymbol{v}$$.  
     Thus we are fitting $$q$$ to $$p$$.  
     However we are doing so with the opposite direction of the KL Divergence. We are unnaturally assuming that $$q$$ is constant and $$p$$ is varying.  
     In the inference optimization problem we choose to use $$D{\mathrm{KL}}\leftq\boldsymbol{h} \vert \boldsymbol{v} \| p\boldsymbol{h} \vert \boldsymbol{v}\right$$ for computational reasons.  
         Specifically computing $$D{\mathrm{KL}}\leftq\boldsymbol{h} \vert \boldsymbol{v} \| p\boldsymbol{h} \vert \boldsymbol{v}\right$$ involves evaluating expectations with respect to $$q$$ so by designing $$q$$ to be simple we can simplify the required expectations.  
         The opposite direction of the KL divergence would require computing expectations with respect to the true posterior.  
            Because the form of the true posterior is determined by the choice of model we cannot design a reduced cost approach to computing $$D{\mathrm{KL}}p\boldsymbol{h} \vert \boldsymbol{v} \| q\boldsymbol{h} \vert \boldsymbol{v}$$ exactly.  
     Three Cases for Optimization  
         If $$q$$ is high and $$p$$ is high then we are happy i.e. low KL divergence.
         If $$q$$ is high and $$p$$ is low then we pay a price i.e. high KL divergence.
         If $$q$$ is low then we dont care i.e. also low KL divergence regardless of $$p$$.
     Optimization based Inference vs Maximum Likelihood ML Learning  
         ML Learning fits a model to data by minimizing $$D{\mathrm{KL}}\leftp{\text {data }} \| p{\text {model }}\right$$.  
            It encourages the model to have high probability everywhere that the data has high probability 
         Optimization based Inference   
            It encourages $$q$$ to have low probability everywhere the true posterior has low probability.  
    
    Variational Bayesian Inference  
    Variational Bayesian Inference AKA Variational Bayes is most often used to infer the conditional distribution over the latent variables given the observations  and parameters.  
    This is also known as the posterior distribution over the latent variables  
    $$pz \vert x \alpha=\frac{pz x \vert \alpha}{\int{z} pz x \vert \alpha}$$  
    which is usually intractable.  

    Notes  
    
     KL Divergence Optimization  
        Optimizing the KL Divergence given by  
        $$D{\mathrm{KL}}q \| p = \mathrm{E} {z \sim q}\left\log \frac{qz}{pz\vert x}\right =\int{z} qz \log \left\frac{qz}{pz\vert x}\right dz$$  
         Three Cases for Optimization  
             If $$q$$ is high and $$p$$ is high then we are happy i.e. low KL divergence.
             If $$q$$ is high and $$p$$ is low then we pay a price i.e. high KL divergence.
             If $$q$$ is low then we dont care i.e. also low KL divergence regardless of $$p$$.
    


Variational Inference and Learning



2. Variational Inference  Discrete Latent Variables
    Variational Inference with Discrete Latent Variables is relatively straightforward.  
    Representing $$q$$  
    We define a distribution $$q$$ where each factor of $$q$$ is just defined by a lookup table over discrete states.  
    In the simplest case $$h$$ is binary and we make the mean field assumption that $$q$$ factorizes over each individual $$h{i}$$.  
    In this case we can parametrize $$q$$ with a vector $$\hat{h}$$ whose entries are probabilities.  
    Then $$q\lefth{i}=1 \vert \boldsymbol{v}\right=\hat{h} {i}$$.  
    Optimizing $$q$$  
    After determining how to represent $$q$$ we simply optimize its parameters.  
    For discrete latent variables this is just a standard optimization problem e.g. gradient descent.  
    However because this optimization must occur in the inner loop of a learning algorithm it must be very fast^1.  
    A popular choice is to iterate fixed point equations; to solve  
    $$\frac{\partial}{\partial \hat{h} {i}} \mathcal{L}=0$$  
    for $$\hat{h} {i}$$.  
    We repeatedly update different elements of $$\hat{\boldsymbol{h}}$$ until we satisfy a convergence criterion.  


    Application  Binary Sparse Coding  
    



3. Variational Inference  Continuous Latent Variables
    Variational Inference and Learning with Continuous Latent Variables requires the use of the calculus of variations for maximizing $$\mathcal{L}$$ with respect to $$q\boldsymbol{h} \vert \boldsymbol{v}$$.  

    In most cases practitioners need not solve any calculus of variations problems themselves. Instead there is a general equation for the mean field fixed point updates.  

    The General Equation for Mean Field Fixed Point Updates  
    If we make the mean field approximation  
    $$q\boldsymbol{h} \vert \boldsymbol{v}=\prod{i} q\lefth{i} \vert \boldsymbol{v}\right$$  
    and fix $$q\lefth{j} \vert \boldsymbol{v}\right$$ for all $$j \neq i$$ then the optimal $$q\lefth{i} \vert \boldsymbol{v}\right$$ may be obtained by normalizing the unnormalized distribution  
    $$\tilde{q}\lefth{i} \vert \boldsymbol{v}\right = \exp \left\mathbb{E}{\mathbf{h}{ i} \sim q\left\mathbf{h} { i} \vert \boldsymbol{v}\right} \log \tilde{p}\boldsymbol{v} \boldsymbol{h}\right = e^{\mathbb{E}{\mathbf{h} { i} \sim q\left\mathbf{h} { i} \vert \boldsymbol{v}\right} \log \tilde{p}\boldsymbol{v} \boldsymbol{h}}$$  
    as long as $$p$$ does not assign $$0$$ probability to any joint configuration of variables.  
      Carrying out the expectation inside the equation will yield the correct functional form of $$q\lefth{i} \vert \boldsymbol{v}\right$$.   
      The General Equation yields the mean field approximation for any probabilistic model.  
      Deriving functional forms of $$q$$ directly using calculus of variations is only necessary if one wishes to develop a new form of variational learning.  
      The General Equation is a fixed point equation designed to be iteratively applied for each value of $$i$$ repeatedly until convergence.  

    Functional Form of the Optimal Distribution/Solution  
    The General Equation tells us the functional form that the optimal solution will take whether we arrive there by fixed point equations or not.  
    This means we can take the functional form from that equation but regard some of the values that appear in it as parameters which we can optimize with any optimization algorithm we like.  
    


    For examples of real applications of variational learning with continuous variables in the context of deep learning see Goodfellow et al. 2013d.  
    


4. Interactions between Learning and Inference
    Using approximate inference as part of a learning algorithm affects the learning process and this in turn affects the accuracy of the inference algorithm.  
    Analysis  
    
     The training algorithm tends to adapt the model in a way that makes the approximating assumptions underlying the approximate inference algorithm become more true.  
     When training the parameters variational learning increases  
        $$\mathbb{E} {\mathbf{h} \sim q} \log p\boldsymbol{v} \boldsymbol{h}$$  
     For a specific $$v$$ this  
         increases $$p\boldsymbol{h} \vert \boldsymbol{v}$$ for values of $$\boldsymbol{h}$$ that have high probability under $$q\boldsymbol{h} \vert \boldsymbol{v}$$ and  
         decreases $$p\boldsymbol{h} \vert \boldsymbol{v}$$ for values of $$\boldsymbol{h}$$ that have low probability under $$q\boldsymbol{h} \vert \boldsymbol{v}$$.  
     This behavior causes our approximating assumptions to become self fulfilling prophecies.  
        If we train the model with a unimodal approximate posterior we will obtain a model with a true posterior that is far closer to unimodal than we would have obtained by training the model with exact inference.  

    Computing the Effect Harm of using Variational Inference  
    Computing the true amount of harm imposed on a model by a variational approximation is thus very difficult.  
    
     There exist several methods for estimating $$\log p\boldsymbol{v}$$  
        We often estimate $$\log p\boldsymbol{v} ; \boldsymbol{\theta}$$ after training the model and find that the gap with $$\mathcal{L}\boldsymbol{v} \boldsymbol{\theta} q$$ is small.  
         From this we can conclude that our variational approximation is accurate for the specific value of $$\boldsymbol{\theta}$$ that we obtained from the learning process.  
         We should not conclude that our variational approximation is accurate in general or that the variational approximation did little harm to the learning process.  
     To measure the true amount of harm induced by the variational approximation  
         We would need to know $$\boldsymbol{\theta}^{ }=\max{\boldsymbol{\theta}} \log p\boldsymbol{v} ; \boldsymbol{\theta}$$.  
         It is possible for $$\mathcal{L}\boldsymbol{v} \boldsymbol{\theta} q \approx \log p\boldsymbol{v} ; \boldsymbol{\theta}$$ and $$\log p\boldsymbol{v} ; \boldsymbol{\theta} \ll \log p\left\boldsymbol{v} ; \boldsymbol{\theta}^{ }\right$$ to hold simultaneously.  
         If $$\max{q} \mathcal{L}\left\boldsymbol{v} \boldsymbol{\theta}^{ } q\right \ll \log p\left\boldsymbol{v} ; \boldsymbol{\theta}^{ }\right$$ because $$\boldsymbol{\theta}^{ }$$ induces too complicated of a posterior distribution for our $$q$$ family to capture then the learning process will never approach $$\boldsymbol{\theta}^{ }$$.  
         Such a problem is very difficult to detect because we can only know for sure that it happened if we have a superior learning algorithm that can find $$\boldsymbol{\theta}^{ }$$ for comparison.  
    


5. Learned Approximate Inference
    Motivation  
    Explicitly performing optimization via iterative procedures such as fixed point equations or gradient based optimization is often very expensive and time consuming.  
    Many approaches to inference avoid this expense by learning to perform approximate inference.  

    Learned Approximate Inference  
    Learns to perform approximate inference by viewing the multistep iterative optimization process as a function $$f$$ that maps an input $$v$$ to an approximate distribution $$q^{ }=\arg \max{q} \mathcal{L}\boldsymbol{v} q$$ and then approximates this function with a neural network that implements an approximation $$f\boldsymbol{v} ; \boldsymbol{\theta}$$.  


    Wake Sleep  
    Motivation  
    
     One of the main difficulties with training a model to infer $$h$$ from $$v$$ is that we do not have a supervised training set with which to train the model.  
     Given a $$v$$ we do not know the appropriate $$h$$.  
     The mapping from $$v$$ to $$h$$ depends on the choice of model family and evolves throughout the learning process as $$\theta$$ changes.  

    Wake Sleep Algorithm  
    The wake sleep algorithm Hinton et al. 1995b; Frey et al. 1996 resolves this problem by drawing samples of both $$h$$ and $$v$$ from the model distribution.  
     For example in a directed model this can be done cheaply by performing ancestral sampling beginning at $$h$$ and ending at $$v$$.  
        The inference network can then be trained to perform the reverse mapping predicting which $$h$$ caused the present $$\boldsymbol{v}$$.  
    
    DrawBacks  
    The main drawback to this approach is that we will only be able to train the inference network on values of $$\boldsymbol{v}$$ that have high probability under the model.  
    Early in learning the model distribution will not resemble the data distribution so the inference network will not have an opportunity to learn on samples that resemble data.  

    Relation to Biological Dreaming  
    



    Generative Modeling  Application  
    Learned approximate inference has recently become one of the dominant approaches to generative modeling in the form of the Variational AutoEncoder Kingma 2013; Rezende et al. 2014.  
    In this elegant approach there is no need to construct explicit targets for the inference network.  
    Instead the inference network is simply used to define $$\mathcal{L}$$ and then the parameters of the inference network are adapted to increase $$\mathcal{L}$$.  





Mathematics of Approximate Inference




1. Calculus of Variations
    
    Method for finding the stationary functions of a functional $$If$$ function of functions by solving a differential equation.  

    Formally calculus of variations seeks to find the function $$y=fx$$ such that the integral functional  
    $$Iy=\int{x{1}}^{x{2}} L\leftx yx y^{\prime}x\right d x$$   
    $$\begin{array}{l}{\text {where}}{x{1} x{2} \text { are constants }}  {yx \text { is twice continuously differentiable }}  {y^{\prime}x=d y / d x}  {L\leftx yx y^{\prime}x\right \text { is twice continuously differentiable with respect to its arguments } x y y^{\prime}}\end{array}$$  
    is stationary.  


    Euler Lagrange Equation  Finding Extrema  
    Finding the extrema of functionals is similar to finding the maxima and minima of functions. The maxima and minima of a function may be located by finding the points where its derivative vanishes i.e. is equal to zero. The extrema of functionals may be obtained by finding functions where the functional derivative is equal to zero. This leads to solving the associated Euler Lagrange equation.  

    The Euler Lagrange Equation is a  order partial differential equation whose solutions are the functions for which a given functional is stationary  
    $$\frac{\partial L}{\partial f} \frac{d}{d x} \frac{\partial L}{\partial f^{\prime}} = 0$$  
    It is defined in terms of the functional derivative  
    $$\frac{\delta J}{\delta fx} = \frac{\partial L}{\partial f} \frac{d}{d x} \frac{\partial L}{\partial f^{\prime}} = 0$$  




    Shortest Path between Two Points  
    Find path such that the distance $$AB$$ between two points is minimized.  
    Using the arc length we define the following functional  
    $$\begin{align}
        I &= \int{A}^{B} dS 
             &= \int{A}^{B} \sqrt{dx^2 + dy^2} 
             &= \int{A}^{B} \sqrt{1 + \left\dfrac{dy}{dx}\right^2} dx  
             &= \int{x1}^{x2} \sqrt{1 + \left\dfrac{dy}{dx}\right^2} dx
        \end{align}
        $$  
     Now we formulate the variational problem  
        Find the extremal function $$y=fx$$ between two points $$A=x1 y1$$ and $$B=x2 y2$$ such that the following integral is minimized  
        $$Iy = \int{x{1}}^{x{2}} \sqrt{1+\lefty^{\prime}x\right^{2}} d x$$   
        where $$y^{\prime}x=\frac{d y}{d x} y{1}=f\leftx{1}\right y{2}=f\leftx{2}\right$$.  
     Solution  
        We use the Euler Lagrange Equation to find the extremal function $$fx$$ that minimizes the functional $$Iy$$  
        $$\frac{\partial L}{\partial f} \frac{d}{d x} \frac{\partial L}{\partial f^{\prime}}=0$$  
        where $$L=\sqrt{1+\leftf^{\prime}x\right^{2}}$$.  
         Since $$f$$ does not appear explicity in $$L$$ the  term in the Euler Lagrange equation vanishes for all $$fx$$  
            $$\frac{\partial L}{\partial f} = 0$$  
         Thus  
            $$\frac{d}{d x} \frac{\partial L}{\partial f^{\prime}}=0$$  
         Substituting for $$L$$ and taking the derivative  
            $$\frac{d}{d x} \frac{f^{\prime}x}{\sqrt{1+\leftf^{\prime}x\right^{2}}}=0$$  
            for some constant $$c$$.  
         If the derivative $$\frac{d}{dx}$$ above is zero then  
            $$\frac{f^{\prime}x}{\sqrt{1+\leftf^{\prime}x\right^{2}}}=c$$  
            for some constant $$c$$.  
         Square both sides  
            $$\frac{\leftf^{\prime}x\right^{2}}{1+\leftf^{\prime}x\right^{2}}=c^{2}$$  
            where $$0 \leq c^{2}<1$$.  
         Solving  
            $$\leftf^{\prime}x\right^{2}=\frac{c^{2}}{1 c^{2}}$$  
            $$\implies$$  
            $$f^{\prime}x=m$$  
            is a constant $$m$$.  
         Integrating  
            $$fx=m x+b$$  
            is an equation of a straight line where $$m=\frac{y{2} y{1}}{x{2} x{1}} \quad$$ and $$\quad b=\frac{x{2} y{1} x{1} y{2}}{x{2} x{1}}$$.  

        In other words the shortest distance between two points is a straight line.  
        
        We have found the extremal function $$fx$$ that minimizes the functional $$Ay$$ so that $$Af$$ is a minimum.
          

    

2. Mean Field Methods

3. Mean Field Approximations



^1 To achieve this speed we typically use special optimization algorithms that are designed to solve comparatively small and simple problems in few iterations.  


 The Partition Function












Introduction  The Partition Function

1. The Partition Function
    The Partition Function is the normalization constant of an unnormalized probability distribution $$\tilde{p}\mathbf{x} ; \boldsymbol{\theta}$$.   
    
    Formally it is the possibly infinite sum over the unnormalized probability $$\tilde{p}\mathbf{x} ; \boldsymbol{\theta}$$ of all the states/events $$\boldsymbol{x} \in X$$  
    
     Discrete Variables  
        $$Z\boldsymbol{\theta} = \sum{\boldsymbol{x}} \tilde{p}\boldsymbol{x}$$  
     Continuous Variables  
        $$Z\boldsymbol{\theta} = \int \tilde{p}\boldsymbol{x} d \boldsymbol{x}$$  

    It is defined such that  
    $$\sum\mathbf{x} p\mathbf{x} ; \boldsymbol{\theta} = \sum\mathbf{x} \dfrac{\tilde{p}\mathbf{x} ; \boldsymbol{\theta}}{Z\boldsymbol{\theta}} = 1$$  

    Notes  
    
     The Partition Function contains an explicit Temperature  
     The Partition Function is a generating function  
    
        Sompolinsky et al. confront the partition function for a Perceptron using statistical mechanics methods developed for spin glasses and simple nets Garder Derrida and applied it to Perceptrons and later to something like MLPs.  
    
        Uses old techniques from non equilibrium statistical mechanics to address the modern problems of inference.  
    

2. Handling the Partition Function  Motivation
    Many Undirected Probabilistic Graphical Models PGMs are defined by an unnormalized probability distribution $$\tilde{p}\mathbf{x} ; \boldsymbol{\theta}$$.  
    To obtain a valid probability distribution we need to normalize $$\tilde{p}$$ by dividing by a partition function $$Z\boldsymbol{\theta}$$  
    $$p\mathbf{x} ; \boldsymbol{\theta}=\dfrac{1}{Z\boldsymbol{\theta}} \tilde{p}\mathbf{x} ; \boldsymbol{\theta}$$  
    Calculating the partition function can be intractable for many interesting models.  

    The Partition Function in Deep Probabilistic Models  
    Deep Probabilistic Models are usually designed with the partition function in mind. There a few approaches taken in the designs  
    
     Some models are designed to have a tractable normalizing constant.  
     Others are designed to be used in ways training/inference that avoid computing the normalized probability altogether.  
     Yet other models directly confront the challenge of intractable partition functions.  
        They use techniques described below for training and evaluating models with intractable $$Z$$.  

    Handling the Partition Function  
    There are a few approaches to handle the intractable partition function  
    
    1. Estimate the partition function as a learned parameter; Noise Contrastive Estimation.  
    2. Estimate the gradient of the partition function directly; Stochastic MLE Contrastive Divergence.  
    3. Avoid computing quantities related to the partition function altogether; Score Matching Pseudolikelihood.  
    4. Estimate the partition function itself explicitly Annealed IS Bridge Sampling Linked IS.   
    

3. The Log Likelihood Gradient
    Phase Decomposition of Learning  
    Learning using MLE requires computing the gradient of the NLL $$\nabla{\boldsymbol{\theta}} \log p\mathbf{x} ; \boldsymbol{\theta}$$.  
    What makes learning undirected models by maximum likelihood particularly difficult is that the partition function depends on the parameters; thus the gradient of the NLL wrt the parameters involves computing the gradient of $$Z\mathbf{\theta}$$.  
    In undirected models this gradient can be written as    
    $$\nabla{\boldsymbol{\theta}} \log p\mathbf{x} ; \boldsymbol{\theta}=\nabla{\boldsymbol{\theta}} \log \tilde{p}\mathbf{x} ; \boldsymbol{\theta} \nabla{\boldsymbol{\theta}} \log Z\boldsymbol{\theta}$$  
    which decomposes the gradient learning into a positive phase and a negative phase.  

    Difficulties in Learning wrt the Decomposition  
    
     Difficulty in the Negative Phase  
          For most undirected models of interest the negative phase is difficult to compute. This is usually due to having to compute the unnormalized probability for all the states.  
          Directed models define many "implicit" conditional independencies between the variables making it easier to compute the normalization due to many terms canceling out.  
         Example  RBMs  
            The quintessential example of a model with a straightforward positive phase and a difficult negative phase is the RBM.  
            It has hidden units that are conditionally independent from each other given the visible units.  

        Word2vec is another example.  
     Difficulty in the Positive Phase  
          Latent Variable Models generally have intractable positive phase.  
          Models with no latent variables or with few interactions between latent variables typically have a tractable positive phase.  
         Example  VAEs  
            VAEs define a continuous distribution over the data with latent variable $$z$$  
            $$p{\theta}x=\int p{\theta}z p{\theta}x \vert z d z$$  
            which is intractable to compute for every $$z$$.  
            Due to complicated interactions between latent variables this integral requires exponential time to compute as it needs to be evaluated over all configurations of latent variable.  
            all $$zi$$ variables are dependent on each other.   

    Positive and Negative Phases  
    The terms positive and negative do not refer to the sign of each term in the equation but rather reflect their effect on the probability density defined by the model.  
      The positive phase increases the probability of training data  by reducing the corresponding free energy  
      The negative phase decreases  the probability of samples generated by the model.  


    Monte Carlo Methods for Approximate LL Maximization  
    To use MC methods for approximate learning we need to rewrite the gradient of the partition function $$\nabla{\boldsymbol{\theta}} \log Z$$ as an expectation of the unnormalized probability $$\tilde{p}$$  
    $$\nabla{\boldsymbol{\theta}} \log Z=\mathbb{E}{\mathbf{x} \sim p\mathbf{x}} \nabla{\boldsymbol{\theta}} \log \tilde{p}\mathbf{x}$$  
    This identity is the basis for a variety of Monte Carlo methods for approximately maximizing the likelihood of models with intractable partition functions.  
     
         Decomposing the gradient of $$\log Z$$  
            $$\begin{aligned} \nabla{\boldsymbol{\theta}} \log Z =& \frac{\nabla{\boldsymbol{\theta}} Z}{Z} =& \frac{\nabla{\boldsymbol{\theta}} \sum{\mathbf{x}} \tilde{p}\mathbf{x}}{Z} =& \frac{\sum{\mathbf{x}} \nabla{\boldsymbol{\theta} \tilde{p}\mathbf{x}}}{Z} \end{aligned}$$  
         For models that guarantee $$p\mathbf{x}>0$$ for all $$\mathbf{x}$$ we can substitute $$\exp \log \tilde{p}\mathbf{x}$$ for $$\tilde{p}\mathbf{x}$$  
            $$\begin{aligned} \frac{\sum{\mathbf{x}} \nabla{\boldsymbol{\theta}} \exp \log \tilde{p}\mathbf{x}}{Z} &= \frac{\sum{\mathbf{x}} \exp \log \tilde{p}\mathbf{x} \nabla{\boldsymbol{\theta}} \log \tilde{p}\mathbf{x}}{Z}  &=\frac{\sum{\mathbf{x}} \tilde{p}\mathbf{x} \nabla{\boldsymbol{\theta}} \log \tilde{p}\mathbf{x}}{Z}  &=\sum{\mathbf{x}} p\mathbf{x} \nabla{\boldsymbol{\theta}} \log \tilde{p}\mathbf{x}  &= \mathbb{E}{\mathbf{x} \sim p\mathbf{x}} \nabla{\boldsymbol{\theta}} \log \tilde{p}\mathbf{x} \end{aligned}$$  


     
         We use Leibniz's rule for diﬀerentiation under the integral sign to obtain the identity  
            $$\nabla{\boldsymbol{\theta}} \int \tilde{p}\mathbf{x} d \boldsymbol{x}=\int \nabla{\boldsymbol{\theta}} \tilde{p}\mathbf{x} d \boldsymbol{x}$$  
             Applicability  Measure Theory  
                This identity is applicable only under certain regularity conditions on $$\tilde{p}$$ and $$\nabla{\boldsymbol{\theta}} \tilde{p}\mathbf{x}$$.  
                In measure theoretic terms the conditions are  
                1. The unnormalized distribution $$\tilde{p}$$ must be a Lebesgue integrable function of $$\boldsymbol{x}$$  for every value of $$\boldsymbol{\theta}$$.  
                2. The gradient $$\nabla{\boldsymbol{\theta}} \tilde{p}\mathbf{x}$$ must exist for all $$\boldsymbol{\theta}$$ and almost all $$\boldsymbol{x}$$.  
                3. There must exist an integrable function $$R\boldsymbol{x}$$ that bounds $$\nabla{\boldsymbol{\theta}} \tilde{p}\mathbf{x}$$ in the sense that $$\max{i}\left\vert\frac{\partial}{\partial \theta{\theta}} \tilde{p}\mathbf{x}\right\vert \leq R\boldsymbol{x}$$ for all $$\boldsymbol{\theta}$$ and almost all $$\boldsymbol{x}$$.   

                Fortunately most machine learning models of interest have these properties..  


    Intuition  
    The Monte Carlo approach to learning provides an intuitive framework in terms of the phases of the learning decomposition  
    
     Positive Phase  
        In the positive phase we increase $$\log \tilde{p}\mathbf{x}$$ for $$\boldsymbol{x}$$ drawn from the data.  
         Parametrize $$\log \tilde{p}$$ in terms of an Energy Function  
            We interpret the positive phase as pushing down on the energy of training examples.   
     Negative Phase  
        In the negative phase we decrease the partition function by decreasing $$\log \tilde{p}\mathbf{x}$$ drawn from the model distribution.  
         Parametrize $$\log \tilde{p}$$ in terms of an Energy Function  
            We interpret the negative phase as pushing up on the energy of samples drawn from the models.  

    

    


4. Stochastic Maximum Likelihood and Contrastive Divergence
    To approximately maximize the log likelihood using the identity derived above we need to use MCMC methods.  

    Motivation  The Naive Approach  
    The naive way to compute the identity above is to approximate it by burning in a set of Markov Chains from a random initialization everytime the gradient is needed.  
    When learning is performed using stochastic gradient descent this means the chains must be burned in once per gradient step.  
    

    The high cost of burning in the Markov chains in the inner loop makes this procedure computationally infeasible.  

    Learning Intuition from naive algorithm  
    
     We can view the MCMC approach to maximum likelihood as trying to achieve balance between two forces  
         One pushing up on the model distribution where the data occurs   
            Corresponds to maximizing $$\log \tilde{p}$$.  
         Another pushing down on the model distribution where the model samples occur.  
            Corresponds to minimizing $$\log Z$$.  
     There are several approximations to the negative phase.  
        Each of these approximations can be understood as making the negative phase computationally cheaper but also making it push down in the wrong locations.  
     Negative Phase Intuition  
         Because the negative phase involves drawing samples from the model’s distribution we can think of it as finding points that the model believes in strongly.  
         Because the negative phase acts to reduce the probability of those points they are generally considered to represent the model’s incorrect beliefs about the world.  
            Referred to in literature as “hallucinations” or “fantasy particles”.  
             
                 In fact the negative phase has been proposed as a possible explanation for dreaming in humans and other animals Crick and Mitchison 1983 the idea being that the brain maintains a probabilistic model of the world and follows the gradient of $$\log \tilde{p}$$ when experiencing real events while awake and follows the negative gradient of $$\log \tilde{p}$$ to minimize $$\log Z$$ while sleeping and experiencing events sampled from the current model. This view explains much of the language used to describe algorithms with a positive and a negative phase but it has not been proved to be correct with neuroscientific experiments.  
                    In machine learning models it is usually necessary to use the positive and negative phase simultaneously rather than in separate periods of wakefulness and REM sleep.  
                    As we will see in section 19.5 other machine learning algorithms draw samples from the model distribution for other purposes and such algorithms could also provide an account for the function of dream sleep.  


    Summary  
    
    The main cost of the naive MCMC algorithm is the cost of burning in the Markov chains from a random initialization at each step.
       


    Contrastive Divergence  
    One way to avoid the high cost in Naive MCMC is to initialize the Markov chains from a distribution that is very close to the model distribution so that the burn in operation does not take as many steps.  
    The Contrastive Divergence CD  or CD $$k$$ to indicate CD with $$k$$ Gibbs steps algorithm initializes the Markov chain at each step with samples from the data distribution Hinton 2000 2010.  
      Obtaining samples from the data distribution is free because they are already available in the dataset.  
      Initially the data distribution is not close to the model distribution so the negative phase is not very accurate.  
      Fortunately the positive phase can still accurately increase the model’s probability of the data.  
      After the positive phase has had some time to act the model distribution is closer to the data distribution and the negative phase starts to become accurate.  
    


    Drawbacks  
    
     Spurious Modes  
        Since CD is still an approximation to the correct negative phase it results in spurious modes; i.e. fails to suppress regions of high probability that are far from actual training examples.  
        Spurious Modes are those regions that have high probability under the model but low probability under the data generating distribution.  
        

          Modes in the distribution that are far from the data distribution will not be visited by Markov chains initialized at training points unless $$k$$ is very large.  
     CD as a Biased Estimator in RBMs and Boltzmann Machines  
          Carreira Perpiñan and Hinton 2005 showed experimentally that the CD estimator is biased for RBMs and fully visible Boltzmann machines in that it converges to different points than the maximum likelihood estimator.  
          They argue that because the bias is small CD could be used as an inexpensive way to initialize a model that could later be fine tuned via more expensive MCMC methods.  
         Interpretation Bengio and Delalleau 2009 show that CD can be interpreted as discarding the smallest terms of the correct MCMC update gradient which explains the bias.  
     Random Gradients  
        Sutskever and Tieleman 2010 showed that the CD update direction is not the gradient of any function.  
        This allows for situations where CD could cycle forever but in practice this is not a serious problem.  
     Difficulty for Deep Models  
         CD is useful for training shallow models like RBMs.  
         The RBMs can be stacked to initialize deeper models like DBNs or DBMs.  
         However CD does NOT provide much help for training deeper models directly.  
             This is because it is difficult to obtain samples of the hidden units given samples of the visible units.  
                 Since the hidden units are not included in the data initializing from training points cannot solve the problem.  
                 Even if we initialize the visible units from the data we will still need to burn in a Markov chain sampling from the distribution over the hidden units conditioned on those visible samples.  


    Relation to Autoencoder Training  
      The CD algorithm can be thought of as penalizing the model for having a Markov chain that changes the input rapidly when the input comes from the data.  
      This means training with CD somewhat resembles autoencoder training.  
      Even though CD is more biased than some of the other training methods it can be useful for pretraining shallow models that will later be stacked.  
    
     This is because the earliest models in the stack are encouraged to copy more information up to their latent variables thereby making it available to the later models.  
        This should be thought of more as an often exploitable side effect of CD training rather than a principled design advantage.  


    Stochastic Maximum Likelihood SML  Persistent Contrastive Divergence PCD PCD $$k$$  
    SML AKA PCD is a method that initializes the Markov Chains in CD at each gradient step with their states from the previous gradient step.  
    This strategy resolves many of the problems with CD.  
    Idea  
    
     The basic idea of this approach is that as long as the steps taken by the stochastic gradient algorithm are small the model from the previous step will be similar to the model from the current step.  
     It follows that the samples from the previous model’s distribution will be very close to being fair samples from the current model’s distribution so a Markov chain initialized with these samples will not require much time to mix.  
    


    Advantages  
    
     SML is considerably more resistant to forming models with spurious modes than CD is  
        Because each Markov chain is continually updated throughout the learning process rather than restarted at each gradient step the chains are free to wander far enough to find all the model’s modes.  
     SML is able to train deep models efficiently  
         SML provides an initialization point for both the hidden and the visible units  
            Because it is possible to store the state of all the sampled variables whether visible or latent.  
         CD is only able to provide an initialization for the visible units and therefore requires burn in for deep models.  
     Performance/Results  In Practice  
        Marlinet al. 2010 compared SML to many other criteria presented in this section. They found that  
         SML results in the best test set log likelihood for an RBM and that 
         if the RBM’s hidden units are used as features for an SVM classifier SML results in the best classification accuracy.  


    Mixing Evaluation  
    


    Sample Evaluation  
    
    Care must be taken when evaluating the samples from a model trained with SML. It is necessary to draw the samples starting from a fresh Markov chain initialized from a random starting point after the model is done training. The samples present in the persistent negative chains used for training have been influenced by several recent versions of the model and thus can make the model appear to have greater capacity than it actually does.


    Bias Variance of CD and SML  
    Berglund and Raiko 2013 performed experiments to examine the bias and variance in the estimate of the gradient provided by CD and SML  
    
     CD proves to have lower variance  than the estimator based on exact sampling.  
        The cause of CD's low variance is its use of the same training points in both the positive and negative phase.  
        If the negative phase is initialized from different training points the variance rises above that of the estimator based on exact sampling.   
     SML has higher variance.  


    Improving CD & SML  
    
     MCMC Algorithms  
        All these methods based on using MCMC to draw samples from the model canin principle be used with almost any variant of MCMC. This means that techniques such as SML can be improved by using any of the enhanced MCMC techniques described in chapter 17 such as parallel tempering Desjardins et al. 2010; Choet al. 2010.  
     Fast PCD FPCD  
        Another approach to accelerating mixing during learning relies not on changing the Monte Carlo sampling technology but rather on changing the parametrization of the model and the cost function.  
        FPCD is such a method that involves replacing the parameters $$\boldsymbol{\theta}$$ of a traditional model with an expression  
        $$\boldsymbol{\theta}=\boldsymbol{\theta}^{\mathrm{slow}}+\boldsymbol{\theta}^{\mathrm{fast}}$$  
        



    Training with Positive Phase Estimators bound based variational methods  
    One key benefit to the MCMC based methods described in this section is that they provide an estimate of the gradient of $$\log Z$$ and thus we can essentially decompose the problem into the $$\log \tilde{p}$$ contribution and the $$\log Z$$ contribution.  
    We can then use any other method to tackle $$\log \tilde{p}\mathbf{x}$$ and just add our negative phase gradient onto the other method’s gradient.  
    In particular this means that our positive phase can make use of methods that provide only a lower bound on $$\tilde{p}$$.  
    Most of the other methods of dealing with $$\log Z$$ presented in this chapter are incompatible with bound based positive phase methods.  
    


5. Pseudolikelihood
    Motivation  
    We can sidestep the issue of approximating the intractable partition function by training the model without computing it at all.  

    Idea  
    Most of these approaches are based on the observation that it is easy to compute ratios of probabilities in an undirected model.  
    This is because the partition function appears in both the numerator and the denominator of the ratio and cancels out  
    $$\frac{p\mathbf{x}}{p\mathbf{y}}=\frac{\frac{1}{Z} \tilde{p}\mathbf{x}}{\frac{1}{Z} \tilde{p}\mathbf{y}}=\frac{\tilde{p}\mathbf{x}}{\tilde{p}\mathbf{y}}$$  

    Pseudolikelihood  
    The Pseudolikelihood is an objective function based on predicting the value of feature $$x  {i}$$ given all the other features $$\boldsymbol{x} { i}$$  
    $$\sum{i=1}^{n} \log p\leftx{i} \vert \boldsymbol{x} { i}\right$$  

    Derivation  
    
     The pseudolikelihood is based on the observation that conditional probabilities take this ratio based form and thus can be computed without knowledge of the partition function.  
     


    Computational Cost  
    
     If each random variable has $$k$$ different values this requires only $$k \times n$$ evaluations of $$\tilde{p}$$ to compute  
     as opposed to the $$k^{n}$$ evaluations needed to compute the partition function.  

    Justification  
    Estimation by maximizing the pseudolikelihood is asymptotically consistent Mase 1995.  
    When the datasets do not approach the large sample limit pseudolikelihood may display different behavior from the maximum likelihood estimator.  

    Generalized Pseudolikelihood Estimator  
    The Generalized Pseudolikelihood Estimator gives us a way to trade off computational complexity for deviation from maximum likelihood behavior.  
    The GPE objective function  
    $$\sum{i=1}^{m} \log p\left\mathbf{x}{\mathbb{S}^{i}} \vert \mathbf{x}{ \mathbb{S}^{i}}\right$$    
    Complexity Consistency Tradeoff  
    It uses $$m$$ different sets $$\mathbb{S}^{i} i=1 \ldots m$$ of indices of variables that appear together on the left side of the conditioning bar  
    
     In the extreme case of $$m=1$$ and $$\mathbb{S}^{1}=1 \ldots n$$ the generalized pseudolikelihood recovers the log likelihood.  
     In the extreme case of $$m=n$$ and $$\mathbb{S}^{i}=\{i\}$$ the generalized pseudolikelihood recovers the pseudolikelihood.  


    Performance  
    The performance of pseudolikelihood based approaches depends largely on how the model will be used  
    
     Pseudolikelihood tends to perform poorly on tasks that require a good model of the full joint $$p\mathbf{x}$$ such as density estimation and sampling. 
     It can perform better than maximum likelihood for tasks that require only the conditional distributions used during training such as filling in small amounts of missing values.  
     Generalized pseudolikelihood techniques are especially powerful if the data has regular structure that allows the $$\mathbb{S}$$ index sets to be designed to capture the most important correlations while leaving out groups of variables that have only negligible correlation.  
        For example in natural images pixels that are widely separated in space also have weak correlation so the generalized pseudolikelihood can be applied with each $$\mathbb{S}$$ set being a small spatially localized window.  

    Drawbacks  Training with Lower Bound Maximization Methods  
    
     One weakness of the pseudolikelihood estimator is that it cannot be used with other approximations that provide only a lower bound on $$\tilde{p}\mathbf{x}$$ e.g. variational inference.  
         This is because $$\tilde{p}$$ appears in the denominator.  
            A lower bound on the denominator provides only an upper bound on the expression as a whole and there is no benefit to maximizing an upper bound.  
            This makes it difficult to apply pseudolikelihood approaches to deep models such as deep Boltzmann machines since variational methods are one of the dominant approaches to approximately marginalizing out the many layers of hidden variables that interact with each other.  
     Nonetheless pseudolikelihood is still useful for deep learning because it can be used to train single layer models or deep models using approximate inference methods that are not based on lower bounds.  

    Pseudolikelihood vs SML/PCD  Computational Cost  
    Pseudolikelihood has a much greater cost per gradient step than SML due to its explicit computation of all the conditionals.  
    But generalized pseudolikelihood and similar criteria can still perform well if only one randomly selected conditional is computed per example Goodfellow et al. 2013b thereby bringing the computational cost down to match that of SML.  


    Relation to the Negative Phase  
    Though the pseudolikelihood estimator does not explicitly minimize $$\log Z$$ it can still be thought of as having something resembling a negative phase.  
    The denominators of each conditional distribution result in the learning algorithm suppressing the probability of all states that have only one variable differing from a training example.  

    Asymptotic Efficiency  
    See Marlin and de Freitas 2011 for a theoretical analysis of the asymptotic efficiency of pseudolikelihood.  
    

6. Score Matching and Ratio Matching
    
    


    Denoising Score Matching  
    

    

7. Noise Contrastive Estimation NCE
    Noise Contrastive Estimation NCE is a method for computing the partition function as a learned parameter in the model; where the probability distribution estimated by the model is represented explicitly as  
    $$\log p{\text {model }}\mathbf{x}=\log \tilde{p} {\text {model}}\mathbf{x} ; \boldsymbol{\theta}+c$$  
    where $$c$$ is explicitly introduced as an approximation of $$ \log Z\boldsymbol{\theta}$$.  

    Rather than estimating only $$\boldsymbol{\theta}$$ the noise contrastive estimation procedure treats $$c$$ as just another parameter and estimates $$\boldsymbol{\theta}$$ and $$c$$ simultaneously using the same algorithm for both.  
    The resulting $$\log p{\text {model}}\mathbf{x}$$ thus may not correspond exactly to a valid probability distribution but it will become closer and closer to being valid as the estimate of $$c$$ improves.^1  

    Derivation  
    
    Problem with Maximum Likelihood Criterion  
        Such an approach would not be possible using maximum likelihood as the criterion for the estimator.  
        The maximum likelihood criterion would choose to set $$c$$ arbitrarily high rather than setting $$c$$ to create a valid probability distribution.  
    Solution  New Estimator of the original problem  
        NCE works by reducing the unsupervised learning problem of estimating $$p\mathrm{x}$$ to that of learning a probabilistic binary classifier in which one of the categories corresponds to the data generated by the model.  
        This supervised learning problem is constructed in such a way that maximum likelihood estimation defines an asymptotically consistent estimator of the original problem.  

        Specifically  
        1. Posit two distributions the model and a noise distribution.
            The Noise Distribution $$p{\text{noise}}\mathbf{x}$$  
                We introduce a new distribution $$p{\text{noise}}\mathbf{x}$$ over the noise.  
                The noise distribution should be tractable to evaluate and to sample from.    
        2. Construct a new joint model over both $$\boldsymbol{x}$$ and a binary variable $$y$$  
             We can now construct a model over both $$\mathbf{x}$$ and a new binary class variable $$y$$. In the new joint model we specify that  
                1 $$p{\mathrm{joint}}y=1=\frac{1}{2}$$  
                2 $$p{\mathrm{joint}}\mathbf{x} \vert y=1=p{\mathrm{model}}\mathbf{x}$$  
                3 $$p{\mathrm{joint}}\mathbf{x} \vert y=0=p{\mathrm{noise}}\mathbf{x}$$  
                In other words $$y$$ is a switch variable that determines whether we will generate $$\mathbf{x}$$ from the model or from the noise distribution.  
             Equivalently We can construct a similar joint model of training data.  
                Formally 
                1 $$p{\text {train}}y=1=\frac{1}{2}$$  
                2 $$p{\text {train}}\mathbf{x} \vert y=1=p{\text {data }}\mathbf{x}$$  
                3 $$p{\text {train}}\mathbf{x} \vert y=0=p{\text {noise}}\mathbf{x}$$  
                In this case the switch variable determines whether we draw $$\mathbf{x}$$ from the data or from the noise distribution.  
        3. Construct the new supervised Binary Classification Task  fitting $$p{\text {joint}}$$ to $$p{\text {train}}$$  
            We can now just use standard maximum likelihood learning on the supervised learning problem of fitting $$p{\text {joint}}$$ to $$p{\text {train}}$$ by swapping $$p{\text {model}}$$ with $$p{\text {joint}}$$  
            $$\boldsymbol{\theta} c=\underset{\boldsymbol{\theta} c}{\arg \max } \mathbb{E}{\mathbf{x} \mathbf{y} \sim p{\text {train}}} \log p{\text {joint}}y \vert \mathbf{x}$$  
             Expanding $$p{\text{joint}}y \vert x$$  
                The distribution $$p{\text{joint}}$$ is essentially a logistic regression model applied to the difference in log probabilities of the model and the noise distribution  
                $$\begin{aligned}  
                    p{\text {joint}}y=1 \vert \mathbf{x} &= \frac{p{\text {model }}\mathbf{x}}{p{\text {model }}\mathbf{x}+p{\text {noise}}\mathbf{x}} 
                    &= \frac{1}{1+\frac{p{\text {noise}}\mathbf{x}}{p{\text {model}} \mathbf{x}}}  
                    &= \frac{1}{1+\exp \left\log \frac{p{\text {noise}}\mathbf{x}}{p{\text {model }}\mathbf{x}}\right} 
                    &= \sigma\left \log \frac{p{\text {noise}}\mathbf{x}}{p{\text {model }}\mathbf{x}}\right 
                    &= \sigma\left\log p{\text {model }}\mathbf{x} \log p{\text {noise}}\mathbf{x}\right 
                    \end{aligned}$$     
     


    Summary  
    
    1. Posit two distributions the model and a noise distribution.
    2. Given a data point predict from which distribution this point was generated.  

    NCE is thus simple to apply as long as  $$ \log \tilde{p}{\text {model}}$$ is easy to back propagate through and as specified above $$p{\text {noise}}$$ is easy to evaluate in order to evaluate $$p{\text {joint}}$$ and sample from to generate the training data.  

    The Noise Distribution  
    
     Practical Implications and Complexity  
     Better Distributions  Parametric $$p{\text{noise}}$$  
        The noise distribution is generally non parametric.  
        However there is nothing stopping us from evolving this distribution and giving it trainable parameters then updating these parameters such that it generates increasingly "optimal" samples.  
         Optimality  
            Of course we would have to design what "optimal" means.  
             Adversarial Contrastive Estimation  



    Weaknesses/Drawbacks  
    
     Problems with Many RVs  
        When NCE is applied to problems with many random variables it becomes less efficient.  
         The logistic regression classifier can reject a noise sample by identifying any one variable whose value is unlikely.  
            This means that learning slows down greatly after $$p{\text {model}}$$ has learned the basic marginal statistics.  
         Imagine learning a model of images of faces using unstructured Gaussian noise as $$p{\text {noise}}$$.  
            If $$p{\text {model }}$$ learns about eyes it can reject almost all unstructured noise samples without having learned anything about other facial features such as mouths.  
     Noise Distribution Complexity  
        The constraint that $$p{\text {noise}}$$ must be easy to evaluate and easy to sample from can be overly restrictive  
         For our training data we require the ability to sample from our noise distribution..  
         For our target we require the ability to compute the likelihood of some data under our noise distribution.  

        When $$p{\text {noise}}$$ is simple most samples are likely to be too obviously distinct from the data to force $$p{\text {model}}$$ to improve noticeably.  
     Training with Lower Bound Maximizing Methods  
        NCE does not work if only a lower bound on $$\tilde{p}$$ is available.  
        Such a lower bound could be used to construct a lower bound on $$p{\text {joint}}y=1 \vert \mathbf{x}$$ but it can only be used to construct an upper bound on $$p{\text {joint}}y=0 \vert \mathbf{x}$$ which appears in half the terms of the NCE objective.  
        Likewise a lower bound on $$p{\text {noise}}$$ is not useful because it provides only an upper bound on $$p{\text {joint}}y=1 \vert \mathbf{x}$$.  


    Self Contrastive Estimation  
    When the model distribution is copied to define a new noise distribution before each gradient step NCE defines a procedure called self contrastive estimation whose expected gradient is equivalent to the expected gradient of maximum likelihood Goodfellow 2014.  
    Interpretation  
    
     Self Contrastive Estimation  
        The special case of NCE where the noise samples are those generated by the model suggests that maximum likelihood can be interpreted as a procedure that forces a model to constantly learn to distinguish reality from its own evolving beliefs   
     NCE  
        However NCE achieves some reduced computational cost by only forcing the model to distinguish reality from a fixed baseline noise model.  


    Connection to Importance Sampling  
    Jozefowicz et al. 2016 show that NCE and IS are not only similar as both are sampling based approaches but are strongly connected.  
    While NCE uses a binary classification task they show that IS can be described similarly using a surrogate loss function Instead of performing binary classification with a logistic loss function like NCE IS then optimises a multi class classification problem with a softmax and cross entropy loss function.  
    They observe that as IS performs multi class classification it may be a better choice for language modeling as the loss leads to tied updates between the data and noise samples rather than independent updates as with NCE.  
    Indeed Jozefowicz et al. 2016 use IS for language modeling and obtain state of the art performance on the 1B Word benchmark.  


    Relation to Generative Adversarial Networks GANs  
    Noise contrastive estimation is based on the idea that a good generative model should be able to distinguish data from noise.  
    A closely related idea is that a good generative model should be able to generate samples that no classifier can distinguish from data.  
    This idea yields generative adversarial networks.  

    Self Normalization  
    Mnih and Teh 2012 and Vaswani et al. 2013 fix $$c = 1$$.  
    They report does not affect the model's performance.  
    This assumption has the nice side effect of reducing the model's parameters while ensuring that the model self normalises by not depending on the explicit normalisation in $$c$$.  
    Indeed Zoph et al. 2016 find that even when learned $$c$$ is close to $$1$$ and has low variance.  
    

8. Negative Sampling
    Negative Sampling NEG can be seen as an approximation to NCE.  
    As we have mentioned above NCE can be shown to approximate the loss of $$\log p{\text{model}}$$ as the number of samples $$k$$ increase.  
    NEG simplifies NCE and does away with this guarantee as the objective of NEG is to learn high quality word representations rather than achieving low perplexity on a test set as is the goal in language modeling.  

    The key difference to NCE is that NEG only approximates this probability by making it as easy to compute as possible.  
    It simplifies NCE as follows  
    
    1. Considers noise distributions whose likelihood we cannot evaluate  
    2. To accommodate it simply set the most expensive term $$p{\text {noise}}x=1$$  
        Equivalently $$k\p{\text {noise}}x=1$$  
         
             Thus Expanding $$p{\text{joint}}y \vert x$$  
                The distribution $$p{\text{joint}}$$ is essentially a logistic regression model applied to the difference in log probabilities of the model and the noise distribution  
                $$\begin{aligned}  
                    p{\text {joint}}y=1 \vert \mathbf{x} &= \frac{p{\text {model }}\mathbf{x}}{p{\text {model }}\mathbf{x}+p{\text {noise}}\mathbf{x}} 
                    &= \frac{1}{1+\frac{p{\text {noise}}\mathbf{x}}{p{\text {model}} \mathbf{x}}}  
                    &= \frac{1}{1+\exp \left\log \frac{p{\text {noise}}\mathbf{x}}{p{\text {model }}\mathbf{x}}\right} 
                    &= \sigma\left \log \frac{p{\text {noise}}\mathbf{x}}{p{\text {model }}\mathbf{x}}\right 
                    &= \sigma\left \log \frac{1}{p{\text {model}}\mathbf{x}}\right 
                    &= \sigma\left\log p{\text {model}}\mathbf{x}\right 
                    \end{aligned}$$  
  

    Equivalence with NCE  
    
     $$k p{\text {noise}}=1$$ is exactly then true when discrete  
        1. $$k=\vert X\vert$$ and  
        2. $$p{\text {noise}}$$ is a uniform distribution.  
        
        In this case NEG is equivalent to NCE.  
     The reason we set $$k p{\text {noise}}=1$$ and not to some other constant can be seen by rewriting the equation as $$Py=1 \vert  \mathbf{x}$$ can simplify the sigmoid function.  
     In all other cases NEG only approximates NCE which means that it will not directly optimize the likelihood $$\log p{\text {model}}\mathbf{x}$$.  
     Asymptotic Consistency  
        Since NEG only approximates NCE it lacks any asymptotic consistency guarantees.  

    Application  Language Modeling and Word Embeddings  
    NEG only approximates NCE which means that it will not directly optimise the likelihood of correct words which is key for language modelling. While NEG may thus be useful for learning word embeddings its lack of asymptotic consistency guarantees makes it inappropriate for language modelling.  
    

9. Self Normalization
    Remember from NCE that we decomposed the log likelihood of the model as  
    $$\log p{\text {model }}\mathbf{x}=\log \tilde{p} {\text {model}}\mathbf{x} ; \boldsymbol{\theta}+c$$  
    where $$c$$ is explicitly introduced as an approximation of $$ \log Z\boldsymbol{\theta}$$.  

    If we are able to constrain our model so that it sets $$c=0$$ i.e. $$e^c = 1$$ then we can avoid computing the normalization in $$c$$ altogether.  
    Devlin et al. 2014 thus propose to add a squared error penalty term to the loss function that encourages the model to keep $$c$$ as close as possible to $$0$$  
    $$\tilde{J} = J + \lambda c 0^{2}$$  
    where $$\lambda$$ allows us to trade off between model accuracy and mean self normalisation.  

    At inference time we set  
    $$p{\text {model }}\mathbf{x}=\dfrac{\tilde{p} {\text {model}}\mathbf{x} ; \boldsymbol{\theta}}{Z\boldsymbol{\theta}} \approx \dfrac{\tilde{p} {\text {model}}\mathbf{x} ; \boldsymbol{\theta}}{1} = \tilde{p} {\text {model}}\mathbf{x} ; \boldsymbol{\theta}$$  

    Results  MT  
    They report that self normalisation achieves a speed up factor of about 15 while only resulting in a small degradation of BLEU scores compared to a regular non self normalizing neural language model.  

    Notes  
    

    





Estimating the Partition Function

1. Estimating the Partition Function
    
    


2. Annealed Importance Sampling AIS
    
    


3. Bridge Sampling
    
    


4. Linked Importance Sampling LIS
    
    


5. Estimating the Partition Function while Training
    
    



^1 NCE is also applicable to problems with a tractable partition function where there is no need to introduce the extra parameter $$c$$. However it has generated the most interest as a means of estimating models with difficult partition functions.  


 TensorFlow 


Calculus of Variations

1. Calculus of Variations


22.Functional
    A Functional 
    

2. Functional Derivative
    The Functional Derivative relates a change in a functional to a change in a function on which the functional depends.  
    In an integral $$L$$ of a functional if a function $$f$$ is varied by adding to it another function $$\delta f$$ that is arbitrarily small and the resulting integrand is expanded in powers of $$\delta f$$ the coefficient of $$\delta$$ in the  order term is called the functional derivative.  
    Consider the functional  
    $$Jf=\int{a}^{b} L\leftx fx f^{\prime}x\right d x$$  
    where $$f^{\prime}x \equiv d f / d x .$$ If is varied by adding to it a function $$\delta f$$ and the resulting integrand $$L\leftx f+\delta f f^{\prime}+\delta f^{\prime}\right$$ is expanded in  powers of  $$\delta f$$ then the change in the value of $$ J$$ to  order in $$\delta f$$ can be expressed as follows  
    $$\delta J=\int{a}^{b} \frac{\delta J}{\delta fx} \delta fx d x$$  
    The coefficient of $$\delta fx$$ denoted as $$\delta J / \delta fx$$ is called the functional derivative of $$J$$ with respect to $$f$$ at the point $$x$$.  
    The functional derivative is the left hand side of the Euler Lagrange equation  
    $$\frac{\delta J}{\delta fx}=\frac{\partial L}{\partial f} \frac{d}{d x} \frac{\partial L}{\partial f^{\prime}}$$  


    Formal Description  
    The definition of a functional derivative may be made more mathematically precise and formal by defining the space of functions more carefully  
    
     Banach Space the functional derivative is the Fréchet derivative  
     Hilbert Space Hilbert is special case of Banach Fréchet derivative  
     General Locally Convex Spaces the functional derivative is the Gateaux derivative  


    Properties  
    
     Linearity  
        $$\frac{\delta\lambda F+\mu G\rho}{\delta \rhox}=\lambda \frac{\delta F\rho}{\delta \rhox}+\mu \frac{\delta G\rho}{\delta \rhox}$$  
        where $$\lambda \mu$$ are constants.  
     Product Rule  
        $$\frac{\deltaF G\rho}{\delta \rhox}=\frac{\delta F\rho}{\delta \rhox} G\rho+F\rho \frac{\delta G\rho}{\delta \rhox}$$  
     Chain Rule  
         If $$F$$ is a functional and $$G$$ another functional  
            $$\frac{\delta FG\rho}{\delta \rhoy}=\int d x \frac{\delta FG}{\delta Gx} {G=G\rho} \cdot \frac{\delta G\rhox}{\delta \rhoy}$$  
         If $$G$$ is an ordinary differentiable function local functional $$g$$ then this reduces to  
            $$\frac{\delta Fg\rho}{\delta \rhoy}=\frac{\delta Fg\rho}{\delta g\rhoy} \frac{d g\rho}{d \rhoy}$$  

    Formula for Determining the Functional Derivative  
    We present a formula to determine functional derivatives for a common class of functionals that can be written as the integral of a function and its derivatives  
    Given a functional $$F\rho=\int f\boldsymbol{r} \rho\boldsymbol{r} \nabla \rho\boldsymbol{r} d \boldsymbol{r}$$ and a function $$\phi\boldsymbol{r}$$ that vanishes on the boundary of the region of integration the functional derivative is  
    $$\frac{\delta F}{\delta \rho\boldsymbol{r}}=\frac{\partial f}{\partial \rho} \nabla \cdot \frac{\partial f}{\partial \nabla \rho}$$  
    where $$\rho=\rho\boldsymbol{r}$$ and $$f=f\boldsymbol{r} \rho \nabla \rho$$.  

    Notes  
    

    

3. Euler Lagrange Equation
    
    Generalization to Manifolds  
    



    Beltrami Identity  
    Beltrami Identity is a special case of the Euler Lagrange Equation where $$\partial L / \partial x=0$$ defined as  
    $$L f^{\prime} \frac{\partial L}{\partial f^{\prime}}=C$$  
    where $$C$$ is a constant.  

    It is applied to many problems where the condition is satisfied like the Brachistochrone problem.  

    Notes  
    

    





 Loss Functions






# Loss Functions


###Loss Functions
Abstractly a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some "cost" associated with the event.  

Formally a loss function is a function $$L \hat{y} y \in \mathbb{R} \times Y \longmapsto L\hat{y} y \in \mathbb{R}$$  that takes as inputs the predicted value $$\hat{y}$$ corresponding to the real data value $$y$$ and outputs how different they are.  


# Loss Functions for Regression



###Introduction
Regression Losses usually only depend on the residual $$r = y  \hat{y}$$ i.e. what you have to add to your prediction to match the target  

Distance Based Loss Functions  
A Loss function $$L\hat{y} y$$ is called distance based if it  
 Only depends on the residual  
    $$L\hat{y} y = \psiy \hat{y}  \\ \text{for some } \psi  \mathbb{R} \longmapsto \mathbb{R}$$  
 Loss is $$0$$ when residual is $$0$$  
    $$\psi0 = 0$$  

Translation Invariance  
Distance based losses are translation invariant  
$$L\hat{y}+a y+a = L\hat{y} y$$  

> Sometimes Relative Error $$\dfrac{\hat{y} y}{y}$$ is a more natural loss but it is NOT translation invariant  




###MSE
The MSE minimizes the sum of squared differences between the predicted values and the target values.  
$$L\hat{y} y = \dfrac{1}{n} \sum{i=1}^{n}\lefty{i} \hat{y} {i}\right^{2}$$  




###MAE
The MAE minimizes the sum of absolute differences between the predicted values and the target values.  
$$L\hat{y} y = \dfrac{1}{n} \sum{i=1}^{n}\vert y{i} \hat{y} {i}\vert$$  

Properties  
 Solution may be Non unique  
 Robustness to outliers  
 Unstable Solutions{ }    
    

    The instability property of the method of least absolute deviations means that for a small horizontal adjustment of a datum the regression line may jump a large amount. The method has continuous solutions for some data configurations; however by moving a datum a small amount one could “jump past” a configuration which has multiple solutions that span a region. After passing this region of solutions the least absolute deviations line has a slope that may differ greatly from that of the previous line. In contrast the least squares solutions is stable in that for any small adjustment of a data point the regression line will always move only slightly; that is the regression parameters are continuous functions of the data. 
  
 Data points "Latching"^3  
     Unique Solution  
        If there are $$k$$ features including the constant then at least one optimal regression surface will pass through $$k$$ of the data points; unless there are multiple solutions.  
     Multiple Solutions  
        The region of valid least absolute deviations solutions will be bounded by at least $$k$$ lines each of which passes through at least $$k$$ data points.  

 


###Huber Loss

AKA Smooth Mean Absolute Error  
$$L\hat{y} y = \left\{\begin{array}{cc}{\frac{1}{2}y \hat{y}^{2}} & {\text { if }|y \hat{y}|<\delta}  {\deltay \hat{y} \frac{1}{2} \delta} & {\text { otherwise }}\end{array}\right.$$  

Properties  
 It’s less sensitive

Code  
```python
def HuberyHat y delta=1.
    return np.wherenp.absy yHat < delta.5y yHat2  deltanp.absy yHat 0.5delta
```


###KL Divergence


$$L\hat{y} y = $$  



###Analysis and Discussion
MSE vs MAE  

| MSE | MAE |
| Sensitive to outliers | Robust to outliers |
| Differentiable Everywhere | Non Differentiable at $$0$$ |
| Stable^1 Solutions | Unstable Solutions |
| Unique Solution | Possibly multiple^2 solutions |

 Statistical Efficiency  
     "For normal observations MSE is about $$12\%$$ more efficient than MAE"  Fisher  
     $$1\%$$ Error is enough to make MAE more efficient  
     2/1000 bad observations make the median more efficient than the mean  
 Subgradient methods are slower than gradient descent  
     you get a lot better convergence rate guarantees for MSE  






###Notes





# Loss Functions for Classification



###$$0 1$$ Loss

$$L\hat{y} y = I\hat{y} \neq y = \left\{\begin{array}{ll}{0} & {\hat{y}=y}  {1} & {\hat{y} \neq y}\end{array}\right.$$  


###MSE

We can write the loss in terms of the margin $$m = y\hat{y}$$  
$$L\hat{y} y=y  \hat{y}^{2}=1 y\hat{y}^{2}=1 m^{2}$$   
> Since $$y \in { 11} \implies y^2 = 1$$  


$$L\hat{y} y = 1 y \hat{y}^{2}$$  







###Hinge Loss

$$L\hat{y} y = \max 01 y \hat{y}=|1 y \hat{y}| {+}$$  

Properties  
 Continuous Convex Non Differentiable  



###Logistic Loss

AKA Log Loss Logarithmic Loss  

$$L\hat{y} y = \log{\left1+e^{ y \hat{y}}\right}$$  


Properties  

 The logistic loss function does not assign zero penalty to any points. Instead functions that correctly classify points with high confidence i.e. with high values of $${\displaystyle \vert f{\vec {x}}\vert}$$ are penalized less. This structure leads the logistic loss function to be sensitive to outliers in the data.  



###Cross Entropy Log Loss

$$L\hat{y} y =  \sum{i} yi \log \left\hat{y} {i}\right$$  

Binary Cross Entropy  
$$L\hat{y} y =  \lefty \log \hat{y}+\left1 y\right \log \left1 \hat{y} {n}\right\right$$  



Cross Entropy and Negative Log Probability  
The Cross Entropy is equal to the Negative Log Probability of predicting the true class in the case that the true distribution that we are trying to match is peaked at a single point and is identically zero everywhere else; this is usually the case in ML when we are using a one hot encoded vector with one class $$y = 0 \ 0 \ \ldots \ 0 \ 1 \ 0 \ \ldots \ 0$$ peaked at the $$j$$ th position   
$$\implies$$  
$$L\hat{y} y =  \sum{i} yi \log \left\hat{y} {i}\right =  \log \hat{y} {j}$$  

Cross Entropy and Log Loss    
The Cross Entropy is equal to the Log Loss in the case of $$0 1$$ classification.  

Equivalence of Binary Cross Entropy and Logistic Loss  
Given $$p \in\{y 1 y\}$$ and $$q \in\{\hat{y} 1 \hat{y}\}$$  
$$Hpq= \sum{x }px\\log qx =  y \log \hat{y} 1 y \log 1 \hat{y} = L\hat{y} y$$  

 Notice the following property of the logistic function $$\sigma$$ used in derivation below   
    $$\sigma x = 1 \sigmax$$  


 onclick="showTextwithParentPopHideevent;"}
  
 $$\hat{y} = \sigmayfx$$^5  
 $$y \in \{ 1 1\}$$   
 $$\hat{y}' = \sigmafx$$  
 $$y' = 1+y/2 = \left\{\begin{array}{ll}{1} & {\text { for }} y' = 1  {0} & {\text { for }} y =  1\end{array}\right. \in \{0 1\}$$^4   
 We start with the modified binary cross entropy  
    $$\begin{aligned}  y' \log \hat{y}' 1 y' \log 1 \hat{y}' &= \left\{\begin{array}{ll}{ \log\hat{y}'} & {\text { for }} y' = 1  { \log1 \hat{y}'} & {\text { for }} y' = 0\end{array}\right.  
    &= \left\{\begin{array}{ll}{ \log\sigmafx} & {\text { for }} y' = 1  { \log1 \sigmafx} & {\text { for }} y' = 0\end{array}\right.  
    &= \left\{\begin{array}{ll}{ \log\sigma1\times fx} & {\text { for }} y' = 1  { \log\sigma 1\times fx} & {\text { for }} y' = 0\end{array}\right.  
    &= \left\{\begin{array}{ll}{ \log\sigmayfx} & {\text { for }} y' = 1  { \log\sigmayfx} & {\text { for }} y' = 0\end{array}\right.  
    &= \left\{\begin{array}{ll}{ \log\hat{y}} & {\text { for }} y' = 1  { \log\hat{y}} & {\text { for }} y' = 0\end{array}\right.  
    &=  \log\hat{y}  
    &= \log\left\dfrac{1}{\hat{y}}\right  
    &= \log\left\hat{y}^{ 1}\right  
    &= \log\left\sigmayfx^{ 1}\right  
    &= \log\left \left\dfrac{1}{1+e^{ yfx}}\right^{ 1}\right  
    &= \log \left1+e^{ yfx}\right\end{aligned}$$  





Cross Entropy as Negative Log Likelihood w/ equal probability outcomes  


Cross Entropy and KL Div  
When comparing a distribution $${\displaystyle q}$$ against a fixed reference distribution $${\displaystyle p}$$ cross entropy and KL divergence are identical up to an additive constant since $${\displaystyle p}$$ is fixed both take on their minimal values when $${\displaystyle p=q}$$ which is $${\displaystyle 0}$$ for KL divergence and $${\displaystyle \mathrm {H} p}$$ for cross entropy.  
> Basically minimizing either will result in the same solution.  


Cross Entropy VS MSE & Classification Loss  
Basically CE > MSE because the gradient of MSE $$z1 z$$ leads to saturation when then output $$z$$ of a neuron is near $$0$$ or $$1$$ making the gradient very small and thus slowing down training.  
CE > Class Loss because Class Loss is binary and doesn't take into account "how well" are we actually approximating the probabilities as opposed to just having the target class be slightly higher than the rest e.g. $$c1=0.3 c2=0.3 c3=0.4$$.  






###Exponential Loss

$$L\hat{y} y = e^{ \beta y \hat{y}}$$  


###Perceptron Loss

$${\displaystyle Lz yi = {\begin{cases}0&{\text{if }}\ yi\cdot zi \geq 0 yi z&{\text{otherwise}}\end{cases}}}$$  



###Notes





^1 Stability  
^2 Reason is that the errors are equally weighted; so tilting the regression line within a region will decrease the distance to a particular point and will increase the distance to other points by the same amount.  

^4 We have to redefine the indicator/target variable to establish the equality.  
^5 $$fx = w^Tx$$ in logistic regression  


 Practical Concepts in Machine Learning




1. Data Snooping

    The Principle  
    If a data set has affected any step in the learning process its ability to assess the outcome has been compromised.  

    Analysis  
    
     Making decision by examining the dataset makes you a part of the learning algorithm.  
        However you didn't consider your contribution to the learning algorithm when making e.g. VC Analysis for Generalization.  
     Thus you are vulnerable to designing the model or choices of learning according to the idiosyncrasies of the dataset.  
     The real problem is that you are not "charging" for the decision you made by examining the dataset.    

    What's allowed?  
    
     You are allowed even encouraged to look at all other information related to the target function and input space.  
        e.g. number/range/dimension/scale/etc. of the inputs correlations properties monotonicity etc.  
     EXCEPT for the specific realization of the training dataset.  


    Manifestations of Data Snooping with Examples one/manifestation  
    
     Changing the Parameters of the model Tricky  
         Complexity  
            Decreasing the order of the fitting polynomial by observing geometric properties of the training set.  
     Using statistics of the Entire Dataset Tricky  
         Normalization  
            Normalizing the data with the mean and variance of the entire dataset training+testing.  
             E.g. In Financial Forecasting; the average affects the outcome by exposing the trend.  
     Reuse of a Dataset  
        If you keep Trying one model after the other on the same data set you will eventually 'succeed'.  
        "If you torture the data long enough it will confess".  
        This bad because the final model you selected is the union of all previous models since some of those models were rejected by you a learning algorithm.  
         Fixed deterministic training set for Model Selection  
            Selecting a model by trying many models on the same fixed deterministic Training dataset.  
     Bias via Snooping  
        By looking at the data in the future when you are not allowed to have the data it wouldn't have been possible; you are creating sampling bias caused by "snooping".  
         E.g. Testing a Trading algorithm using the currently traded companies in S&P500.  
            You shouldn't have been able to know which companies are being currently traded future.  



    Remedies/Solutions to Data Snooping  
    
    1. Avoid Data Snooping  
        A strict discipline very hard.  
    2. Account for Data Snooping  
        By quantifying "How much data contamination".  

    


2. Mismatched Data

3. Mismatched Classes

4. Sampling Bias
    Sampling Bias occurs when $$\exists$$ Region with zero probability $$P=0$$ in training but with positive probability $$P>0$$ in testing.  

    The Principle  
    If the data is sampled in a biased way learning will produce a similarly biased outcome.  

    Example 1948 Presidential Elections  
    
     Newspaper conducted a Telephone poll between Jackson and Truman  
     Jackson won the poll decisively.  
     The result was NOT unlucky  
        No matter how many times the poll was re conducted and no matter how many times the sample sized is increased; the outcome will be fixed.  
     The reason is the Telephone  
        1 Telephones were expensive and only rich people had Telephones.  
        2 Rich people favored Jackson.  
        Thus the result was well reflective of the mini population being sampled.  

    How to sample  
    Sample in a way that matches the distributions of train and test samples.  

    The solution Fails doesn't work if  
    $$\exists$$ Region with zero probability $$P=0$$ in training but with positive probability $$P>0$$ in testing.  
    This is when sampling bias exists.  


    Notes  
    
     Medical sources sometimes refer to sampling bias as ascertainment bias.  
     Sampling bias could be viewed as a subtype of selection bias.  
    


5. Model Uncertainty

    Interpreting Softmax Output Probabilities  

    In the same way that in regression a NN with two outputs one representing mean and one variance that parameterise a Gaussian can capture aleatoric uncertainty even though the model is deterministic.  
    Bayesian NNs dropout included aim to capture epistemic aka model uncertainty.  

    Dropout for Measuring Model epistemic Uncertainty  
    Dropout can give us principled uncertainty estimates.  
    Principled in the sense that the uncertainty estimates basically approximate those of our Gaussian process/workfiles/research/dl/archits/nns.  

    Theoretical Motivation dropout neural networks are identical to variational inference in Gaussian processes.  
    Interpretations of Dropout  
    
     Dropout is just a diagonal noise matrix with the diagonal elements set to either 0 or 1.  

    

6. Probability Calibration
    Modern NN are miscalibrated not well calibrated. They tend to be very confident. We cannot interpret the softmax probabilities as reflecting the true probability distribution or as a measure of confidence.  

    Miscalibration is the discrepancy between model confidence and model accuracy.  
    You assume that if a model gives $$80\%$$ confidence for 100 images then $$80$$ of them will be accurate and the other $$20$$ will be inaccurate.  
    


    Model Confidence probability of correctness.  
    Calibrated Confidence softmax scores $$\hat{p}$$ $$\hat{p}$$ represents a true probability.  


        Paper that defines the problem and gives multiple effective solution for calibrating Neural Networks. 
     Calibration of Convolutional Neural Networks Thesis!file///Users/ahmadbadary/Downloads/KängseppComputerScience2018.pdf  
    

7. Debugging Strategies for Deep ML Models
    
    1. Visualize the model in action  
        Directly observing qualitative results of a model e.g. located objects generated speech can help avoid evaluation bugs or mis leading evaluation results. It can also help guide the expected quantitative performance of the model.  
    2. Visualize the worst mistakes  
        By viewing the training set examples that are the hardest to model correctly by using a confidence measure e.g. softmax probabilities one can often discover problems with the way the data have been preprocessed or labeled.  
    3. Reason about software using training and test error  
        It is hard to determine whether the underlying software is correctly implemented.  
        We can use the training/test errors to help guide us  
         If training error is low but test error is high then  
             it is likely that that the training procedure works correctlyand the model is overfitting for fundamental algorithmic reasons.  
             or that the test error is measured incorrectly because of a problem with saving the model after training then reloading it for test set evaluation or because the test data was prepared differently from the training data.  
         If both training and test errors are high then  
            it is difficult to determine whether there is a software defect or whether the model is underfitting due to fundamental algorithmic reasons.  
            This scenario requires further tests described next.  
    3. Fit a tiny dataset  
        If you have high error on the training set determine whether it is due to genuine underfitting or due to a software defect.  
        Usually even small models can be guaranteed to be able fit a suﬃciently small dataset. For example a classification dataset with only one example can be fit just by setting the biase sof the output layer correctly.  
        This test can be extended to a small dataset with few examples.  
    4. Monitor histograms of activations and gradients  
        It is often useful to visualize statistics of neural network activations and gradients collected over a large amount of training iterations maybe one epoch.  
        The preactivation value of hidden units can tell us if the units saturate or how often they do.  
        For example for rectifiershow often are they off? Are there units that are always off?  
        For tanh unitsthe average of the absolute value of the preactivations tells us how saturated the unit is.  
        In a deep network where the propagated gradients quickly grow or quickly vanish optimization may be hampered.  
        Finally it is useful to compare the magnitude of parameter gradients to the magnitude of the parameters themselves. As suggested by Bottou 2015 we would like the magnitude of parameter updates over a minibatch to represent something like 1 percent of the magnitude of the parameter not 50 percent or 0.001 percent which would make the parametersmove too slowly. It may be that some groups of parameters are moving at a good pace while others are stalled. When the data is sparse like in natural language some parameters may be very rarely updated and this should be kept in mind when monitoring their evolution.  
    5. Finally many deep learning algorithms provide some sort of guarantee about the results produced at each step.  
        For example in part III we will see some approximate inference algorithms that work by using algebraic solutions to optimization problems.  
        Typically these can be debugged by testing each of their guarantees.Some guarantees that some optimization algorithms offer include that the objective function will never increase after one step of the algorithm that the gradient with respect to some subset of variables will be zero after each step of the algorithmand that the gradient with respect to all variables will be zero at convergence.Usually due to rounding error these conditions will not hold exactly in a digital computer so the debugging test should include some tolerance parameter. 










 Normalization Methods in Deep Learning








Normalization

1. Normalization
    Normalization aka Feature Scaling is a method used to normalize the range of independent variables or features of data. It is generally performed during the data preprocessing step.  

    Motivation  
    Since the range of values of raw data varies widely in some machine learning algorithms objective functions will not work properly without normalization e.g. dot product based functions are scale sensitive.  
    Moreover normalizing the inputs leads to spherical contours of the objective which makes the optimization easier for vanilla sgd and speeds up the convergence.  



          

    

2. Input Normalization
    

    Rescaling min max normalization  
    is the simplest method and consists in rescaling the range of features to scale the range in $$0 1$$ or $$−1 1$$. Selecting the target range depends on the nature of the data. To rescale to $$0 1$$ range  
    $$x'=\dfrac{x {\text{min}}x}{ {\text{max}}x {\text{min}}x}$$  

    To rescale a range between an arbitrary set of values $$a b$$ the formula becomes  
    $${\displaystyle x'=a+{\frac {x {\text{min}}xb a}{ {\text{max}}x {\text{min}}x}}}$$  
    where $$a\ b$$ are the min max values.  


    Zero  Centering  Mean Normalization  
     Define the mean $$\muj$$ of each feature of the datapoints $$x^{i}$$  
        $$\mu{j}=\frac{1}{m} \sum{i=1}^{m} x{j}^{i}$$  
     Replace each $$xj^{i}$$ with $$xj  \muj$$  


    Standardization Z score Normalization  
    Feature standardization makes the values of each feature in the data have zero mean when subtracting the mean in the numerator and unit variance. Subtract the mean from each feature then divide the values of each feature by its standard deviation.  
    $$x' = \frac{x  \bar{x}}{\sigma}$$  


    Scaling to Unit Length Normalization  
    Another option that is widely used in machine learning is to scale the components of a feature vector such that the complete vector has length one. This usually means dividing each component by the Euclidean length of the vector  
    $${\displaystyle x'={\frac {x}{\left\|{x}\right\|}}}$$  
    We can use $$L1$$ norm or other metrics based on problem.  

    

3. Activation Normalization
    Extends the idea of input normalization to deeper networks. Interprets activations in a layer as a featurized input abstract representation of the input and aims to normalize those layer outputs/activations.  

    The difference however is in the target mean and variance we want to achieve. Unlike inputs we might not want to force the activations to have mean$$=0$$ and variance$$=1$$. E.g. if we are using the sigmoid activation; mean$$=0$$ and variance$$=1$$ will utilize the linear part of sigmoid. So changing the mean and variance will allow the network to take advantage of the non linearity.   

    In practice it's much more common to normalize the outputs before applying the activation e.g. in Batch Norm  

    


    Let $$X$$ be $$n \times d$$ design matrix of sample pts.  
    Each row $$i$$ of $$X$$ is a sample pt $$Xi^T$$.  

    Centering Transform  
    AKA Mean Normalization is just removing the mean from each observation. It centers the data around $$0$$.  
    $$X' = X  \mathbf{\mu}$$   
    where $$\mathbf{\mu}$$ is the mean of all the rows of $$X$$.  

    Decorrelation Transform  
    removes only the correlations but leaves variances intact  
    $$Z = X'V$$  
    where $$\text{Var}X' = \Sigma = \dfrac{1}{n}X'^TX' = V\Lambda V^T$$ and $$\text{Var}Z = \Lambda$$ is the sample covariance matrix.  

    It transforms the sample points to the eigenvector coordinate system.  

    Standardization Transform  
    sets variances to $$1$$ but leaves correlations intact  
    $$X's = \dfrac{X  \mathbf{\mu}}{\mathbf{\sigma} X}$$  
    where $$\mathbf{\sigma} X$$ is the standard deviation of all the rows of $$X$$.  

    Sphering Transform  
    Rescales the uncorrelated matrix $$Z$$ in order to obtain a covariance matrix corresponding to the identity matrix. To do that we scale our decorrelated data by dividing each dimension by the square root of its corresponding eigenvalue.  
    $$W = X'\Sigma^{ 1/2} = X'V \Lambda^{ 1/2} V^T$$  
    this is ZCA whitening.  

    Whitening Transform  

    = Centering + Sphering  

    Then $$W$$ has covariance matrix $$I . \left\text { If } X{i} \sim \mathcal{N}\mu \Sigma \text { then approximately } W{i} \sim \mathcal{N}0 I\right$$.  



    Notes  
    
     Decorrelation intuitively it means that we want to rotate the data until there is no correlation anymore.  
     Centering seems to make it easier for hidden units to get into a good operating region of the sigmoid or ReLU  
     Standardization normalizing variance makes the objective function better conditioned so gradient descent converges faster  

    
  


Batch Normalization

1. Batch Normalization
    Batch Normalization is a normalization method that normalizes activations in a network across the mini batch. For each feature batch normalization computes the mean and variance of that feature in the mini batch. It then subtracts the mean and divides the feature by its mini batch standard deviation.  
    This restricts the activations to have $$0$$ mean and unit variance.  
    This is followed by an affine transformation of the normalized activations to rescale the mean and variance in a learnable way to whatever the network sees fit. This is done because restricting the mean and variance of the activations might hinder the network from taking advantage of the freedom of setting the distribution of the activations to something that might help the later layers learn faster#affinetransform. This means that the expressiveness of the network does not change.  

    
     onclick="showTextwithParentPopHideevent;"}


    

2. Effectiveness of Batch Normalization
    There are many different proposed reasons that try to explain the wide success and effectiveness of the method. We summarize here some of those reasons and give an intuition to why those reasons apply.  

    Summarizing the intuition of why BN works  
    The overall intuition is that batch normalization makes the loss surface “easier to navigate” making optimization easier enabling the use of higher learning rates and improving model performance across multiple tasks.  
    Further there is a regularization effect when using BN induced by added noise to the estimate of the mean and variance of the data due to using mini batches instead.  



    

22.Internal Covariate Shift as an Intuitive but Wrong Motivation for the Success of BN
    The correlation between batch normalization and internal covariate shift is widely accepted but was not supported by experimental results. Scholars recently show with experiments that the hypothesized relationship is not an accurate one. Rather the enhanced accuracy with the batch normalization layer seems to be independent of internal covariate shift.  

    Two Problems with the ICS Explanation  
    1. Even if the mean and variance are constant the distribution of activations can still change. Why are the mean and variance so important?  
    2. If we introduce $$\gamma$$  and $$\beta$$ the mean and variance will deviate from $$0$$ and $$1$$ anyway. What then is the point of batch normalization?  
    


3. Suppressing Higher Order Effects Goodfellow

    Quick Intuition  
    
     In a neural network changing one weight affects subsequent layers which then affect subsequent layers and so on.
     This means changing one weight can make the activations fly all over the place in complex ways.
     This forces us to use lower learning rates to prevent the gradients from exploding or   if we use sigmoid activations   to prevent the gradients from disappearing.
     Batch normalization to the rescue! Batch normalization reparameterizes the network to make optimization easier.
     With batch normalization we can control the magnitude and mean of the activations independent of all other layers.
     This means weights don’t fly all over the place as long as we have sensible initial means and magnitudes and we can optimize much easier.  

    Further Intuition  
    Deep Neural networks have higher order interactions which means changing weights of one layer might also effect the statistics of other layers in addition to the loss function. These cross layer interactions when unaccounted lead to internal covariate shift. Every time we update the weights of a layer there is a chance that it effects the statistics of a layer further in the neural network in an unfavorable way.  
    Convergence may require careful initializing hyperparameter tuning and longer training durations in such cases. However when we add the batch normalized layer between the layers the statistics of a layer are only effected by the two hyperparameters  $$\gamma$$  and $$\beta$$. Now our optimization algorithm has to adjust only two hyperparameters to control the statistics of any layer rather than the entire weights in the previous layer. This greatly speeds up convergence and avoids the need for careful initialization and hyperparameter tuning. Therefore Batch Norm acts more like a check pointing mechanism.  
    Notice that the ability to arbitrarily set the mean and the standard deviation of a layer also means that we can recover the original distribution if that was sufficient for proper training.  




          


    



    In a new paper the authors claim that BN works because it makes the loss surface smoother. Concretely it improves the $$\beta$$ smoothness or the Lipschitzness of the function. This smoothness induces a more predictive and stable behavior of the gradients allowing for faster training.  




          

    

5. Batch Norm as a Regularizer
    BN also acts a regularizer. The mean and the variance estimated for each batch is a noisier version of the true mean and this injects randomness in our optima search. This helps in regularization.  
    

6. Length Direction Decoupling
    It is argued that the success of batch normalization could be at least partially credited to the length direction decoupling effect that the method provides.  

    By interpreting the batch normalization procedure as the reparametrization of weight space it could be shown that the length and the direction of the weights are separated after the procedure and they could thus be trained separately.  

    This property could then be used to prove the faster convergence of problems with batch normalization  
    

    

    

7. Limitations/Problems of BN
    
    When doing normalization we ideally want to the use the global mean and variance to standardize our data. Computing this for each layer is far too expensive though so we need to approximate using some other measures. BN approximates the mean and variance with those of the mini batch which is a noisy estimate. Although we motivated some of the effectiveness of BN to its regularizing effects due to this same noisy estimate this estimate can be problematic in the following scenarios  
    1. Small Batch Sizes  
        If the batch size is $$1$$ the variance is $$0$$ so batch normalization cannot be applied. Slightly larger mini batch sizes won’t have this problem but small mini batches make our estimates very noisy and can negatively impact training meaning batch normalization imposes a certain lower bound on our batch size.  
    2. Recurrent Connections in an RNN  
        In an RNN the recurrent activations of each time step will have different statistics. This means that we have to fit a separate batch normalization layer for each time step. This makes the model more complicated and   more importantly   it forces us to store the statistics for each time step during training.  
    3. Dependence of the loss between samples in a mini batch  
        BN makes the loss value for each sample in a mini batch dependent on other samples in the mini batch. For instance if a sample causes a certain layer’s activations to become much larger this will make the mean and variance larger as well. This will change the activation for all other samples in the mini batch as well. Furthermore the mini batch statistics will depend on the mini batch size as well a smaller mini batch size will increase the random variation in the mean and variance statistics.
        
    4. BN parameters in Fine Tuning Applications  
        When Fine Tuning a larger network by freezing all the layers except the last layer; it is unclear if one should use the mean and variance computed on the original dataset or use the mean and variance of the mini batches. Though most frameworks use the mini batch statistics if we are using a different mini batch size there will be a mismatch between the optimal batch normalization parameters and the parameters in the network.  
        If there is a mis match in the mini batch sizes it seems to be better to use the statistics of the original dataset instead.  



    




Weight Normalization



     onclick="showTextwithParentPopHideevent;"}



1. Weight Normalization
    Weight Normalization is a normalization method that instead of normalizing the mini batch normalizes the weights of the layer.  

    WN reparameterizes the weights $$\boldsymbol{w}$$ of any layer in the network in the following way  
    $$\boldsymbol{w}=\frac{g}{\|\boldsymbol{v}\|} \boldsymbol{v} \\\\\\\ $$  
    where $$\boldsymbol{v}$$ is a $$k$$ dimensional vector $$g$$ is a scalar and $$\|\boldsymbol{v}\|$$ denotes the Euclidean norm of $$\boldsymbol{v}$$.  
    This reparameterization has the effect of fixing the Euclidean norm of the weight vector $$\boldsymbol{w}$$ we now have $$\|\boldsymbol{w}\| = g$$ independent of the parameters $$\boldsymbol{v}$$.  

    Weight Normalization separates the norm of the weight vector from its direction without reducing expressiveness  
    
     For variance this has a similar effect to dividing the inputs by the standard deviation in batch normalization.  
     As for the mean the authors of the paper proposed using a method called “mean only batch normalization” together with weight normalization.  
    

2. Advantages
     Since WN separates the norm of the weight vector from its direction and then optimizes both $$g$$ and $$\boldsymbol{v}$$ using gradient descent. This change in learning dynamics makes optimization easier.  
     It makes the mean and variance independent of the batch; since now they are connected to the weights of the network.  
     It is often much faster than BN. In CNNs the number of weights tends to be far smaller than the number of inputs meaning weight normalization is computationally cheaper compared to batch normalization CNNs share weights.   
    

3. Mean Only Batch Normalization
    Although weight normalization on its own can assist training the authors of the paper proposed using a method called “mean only batch normalization” in conjunction with weight normalization.  

    Mean Only Batch Normalization is a method that subtracts out the mean of the mini batch but does not divide the inputs by the standard deviation or rescales them.  

    Though this method counteracts some of the computational speed up of weight normalization it is cheaper than batch normalization since it does not need to compute the standard deviations. The authors claim that this method provides the following benefits  
    
    1. It makes the mean of the activations independent from $$\boldsymbol{v}$$  
        Weight normalization independently cannot isolate the mean of the activations from the weights of the layer causing high level dependencies between the means of each layer. Mean only batch normalization can resolve this problem.  
    2. It adds “gentler noise” to the activations  
        One of the side effects of batch normalization is that it adds some stochastic noise to the activations as a result of using noisy estimates computed on the mini batches. This has a regularization effect in some applications but can be potentially harmful in some noise sensitive domains like reinforcement learning. The noise caused by the mean estimations however are “gentler” since the law of large numbers ensures the mean of the activations is approximately normally distributed.  
        Thus weight normalization can still work in settings with a smaller mini batch size.  
    
 




Layer Normalization


     onclick="showTextwithParentPopHideevent;"}



1. Layer Normalization
    Layer Normalization is a normalization method developed by Hinton that instead of normalizing the inputs across batches like BN normalizes the inputs across the features  
    $$\begin{aligned} \mu{i} &=\frac{1}{m} \sum{j=1}^{m} x{i j}  \sigma{i}^{2} &=\frac{1}{m} \sum{j=1}^{m}\leftx{i j} \mu{i}\right^{2}  \hat{x}{i j} &=\frac{x{i j} \mu{i}}{\sqrt{\sigma{i}^{2}+\epsilon}} \end{aligned}$$  

    This is deceptively similar to the batch norm equations  
    $$\begin{aligned} \mu{j} &=\frac{1}{m} \sum{i=1}^{m} x{i j}  \sigma{j}^{2} &=\frac{1}{m} \sum{i=1}^{m}\leftx{i j} \mu{j}\right^{2}  \hat{x}{i j} &=\frac{x{i j} \mu{j}}{\sqrt{\sigma{j}^{2}+\epsilon}} \end{aligned}$$  
    where $$x{i j}$$  is the $$ij$$ th element of the input the  dimension represents the batch and the  represents the feature I have modified the notation from the original papers to make the contrast clearer.  

    
    

    In batch normalization the statistics are computed across the batch and are the same for each example in the batch. In contrast in layer normalization the statistics are computed across each feature and are independent of other examples.  

    This means that layer normalization is not a simple reparameterization of the network unlike the case of weight normalization and batch normalization which both have the same expressive power as an unnormalized neural network. The layer normalized model thus has different invariance properties than the other methods.   
    

2. Advantages
     The independence between inputs means that each input has a different normalization operation allowing arbitrary mini batch sizes to be used.
     The experimental results show that layer normalization performs well for recurrent neural networks.  
    

3. Analysis of the Invariance Properties of LN
    


Instance Normalization


     onclick="showTextwithParentPopHideevent;"}






1. Instance Normalization
    Instance Normalization is similar to layer normalization but with an extra restriction it computes the mean/standard deviation and normalize across each channel in each training example.  

    Motivation  
    In Style Transfer problems the network should be agnostic to the contrast of the original image.   
    Therefore it is specific to images and not trivially extendable to RNNs.  


    Experimental results show that instance normalization performs well on style transfer when replacing batch normalization. Recently instance normalization has also been used as a replacement for batch normalization in GANs.   
    





Group Normalization


     onclick="showTextwithParentPopHideevent;"}



1. Group Normalization
    Group Normalization computes the mean and standard deviation over groups of channels for each training example.  
    You can think of GN as being half way between layer normalization and instance normalization  
    
     When we put all the channels into a single group it becomes Layer Normalization  
     When we put each channel in a different group it becomes Instance Normalization  
    

2. Motivation/Advantages
    Layer Norm and Instance Norm were significantly inferior to BN on image recognition tasks. Group Normalization was able to achieve much closer performance to BN with a batch size of 32 on ImageNet and outperformed it on smaller batch sizes.  

    For tasks like object detection and segmentation that use much higher resolution images and therefore cannot increase their batch size due to memory constraints Group Normalization was shown to be a very effective normalization method.  
    

3. Effectiveness of Group Normalization
    Why is group normalization so effective compared to layer normalization and instance normalization?  

    Layer Norm makes an implicit assumption that all channels are “equally important” when computing the mean. This assumption is often not true in convolution layers.  
    For instance neurons near the edge of an image and neurons near the center of an image will have very different activation statistics.  This means that computing different statistics for different channels can give models much needed flexibility.  

    Instance Norm on the other hand assumes that the channels are completely independent from each other. Channels in an image are not completely independent though so being able to leverage the statistics of nearby channels is an advantage group normalization has over instance normalization.  
    



Batch ReNormalization


     onclick="showTextwithParentPopHideevent;"}



1. Batch ReNormalization
    Batch ReNormalization is an extension of BN for applying batch normalization to small batch sizes.  
    In the Authors words "an effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire mini batch"  

    The authors propose to use a moving average while also taking the effect of previous layers on the statistics into account. Their method is   at its core   a simple reparameterization of normalization with the moving average. If we denote the moving average mean and standard deviation as $$\mu$$ and $$\sigma$$ and the mini batch mean and standard deviation as $$\muB$$ and $$\sigmaB$$ the batch renormalization equation is  
    $$\frac{x{i} \mu}{\sigma}=\frac{x{i} \mu{\mathcal{B}}}{\sigma{\mathcal{B}}} \cdot r+d \quad \text { where } r=\frac{\sigma{\mathcal{B}}}{\sigma} \quad d=\frac{\mu{\mathcal{B}} \mu}{\sigma}$$  

    In other words we multiply the batch normalized activations by $$r$$ and add $$d$$ where both $$r$$ and $$d$$ are computed from the mini batch statistics and moving average statistics. The trick here is to not backpropagate through $$r$$ and $$d$$. Though this means we ignore some of the effects of previous layers on previous mini batches since the mini batch statistics and moving average statistics should be the same on average the overall effect of this should cancel out on average as well.  
    


2. Motivation derivation
    The basic idea behind batch renormalization comes from the fact that we do not use the individual mini batch statistics for batch normalization during inference. Instead we use a moving average of the mini batch statistics. This is because a moving average provides a better estimate of the true mean and variance compared to individual mini batches.  

    Why don’t we use the moving average during training?  
    The answer has to do with the fact that during training we need to perform backpropagation. In essence when we use some statistics to normalize the data we need to backpropagate through those statistics as well. If we use the statistics of activations from previous mini batches to normalize the data we need to account for how the previous layer affected those statistics during backpropagation. If we ignore these interactions we could potentially cause previous layers to keep on increasing the magnitude of their activations even though it has no effect on the loss. This means that if we use a moving average we would need to store the data from all previous mini batches during training which is far too expensive.  
    

3. Performance
    Unfortunately batch renormalization’s performance still degrades when the batch size decreases though not as badly as batch normalization meaning group normalization still has a slight advantage in the small batch size regime.
    


Batch Instance Normalization


     onclick="showTextwithParentPopHideevent;"}



1. Batch Instance Normalization
    Batch Instance Normalization is an extension of instance normalization that attempts to account for differences in contrast and style in images. It is basically just an interpolation between batch normalization and instance normalization.  

    Denoting the batch normalized outputs and the instance normalized outputs as $$\hat{x}^{B}$$ and $$\hat{x}^{I}$$ each the batch instance normalized output can be expressed as  
    $$\mathbf{y}=\left\rho \cdot \hat{\mathbf{x}}^{B}+1 \rho \cdot \hat{\mathbf{x}}^{I}\right \cdot \gamma+\beta$$  
    The interesting aspect of batch instance normalization is that the balancing parameter $$\rho$$ is learned through gradient descent.  
    

2. Motivation
    The problem with instance normalization is that it completely erases style information. Though this is beneficial in certain settings like style transfer it can be problematic in settings like weather classification where the style e.g. the brightness of the image can be a crucial feature. In other words the degree of style information that should be removed is dependent on the task at hand. Batch instance normalization attempts to deal with this by learning how much style information should be used for each task and feature map channel.  
    Thus B IN extends instance normalization to account for differences in contrast and style in images.  
    

3. Performance
    Batch instance normalization outperformed batch normalization on CIFAR 10/100 ImageNet domain adaptation and style transfer. In image classification tasks the value of $$\rho$$  tended to be close to $$0$$ or $$1$$ meaning many layers used either instance or batch normalization almost exclusively. In addition layers tended to use batch normalization more than instance normalization which fits the intuition proposed by the authors that instance normalization serves more as a method to eliminate unnecessary style variation. On style transfer   on the other hand   the model tended to use instance normalization more which makes sense given style is much less important in style transfer.  

    The authors also found that in practice using a higher learning rate for $$\rho$$ improves performance.  

    One important contribution of batch instance normalization is that it showed that models could learn to adaptively use different normalization methods using gradient descent. This raises the question could models learn to use an even wider variety of normalization methods?  
    This nicely leads us to the next normalization method Switchable Normalization.    


Switchable Normalization


     onclick="showTextwithParentPopHideevent;"}



     onclick="showTextwithParentPopHideevent;"}



1. Switchable Normalization
    Switchable Normalization is a method that uses a weighted average of different mean and variance statistics from batch normalization instance normalization and layer normalization. Similar to batch instance normalization the weights were learned through backpropagation.  
    

2. Motivation
    Given the different methods proposed for normalization common questions arise including  
    Is batch normalization still the best normalization method out of the box? What if we combine different normalization methods? What if the best normalization method actually differs depending on the depth of the layer?  
    Switchable Normalization aims to answer those questions.  
    

3. Performance
    The authors showed that switch normalization could potentially outperform batch normalization on tasks such as image classification and object detection.  

    Perhaps more interestingly the paper showed that the statistics of instance normalization were used more heavily in earlier layers whereas layer normalization was preferred in the later layers and batch normalization being used in the middle. Smaller batch sizes lead to a preference towards layer normalization and instance normalization as is expected.  
    


Spectral Normalization


     onclick="showTextwithParentPopHideevent;"}



1. Spectral Normalization
    Spectral Normalization is a method proposed to improve the training of GANs by limiting the Lipschitz constant of the discriminator.  
    The authors restrict the Lipschitz constant by normalizing the weight matrices by their largest eigenvalue or their spectral norm   hence the name. The largest eigenvalue is computed using the power method which makes the computational cost of this method very cheap. Compared to weight normalization spectral normalization does not reduce the rank of the weight matrix.^1  
    

    SN is not designed to be a replacement for batch normalization but it gives us a very interesting look into normalization in deep learning in general.  
    

2. Performance
    Experimental results show that spectral normalization improves the training of GANs with minimal additional tuning.  
    


Further Exploration/Discussions



^1 Most likely a wrong statement


 Data Processing






Dimensionality Reduction



###Dimensionality Reduction
Dimensionality Reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. It can be divided into feature selection


###t SNE \| T distributed Stochastic Neighbor Embeddings


     onclick="showTextwithParentPopHideevent;"}










SNE  Stochastic Neighbor Embeddings  
SNE is a method that aims to match distributions of distances between points in high and low dimensional space via conditional probabilities.  
It Assumes distances in both high and low dimensional space are Gaussian distributed.  


      
 


t SNE  
t SNE is a machine learning algorithm for visualization developed by Laurens van der Maaten and Geoffrey Hinton.  
It is a nonlinear dimensionality reduction technique well suited for embedding high dimensional data for visualization in a low dimensional space of two or three dimensions.  
Specifically it models each high dimensional object by a two or three dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points  with high probability.  
> It tends to preserve local structure while at the same time preserving the global structure as much as possible.  



Stages  

1. It Constructs a probability distribution over pairs of high dimensional objects in such a way that similar objects have a high probability of being picked while dissimilar points have an extremely small probability of being picked.  
2. It Defines a similar probability distribution over the points in the low dimensional map and it minimizes the Kullback Leibler divergence between the two distributions with respect to the locations of the points in the map.  


Key Ideas  
It solves two big problems that SNE faces  

1. The Crowding Problem  
    The "crowding problem" that are addressed in the paper is defined as "the area of the two dimensional map that is available to accommodate moderately distant datapoints will not be nearly large enough compared with the area available to accommodate nearby datepoints". This happens when the datapoints are distributed in a region on a high dimensional manifold around i and we try to model the pairwise distances from i to the datapoints in a two dimensional map. For example it is possible to have 11 datapoints that are mutually equidistant in a ten dimensional manifold but it is not possible to model this faithfully in a two dimensional map. Therefore if the small distances can be modeled accurately in a map most of the moderately distant datapoints will be too far away in the two dimensional map. In SNE this will result in very small attractive force from datapoint i to these too distant map points. The very large number of such forces collapses together the points in the center of the map and prevents gaps from forming between the natural clusters. This phenomena crowding problem is not specific to SNE and can be observed in other local techniques such as Sammon mapping as well.  
     Solution  Student t distribution for $$q$$  
        Student t distribution is used to compute the similarities between data points in the low dimensional space $$q$$.  
2. Optimization Difficulty of KL div  
    The KL Divergence is used over the conditional probability to calculate the error in the low dimensional representation. So the algorithm will be trying to minimize this loss and will calculate its gradient  
    $$\frac{\delta C}{\delta y{i}}=2 \sum{j}\leftp{j | i} q{j | i}+p{i | j} q{i | j}\right\lefty{i} y{j}\right$$  
    This gradient involves all the probabilities for point $$i$$ and $$j$$. But these probabilities were composed of the exponentials. The problem is that We have all these exponentials in our gradient which can explode or display other unusual behavior very quickly and hence the algorithm will take a long time to converge.  
     Solution  Symmetric SNE  
        The Cost Function is a symmetrized version of that in SNE. i.e. $$p{i\vert j} = p{j\vert i}$$ and $$q{i\vert j} = q{j\vert i}$$.  


Application  
It is often used to visualize high level representations learned by an artificial neural network.  


Motivation  
There are a lot of problems with traditional dimensionality reduction techniques that employ feature projection; e.g. PCA. These techniques attempt to preserve the global structure and in that process they lose the local structure. Mainly projecting the data on one axis or another may most likely not preserve the neighborhood structure of the data; e.g. the clusters in the data  
t SNE finds a way to project data into a low dimensional space 1 d in this case such that the clustering "local structure" in the high dimensional space is preserved.  



t SNE Clusters  
While t SNE plots often seem to display clusters the visual clusters can be influenced strongly by the chosen parameterization and therefore a good understanding of the parameters for t SNE is necessary. Such "clusters" can be shown to even appear in non clustered data and thus may be false findings.  



Properties  

 It preserves the neighborhood structure of the data  
 Does NOT preserve distances nor density  
 Only to some extent preserves nearest neighbors?  

 It learns a non parametric mapping which means that it does NOT learn an explicit function that maps data from the input space to the map  


Algorithm  

     onclick="showTextwithParentPopHideevent;"}




Issues/Weaknesses/Drawbacks  

1. The paper only focuses on the date visualization using t SNE that is embedding high dimensional date into a two or three dimensional space. However this behavior of t SNE presented in the paper cannot readily be extrapolated to $$d>3$$ dimensions due to the heavy tails of the Student t distribution.  
2. It might be less successful when applied to data sets with a high intrinsic dimensionality. This is a result of the local linearity assumption on the manifold that t SNE makes by employing Euclidean distance to present the similarity between the datapoints. 
3. The cost function is not convex. This leads to the problem that several optimization parameters hyperparameters need to be chosen and tuned and the constructed solutions depending on these parameters may be different each time t SNE is run from an initial random configuration of the map points.  
4. It cannot work "online". Since it learns a non parametric mapping which means that it does not learn an explicit function that maps data from the input space to the map. Therefore it is not possible to embed test points in an existing map. You have to re run t SNE on the full dataset.  
    A potential approach to deal with this would be to train a multivariate regressor to predict the map location from the input data.  
    



t SNE Optimization  






Discussion and Information  
 What is perplexity?  
    Perplexity is a measure for information that is defined as 2 to the power of the Shannon entropy. The perplexity of a fair die with k sides is equal to k. In t SNE the perplexity may be viewed as a knob that sets the number of effective nearest neighbors. It is comparable with the number of nearest neighbors k that is employed in many manifold learners.  
 Choosing the perplexity hp   
    The performance of t SNE is fairly robust under different settings of the perplexity. The most appropriate value depends on the density of your data. Loosely speaking one could say that a larger / denser dataset requires a larger perplexity. Typical values for the perplexity range between $$5$$ and $$50$$.  
 Every time I run t SNE I get a slightly different result?  
    In contrast to e.g. PCA t SNE has a non convex objective function. The objective function is minimized using a gradient descent optimization that is initiated randomly. As a result it is possible that different runs give you different solutions. Notice that it is perfectly fine to run t SNE a number of times with the same data and parameters and to select the visualization with the lowest value of the objective function as your final visualization.  
 Assessing the "Quality of Embeddings/visualizations"  
    Preferably just look at them! Notice that t SNE does not retain distances but probabilities so measuring some error between the Euclidean distances in high D and low D is useless. However if you use the same data and perplexity you can compare the Kullback Leibler divergences that t SNE reports. It is perfectly fine to run t SNE ten times and select the solution with the lowest KL divergence.  
        








Feature Selection



###Feature Selection
Feature Selection is the process of selecting a subset of relevant features variables predictors for use in model construction.  

Applications  

 Simplification of models to make them easier to interpret by researchers/users  
 Shorter training time  
 A way to handle curse of dimensionality  
 Reduction of Variance $$\rightarrow$$ Reduce Overfitting $$\rightarrow$$ Enhanced Generalization  

Strategies/Approaches  

 Wrapper Strategy  
    Wrapper methods use a predictive model to score feature subsets. Each new subset is used to train a model which is tested on a hold out set. Counting the number of mistakes made on that hold out set the error rate of the model gives the score for that subset. As wrapper methods train a new model for each subset they are very computationally intensive but usually provide the best performing feature set for that particular type of model.  
    e.g. Search Guided by Accuracy
 Filter Strategy  
    Filter methods use a proxy measure instead of the error rate to score a feature subset. This measure is chosen to be fast to compute while still capturing the usefulness of the feature set.  
    Filter methods produce a feature set which is not tuned to a specific model usually giving lower prediction performance than a wrapper but are more general and more useful for exposing the relationships between features.  
    e.g. Information Gain
 Embedded Strategy  
    Embedded methods are a catch all group of techniques which perform feature selection as part of the model construction process.  
    e.g. LASSO




###Correlation Feature Selection
The Correlation Feature Selection CFS measure evaluates subsets of features on the basis of the following hypothesis  
"Good feature subsets contain features highly correlated with the classification yet uncorrelated to each other

The following equation gives the merit of a feature subset $$S$$ consisting of $$k$$ features  
$${\displaystyle \mathrm {Merit} {S{k}}={\frac {k{\overline {r{cf}}}}{\sqrt {k+kk 1{\overline {r{ff}}}}}}.}$$  
where $${\displaystyle {\overline {r{cf}}}}$$ is the average value of all feature classification correlations and $${\displaystyle {\overline {r{ff}}}}$$ is the average value of all feature feature correlations.  

The CFS criterion is defined as follows  
$$\mathrm {CFS} =\max {S{k}}\left{\frac {r{cf{1}}+r{cf{2}}+\cdots +r{cf{k}}}{\sqrt {k+2r{f{1}f{2}}+\cdots +r{f{i}f{j}}+\cdots +r{f{k}f{1}}}}}\right$$  



###Feature Selection Embedded in Learning Algorithms
 $$l{1}$$ regularization techniques such as sparse regression LASSO and $${\displaystyle l{1}}$$ SVM
 Regularized trees e.g. regularized random forest implemented in the RRF package
 Decision tree
 Memetic algorithm
 Random multinomial logit RMNL
 Auto encoding networks with a bottleneck layer
 Submodular feature selection



###Information Theory Based Feature Selection Mechanisms
There are different Feature Selection mechanisms around that utilize mutual information for scoring the different features.  
They all usually use the same algorithm  
1. Calculate the mutual information as score for between all features $${\displaystyle f{i}\in F}$$ and the target class $$c$$
1. Select the feature with the largest score e.g. $${\displaystyle argmax{f{i}\in F}If{i}c}$$ and add it to the set of selected features $$S$$
1. Calculate the score which might be derived form the mutual information
1. Select the feature with the largest score and add it to the set of select features e.g. $${\displaystyle {\arg \max }{f{i}\in F}I{derived}f{i}c}$$
5. Repeat 3. and 4. until a certain number of features is selected e.g. $${\displaystyle \vert S\vert =l}$$  




Feature Extraction


###Feature Extraction
Feature Extraction starts from an initial set of measured data and builds derived values features intended to be informative and non redundant facilitating the subsequent learning and generalization steps and in some cases leading to better human interpretations.  

In dimensionality reduction feature extraction is also called Feature Projection which is a method that transforms the data in the high dimensional space to a space of fewer dimensions. The data transformation may be linear as in principal component analysis PCA but many nonlinear dimensionality reduction techniques also exist.  

Methods/Algorithms  

 Independent component analysis  
 Isomap  
 Kernel PCA  
 Latent semantic analysis  
 Partial least squares  
 Principal component analysis  
 Autoencoder  
 Linear Discriminant Analysis LDA  
 Non negative matrix factorization NMF


















 Activation Functions


Comprehensive list of activation functions in neural networks with pros/cons




Introduction

1. Activation Functions
    In NNs the activation function of a node defines the output of that node given an input or set of inputs.  
    The activation function is an abstraction representing the rate of action potential firing in the cell.  
    

2. Motivation

3. Desirable Properties
     Non Linearity  
    When the activation function is non linear then a two layer neural network can be proven to be a universal function approximator. The identity activation function does not satisfy this property. When multiple layers use the identity activation function the entire network is equivalent to a single layer model. 
     Range  
    When the range of the activation function is finite gradient based training methods tend to be more stable because pattern presentations significantly affect only limited weights. When the range is infinite training is generally more efficient because pattern presentations significantly affect most of the weights. In the latter case smaller learning rates are typically necessary.  
     Continuously Differentiable  
    This property is desirable for enabling gradient based optimization methods. The binary step activation function is not differentiable at 0 and it differentiates to 0 for all other values so gradient based methods can make no progress with it.
     Monotonicity  
     When the activation function is monotonic the error surface associated with a single layer model is guaranteed to be convex.  
     During the training phase backpropagation informs each neuron how much it should influence each neuron in the next layer. If the activation function isn't monotonic then increasing the neuron's weight might cause it to have less influence the opposite of what was intended.  
        However Monotonicity isn't required. Several papers use non monotonic trained activation functions.  
        Gradient descent finds a local minimum even with non monotonic activation functions. It might only take longer.  
     From a biological perspective an "activation" depends on the sum of inputs and once the sum surpasses a threshold "firing" occurs. This firing should happen even if the sum surpasses the threshold by a small or a large amount; making monotonicity a desirable property to not limit the range of the "sum".  
     Smoothness with Monotonic Derivatives  
    These have been shown to generalize better in some cases.  
     Approximating Identity near Origin  
    Equivalent to $${\displaystyle f0=0}$$ and $${\displaystyle f'0=1}$$ and $${\displaystyle f'}$$ is continuous at $$0$$.  
    When activation functions have this property the neural network will learn efficiently when its weights are initialized with small random values. When the activation function does not approximate identity near the origin special care must be used when initializing the weights.  
     Zero Centered Range  
    Has effects of centering the data zero mean by centering the activations. Makes learning easier.   


    

4. Undesirable Properties
     Saturation  
    An activation functions output with finite range may saturate near its tail or head e.g. $$\{0 1\}$$ for sigmoid. This leads to a problem called vanishing gradient.  
     Vanishing Gradients  
    Happens when the gradient of an activation function is very small/zero. This usually happens when the activation function saturates at either of its tails.  
    The chain rule will multiply the local gradient of activation function with the whole objective. Thus when gradient is small/zero it will "kill" the gradient $$\rightarrow$$ no signal will flow through the neuron to its weights or to its data.  
    Slows/Stops learning completely.  
     Range Not Zero Centered  
    This is undesirable since neurons in later layers of processing in a Neural Network would be receiving data that is not zero centered. This has implications on the dynamics during gradient descent because if the data coming into a neuron is always positive e.g. $$x>0$$ elementwise in $$f=w^Tx+b$$ then the gradient on the weights $$w$$ will during backpropagation become either all be positive or all negative depending on the gradient of the whole expression $$f$$. This could introduce undesirable zig zagging dynamics in the gradient updates for the weights. However notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs somewhat mitigating this issue. Therefore this is an inconvenience but it has less severe consequences compared to the saturated activation problem above.  
    Makes optimization harder.   
    



Activation Functions


1. Sigmoid
    $$Sz=\frac{1}{1+e^{ z}}  S^{\prime}z=Sz \cdot1 Sz$$  
    Properties  
    Never use as activation use as an output unit for binary classification.  
     Pros  
         Has a nice interpretation as the firing rate of a neuron  
     Cons  
         They Saturate and kill gradients $$\rightarrow$$ Gives rise to vanishing gradients^1 $$\rightarrow$$ Stop Learning  
         Happens when initialization weights are too large  
         or sloppy with data preprocessing  
         Neurons Activation saturates at either tail of $$0$$ or $$1$$  
         Output NOT Zero Centered $$\rightarrow$$ Gradient updates go too far in different directions $$\rightarrow$$ makes optimization harder   
         The local gradient $$z  1 z$$ achieves maximum at $$0.25$$ when $$z = 0.5$$. $$\rightarrow$$ very time the gradient signal flows through a sigmoid gate its magnitude always diminishes by one quarter or more $$\rightarrow$$ with basic SGD the lower layers of a network train much slower than the higher one  

2. Tanh
    $$\tanh z=\frac{e^{z} e^{ z}}{e^{z}+e^{ z}}  \tanh ^{\prime}z=1 \tanh z^{2}$$  

    Properties  
    Strictly superior to Sigmoid scaled version of sigmoid \| stronger gradient. Good for activation.  
     Pros  
         Zero Mean/Centered  
     Cons  
         They Saturate and kill gradients $$\rightarrow$$ Gives rise to vanishing gradients^1 $$\rightarrow$$ Stop Learning  
    

3. ReLU
    $$Rz=\left\{\begin{array}{cc}{z} & {z>0}  {0} & {z<=0}\end{array}\right\}   R^{\prime}z=\left\{\begin{array}{ll}{1} & {z>0}  {0} & {z<0}\end{array}\right\}$$  

    Properties  
    The best for activation Better gradients.  
     Pros  
         Non saturation of gradients which accelerates convergence of SGD  

         Not computationally expensive  
     Cons  
         ReLU not zero centered problem  
        The problem that ReLU is not zero centered can be solved/mitigated by using batch normalization which normalizes the signal before activation  
        From paper We add the BN transform immediately before the nonlinearity by normalizing $$x =  Wu + b$$; normalizing it is likely to produce activations with a stable distribution.  

         Dying ReLUs Dead Neurons  
        If a neuron gets clamped to zero in the forward pass it doesn’t "fire" / $$x<0$$ then its weights will get zero gradient. Thus if a ReLU neuron is unfortunately initialized such that it never fires or if a neuron’s weights ever get knocked off with a large update during training into this regime usually as a symptom of aggressive learning rates then this neuron will remain permanently dead.  


              
         Infinite Range  
        Can blow up the activation.  
    

4. Leaky ReLU
    $$Rz=\left\{\begin{array}{cc}{z} & {z>0}  {\alpha z} & {z<=0}\end{array}\right\}  
        R^{\prime}z=\left\{\begin{array}{ll}{1} & {z>0}  {\alpha} & {z<0}\end{array}\right\}$$  

    Properties  
    Sometimes useful. Worth trying.  
     Pros  
         Leaky ReLUs are one attempt to fix the “dying ReLU” problem by having a small negative slope of 0.01 or so.  
     Cons  
        The consistency of the benefit across tasks is presently unclear.  
    

5. ELU

8. Notes
     It is very rare to mix and match different types of neurons in the same network even though there is no fundamental problem with doing so.  
     Identity Mappings  
    When an activation function cannot achieve an identity mapping e.g. ReLU map all negative inputs to zero; then adding extra depth actually decreases the best performance in the case a shallower one would suffice Deep Residual Net paper.  
    

1. Softmax
    
    Motivation  
    
     Information Theory  from the perspective of information theory the softmax function can be seen as trying to minimize the cross entropy between the predictions and the truth.  
     Probability Theory  from this perspective since $$\hat{y} i$$ represent log probabilities we are in fact looking at the log probabilities thus when we perform exponentiation we end up with the raw probabilities. In this case the softmax equation find the MLE Maximum Likelihood Estimate.  
        If a neuron's output is a log probability then the summation of many neurons' outputs is a multiplication of their probabilities. That's more commonly useful than a sum of probabilities.  
     It is a softened version of the argmax function limit as $$T \rightarrow 0$$  

     Properties
         There is one nice attribute of Softmax as compared with standard normalisation  
            It react to low stimulation think blurry image of your neural net with rather uniform distribution and to high stimulation ie. large numbers think crisp image with probabilities close to 0 and 1.   
            While standard normalisation does not care as long as the proportion are the same.  
            Have a look what happens when soft max has 10 times larger input ie your neural net got a crisp image and a lot of neurones got activated.  
            
            >>> softmax12              # blurry image of a ferret  
                0.26894142      0.73105858  #     it is a cat perhaps !?  
                >>> softmax1020            # crisp image of a cat  
                0.0000453978687 0.999954602 #     it is definitely a CAT !   
  
            And then compare it with standard normalisation  
            
            >>> stdnorm12                      # blurry image of a ferret  
                0.3333333333333333 0.6666666666666666 #     it is a cat perhaps !?  
                >>> stdnorm1020                    # crisp image of a cat  
                0.3333333333333333 0.6666666666666666 #     it is a cat perhaps !?  


    Notes  
    
     Alternatives to Softmax  
        
        
    
    






 Gradient Based Optimization
















Gradient Based Optimization

1. Gradient Based Optimization
    Gradient Methods are algorithms to solve problems of the form  
    $$\min{x \in \mathbb{R}^{n}} fx$$  
    with the search directions defined by the gradient of the function at the current point.  
    

2. Gradient Based Algorithms
    Examples include  
     Gradient Descent minimizes arbitrary differentiable functions. 
     Conjugate Gradient minimizes sparse linear systems w/ symmetric & positive definite matrices.  
     Coordinate Descent minimizes functions of two variables.  
    



Gradient Descent

1. Gradient Descent
    Gradient Descent is a  order iterative algorithm to minimize an objective function $$J\theta$$ parameterized by a model's parameters $$\theta \in \mathbb{R}^{d}$$ by updating the parameters in the opposite direction of the gradient of the objective function $$\nabla{\theta} J\theta$$  w.r.t. to the parameters.  
    

    
    Intuition for derivation  
    1. Local Search from a starting location on a hill
    1. Feel around how a small movement/step around your location would change the height of the surrounding hill is the ground higher or lower
    1. Make the movement/step consistent as a small fixed step along some direction
    1. Measure the steepness of the hill at the new location in the chosen direction
    1. Do so by Approximating the steepness with some local information
    1. Find the direction that decreases the steepness the most

  

    1. Local Search from an initial point $$x0$$ on a function
    1. Explore the value of the function at different small nudges around $$x0$$
    1. Make the nudges consistent as a small fixed step $$\delta$$ along a normalized direction $$\hat{\boldsymbol{u}}$$
    1. Evaluate the function at the new location $$x0 + \delta \hat{\boldsymbol{u}}$$
    1. Do so by Approximating the function w/  order information Taylor expansion
    1. Find the direction $$\hat{\boldsymbol{u}}$$ that minimizes the function the most
    

2. Derivation
    A small change in $$\boldsymbol{x}$$  
    We would like to know how would a small change in $$\boldsymbol{x}$$ namely $$\Delta \boldsymbol{x}$$ would affect the value of the function $$fx$$. This will allow us to evaluate the function  
    $$f\mathbf{x}+\Delta \mathbf{x}$$  
    to find the direction that makes $$f$$ decrease the fastest.  

    Let's set up $$\Delta \boldsymbol{x}$$ the change in $$\boldsymbol{x}$$ as a fixed step $$\delta$$ along some normalized direction $$\hat{\boldsymbol{u}}$$  
    $$\Delta \boldsymbol{x} = \delta \hat{\boldsymbol{u}}$$  

    The Gradient  
    The gradient tells us how that small change in $$f\mathbf{x}+\Delta \mathbf{x}$$ affects $$f$$ through the  order approximation  
    $$f\mathbf{x}+\Delta \mathbf{x} \approx f\mathbf{x}+\Delta \mathbf{x}^{T} \nabla{\mathbf{x}} f\mathbf{x}$$  

    In the single variable case $$fx+\delta \approx fx+\delta f'x$$ we know that $$f\leftx \delta \operatorname{sign}\leftf^{\prime}x\right\right$$ is less than $$fx$$ for small enough $$\delta$$.  
    We can thus reduce $$fx$$ by moving $$x$$ in small steps with the opposite sign of the derivative.   


    The Change in $$f$$  
    The change in the objective function is  
    $$\begin{aligned} \Delta f &= f\boldsymbol{x} 0 + \Delta \boldsymbol{x}  f\boldsymbol{x} 0 
        &= f\boldsymbol{x} 0 + \delta \hat{\boldsymbol{u}}  f\boldsymbol{x} 0
        &= \delta \nablax f\boldsymbol{x} 0^T\hat{\boldsymbol{u}} + \mathcal{O}\delta^2 
        &= \delta \nablax f\boldsymbol{x} 0^T\hat{\boldsymbol{u}} 
        &\geq  \delta\|\nabla f\boldsymbol{x} 0\| 2
        \end{aligned}
    $$   
    using the  order approximation above.  
    Notice  
    $$\nablax f\boldsymbol{x} 0^T\hat{\boldsymbol{u}} \in \left \|\nabla f\boldsymbol{x} 0\| 2 \|\nabla f\boldsymbol{x} 0\| 2\right$$  
    since $$\hat{\boldsymbol{u}}$$ is a unit vector; either aligned with $$\nablax f\boldsymbol{x} 0$$ or in the opposite direction; it contributes nothing to the magnitude of the dot product.  

    So the $$\hat{\boldsymbol{u}}$$ that changes the above inequality to equality achieves the largest negative value moves the most downhill. That vector $$\hat{\boldsymbol{u}}$$ is then the one in the negative direction of $$\nablax f\boldsymbol{x} 0$$; the opposite direction of the gradient.  


    The Directional Derivative  
    The directional derivative in direction $$\boldsymbol{u}$$ a unit vector is the slope of the function $$f$$ in direction $$\boldsymbol{u}$$. In other words the directional derivative is the derivative of the function $$f\boldsymbol{x}+\alpha \boldsymbol{u}$$ with respect to $$\delta$$ evaluated at $$\delta= 0$$.  
    Using the chain rule we can see that $$\frac{\partial}{\partial \delta} f\boldsymbol{x}+\delta \boldsymbol{u}$$ evaluates to $$\boldsymbol{u}^{\top} \nabla{\boldsymbol{x}} f\boldsymbol{x}$$ when $$\delta=0$$.  


    Minimizing $$f$$  
    To minimize $$f$$ we would like to find the direction in which $$f$$ decreases the fastest. We do so by using the directional derivative  
    $$\begin{aligned} & \min {\boldsymbol{u} \boldsymbol{u}^{\top} \boldsymbol{u}=1} \boldsymbol{u}^{\top} \nabla{\boldsymbol{x}} f\boldsymbol{x} 
    =& \min{\boldsymbol{u} \boldsymbol{u}^{\top} \boldsymbol{u}=1}\|\boldsymbol{u}\|{2}\left\|\nabla{\boldsymbol{x}} f\boldsymbol{x}\right\|{2} \cos \theta 
    =& \min{\boldsymbol{u}} \cos \theta  \implies& \boldsymbol{u} =  \nablax fx\end{aligned}$$    
    by substituting $$\|\boldsymbol{u}\|2 = 1$$ and ignoring factors that do not depend on $$\boldsymbol{u}$$ we get $$\min{\boldsymbol{u}} \cos \theta$$; this is minimized when $$\boldsymbol{u}$$ points in the opposite direction as the gradient.  
    Or rather because $$\hat{\boldsymbol{u}}$$ is a unit vector we need  
    $$\hat{\boldsymbol{u}} =  \dfrac{\nablax fx}{\|\nablax fx\| 2}$$  

    In other words the gradient points directly uphill and the negative gradient points directly downhill.  We can decrease $$f$$ by moving in the direction of the negative gradient.  

    The method of steepest/gradient descent  
    Proposes a new point to decrease the value of $$f$$  
    $$\boldsymbol{x}^{\prime}=\boldsymbol{x} \epsilon \nabla{\boldsymbol{x}} f\boldsymbol{x}$$  
    where $$\epsilon$$ is the learning rate defined as  
    $$\epsilon = \dfrac{\delta}{\left\|\nabla{x} fx\right\| {2}}$$    



          

    

3. The Learning Rate
    The learning rate is a hyper parameter that controls how much we are adjusting the weights of our network with respect the loss gradient.  

    The learning rate comes from a modification of the step size in the GD derivation.  
    We get the learning rate by employing a simple idea  
    We have a fixed step size $$\delta$$ that dictated how much we should be moving in the direction of steepest descent. However we would like to keep the step size from being too small or overshooting. The idea is to make the step size proportional to the magnitude of the gradient i.e. some constant multiplied by the magnitude of the gradient  
    $$\delta = \epsilon \left\|\nabla{x} fx\right\| {2}$$   
    If we do so we get a nice cancellation as follows  
    $$\begin{aligned}\Delta \boldsymbol{x} &= \delta \hat{\boldsymbol{u}}  
        &=  \delta \dfrac{\nablax fx}{\|\nablax fx\| 2} 
        &=  \epsilon \left\|\nabla{x} fx\right\| {2} \dfrac{\nablax fx}{\|\nablax fx\| 2} 
        &=  \dfrac{\epsilon \left\|\nabla{x} fx\right\| {2}}{\|\nablax fx\| 2} \nablax fx 
        &=  \epsilon \nablax fx
    \end{aligned}$$  
    where now we have a fixed learning rate instead of a fixed step size.  

    Choosing the Learning Rate  
    
     Set it to a small constant  
     Line Search evaluate $$f\left\boldsymbol{x} \epsilon \nabla{\boldsymbol{x}} f\boldsymbol{x}\right$$ for several values of $$\epsilon$$ and choose the one that results in the smallest objective value.  
        Finds a local minimum along a search direction by solving an optimization problem in 1 D.  
        e.g. for smooth $$f$$ Secant Method Newton Raphson Method may need Hessian hard for large dims  
            for non smooth $$f$$ use direct line search e.g. golden section search  
        Note usually NOT used in DL  
     Trust Region Method  
     Grid Search is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. It is guided by some performance metric typically measured by cross validation on the training set or evaluation on a held out validation set.  

        In PBT a population of models are created. They are all continuously trained in parallel. When any member of the population has had sufficiently long to train to show improvement its validation accuracy is compared to the rest of the population. If its performance is in the lowest $$20\%$$ then it copies and mutates the hyper parameters and variables of one of the top $$20\%$$ performers.  
        In this way the most successful hyper parameters spawn many slightly mutated variants of themselves and the best hyper parameters are likely discovered.  
     Bayesian Optimization is a global optimization method for noisy black box functions. Applied to hp optimization it builds a probabilistic model of the function mapping from hp values to the objective evaluated on a validation set. By iteratively evaluating a promising hp configuration based on the current model and then updating it it aims to gather observations revealing as much information as possible about this function and in particular the location of the optimum. It tries to balance exploration hps for which the outcome is most uncertain and exploitation hps expected close to the optimum.  
        In practice it has been shown to obtain better results in fewer evaluations compared to grid search and random search due to the ability to reason about the quality of experiments before they are run.  
            

    Line Search VS Trust Region  
    Trust region methods are in some sense dual to line search methods trust region methods  choose a step size the size of the trust region and then a step direction while line search methods  choose a step direction and then a step size.  


    Learning Rate Schedule  
    A learning rate schedule changes the learning rate during learning and is most often changed between epochs/iterations. This is mainly done with two parameters decay and momentum.  
     Decay serves to settle the learning in a nice place and avoid oscillations a situation that may arise when a too high constant learning rate makes the learning jump back and forth over a minima and is controlled by a hyperparameter.  
     Momentum is analogous to a ball rolling down a hill; we want the ball to settle at the lowest point of the hill corresponding to the lowest error. Momentum both speeds up the learning increasing the learning rate when the error cost gradient is heading in the same direction for a long time and also avoids local minima by 'rolling over' small bumps.  
            

    Types of learning rate schedules for Decay  
    
     Time based learning schedules alter the learning rate depending on the learning rate of the previous time iteration. Factoring in the decay the mathematical formula for the learning rate is  
        $${\displaystyle \eta{n+1}={\frac {\eta{n}}{1+dn}}}$$  
        where $$\eta$$ is the learning rate $$d$$ is a decay parameter and $$n$$ is the iteration step.  
     Step based learning schedules changes the learning rate according to some pre defined steps  
        $${\displaystyle \eta{n}=\eta{0}d^{floor{\frac {1+n}{r}}}}$$   
        where $${\displaystyle \eta{n}}$$ is the learning rate at iteration $$n$$ $$\eta{0}$$ is the initial learning rate $$d$$ is how much the learning rate should change at each drop 0.5 corresponds to a halving and $$r$$ corresponds to the droprate or how often the rate should be dropped $$10$$ corresponds to a drop every $$10$$ iterations. The floor function here drops the value of its input to $$0$$ for all values smaller than $$1$$.  
     Exponential learning schedules are similar to step based but instead of steps a decreasing exponential function is used. The mathematical formula for factoring in the decay is  
        $$ {\displaystyle \eta{n}=\eta{0}e^{ dn}}$$  
        where $$d$$ is a decay parameter.  
    

4. Convergence
    Gradient Descent converges when every element of the gradient is zero or very close to zero within some threshold.  

    With certain assumptions on $$f$$ convex $$\nabla f$$ lipschitz and particular choices of $$\epsilon$$ chosen via line search etc. convergence to a local minimum can be guaranteed.  
    Moreover if $$f$$ is convex all local minima are global minimia so convergence is to the global minimum.  
    

5. Choosing tuning the hyperparameters
    We can set/tune most hyperparameters by reasoning about their effect on model capacity.  
    


    Important HPs  
    
    1. Learning Rate  
    1. \# Hidden Units  
    1. Mini batch Size  
    1. Momentum Coefficient  

    
    Hyperparameter search  
    Sample at random in a grid hypercube of different parameters then zoom in to a tighter range of "good" values.  
    Search sample on a logarithmic scale to get uniform sizes between values  
     Select value $$r \in a b$$ e.g. $$\in  4 0$$ and set your hp as $$10^r$$ e.g. $$\epsilon = 10^{r}$$. You'll be effectively sampling $$\in 10^{ 4} 10^0 \iff 0.0001 1$$.     
    



8. Notes
    
     Neural nets are unconstrained optimization problems with many many local minima. They sometimes benefit from line search or  order optimization algorithms but when the input data set is very large researchers often favor the dumb blind stochastic versions of gradient descent.  
     Grid search suffers from the curse of dimensionality but is often embarrassingly parallel because typically the hyperparameter settings it evaluates are independent of each other.  
    


Gradient Descent Variants

There are three variants of gradient descent which differ in the amount of data used to compute the gradient. The amount of data imposes a trade off between the accuracy of the parameter updates and the time it takes to perform the update.  


1. Batch Gradient Descent
    Batch Gradient Descent AKA Vanilla Gradient Descent computes the gradient of the objective wrt. the parameters $$\theta$$ for the entire dataset  
    $$\theta=\theta \epsilon \cdot \nabla{\theta} J\theta$$  

    Since we need to compute the gradient for the entire dataset for each update this approach can be very slow and is intractable for datasets that can't fit in memory.  
    Moreover batch GD doesn't allow for an online learning approach.  
    

2. Stochastic Gradient Descent
    SGD performs a parameter update for each data point  
    $$\theta=\theta \epsilon \cdot \nabla{\theta} J\left\theta ; x^{i} ; y^{i}\right$$  

    SGD exhibits a lot of fluctuation and has a lot of variance in the parameter updates. However although SGD can potentially move in the wrong direction due to limited information; in practice if we slowly decrease the learning rate it shows the same convergence behavior as batch gradient descent almost certainly converging to a local or the global minimum for non convex and convex optimization respectively.  
    Moreover the fluctuations it exhibits enables it to jump to new and potentially better local minima.  

    Notes  
    
     Why reduce the learning rate after every epoch?  
        This is due to the fact that the random sampling of batches acts as a source of noise which might make SGD keep oscillating around the minima without actually reaching it.  
        It is necessary to guarantee convergence.  
     The following conditions guarantee convergence under convexity conditions for SGD  
        $$\begin{array}{l}{\sum{k=1}^{\infty} \epsilon{k}=\infty \quad \text { and }}  {\sum{k=1}^{\infty} \epsilon{k}^{2}<\infty}\end{array}$$  

            
    

3. Mini batch Gradient Descent
    A hybrid approach that perform updates for a pre specified mini batch of $$n$$ training examples  
    $$\theta=\theta \epsilon \cdot \nabla{\theta} J\left\theta ; x^{i  i+n} ; y^{i  i+n}\right$$

    This allows it to  
    1. Reduce the variance of the parameter updates $$\rightarrow$$ more stable convergence
    2. Makes use of matrix vector highly optimized libraries  




Gradient Descent "Optimization"

1. Challenges in vanilla approaches to gradient descent
    All the variants described above however do not guarantee "good" convergence due to some challenges  
     Choosing a proper learning rate is usually difficult  
        A learning rate that is too small leads to painfully slow convergence while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.  
      Learning rate schedules^1 try to adjust the learning rate during training by e.g. annealing i.e. reducing the learning rate according to a pre defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds however have to be defined in advance and are thus unable to adapt to a dataset's characteristics^2.  
     The learning rate is fixed for all parameter updates  
        If our data is sparse and our features have very different frequencies we might not want to update all of them to the same extent but perform a larger update for rarely occurring features.  
     Another key challenge of minimizing highly non convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima. Dauphin et al.^3 argue that the difficulty arises in fact not from local minima but from saddle points i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error which makes it notoriously hard for SGD to escape as the gradient is close to zero in all dimensions.  
    

22.Preliminaries  Important Concepts
    Exponentially Weighted Averages  

    $$v{t}=\beta v{t 1}+ 1 \beta \theta{t}$$  
    You can think of $$vt$$ as approximately averaging over $$\approx \dfrac{1}{1 \beta}$$ previous values $$\thetai$$.  

    The larger the $$\beta$$ the slower $$vt$$ adapts to changes in new $$\theta$$ and the less noisy the value of $$vt$$.  

    Intuition
    It is a recursive equation. Thus 
    $$v{100} = 1 \beta \theta{100} + 1 \beta\beta \theta{99} + 1 \beta\beta^{2} \theta{98} + 1 \beta\beta^{3} \theta{97} + 1 \beta\beta^{4} \theta{96} + \ldots + 1 \beta\beta^{100} \theta{1}$$  
     It is an element wise product between the values of $$\thetai$$ and an exponentially decaying function $$vi$$.  
        For $$ T=100 \beta=0.9$$  

     The sum of the coefficients of $$\thetai$$ is equal to $$\approx 1$$   
        But not exactly $$1$$ which is why bias correction is needed.  
     It takes about $$1 \beta^{\dfrac{1}{\beta}}$$ time steps for $$v$$ to decay to about a  of its peak value. So after $$1 \beta^{\dfrac{1}{\beta}}$$ steps the weight decays to about a  of the weight of the current time step $$\theta$$ value.  
        In general   
        $$1 \epsilon^{\dfrac{1}{\epsilon}} \approx \dfrac{1}{e} \approx 0.35 \approx \dfrac{1}{3}$$    

    
    

    Exponentially Weighted Averages Bias Correction  

    The Problem
    The estimate of the  value $$\theta1$$ will not be a good estimate of because it will be multiplied by $$1 \beta << 1$$. This will be a much lower estimate especially during the initial phase of the estimate. It will produce the purple curve instead of the green curve  


    Bias Correction
    Replace $$vt$$ with  
    $$\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ \dfrac{v{t}}{1 \beta^{t}}$$   
     Small $$t$$ $$\implies \beta^t$$ is large $$\implies \dfrac{1}{1 \beta^t}$$ is large  
     Large $$t$$ $$\implies \beta^t$$ is large $$\implies \dfrac{1}{1 \beta^t} \approx 1$$  
     


2. Momentum
    Motivation  
    SGD has trouble navigating ravines i.e. areas where the surface curves much more steeply in one dimension than in another^4 which are common around local optima.  
    In these scenarios SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.  

    Momentum  
    Momentum^5 is a method that helps accelerate SGD in the relevant direction and dampens oscillations image^. It does this by adding a fraction $$\gamma$$ of the update vector of the past time step to the current update vector  
    $$\begin{aligned} v{t} &=\gamma v{t 1}+\eta \nabla{\theta} J\theta  \theta &=\theta v{t} \end{aligned}$$  
    Note Some implementations exchange the signs in the equations. The momentum term $$\gamma$$ is usually set to $$0.9$$ or a similar value and $$v0 = 0$$.  



    Essentially when using momentum we push a ball down a hill. The ball accumulates momentum as it rolls downhill becoming faster and faster on the way until it reaches its terminal velocity if there is air resistance i.e.  $$\gamma < 1$$. The same thing happens to our parameter updates The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result we gain faster convergence and reduced oscillation.  
    In this case we think of the equation as  
    $$v{t} =\underbrace{\gamma}{\text{friction }} \ \underbrace{v{t 1}}{\text{velocity}}+\eta \underbrace{\nabla{\theta} J\theta} {\text{acceleration}}$$  
    Instead of using the gradient to change the position of the weight "particle" use it to change the velocity.  Hinton  




          
        Momentum Calculation EWAs  
        $$\begin{align} 
            v{dw} &= \beta\v{dw}+1 \beta dw  
            v{db} &=\beta\v{db}+1 \beta db \end{align}$$  
        Parameter Updates  
        $$\begin{align} 
            w &= w  \epsilon\v{dw}  
            b &= b  \epsilon\v{db} \end{align}$$  


          

    Notes  
    
     Bias Correction is NOT used in practice; only 10 iterations needed to catch up.  
     The $$1 \beta$$ coefficient usually gets dropped in the literature. The effect is that that lr needs to be rescaled which is not a problem.  
    

    


3. Nesterov Accelerated Gradient
    Motivation  
    Momentum is good however a ball that rolls down a hill blindly following the slope is highly unsatisfactory. We'd like to have a smarter ball a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.  

    Nesterov Accelerated Gradient NAG  
    NAG^6 is a way to five our momentum term this kind of prescience. Since we know that we will use the momentum term $$\gamma\v{t 1}$$ to move the parameters $$\theta$$ we can compute a rough approximation of the next position of the parameters with $$\theta  \gamma v{t 1}$$ w/o the gradient. This allows us to effectively look ahead by calculating the gradient not wrt. our current parameters $$\theta$$ but wrt. the approximate future position of our parameters  
    $$\begin{aligned} v{t} &=\gamma v{t 1}+\eta \nabla{\theta} J\left\theta \gamma v{t 1}\right  \theta &=\theta v{t} \end{aligned}$$  
    $$\gamma = 0.9$$  

    While Momentum  computes the current gradient small blue vector and then takes a big jump in the direction of the updated accumulated gradient big blue vector NAG  makes a big jump in the direction of the previous accumulated gradient brown vector measures the gradient and then makes a correction red vector which results in the complete NAG update green vector. This anticipatory update prevents us from going too fast and results in increased responsiveness which has significantly increased the performance of RNNs on a number of tasks^7.  

    Notes  
    
     This really helps the optimization of recurrent neural networks^8  
     Momentum allows us to adapt our updates to the slope of our error function and speed up SGD  

     

4. Adagrad
    Motivation  
    Now that we are able to adapt our updates to the slope of our error function
    The magnitude of the gradient can be very different for different weights and can change during learning This makes it hard to choose single global learning rate.   Hinton


    Adagrad  
    Adagrad^9 is an algorithm for gradient based optimization that does just this It adapts the learning rate to the parameters performing smaller updates i.e. low learning rates for parameters associated with frequently occurring features and larger updates i.e. high learning rates for parameters associated with infrequent features.  

    Adagrad per parameter update  
    Adagrad uses a different learning rate for every parameter $$\thetai$$ at every time step $$t$$ so we  show Adagrad's per parameter update.  
    
    The SGD update for every parameter $$\thetai$$ at each time step $$t$$ is  
    $$\theta{t+1 i}=\theta{t i} \eta \cdot g{t i}$$  
    where $$g{t i}=\nabla{\theta} J\left\theta{t i}\right$$ is the partial derivative of the objective function w.r.t. to the parameter $$\thetai$$ at time step $$t$$ and $$g{t}$$ is the gradient at time step $$t$$.  

    In its update rule Adagrad modifies the general learning rate $$\eta$$ at each time step $$t$$ for every parameter $$\thetai$$ based on the past gradients that have been computed for $$\thetai$$  
    $$\theta{t+1 i}=\theta{t i} \frac{\eta}{\sqrt{G{t i i}+\epsilon}} \cdot g{t i}$$  

    $$Gt \in \mathbb{R}^{d \times d}$$ here is a diagonal matrix where each diagonal element $$i i$$ is the sum of the squares of the gradients wrt $$\thetai$$ up to time step $$t$$^12 while $$\epsilon$$ is a smoothing term that avoids division by zero $$\approx 1e  8$$.  
    Without the sqrt the algorithm performs much worse  

    As $$Gt$$ contains the sum of the squares of the past gradients w.r.t. to all parameters $$\theta$$ along its diagonal we can now vectorize our implementation by performing a matrix vector product $$\odot$$ between $$Gt$$ and  $$gt$$  
    $$\theta{t+1}=\theta{t} \frac{\eta}{\sqrt{G{t}+\epsilon}} \odot g{t}$$  


    Properties  
    
     Well suited for dealing with sparse data because it adapts the lr of each parameter wrt feature frequency  
         Pennington et al.^11 used Adagrad to train GloVe word embeddings as infrequent words require much larger updates than frequent ones.  
     Eliminates need for manual tuning of lr  
        One of Adagrad's main benefits is that it eliminates the need to manually tune the learning rate. Most implementations use a default value of $$0.01$$ and leave it at that.  
     Weakness  > Accumulation of the squared gradients in the denominator  
        Since every added term is positive the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small at which point the algorithm is no longer able to acquire additional knowledge.  

    

5. Adadelta
    Motivation  
    Adagrad has a weakness where it suffers from aggressive monotonically decreasing lr

    Adadelta  
    Adadelta^13 is an extension of Adagrad that seeks to reduce its aggressive monotonically decreasing learning rate. Instead of accumulating all past squared gradients Adadelta restricts the window of accumulated past gradients to some fixed size $$w$$.  

    Instead of inefficiently storing $$w$$ previous squared gradients the sum of gradients is recursively defined as a decaying average of all past squared gradients. The running average $$E\leftg^{2}\right {t}$$ at time step $$t$$ then depends as a fraction $$\gamma$$ similarly to the Momentum term only on the previous average and the current gradient  
    $$E\leftg^{2}\right{t}=\gamma E\leftg^{2}\right{t 1}+1 \gamma g{t}^{2}$$  

    We set $$\gamma$$ to a similar value as the momentum term around $$0.9$$.  
    For clarity we now rewrite our vanilla SGD update in terms of the parameter update vector $$\Delta \theta{t}$$
    $$\begin{aligned} \Delta \theta{t} &= \eta \cdot g{t i}  \theta{t+1} &=\theta{t}+\Delta \theta{t} \end{aligned}$$  

    In the parameter update vector of Adagrad we replace the diagonal matrix $$Gt$$ with the decaying average over pas squared gradients $$Eg^2 t$$  
    $$ \frac{\eta}{\sqrt{Eg^2 t+\epsilon}} \odot g{t}$$  
    Since the denominator is just the root mean squared RMS error criterion of the gradient we can replace it with the criterion short hand  
    $$ \frac{\eta}{RMSg{t}} \odot g{t}$$  

    This modified parameter update vector does NOT have the same hypothetical units as the parameter.  
    We accomplish this by  defining another exponentially decaying average this time not of squared gradients but of squared parameter updates  
    $$E\Delta \theta^2t = \gamma E\Delta \theta^2{t 1} + 1  \gamma \Delta \theta^2t$$  
    The RMS Error of parameter updates is thus  
    $$RMS\Delta \theta {t} = \sqrt{E\Delta \theta^2 t + \epsilon}$$  

    Since $$RMS\Delta \theta {t}$$ is unknown we approximate it with the $$RMS$$ of parameter updates up to until the previous time step. Replacing the learning rate $$\eta$$ in the previous update rule with $$RMS\Delta \theta {t 1}$$ finally yields the Adadelta update rule  
    $$\begin{align}  \begin{split}  \Delta \thetat &=  \dfrac{RMS\Delta \theta{t 1}}{RMSg{t}} g{t}   \theta{t+1} &= \thetat + \Delta \thetat  \end{split}  \end{align}$$  
    

    Properties  
    
     Eliminates need for lr completely  
        With Adadelta we do not even need to set a default learning rate as it has been eliminated from the update rule.  
    


6. RMSprop
    Motivation   
    RMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad's radically diminishing learning rates.  

    RMSprop  
    

    RMSprop in fact is identical to the  update vector of Adadelta that we derived above  
    $$\begin{align}  \begin{split}  Eg^2t &= 0.9 Eg^2{t 1} + 0.1 g^2t   \theta{t+1} &= \theta{t}  \dfrac{\eta}{\sqrt{Eg^2t + \epsilon}} g{t}  \end{split}  \end{align}$$  

    RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients
    Hinton suggests $$\gamma$$ to be set to $$0.9$$ while a good default value for the learning rate $$\eta$$ is $$0.001$$.  


    RMSprop as an extension of Rprop  
    Hinton actually thought of RMSprop as a way of extending Rprop to work with mini batches.  
    
     onclick="showTextwithParentPopHideevent;"}

    Rprop is equivalent to using the gradient but also dividing by the magnitude of the gradient.  
    The problem with mini batch rprop is that we divide by a different number for each mini batch.  
    So why not force the number we divide by to be very similar for adjacent mini batches?  
    That is the idea behind RMSprop.  
    

    Notes  
    
     It is of note that Hinton has tried to add momentum to RMSprop and found that "it does not help as much as it normally does  needs more investigation".  



7. Adam
    Motivation  
    Adding momentum to Adadelta/RMSprop.  

    Adam  
    Adaptive Moment Estimation Adam^14 is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients $$vt$$ like Adadelta and RMSprop Adam also keeps an exponentially decaying average of past gradients $$mt$$ similar to momentum.  

    Adam VS Momentum  
    Whereas momentum can be seen as a ball running down a slope Adam behaves like a heavy ball with friction which thus prefers flat minima in the error surface^15.  

    We compute the decaying averages of past and past squared gradients $$mt$$ and $$vt$$ respectively as follows  
    $$\begin{align}  \begin{split}  mt &= \beta1 m{t 1} + 1  \beta1 gt   vt &= \beta2 v{t 1} + 1  \beta2 gt^2  \end{split}  \end{align}$$  

    $$mt$$ and $$vt$$ are estimates of the  moment the mean and the  moment the uncentered variance of the gradients respectively hence the name of the method.  
    As $$mt$$ and $$vt$$ are initialized as vectors of $$0$$'s the authors of Adam observe that they are biased towards zero especially during the initial time steps and especially when the decay rates are small i.e. $$\beta1$$ and $$\beta2$$ are close to $$1$$.  

    They counteract these biases by computing bias corrected  and  moment estimates  
    $$\begin{align}  \begin{split}  \hat{m} t &= \dfrac{mt}{1  \beta^t1}   \hat{v} t &= \dfrac{vt}{1  \beta^t2} \end{split}  \end{align}$$  

    They then use these to update the parameters just as we have seen in Adadelta and RMSprop which yields the Adam update rule
    $$\theta{t+1} = \theta{t}  \dfrac{\eta}{\sqrt{\hat{v} t} + \epsilon} \hat{m} t$$  

    The authors propose default values of $$0.9$$ for $$\beta1$$ $$0.999$$ for $$\beta2$$ and $$10^{ −8}$$ for $$\epsilon$$. They show empirically that Adam works well in practice and compares favorably to other adaptive learning method algorithms.  
    







11.Visualization of the Algorithms
    SGD optimization on loss surface contours  

    We see their behavior on the contours of a loss surface the Beale function over time. Note that Adagrad Adadelta and RMSprop almost immediately head off in the right direction and converge similarly fast while Momentum and NAG are led off track evoking the image of a ball rolling down the hill. NAG however is quickly able to correct its course due to its increased responsiveness by looking ahead and heads to the minimum.  

    

    SGD optimization on saddle point  

    Image shows the behavior of the algorithms at a saddle point i.e. a point where one dimension has a positive slope while the other dimension has a negative slope which pose a difficulty for SGD as we mentioned before. Notice here that SGD Momentum and NAG find it difficulty to break symmetry although the two latter eventually manage to escape the saddle point while Adagrad RMSprop and Adadelta quickly head down the negative slope.  


    Analysis  
    As we can see the adaptive learning rate methods i.e. Adagrad Adadelta RMSprop and Adam are most suitable and provide the best convergence for these scenarios.  





12.Analysis  Choosing an Optimizer
     Sparse Input Data  
        If your input data is sparse then you likely achieve the best results using one of the adaptive learning rate methods. An additional benefit is that you won't need to tune the learning rate but likely achieve the best results with the default value.  

    In summary RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta except that Adadelta uses the RMS of parameter updates in the numerator update rule. Adam finally adds bias correction and momentum to RMSprop. Insofar RMSprop Adadelta and Adam are very similar algorithms that do well in similar circumstances. Kingma et al.^14 show that its bias correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar Adam might be the best overall choice.  

    Interestingly many recent papers use vanilla SGD without momentum and a simple learning rate annealing schedule. As has been shown SGD usually achieves to find a minimum but it might take significantly longer than with some of the optimizers is much more reliant on a robust initialization and annealing schedule and may get stuck in saddle points rather than local minima. Consequently if you care about fast convergence and train a deep or complex neural network you should choose one of the adaptive learning rate methods.








For a great overview of some other common tricks refer to^24  




    Generally we want to avoid providing the training examples in a meaningful order to our model as this may bias the optimization algorithm. Consequently it is often a good idea to shuffle the training data after every epoch.  

    On the other hand for some cases where we aim to solve progressively harder problems supplying the training examples in a meaningful order may actually lead to improved performance and better convergence. The method for establishing this meaningful order is called Curriculum Learning^25.  

    Zaremba and Sutskever^26 were only able to train LSTMs to evaluate simple programs using Curriculum Learning and show that a combined or mixed strategy is better than the naive one which sorts examples by increasing difficulty.  
    


    To facilitate learning we typically normalize the initial values of our parameters by initializing them with zero mean and unit variance. As training progresses and we update parameters to different extents we lose this normalization which slows down training and amplifies changes as the network becomes deeper.  

    Batch normalization^27 reestablishes these normalizations for every mini batch and changes are back propagated through the operation as well. By making normalization part of the model architecture we are able to use higher learning rates and pay less attention to the initialization parameters. Batch normalization additionally acts as a regularizer reducing and sometimes even eliminating the need for Dropout.  



    


    
    


    Neelakantan et al.^28 add noise that follows a Gaussian distribution $$\mathcal{N}0 \sigma^2t$$ to each gradient update  
    $$g{t i} = g{t i} + \mathcal{N}0 \sigma^2t$$  

    They anneal the variance according to the following schedule  
    $$\sigma^2t = \dfrac{\eta}{1 + t^\gamma}$$  

    They show that adding this noise makes networks more robust to poor initialization and helps training particularly deep and complex networks. They suspect that the added noise gives the model more chances to escape and find new local minima which are more frequent for deeper models.  






^1 H. Robinds and S. Monro “A stochastic approximation method” Annals of Mathematical Statistics vol. 22 pp. 400 407 1951.


^4 Sutton R. S. 1986. Two problems with backpropagation and other steepest descent learning procedures for networks. Proc. 8th Annual Conf. Cognitive Science Society.

^6 Nesterov Y. 1983. A method for unconstrained convex minimization problem with the rate of convergence o1/k2. Doklady ANSSSR translated as Soviet.Math.Docl. vol. 269 pp. 543  547.

^8 Sutskever I. 2013. Training Recurrent neural Networks. PhD Thesis.



^12 Duchi et al. 3 give this matrix as an alternative to the full matrix containing the outer products of all previous gradients as the computation of the matrix square root is infeasible even for a moderate number of parameters $$d$$.  

^14 Kingma D. P. &amp; Ba J. L. 2015. Adam a Method for Stochastic Optimization. International Conference on Learning Representations 1 13.



^27 Ioffe S. &amp; Szegedy C. 2015. Batch Normalization  Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv Preprint arXiv1502.03167v3.



 Sentence and Contextualized Word Representations  Multi Task/Transfer Learning


Sentence Representations

1. What?
       Sentence Representation/Embedding Learning is focused on producing one feature vector to represent a sentence in a latent semantic space while preserving linear properties distances angles.  

2. Tasks
     Sentence Classification
     Paraphrase Identification
     Semantic Similarity
     Textual Entailment i.e. Natural Language Inference
     Retrieval

3. Methods
     Multi Task Learning  
        In particular people do Pre Training on other tasks and then use the pre trained weights and fine tune them on a new task.  

4. End To End VS Pre Training
    We can always use End To End objectives however there are two problems that arise and can be mitigated by pre training  
     Paucity of Training Data  
     Weak Feedback from end of sentence only for text classification explain?


Training Sentence Representations

1. Language Model Transfer Dai and Le 2015
     Model LSTM
     Objective Language modeling objective
     Data Classification data itself or Amazon reviews
     Downstream On text classification initialize weights and continue training

2. Unidirectional Training + Transformer  OpenAI GPT Radford et al. 2018
     Model  Masked self attention
     Objective  Predict the next word left >right
     Data  BooksCorpus
     Downstream  Some task fine tuning other tasks additional multi sentence training

3. Auto encoder Transfer Dai and Le 2015
     Model LSTM
     Objective From single sentence vector reconstruct the sentence
     Data Classification data itself or Amazon reviews
     Downstream On text classification initialize weights and continue training

4. Context Prediction Transfer  SkipThought Vectors Kiros et al. 2015
     Model  LSTM
     Objective  Predict the surrounding sentences
     Data  Books important because of context
     Downstream  Train logistic regression on $$\|u v\|; u  v$$ component wise

5. Paraphrase ID Transfer Wieting et al. 2015
     Model Try many different ones
     Objective Predict whether two phrases are paraphrases or not from

         Large Scale Paraphrase Data  ParaNMT 50MT Wieting and Gimpel 2018  
             Automatic construction of large paraphrase DB
                 Get large parallel corpus English Czech 
                 Translate the Czech side using a SOTA NMT system 
                 Get automated score and annotate a sample  
             Corpus is huge but includes noise 50M sentences about 30M are high quality 
             Trained representations work quite well and generalize  
     Downstream Usage Sentence similarity classification etc.
     Result Interestingly LSTMs work well on in domain data but word averaging generalizes better


7. Entailment Transfer  InferSent Conneau et al. 2017
     Previous objectives use no human labels but what if
     Objective supervised training for a task such as entailment learn generalizable embeddings?  
         Task is more difficult and requires capturing nuance → yes? or data is much smaller → no?  
     Model Bi LSTM + max pooling  
     Data Stanford NLI MultiNLI
     Results Tends to be better than unsupervised objectives such as SkipThought  



Contextualized Word Representations










FOURTH











 2.1  Basics and Definitions








Linear Functions and Transformations and Maps

1. Linear Functions
        Linear functions are functions which preserve scaling and addition of the input argument.
    Formally
         A function $$f \mathbf{R}^n \rightarrow \mathbf{R}$$ is linear if and only if $$f$$ preserves scaling and addition of its arguments  
        for every $$x \in \mathbf{R}^n$$ and $$\alpha \in \mathbf{R} \ f\alpha x = \alpha fx$$; and
        for every $$x1 x2 \in \mathbf{R}^n fx1+x2 = fx1+fx2$$.

2. Affine Functions
       Affine functions are linear functions plus constant functions.
       Formally  
       A function f is affine if and only if the function $$\tilde{f} \mathbf{R}^n \rightarrow \mathbf{R}$$ with values $$\tilde{f}x = fx f0$$ is linear. $$\diamondsuit$$
    Equivalently
       A map $$f  \mathbf{R}^n \rightarrow \mathbf{R}^m$$ is affine if and only if the map $$g  \mathbf{R}^n \rightarrow \mathbf{R}^m$$ with values $$gx = fx  f0$$ is linear.

0. Quadratic Function
       A function $$q  \mathbf{R}^n \rightarrow \mathbf{R}$$ is said to be a quadratic function if it can be expressed as
       $$
        qx = \sum{i=1}^n \sum{j=1}^n A{ij} xi xj + 2 \sum{i=1}^n bi xi + c 
        $$  
       for numbers $$A{ij} bi$$ and $$c i j \in {1 \ldots n}$$.
        A quadratic function is thus an affine combination of the $$\ xi$$'s and all the "cross products" $$xixj$$.  
    We observe that the coefficient of $$xixj$$ is $$A{ij} + A{ji}$$.  
    The function is said to be a quadratic form if there are no linear or constant terms in it $$bi = 0 c=0.$$
    The Hessian of a quadratic function is always constant.


3. Equivalent Definitions of Linear Functions Theorem
       A map $$f  \mathbf{R}^n \rightarrow \mathbf{R}^m$$ is linear if and only if either one of the following conditions hold
        $$f$$ preserves scaling and addition of its arguments
              for every $$x \in \mathbf{R}^n$$ and $$\alpha \in \mathbf{R}  f\alpha x = \alpha fx$$; and
             for every $$x1 x2 \in \mathbf{R}^n fx1+x2 =  fx1+fx2.$$
        $$f$$ vanishes at the origin
             $$f0 = 0$$ and
             It transforms any line segment $$\in \mathbf{R}^n$$ into another segment $$\in \mathbf{R}^m$$
            $$\forall \ x y \in \mathbf{R}^n \; \forall \ \lambda \in 01 ~~ f\lambda x + 1 \lambda y = \lambda fx + 1 \lambda fy$$.  
                 $$f$$ is differentiable vanishes at the origin and the matrix of its derivatives is constant.
                 There exist $$A \in \mathbf{R}^{m \times n}$$ such that $$\ \forall  x \in \mathbf{R}^n ~~ fx = Ax$$. 
    
    


4. Vector Form and the scalar product
    Theorem Representation of affine function via the scalar product.  
    $$\ \ \ \ \ \ \ \ $$    A function $$f \mathbf{R}^n \rightarrow \mathbf{R}$$ is affine if and only if it can be expressed via a scalar product  
        $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ \ \ \ \ \ \ \ \ $$  $$fx = a^Tx + b$$   
        $$\ \ \ \ \ \ \ \ $$ for some unique pair $$ab$$ with $$a \in \mathbf{R}^{n}$$ and $$b \in \mathbf{R}$$ given by $$ai = fei f0$$ with $$ei$$ $$\ \ \ \ \ \ \ \ \ $$the $$i th$$ unit vector $$\in \mathbf{R}^n i=1 \ldots n$$ and $$\ b = f0$$.  
    The function is linear $$\iff b = 0$$.  

    The theorem shows that a vector can be seen as a linear function from the "input" space $$\mathbf{R}^n$$ to the "output" space $$\mathbf{R}$$.  

    Both points of view matrices as simple collections of numbers or as linear functions are useful.

0. Gradient of a Linear Function
    

5. Gradient of an Affine Function
       The gradient of a function $$f  \mathbf{R}^n \rightarrow \mathbf{R}$$ at a point $$x$$ denoted $$\nabla fx$$ is the vector of  derivatives with respect to $$x1 \ldots xn$$.
    When $$n=1$$ there is only one input variable the gradient is simply the derivative.  
       An affine function $$f  \mathbf{R}^n \rightarrow \mathbf{R}$$ with values $$fx = a^Tx+b$$ has the gradient
       $$\nabla fx = a$$.  
    i.e. For all Affine Functions the gradient is the constant vector $$a$$.

6. Interpreting $$a$$ and $$b$$
        The $$b=f0$$ is the constant term. For this reason it is sometimes referred to as the bias or intercept.  
            as it is the point where $$f$$ intercepts the vertical axis if we were to plot the graph of the function.
        The terms $$aj j=1 \ldots n$$ which correspond to the gradient of $$f$$ give the coefficients of influence of $$xj$$ on $$f$$. 
            For example if $$a1 >> a3$$ then the  component of $$x$$ has much greater influence on the value of $$fx$$ than the .

7.  order approximation of non linear functions
        One dimensional case  
        Consider a function of one variable $$f  \mathbf{R} \rightarrow \mathbf{R}$$ and assume it is differentiable everywhere.  
        Then we can approximate the values function at a point $$x$$ near a point $$x0$$ as follows  
       $$ fx \simeq lx = fx0 + f'x0 x x0  $$
       $$\ \ \ \ \  \ \ \ $$ where $$f'x$$ denotes the derivative of $$f$$ at $$x$$.
        Multi dimensional  
        Let us approximate a differentiable function $$f  \mathbf{R}^n \rightarrow \mathbf{R}$$ by a linear function $$l$$ so that $$f$$ and $$l$$ coincide up and including to the  derivatives.  
        The approximate function l must be of the form  
       $$lx = a^Tx + b $$  
       $$\ \ \ \ \  \ \ \ $$ where $$a \in \mathbf{R}^n$$ and $$b \in \mathbf{R}$$.  
    The corresponding approximation $$l$$ is called the  order approximation to $$f$$ at $$x0$$.  

        Our condition that $$l$$ coincides with $$f$$ up and including to the  derivatives shows that we must have  
       $$  \nabla lx = a = \nabla fx0 \;\; a^Tx0 + b = fx0 $$  
       $$\ \ \ \ \  \ \ \ $$   where $$\nabla fx0$$ is the gradient of $$f$$ at $$x0$$. 

8.  order Expansion of a function Theorem
       The  order approximation of a differentiable function $$f$$ at a point $$x0$$ is of the form  
       $$fx \approx lx = fx0 + \nabla fx0^T x x0$$   
       where $$\nabla fx0 \in \mathbf{R}^n$$ is the gradient of $$f$$ at $$x0$$.
    
    





Matrices

0. Matrix Transpose
       $$ A{ij} =  A{ji}^T \; \forall i j \in \mathbf{F}$$  
     Properties  
         $$AB^T = B^TA^T.$$  

1. Matrix vector product
        $$Axi = \sum{j=1}^n A{ij}xj  \;\; i=1 \ldots m. $$
        Where the Matrix is $$\in {\mathbf{R}}^{m \times n}$$ and the vector is $$ \in {\mathbf{R}}^m$$.
        Interpretations  
           1. A linear combination of the columns of $$A$$    
            $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \   Ax = \left \begin{array}{c} a1^Tx  \ldots  am^Tx \end{array} \right^T$$ .   
            where the columns of $$A$$ are given by the vectors $$ai i=1 \ldots n$$ so that $$A = a1  \ldots an$$.

            2. Scalar Products of Rows of $$A$$ with $$x$$    
            $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  Ax = \sum{i=1}^n xi ai$$ .   
            where the rows of $$A$$ are given by the vectors $$ai^T i=1 \ldots m$$
            $$A = \left \begin{array}{c} a1^T  \ldots  am^T \end{array} \right^T$$.

    
    


2. Left Product
        If $$z \in \mathbf{R}^m$$ then the notation $$z^TA$$ is the row vector of size $$n$$ equal to the transpose of the column vector $$A^Tz \in \mathbf{R}^n$$  
       $$ z^TAj = \sum{i=1}^m A{ij}zi  \;\; j=1 \ldots n. $$
    
    



3. Matrix matrix product
       $$  AB{ij} = \sum{k=1}^n A{ik} B{kj}$$.  
       where $$A \in \mathbf{R}^{m \times n}$$ and $$B \in \mathbf{R}^{n \times p}$$ and the notation $$AB$$ denotes the $$m \times p$$ matrix given above.
        Interpretations  
           1. Transforming the columns of $$B$$ into $$Abi$$    
            $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \    AB = A \left \begin{array}{ccc} b1 & \ldots & bn \end{array} \right =  \left \begin{array}{ccc} Ab1 & \ldots & Abn \end{array} \right$$ .   
            where the columns of $$B$$ are given by the vectors $$bi i=1 \ldots n$$ so that $$B = b1  \ldots bn$$.  
            2. Transforming the Rows of $$A$$ into $$ai^TB$$      
            $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  AB = \left\begin{array}{c} a1^T  \vdots  am^T \end{array}\right B = \left\begin{array}{c} a1^TB  \vdots  am^TB \end{array}\right$$.   
            where the rows of $$A$$ are given by the vectors $$ai^T i=1 \ldots m$$
            $$A = \left \begin{array}{c} a1^T  \ldots  am^T \end{array} \right^T$$.

4. Block Matrix Products

5. Outer Products

6. Trace
       The trace of a square $$n \times n$$ matrix $$A$$ denoted by $$\mathbf{Tr} A$$ is the sum of its diagonal elements  
       $$ \mathbf{Tr} A = \sum{i=1}^n A{ii}$$.  
     Properties  
         $$\mathbf{Tr} A = \mathbf{Tr} A^T$$.  
         $$\mathbf{Tr} AB = \mathbf{Tr} BA$$.
         $$\mathbf{Tr}XYZ = \mathbf{Tr}ZXY = \mathbf{Tr}YZX$$.
         $${\displaystyle \operatorname{tr} A+B = \operatorname{tr} A+\operatorname{tr} B}$$.
         $${\displaystyle \operatorname{tr} cA = c\operatorname{tr} A}$$.
         $${\displaystyle \operatorname{tr} \leftX^{\mathrm {T} }Y\right=\operatorname{tr} \leftXY^{\mathrm {T} }\right=\operatorname{tr} \leftY^{\mathrm {T} }X\right=\operatorname{tr} \leftYX^{\mathrm {T} }\right=\sum {ij}X{ij}Y{ij}}$$.
         $${\displaystyle \operatorname{tr} \leftX^{\mathrm {T} }Y\right=\sum {ij}X\circ Y{ij}}\ \ \ \ $$ The Hadamard product.
         Arbitrary permutations of the product of matrices is not allowed. Only cyclic permutations are.
            However if products of three symmetric matrices are considered any permutation is allowed.
         The trace of an idempotent matrix $$A$$ is the dimension of A.
         The trace of a nilpotent matrix is zero.
         If $$fx = x − \lambda1^{d1} \cdots x − \lambdak^{dk}$$ is the characteristic polynomial of a matrix $$A$$ then $${\displaystyle \operatorname{tr} A=d{1}\lambda{1} + \cdots + d{k} \lambda{k}}$$.
         When both $$A$$ and $$B$$ are $$n \times n$$ the trace of the ring theoretic commutator of $$A$$ and $$B$$ vanishes $$\mathbf{tr}A B = 0$$; one can state this as "the trace is a map of Lie algebras $${\displaystyle \mathbf{GL{n}} \to k}$$ from operators to scalars" as the commutator of scalars is trivial it is an abelian Lie algebra.
         The trace of a projection matrix is the dimension of the target space.
            $${\displaystyle 
            P{X} = X\leftX^{\mathrm {T} }X\right^{ 1}X^{\mathrm {T} } 
            \Rightarrow 
            \operatorname {tr} \leftP{X}\right = \operatorname {rank} \leftX\right}$$



7. Scalar Product
       $$\langle A B \rangle = \mathbf{Tr}A^TB = \displaystyle\sum{i=1}^m\sum{j=1}^n A{ij}B{ij}.$$  
    The above definition is Symmetric  
       $$\implies \langle AB \rangle =  \mathbf{Tr} A^TB = \mathbf{Tr} A^TB^T =  \mathbf{Tr} B^TA = \langle BA \rangle .$$  
    We can interpret the matrix scalar product as the vector scalar product between two long vectors of length $$mn$$ each obtained by stacking all the columns of $$A B$$ on top of each other.

8. Special Matrices
     Diagonal matrices/workfiles/research/la/symmat are square matrices $$A$$ with $$A{ij} = 0$$ when $$i \ne j$$.  
     Symmetric matrices are square matrices that satisfy $$A{ij} = A{ji} $$for every pair $$ij$$.
     Triangular matrices are square matrices that satisfy $$A{ij} = A{ji} $$for every pair $$ij$$.    
$${F}}^{2}=\|AR\|{\rm {F}}^{2}=\|RA\|{\rm {F}}^{2}}$$ for any rotation matrix $$R$$.

        3. Invariant under a unitary transformation for complex matrices.

        4. $${\displaystyle \|A^{\rm {T}}A\|{\rm {F}}=\|AA^{\rm {T}}\|{\rm {F}}\leq \|A\|{\rm {F}}^{2}}$$.

        5. $${\displaystyle \|A+B\|{\rm {F}}^{2}=\|A\|{\rm {F}}^{2}+\|B\|{\rm {F}}^{2}+2\langle AB\rangle {\mathrm {F} }}$$.


4. $$l{\infty\infty}$$ Max Norm
       $$ \|A\|{\max} = \max{ij} |a{ij}|.$$

     Properties  
        1. NOT Submultiplicative.

5. The Spectral Norm
       $${\displaystyle \|A\|{2}={\sqrt {\lambda {\max }A^{^{}}A}}=\sigma {\max }A} = {\displaystyle \max{\|x\|2!=0}\|Ax\|2/\|x\|2}.$$  
    The spectral norm of a matrix $${\displaystyle A} $$ is the largest singular value of $${\displaystyle A}$$. 
    i.e. the square root of the largest eigenvalue of the positive semidefinite matrix $${\displaystyle A^{}A}.$$

     The Spectral Radius of $$A \ $$  denoted $$\rhoA$$
       $$ \lim{r\rightarrow\infty}\|A^r\|^{1/r}=\rhoA.$$

     Properties  
        1. Submultiplicative.

        2. Satisfies $${\displaystyle \|A^{r}\|^{1/r}\geq \rho A}$$ where $$\rhoA$$ is the spectral radius of $$A$$.

        3. It is an "induced vector norm".



8. Equivalence of Norms
    
    


8. Applications
    1. RMS Gain Frobenius Norm.

    2. Peak Gain Spectral Norm.

    3. Distance between Matrices Frobenius Norm.
        
        


    4. Direction of Maximal Variance Spectral Norm.
        
        





 Contextual Word Representations and Pretraining





Word Representations and their progress

1. Word Representations Accepted Methods
    The current accepted methods provide one representation of words  
    1. Word2Vec
    2. GloVe 
    3. FastText  

    

    Problems  
     Word Senses Always the same representation for a word type regardless of the context in which a word token occurs  
         We might want very fine grained word sense disambiguation e.g. not just 'holywood star' and 'astronomical star'; but also 'rock star' 'star student' etc.  

     We just have one representation for a word but words have different aspects including semantics syntactic behavior and register/connotations e.g. when is it appropriate to use 'bathroom' vs 'shithole' etc.; 'can' noun vs 'can' verb have same vector  

    Possible Solution that we always had?  
     In a NLM LSTM layers are trained to predict the next word producing hidden/state vectors that are basically context specific word representations at each position  
      

2. TagLM Peters et al. 2017 — Pre Elmo
    Idea 
     Want meaning of word in context but standardly learn task RNN only on small task labeled data e.g. NER.  
     Do semi supervised approach where we train NLM on large unlabeled corpus rather than just word vectors.  
     Run a BiRNN LM and concatenate the For and Back representations
     Also train a traditional word embedding w2v on the word and concatenate with Bi LM repr.
     Also train a Char CNN/RNN to get character level embedding and concatenate all of them together

    Details  
     Language model is trained on 800 million training words of "Billion word benchmark"  
     Language model observations  
         An LM trained on supervised data does not help 
         Having a bidirectional LM helps over only forward by about 0.2 
         Having a huge LM design ppl 30 helps over a smaller model ppl 48 by about 0.3 
     Task specific BiLSTM observations  
         Using just the LM embeddings to predict isn't great 88.17 F1  
             Well below just using an BiLSTM tagger on labeled data      

    
    

3. Cove  Pre Elmo
     Also has idea of using a trained sequence model to provide context to other NLP models 
     Idea Machine translation is meant to preserve meaning so maybe that's a good objective? 
     Use a 2 layer bi LSTM that is the encoder of seq2seq + attention NMT system as the context provider 
     The resulting CoVe vectors do outperform GloVe vectors on various tasks 
     But the results aren't as strong as the simpler NLM training described in the rest of these slides so seems abandoned 
         Maybe NMT is just harder than language modeling? 
         Maybe someday this idea will return?

    

4. Elmo  Embeddings from Language Models Peters et al. 2018
    Idea  
     Train a bidirectional LM  
     Aim at performant but not overly large LM  
         Use 2 biLSTM layers  
         Use character CNN to build initial word representation only  
             2048 char n gram filters and 2 highway layers 512 dim projection  
         Use 4096 dim hidden/cell LSTM states with 512 dim projections to next input  
         Use a residual connection  
         Tie parameters of token input and output softmax and tie these between forward and backward LMs  

    Key Results  
     ELMo learns task specific combination of BiLM representations  
     This is an innovation that improves on just using top layer of LSTM stack
    $$\begin{aligned} R{k} &=\left\{\mathbf{x}{k}^{L M} \overrightarrow{\mathbf{h}}{k j}^{L M} \mathbf{h}{k j}^{L M} | j=1 \ldots L\right\}  &=\left\{\mathbf{h}{k j}^{L M} | j=0 \ldots L\right\} \end{aligned}$$  
    $$\mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}{k}^{t a s k}=E\leftR{k} ; \Theta^{t a s k}\right=\gamma^{task} \sum{j=0}^{L} s{j}^{task} \mathbf{h} {k j}^{L M}$$  
     $$\gamma^{\text { task }}$$ scales overall usefulness of ELMo to task;  
     $$s^{\text { task }}$$ are softmax normalized mixture model weights  
    Possibly this is a way of saying different semantic and syntactic meanings of a word are represented in different layers; and by doing a weighted average of those in a task specific manner we can leverage the appropriate kind of information for each task.  

    Using ELMo with a Task  
      run biLM to get representations for each word
     Then let whatever end task model use them
         Freeze weights of ELMo for purposes of supervised model
         Concatenate ELMo weights into task specific model
             Details depend on task
                 Concatenating into intermediate layer as for TagLM is typical
                 Can provide ELMo representations again when producing outputs as in a question answering system  

    
    Weighting of Layers  
     The two Bi LSTM NLP Layers have differentiated uses/meanings  
         Lower layer is better for lower level syntax etc.  
             POS Tagging Syntactic Dependencies NER  
         Higher layer is better for higher level semantics  
             Sentiment Semantic role labeling QA SNLI  


    Reason for Excitement  
    ELMo proved to be great for all NLP tasks even tho the core of the idea was in TagLM   
     
            
    
5. ULMfit  Universal Language Model Fine Tuning Howard and Ruder 2018
    ULMfit  Universal Language Model Fine Tuning for Text Classification  


6. BERT Devlin et al. 2018
    BERT  Bidirectional Encoder Representations from Transformers  
    Idea Pre training of Deep Bidirectional Transformers for Language Understanding.  

    Model Architecture  
     Transformer Encoder
     Self attention   > no locality bias
         Long distance context has "equal opportunity"  
     Single multiplication per layer   > efficiency on GPU/TPU
     Architectures  
         BERT Base 12 layer 768 hidden 12 head  
         BERT Large 24 layer 1024 hudden 16 heads  


    Model Training  
     Train on Wikipedia + BookCorpus
     Train 2 model sizes  
         BERT Base
         BERT Large
     Trained on $$4\times 4$$  or $$8\times 8$$ TPU slice for 4 days  


    Model Fine Tuning  
     Simply learn a classifier built on top layer for each task that you fine tune for.
                

    Problem with Unidirectional and Bidirectional LMs  
     Uni build representation incrementally; not enough context from the sentence  
     Bi  Cross Talk; words can "see themselves"  

    Solution  
     Mask out $$k\%$$ of the input words and then predict the masked words
         They always use $$k=15%$$  
        Ex "The man went to the MASK to buy a MASK of milk."  

         Too little Masking  Too Expensive to train
         Too much Masking  Not enough context   
     Other Benefits  
         In ELMo bidirectional training is done independently for each direction and then concatenated. No joint context in the model during the building of contextual reprs.
         In GPT there is only unidirectional context.  
            
    Another Objective  Next Sentence Prediction  
    To learn relationships between sentences predict whether sentence B is actual sentence that proceeeds sentence A or a random sentence for QA NLU etc..  


    Results  
    Beats every other architecture in every GLUE task NL Inference.  



    



The Transformer


1. Self Attention
     Computational Complexity Comparison  
     Self Attention/Relative Attention Interpretations  
         Can achieve Translational Equivariance like convs by removing pos encoding.
         Can model similarity graphs. 
         Connected to message passing NNs. Can think of self atten as passing messages bet pairs of nodes in graph; equiv. imposing a complete bipartite graph and you're passing messages between nodes.  
            Mathematically the difference is message passing NNs impose condition that messages pass ONLY bet pairs of nodes; while self atten uses softmax and thus passes messages between all nodes.  

     Self Attention Summary  
         Constant path length between any two positions
         Unbounded memory ie no fixed size hstate 
         Trivial to parallelize per layer
         Models self similarity
         Relative attention provides expressive timing equivariance and extends naturally to graphs  

     Current Issues  
         Slow Generation  
            Mainly due to Auto Regressive generation which is necessary to break the multimodality of generation. Multimodality prohibits naive parallel generation.  
            Multimodality refers to the fact that there are multiple different sentences in german that are considered a correct translation of a sentence in english and they all depend on the word that was generated  ie no parallelization.   
         Active Area of Research  
            Non Auto Regressive Transformers.  
             Papers  
                 Non autoregressive transformer Gu and Bradbury et al. 2018  
                 Deterministic Non Autoregressive Neural Sequence Modeling by Iterative Refinement Lee Manismov and Cho 2018  
                 Fast Decoding in Sequence Models Using Discrete Latent Variables ICML 2018 Kaiser Roy Vaswani Pamar Bengio Uszkoreit Shazeer  
                 Towards a Better Understanding of Vector Quantized Autoencoders Roy Vaswani Parmar Neelakantan 2018  
                 Blockwise Parallel Decoding For Deep Autogressive Models NeurIPS 2019 Stern Shazeer Uszkoreit   




 Word Vector Representations  word2vec






An overview of word embeddings and their connection to distributional semantic models






Word Meaning

1. Representing the Meaning of a Word
    Commonest linguistic way of thinking of meaning  
    Signifier $$\iff$$ Signified idea or thing = denotation
    
2. How do we have usable meaning in a computer
    Commonly  Use a taxonomy like WordNet that has hypernyms is a relationships and synonym sets
    
3. Problems with this discrete representation
     Great as a resource but missing nuances  
         Synonyms  
            adept expert good practiced proficient skillful
     Missing New Words
     Subjective  
     Requires human labor to create and adapt  
     Hard to compute accurate word similarity  
         One Hot Encoding in vector space terms this is a vector with one 1 at the position of the word and a lot of zeroes elsewhere.  
             It is a localist representation   
             There is no natural notion of similarity in a set of one hot vectors   
    
4. Distributed Representations of Words
    A method where vectors encode the similarity between the words.  
    
    The meaning is represented with real valued numbers and is "smeared" across the vector.  
    
    Contrast with one hot encoding.  
    

5. Distributional Similarity
    is an idea/hypothesis that one can describe the meaning of words by the context in which they appear in.   
    
    Contrast with Denotational Meaning of words.  
    
6. The Big Idea
    We will build a dense vector for each word type chosen so that it is good at predicting other words appearing in its context.  
    
7. Learning Neural Network Word Embeddings
    We define a model that aims to predict between a center word $$wt$$ and context words in terms of word vectors.    
    $$p\text{context} \vert  wt = \ldots$$   

    The Loss Function    
    $$J = 1  pw{ t} \vert  wt$$    

    We look at many positions $$t$$ in a big language corpus  
    We keep adjusting the vector representations of words to minimize this loss  

    
8. Relevant Papers
     Learning representations by back propagating errors Rumelhart et al. 1986 
     A neural probabilistic language model Bengio et al. 2003 
     NLP almost from Scratch Collobert & Weston 2008 
     A recent even simpler and faster model word2vec Mikolov et al. 2013 à intro now
    

Word Embeddings

1. Main Ideas
     Words are represented as vectors of real numbers
     Words with similar vectors are semantically similar 
     Sometimes vectors are low dimensional compared to the vocabulary size  
    
2. The Clusterings
    Relationships attributes Captured    
     Synonyms car auto
     Antonyms agree disagree
     Values on a scale hot warm cold
     Hyponym Hypernym "Truck" is a type of "car" "dog" is a type of "pet"
     Co Hyponyms "cat"&"dog" is a type of "pet"
     Context Drink Eat Talk Listen

3. Word Embeddings Theory
    Distributional Similarity Hypothesis

4. History and Terminology
    Word Embeddings = Distributional Semantic Model = Distributed Representation = Semantic Vector Space = Vector Space Model  

5. Applications
     Word Similarity
     Word Grouping
     Features in Text Classification
     Document Clustering
     NLP  
         POS Tagging
         Semantic Analysis
         Syntactic Parsing

6. Approaches
     Count word count/context co occurrences   
         Distributional Semantics    
            1. Summarize the occurrence statistics for each word in a large document set   
            2. Apply some dimensionality reduction transformation SVD to the counts to obtain dense real valued vectors   
            3. Compute similarity between words as vector similarity  
     Predict word based on context  
         word2vec  
            1. In one setup the goal is to predict a word given its context.  
            2. Update word representations for each context in the data set  
            3. Similar words would be predicted by similar contexts

7. Parameters
     Underlying Document Set   
     Context Size
     Context Type

8. Software
    

Word2Vec

11.Word2Vec
    Word2Vec Mikolov et al. 2013 is a framework for learning word representations as vectors. It is based on the idea of distributional similarity.  
    

1. Main Idea
     Given a large corpus of text
     Represent every word in a fixed vocabulary by a vector 
     Go through each position $$t$$ in the text which has a center word $$c$$ and context words $$o$$ 
     Use the similarity of the word vectors for $$c$$ and $$o$$ to calculate the probability of $$o$$ given $$c$$ SG  
     Keep adjusting the word vectors to maximize this probability  

    
2. Algorithms
    1. Skip grams SG  
        Predict context words given target position independent
    2. Continuous Bag of Words CBOW  
        Predict target word from bag of words context  
    
    
3. Training Methods
     Basic    
        1. Naive Softmax  
     Moderately Efficient  
        1. Hierarchical Softmax
        2. Negative Sampling   
    
    
4. Skip Gram Prediction Method
    Skip Gram Models aim to predict the distribution probability of context words from a center word.  
    CBOW does the opposite and aims to predict a center word from the surrounding context in terms of word vectors.   

    The Algorithm    
    1. We generate our one hot input vector $$x \in \mathbf{R}^{\vert V\vert }$$ of the center word.  
    2. We get our embedded word vector for the center word $$vc = Vx \in \mathbf{R}^n$$  
    3. Generate a score vector $$z = \mathcal{U} {vc}$$ 
    4. Turn the score vector into probabilities $$\hat{y} = \text{softmax}z$$ 
        Note that $$\hat{y}{c−m} \ldots \hat{y}{c−1} \hat{y}{c+1} \ldots \hat{y}{c+m}$$ are the probabilities of observing each context word.  
    5. We desire our probability vector generated to match the true probabilities which is  
        $$ y^{c−m}  \ldots y^{c−1}  y^{c+1}  \ldots y^{c+m}$$  
        the one hot vectors of the actual output.  
    
    
5. Word2Vec Details
     For each word position $$t = 1 \ldots T$$ predict surrounding context words in a window of “radius” $$m$$ of every word.  

    Calculating $$po \vert c$$^2 the probability of outside words given center word  
     We use two vectors per word $$w$$  
         $$v{w}$$ $$\$$  when $$w$$ is a center word  
         $$u{w}$$ $$\$$ when $$w$$ is a context word  
     Now for a center word $$c$$ and a context word $$o$$ we calculate the probability  
        $${\displaystyle po \vert  c = \dfrac{e^{uo^Tvc}}{\sum{w\in V} e^{uw^Tvc}}} \\\\\\\\\\\\$$  
        
         The Probability Distribution $$po \vert c$$ is an application of the softmax function on the dot product similarity function $$uo^Tvc$$  
         The Softmax function allows us to construct a probability distribution by making the numerator positive and normalizing the function to $$1$$ with the denominator  
         The similarity function $$uo^Tvc$$ allows us to model as follows the more the similarity $$\rightarrow$$ the larger the dot product; the larger the exponential in the softmax    

    
    
6. The Objective
    Goal   
    Maximize the probability of any context word given the current center word.  

    We start with the Likelihood of being able to predict the context words given center words and the parameters $$\theta$$ only the wordvectors.  
    The Likelihood  
    $$L\theta=\prod{t=1}^{T} \prod{ m \leq j \leq m \atop j \neq 0} P\leftw{t+j} | w{t} ; \theta\right$$  
    
    The objective    
    The Objective is just the average negative log likelihood  
    $$J\theta =  \frac{1}{T} \log L\theta=  \dfrac{1}{T} \sum{t=1}^{t} \sum{ m \leq j \leq m  \\\\j\neq 0} \log pw{t+j} \vert  wt ; \theta$$  

    Notice Minimizing objective function $$\iff$$ Maximizing predictive accuracy^1  
    
    
7. The Gradients
    We have a vector of parameters $$\theta$$ that we are trying to optimize over and We need to calculate the gradient of the two sets of parameters in $$\theta$$; namely $$\dfrac{\partial}{\partial vc}$$ and $$\dfrac{\partial}{\partial uo}$$.  

    The gradient $$\dfrac{\partial}{\partial vc}$$  
    $$\dfrac{\partial}{\partial vc} \log po\vert c = uo  \sum{w'\in V} p{w' | c} \cdot u{w'}$$  

    Interpretation  
    We are getting the slope by taking the observed representation of the context word and subtracting away "what the model thinks the context should look like" the weighted average of the representations of each word multiplied by its probability in the current model  
    i.e. the Expectation of the context word vector i.e. the expected context word according to our current model   
    I.E.  
        The difference between the expected context word and the actual context word
    

    Importance Sampling  
    $$\sum{w{i} \in V} \left\frac{\exp \left \mathcal{E}\leftw{i}\right\right}{\sum{w{i} \in V} \exp \left \mathcal{E}\leftw{i}\right\right}\right \nabla{\theta} \mathcal{E}\leftw{i}\right  = \sum{w{i} \in V} P\leftw{i}\right \nabla{\theta} \mathcal{E}\leftw{i}\right$$  
    

    $$\mathbb{E}{w{i} \sim P}\left\nabla{\theta} \mathcal{E}\leftw{i}\right\right =\sum{w{i} \in V} P\leftw{i}\right \nabla{\theta} \mathcal{E}\leftw{i}\right$$  

     $$P\leftw{i}\right \approx \frac{rwi}{R}$$  

    $$\mathbb{E}{w{i} \sim P}\left\nabla{\theta} \mathcal{E}\leftw{i}\right\right \approx \sum{w{i} \in V} \frac{rwi}{R} \nabla{\theta} \mathcal{E}\leftw{i}\right$$  

    $$\mathbb{E}{w{i} \sim P}\left\nabla{\theta} \mathcal{E}\leftw{i}\right\right \approx \frac{1}{R} \sum{i=1}^{m} r\leftw{i}\right \nabla{\theta} \mathcal{E}\leftw{i}\right$$  
    
    where $$rw=\frac{\exp  \mathcal{E}w}{Qw}$$ $$R=\sum{j=1}^{m} r\leftw{j}\right$$ and $$Q$$ is the unigram distribution of the training set.    

8. Notes
     Mikolov on SkipGram vs CBOW  
         Skip gram works well with small amount of the training data represents well even rare words or phrases.  
         CBOW several times faster to train than the skip gram slightly better accuracy for the frequent words.  
     Further Readings  



        
            
    
^1 accuracy of predicting words in the context of another word  
^2 I.E. $$pw{t+j} \vert  wt$$  








































 ASR  Research Papers


Deep Speech 

1. Introduction
       This paper takes a  attempt at an End to End system for ASR.  

2. Structure
        Input vector of speech spectrograms  
             An utterance $$x^{i}$$ is a time series of length $$T^{i}$$ composed of time slices where each is a vector of audio spectrogram features $$x{tp}^{i} t=1...T^{i}$$ where $$p$$ denotes the power of the p'th frequency bin in the audio frame at time $$t$$.  
         Output English text transcript $$y$$  
        Goal  
            The goal of the RNN is to convert an input sequence $$x$$ into a sequence of character probabilities for the transcription $$y$$ with $$\tilde{y}t = Pct\vert x$$ where $$ct \in \{\text{a b c } \ldots \text{  z space  apostrophe blank}\}$$.
                
3. Strategy
       The goal is to replace the multi part model with a single RNN network that captures as much of the information needed to do transcription in a single system.  

4. Solves
        Previous models only used DNNs as a single component in a complex pipeline.  
            NNs are trained to classify individual frames of acoustic data and then their output distributions are reformulated as emission probabilities for a HMM.  
            In this case the objective function used to train the networks is therefore substantially different from the true performance measure sequence level transcription accuracy.  
            This leads to problems where one system might have an improved accuracy rate but the overall transcription accuracy can still decrease.  
          An additional problem is that the frame level training targets must be inferred from the alignments determined by the HMM. This leads to an awkward iterative procedure where network retraining is alternated with HMM re alignments to generate more accurate targets.  

5. Key Insights
        As an End to End model this system avoids the problems of multi part systems that lead to inconsistent training criteria and difficulty of integration.   
            The network is trained directly on the text transcripts no phonetic representation and hence no pronunciation dictionary or state tying is used.  
         Using CTC objective the system is able to better approximate and solve the alignment problem avoiding HMM realignment training.  
            Since CTC integrates out over all possible input output alignments no forced alignment is required to provide training targets.  
         The Dataset is augmented with newly synthesized data and modified to include all the variations and effects that face ASR problems.    
            This greatly increases the system performance on particularly noisy/affected speech.  

6. Preparing Data Pre Processing
       The paper uses spectrograms of power normalized audio clips as features.  

7. Architecture
       The system is composed of  
         An RNN    
             5 layers of hidden units  
                 3 Layer of Feed forward Nets  
                     For the input layer the output depends on the spectrogram frame $$xt$$ along with a context of $$C$$ frames on each side.  
                        $$C \in \{5 7 9\}$$  
                     The non recurrent layers operate on independent data for each time step  
                        $$ht^{l} = gW^{l} h{t}^{l 1} + b^{l}$$  
                        where $$gz = \min \{\max \{0 z\} 20\}$$ is the clipped RELU.    
                 2 layers of Recurrent Nets  
                     1 layer of a Bi LSTM  
                         Includes two sets of hidden units 
                            A set with forward recurrence $$h^{f}$$  
                            A set with backward recurrence $$h^{b}$$  
                            $$ht^{f} = gW^{4}ht^{3} + Wr^{b} h{t 1}^{b} + b ^{4}  
                            ht^{b} = gW^{4}ht^{3} + Wr^{b} h{t+1}^{b} + b ^{4}$$  
                            Note that $$h^{f}$$ must be computed sequentially from $$t = 1$$ to $$t = T^{i}$$ for the i’th utterance while the units $$h^{b}$$ must be computed sequentially in reverse from $$t = T^{i}$$ to $$t = 1$$.  
                     1 layer of Feed forward Nets   
                         The fifth non recurrent layer takes both the forward and backward units as inputs  
                            $$ht^{5} = gW ^{5}ht ^{4} + b ^{5}$$  
                            where $$ht^{4} = ht^{f} + ht^{b}$$ 
             An Output layer made of a standard softmax function that yields the predicted character probabilities for each time slice $$t$$ and character $$k$$ in the alphabet   
                $$\displaystyle{h {tk} ^{6} = \hat{y} {tk} = Pct = k \vert x = \dfrac{\exp Wk ^{6} ht ^{5} + bk ^{6}}{\sumj \exp Wj ^{6}ht ^{5} + bj ^{6}}}$$  
                where $$Wk ^{6}$$ and $$bk ^{6}$$ denote the k'th column of the weight matrix and k'th bias.  
         A CTC Loss Function $$\mathcal{L}\hat{y} y$$  
         An N gram Language Model 
         A combined Objective Function  
       $$Qc = \log Px \vert x + \alpha \log P{\text{LM}}c + \beta \text{wordcount}c$$   

8. Algorithm
        Given the output $$Pc \vert x$$ of the RNN perform a search to find the sequence of characters $$c1 c2 ...$$ that is most probable according to both  
            1. The RNN Output
            2. The Language Model  
         We maximize the combined objective  
            $$Qc = \log Px \vert x + \alpha \log P{\text{LM}}c + \beta \text{wordcount}c$$  
            where the term $$P{\text{lm}}$$ denotes the probability of the sequence $$c$$ according to the N gram model.  
         The objective is maximized using a highly optimized beam search algorithm  
            beam size 1000 8000

9. Training
        The gradient of the CTC Loss $$\nabla{\hat{y}} \mathcal{L}\hat{y} y$$ with respect to the net outputs given the ground truth character sequence $$y$$ is computed
        Nesterov’s Accelerated gradient
         Nesterov Momentum
         Annealing the learning rate by a constant factor
         Dropout  
         Striding   shortening the recurrent layers by taking strides of size $$2$$.  
            The unrolled RNN will have half as many steps.  
            similar to a convolutional network with a step size of 2 in the  layer.  

10.Parameters
        Momentum $$0.99$$ 
         Dropout $$5 10 \%$$ FFN only   
         Trade Off Params use cross validation for $$\alpha \beta$$  

11.Issues/The Bottleneck
       

12.Results
        SwitchboardHub5’00  
    WER 
             Standard $$16.0\%$$  
             w/Lexicon of allowed words $$21.9\%$$ 
             Trigram LM $$8.2\%$$ 
             w/Baseline system $$6.7\%$$

13.Discussion
        Why avoid LSTMs  
            One disadvantage of LSTM cells is that they require computing and storing multiple gating neuron responses at each step.  
            Since the forward and backward recurrences are sequential this small additional cost can become a computational bottleneck.  
        Why a homogeneous model  
             By using a homogeneous model we have made the computation of the recurrent activations as efficient as possible computing the ReLu outputs involves only a few highly optimized BLAS operations on the GPU and a single point wise nonlinearity.

14.Further Development
       


Towards End to End Speech Recognition with Recurrent Neural Networks

1. Introduction
       This paper presents an ASR system that directly transcribes audio data with text without requiring an intermediate phonetic representation.

2. Structure
        Input 
         Output  
                

3. Strategy
       The goal of this paper is a system where as much of the speech pipeline as possible is replaced by a single recurrent neural network RNN architecture.  
        The language model however will be lacking due to the limitation of the audio data to learn a strong LM. 

4. Solves
         attempts used RNNs or standard LSTMs. These models lacked the complexity that was needed to capture all the models required for ASR. 

5. Key Insights
        The model uses Bidirectional LSTMs to capture the nuances of the problem.  
         The system uses a new objective function that trains the network to directly optimize the WER.  

6. Preparing the Data Pre Processing
       The paper uses spectrograms as a minimal preprocessing scheme.  

7. Architecture
       The system is composed of  
         A Bi LSTM  
         A CTC output layer  
         A combined objective function  
            The new objective function at allows an RNN to be trained to optimize the expected value of an arbitrary loss function defined over output transcriptions such as WER.  
            Given input sequence $$x$$ the distribution $$Py\vert x$$ over transcriptions sequences $$y$$ defined by CTC and a real valued transcription loss function $$\mathcal{L}x y$$ the expected transcription loss $$\mathcal{L}x$$ is defined  
            $$\begin{align}
                \mathcal{L}x &= \sumy Py \vert x\mathcal{L}xy  
                &= \sumy \sum{a \in \mathcal{B}^{ 1}y} Pa \vert x\mathcal{L}xy 
                &= \suma Pa \vert x\mathcal{L}x\mathcal{B}a
                \end{align}$$  
        


8. Algorithm
       

9. Issues/The Bottleneck
       

10.Results
        WSJC 
    WER 
             Standard $$27.3\%$$  
             w/Lexicon of allowed words $$21.9\%$$ 
             Trigram LM $$8.2\%$$ 
             w/Baseline system $$6.7\%$$


Attention Based Models for Speech Recognition

1. Introduction
       This paper introduces and extends the attention mechanism with features needed for ASR. It adds location awareness to the attention mechanism to add robustness against different lengths of utterances.  

2. Motivation
       Learning to recognize speech can be viewed as learning to generate a sequence transcription given another sequence speech.  
        From this perspective it is similar to machine translation and handwriting synthesis tasks for which attention based methods have been found suitable. 
       How ASR differs  
        Compared to Machine Translation speech recognition differs by requesting much longer input sequences which introduces a challenge of distinguishing similar speech fragments in a single utterance.  
        thousands of frames instead of dozens of words   
       It is different from Handwriting Synthesis since the input sequence is much noisier and does not have a clear structure.  

2. Structure
        Input $$x=x1 \ldots x{L'}$$ is a sequence of feature vectors  
             Each feature vector is extracted from a small overlapping window of audio frames
         Output $$y$$ a sequence of phonemes   

3. Strategy
       The goal of this paper is a system that uses attention mechanism with location awareness whose performance is comparable to that of the conventional approaches.   
        For each generated phoneme an attention mechanism selects or weighs the signals produced by a trained feature extraction mechanism at potentially all of the time steps in the input sequence speech frames.  
         The weighted feature vector then helps to condition the generation of the next element of the output sequence.  
         Since the utterances in this dataset are rather short mostly under 5 s we measure the ability of the considered models in recognizing much longer utterances which were created by artificially concatenating the existing utterances.

4. Solves
        Problem  

            The paper argues that  this model adapted to track the absolute location in the input sequence of the content it is recognizing a strategy feasible for short utterances from the original test set but inherently unscalable.  
         Solution  
            The attention mechanism is modified to take into account the location of the focus from the previous step and the features of the input sequence by adding as inputs to the attention mechanism auxiliary Convolutional Features which are extracted by convolving the attention weights from the previous step with trainable filters.  

5. Key Insights
        Introduces attention mechanism to ASR
         The attention mechanism is modified to take into account  
             location of the focus from the previous step  
             features of the input sequence
         Proposes a generic method of adding location awareness to the attention mechanism
         Introduce a modification of the attention mechanism to avoid concentrating the attention on a single frame  

7. Attention based Recurrent Sequence Generator ARSG
       is a recurrent neural network that stochastically generates an output sequence $$y1 \ldots yT$$ from an input $$x$$.  
    In practice $$x$$ is often processed by an encoder which outputs a sequential input representation $$h = h1 \ldots hL$$ more suitable for the attention mechanism to work with.  
       The Encoder a deep bidirectional recurrent network.  
        It forms a sequential representation h of length $$L = L'$$.  
      Structure    
         Input $$x = x1 \ldots x{L'}$$ is a sequence of feature vectors   
            Each feature vector is extracted from a small overlapping window of audio frames.  
         Output $$y$$ is a sequence of phonemes
      Strategy    
        At the $$i$$ th step an ARSG generates an output $$yi$$ by focusing on the relevant elements of $$h$$  
       $$\begin{align}
        \alphai &= \text{Attend}s{i 1} \alpha {i 1} h & 1 
        gi &= \sum{j=1}^L \alpha{ij} hj & 2 //
        yi &\sim \text{Generate}s{i 1} gi & 3  
        \end{align}$$
       where $$s{i−1}$$ is the $$i − 1$$ th state of the recurrent neural network to which we refer as the generator $$\alphai \in \mathbb{R}^L$$ is a vector of the attention weights also often called the alignment; and $$gi$$ is the glimpse.  
        The step is completed by computing a new generator state  
       $$si = \text{Recurrency}s{i 1} gi yi$$  
       where the Recurrency is an RNN.  

12.Attention mechanism Types and Speech Recognition
      Types of Attention      
         Generic Hybrid Attention $$\alphai = \text{Attend}s{i 1} \alpha{i 1} h$$  
         Content based Attention $$\alphai = \text{Attend}s{i 1} h$$   
            In this case Attend is often implemented by scoring each element in h separately and normalizing the scores  
            $$e{ij} = \text{Score}s{i 1} hj $$ 
              $$\alpha{ij} = \dfrac{\text{exp} e{ij} }{\sum{j=1}^L \text{exp}e{ij}}$$  
             Limitations  
                The main limitation of such scheme is that identical or very similar elements of $$h$$ are scored equally regardless of their position in the sequence.  
                Often this issue is partially alleviated by an encoder such as e.g. a BiRNN or a deep convolutional network that encode contextual information into every element of h . However capacity of h elements is always limited and thus disambiguation by context is only possible to a limited extent.  
         Location based Attention $$\alphai = \text{Attend}s{i 1} \alpha{i 1}$$   
            a location based attention mechanism computes the alignment from the generator state and the previous alignment only.  
             Limitations  
                the model would have to predict the distance between consequent phonemes using $$s{i−1}$$ only which we expect to be hard due to large variance of this quantity.  
       Thus we conclude that the Hybrid Attention mechanism is a suitable candidate.  
        Ideally we need an attention model that uses the previous alignment $$\alpha{i 1}$$ to select a short list of elements from $$h$$ from which the content based attention will select the relevant ones without confusion.  

6. Preparing the Data Pre Processing
       The paper uses spectrograms as a minimal preprocessing scheme.  

7. Architecture
       Start with the ARSG based model  
         Encoder is a Bi RNN  
        $$e{ij} = w^T \tanh Ws{i 1} + Vhj + b$$
         Attention Content Based Attention extended for location awareness  
            $$e{ij} = w^T \tanh Ws{i 1} + Vhj + Uf{ij} + b$$
       Extending the Attention Mechanism  
        Content Based Attention extended for location awareness by making it take into account the alignment produced at the previous step.  
          we extract $$k$$ vectors $$f{ij} \in \mathbb{R}^k$$ for every position $$j$$ of the previous alignment $$\alpha{i−1}$$ by convolving it with a matrix $$F \in \mathbb{R}^{k\times r}$$  
            $$fi = F  \alpha{i 1}$$
         These additional vectors $$f{ij} $$ are then used by the scoring mechanism $$e{ij}$$  
            $$e{ij} = w^T \tanh Ws{i 1} + Vhj + Uf{ij} + b$$  

                
            

8. Algorithm
       

9. Issues/The Bottleneck
       



A Neural Transducer

 


Deep Speech 2

1. Introduction
       This paper improves on the previous attempt at an End to End system for ASR. It increases the complexity of the architecture and is able to achieve high accuracy on two different languages   English and Chinese.   

2. Structure
        Input vector of speech spectrograms  
             An utterance $$x^{i}$$ is a time series of length $$T^{i}$$ composed of time slices where each is a vector of audio spectrogram features $$x{tp}^{i} t=1...T^{i}$$ where $$p$$ denotes the power of the p'th frequency bin in the audio frame at time $$t$$.  
         Output English text transcript $$y$$  
        Goal  
            The goal of the RNN is to convert an input sequence $$x$$ into a sequence of character probabilities for the transcription $$y$$ with $$\tilde{y}t = Pct\vert x$$ where $$ct \in \{\text{a b c } \ldots \text{  z space  apostrophe blank}\}$$.
                
3. Strategy
       The goal is to replace the multi part model with a single RNN network that captures as much of the information needed to do transcription in a single system.  

4. Solves
        Previous models only used DNNs as a single component in a complex pipeline.  
            NNs are trained to classify individual frames of acoustic data and then their output distributions are reformulated as emission probabilities for a HMM.  
            In this case the objective function used to train the networks is therefore substantially different from the true performance measure sequence level transcription accuracy.  
            This leads to problems where one system might have an improved accuracy rate but the overall transcription accuracy can still decrease.  
          An additional problem is that the frame level training targets must be inferred from the alignments determined by the HMM. This leads to an awkward iterative procedure where network retraining is alternated with HMM re alignments to generate more accurate targets.  

5. Key Insights
        As an End to End model this system avoids the problems of multi part systems that lead to inconsistent training criteria and difficulty of integration.   
            The network is trained directly on the text transcripts no phonetic representation and hence no pronunciation dictionary or state tying is used.  
         Using CTC objective the system is able to better approximate and solve the alignment problem avoiding HMM realignment training.  
            Since CTC integrates out over all possible input output alignments no forced alignment is required to provide training targets.  
         The Dataset is augmented with newly synthesized data and modified to include all the variations and effects that face ASR problems.    
            This greatly increases the system performance on particularly noisy/affected speech.  

6. Preparing Data Pre Processing
       The paper uses spectrograms as a minimal preprocessing scheme.  

7. Architecture
       The system is composed of  
         An RNN    
             5 layers of hidden units  
                 3 Layer of Feed forward Nets  
                     For the input layer the output depends on the spectrogram frame $$xt$$ along with a context of $$C$$ frames on each side.  
                        $$C \in \{5 7 9\}$$  
                     The non recurrent layers operate on independent data for each time step  
                        $$ht^{l} = gW^{l} h{t}^{l 1} + b^{l}$$  
                        where $$gz = \min \{\max \{0 z\} 20\}$$ is the clipped RELU.    
                 2 layers of Recurrent Nets  
                     1 layer of a Bi LSTM  
                         Includes two sets of hidden units 
                            A set with forward recurrence $$h^{f}$$  
                            A set with backward recurrence $$h^{b}$$  
                            $$ht^{f} = gW^{4}ht^{3} + Wr^{b} h{t 1}^{b} + b ^{4}  
                            ht^{b} = gW^{4}ht^{3} + Wr^{b} h{t+1}^{b} + b ^{4}$$  
                            Note that $$h^{f}$$ must be computed sequentially from $$t = 1$$ to $$t = T^{i}$$ for the i’th utterance while the units $$h^{b}$$ must be computed sequentially in reverse from $$t = T^{i}$$ to $$t = 1$$.  
                     1 layer of Feed forward Nets   
                         The fifth non recurrent layer takes both the forward and backward units as inputs  
                            $$ht^{5} = gW ^{5}ht ^{4} + b ^{5}$$  
                            where $$ht^{4} = ht^{f} + ht^{b}$$ 
             An Output layer made of a standard softmax function that yields the predicted character probabilities for each time slice $$t$$ and character $$k$$ in the alphabet   
                $$\displaystyle{h {tk} ^{6} = \hat{y} {tk} = Pct = k \vert x = \dfrac{\exp Wk ^{6} ht ^{5} + bk ^{6}}{\sumj \exp Wj ^{6}ht ^{5} + bj ^{6}}}$$  
                where $$Wk ^{6}$$ and $$bk ^{6}$$ denote the k'th column of the weight matrix and k'th bias.  
         A CTC Loss Function $$\mathcal{L}\hat{y} y$$  
         An N gram Language Model 
         A combined Objective Function  
       $$Qc = \log Px \vert x + \alpha \log P{\text{LM}}c + \beta \text{wordcount}c$$   
8. Algorithm
        Given the output $$Pc \vert x$$ of the RNN perform a search to find the sequence of characters $$c1 c2 ...$$ that is most probable according to both  
            1. The RNN Output
            2. The Language Model  
         We maximize the combined objective  
            $$Qc = \log Px \vert x + \alpha \log P{\text{LM}}c + \beta \text{wordcount}c$$  
            where the term $$P{\text{lm}}$$ denotes the probability of the sequence $$c$$ according to the N gram model.  
         The objective is maximized using a highly optimized beam search algorithm  
            beam size 1000 8000

9. Training
        The gradient of the CTC Loss $$\nabla{\hat{y}} \mathcal{L}\hat{y} y$$ with respect to the net outputs given the ground truth character sequence $$y$$ is computed
        Nesterov’s Accelerated gradient
         Nesterov Momentum
         Annealing the learning rate by a constant factor
         Dropout  
         Striding   shortening the recurrent layers by taking strides of size $$2$$.  
            The unrolled RNN will have half as many steps.  
            similar to a convolutional network with a step size of 2 in the  layer.  

10.Parameters
        Momentum $$0.99$$ 
         Dropout $$5 10 \%$$ FFN only   
         Trade Off Params use cross validation for $$\alpha \beta$$  

11.Issues/The Bottleneck
       

12.Results
        SwitchboardHub5’00  
    WER 
             Standard $$16.0\%$$  
             w/Lexicon of allowed words $$21.9\%$$ 
             Trigram LM $$8.2\%$$ 
             w/Baseline system $$6.7\%$$

13.Discussion
        Why avoid LSTMs  
            One disadvantage of LSTM cells is that they require computing and storing multiple gating neuron responses at each step.  
            Since the forward and backward recurrences are sequential this small additional cost can become a computational bottleneck.  
        Why a homogeneous model  
            By using a homogeneous model we have made the computation of the recurrent activations as efficient as possible computing the ReLu outputs involves only a few highly optimized BLAS operations on the GPU and a single point wise nonlinearity.

14.Further Development
       
 

Listen Attend and Spell LAS

1. Introduction
       This paper presents a a neural network that learns to transcribe speech utterances to characters.  Unlike traditional DNN HMM models this model learns all the components of a speech recognizer jointly.  
       The system has two components a listener and a speller.  
       LAS is based on the sequence to sequence learning framework with attention.  

2. Structure
        Input $$\mathbb{x} = x1 \ldots xT$$ a sequence of filter bank spectra acoustic features
            
         Output $$\mathbb{y} = \text{<sos>} y1 \ldots yS \text{<eos>} yi \in \{\text{a b c · · ·  z 0 · · ·  9 <spacei<comma><period><apostrophe><unk> }\}$$ the output sequence of characters  
        Goal  
            The goal of the RNN is to convert an input sequence $$x$$ into a sequence of character probabilities for the transcription $$y$$ with $$\tilde{y}t = Pct\vert x$$ where $$ct \in \{\text{a b c } \ldots \text{  z space  apostrophe blank}\}$$.
                
3. Strategy
       LAS is based on the sequence to sequence learning framework with attention.  
        We want to model each character output $$yi$$ as a conditional distribution over the previous characters $$y{\leq i+1}$$ and the input signal $$\mathbb{x}$$ using the chain rule  
        $$P\mathbb{y} \vert \mathbb{x} = \prodi Pyi \vert \mathbb{x} y{\leq i+1} \\\\ 1$$
         

4. Solves
        CTC  
            CTC assumes that the label outputs are conditionally independent of each other
         Seq2Seq  
            the sequence to sequence approach has only been applied to phoneme sequences and not trained end to end for speech recognition. 
         

5. Key Insights
        Use a pyramidal RNN model for the listener which reduces the number of time steps that the attention model has to extract relevant information from.  
        The pyramid structure also reduces the computational complexity.    
         Character based transcription allows the handling of rare and OOV words automatically  
         Attention 

77.The Model
   Listen  
    Uses a Bi directional LSTM with a pyramid structure.  
    The pyramid structure is needed to reduce the length $$U$$ of $$\mathbf{h}$$ from $$T$$  the length of the input $$\mathbb{h}$$  because the input speech signals can be hundreds to thousands of frames long.  
     Pyramidal LSTM  
        The output at the i th time step from the j th layer is changed from  
        $$hi^j = \text{BLSTM}h{i 1}^j h{i}^{j 1}$$  
        to instead we concatenate the outputs at consecutive steps of each layer before feeding it to the next layer     
        $$hi^j = \text{pBLSTM}h{i 1}^j \lefth{2i}^{j 1} h{2i+1}^{j 1}\right$$  
        In the model we stack 3 pBLSTMs on top of the bottom BLSTM layer to reduce the time resolution $$2^3 = 8$$ times.  
   Attend and Spell  
    The function is computed using an attention based LSTM transducer.  
    At every output step the transducer produces a probability distribution over the next character conditioned on all the characters seen previously.  
    The distribution for $$yi$$ is a function of the decoder state $$si$$ and context $$ci$$.  
    The decoder state $$si$$ is a function of the previous state $$s{i−1}$$ the previously emitted character $$y{i−1}$$ and context $$c{i−1}$$.  
    The context vector $$ci$$ is produced by an attention mechanism  
    $$ ci = \text{AttentionContext}si \mathbf{h} 
        si = \text{RNN}s{i 1} y{i 1} c{i 1} 
    Pyi \vert \mathbf{x} y{\leq i+1} = \text{CharacterDistribution}si ci$$    
    where CharacterDistribution is an MLP with softmax outputs over characters and RNN is a 2 layer LSTM.  
    The Attention Mechanism  
    At each step $$i$$ the attention mechanism AttentionContext generates a context vector $$ci$$ encapsulating the information in the acoustic signal needed to generate the next character.  
    The attention model is content based  the contents of the decoder state $$si$$ are matched to the contents of $$hu$$ representing time step $$u$$ of $$\mathbf{h}$$ to generate an attention vector $$\alphai$$.  
    $$\alphai$$ is used to linearly blend vectors $$hu$$ to create $$ci$$.  
    Specifically at each decoder timestep $$i$$  the AttentionContext function computes the scalar energy
    $$e{iu}$$ for each time step $$u$$  using vector $$hu \in h$$ and $$si$$.  
    The scalar energy $$e{iu}$$ is converted into a
    probability distribution over times steps or attention $$\alphai$$  using a softmax function. This is used to create the context vector $$ci$$  by linearly blending the listener features $$hu$$ at different time steps  
      
    $$\begin{align}
        e{iu} &= <\phisi \psihu> 
        \alpha{iu} &= \dfrac{\expe{iu}}{\sumu \expe{iu}} 
        ci &= \sumu \alpha{iu}hu 
        \end{align}
        $$ 
      
    where $$\phi$$ and $$\psi$$ are MLP Networks.  
    On convergence the $$\alphai$$  distribution is typically very sharp and focused on only a few frames of $$\mathbf{h}$$ ; $$ci$$ can be seen as a continuous bag of weighted features of $$\mathbf{h}$$.  

6. Preparing Data Pre Processing

7. Architecture
        Encoder listener  
            An acoustic model encoder whose key operation is ```Listen```.  
            It converts low level speech signals into higher level features.  
             pyramidal RNN  
                 Bi Directional LSTM  
                            
                     Structure  
                         Input original signal $$ \mathbb{x}$$ 
                         Output a high level representation $$\mathbf{h} = h1 ldots hU$$ with $$U \leq T$$   
                        
         Decoder speller  
            The speller is an attention based character decoder whose key operation is ```AttendAndSpell```.  
            It converts the higher level features into output utterances by specifying a probability distribution over sequences of characters using the attention mechanism.  
             RNN  
                 Structure  
                     Input features $$ \mathbf{h}$$ 
                     Output a probability distribution over character sequences   $$\mathbf{h} = h1 ldots hU$$ with $$U \leq T$$
    
8. Algorithm
       

9. Training
        The gradient of the CTC Loss $$\nabla{\hat{y}} \mathcal{L}\hat{y} y$$ with respect to the net outputs given the ground truth character sequence $$y$$ is computed
        Nesterov’s Accelerated gradient
         Nesterov Momentum
         Annealing the learning rate by a constant factor
         Dropout  
         Striding   shortening the recurrent layers by taking strides of size $$2$$.  
            The unrolled RNN will have half as many steps.  
            similar to a convolutional network with a step size of 2 in the  layer.  

99.Inference Decoding and Rescoring


10.Parameters
        Momentum $$0.99$$ 
         Dropout $$5 10 \%$$ FFN only   
         Trade Off Params use cross validation for $$\alpha \beta$$  


11.Issues/The Bottleneck
       

12.Results
        SwitchboardHub5’00  
    WER 
             Standard $$16.0\%$$  
             w/Lexicon of allowed words $$21.9\%$$ 
             Trigram LM $$8.2\%$$ 
             w/Baseline system $$6.7\%$$

13.Discussion
        Without the attention mechanism the model overfits the training data significantly in spite of our large training set of three million utterances  it memorizes the training transcripts without paying attention to the acoustics.  
         Without the pyramid structure in the encoder side our model converges too slowly  even after a month of training the error rates were significantly higher than the errors reported
         To reduce the overfitting of the speller to the training transcripts use a sampling trick during training

14.Further Development
       



 Neural Machine Translation  


Machine Translation

1. Methods
        Methods are statistical  
         Uses parallel corpora

3. Traditional MT
       Traditional MT was very complex and included multiple disciplines coming in together.  
        The systems included many independent parts and required a lot of human engineering and experts.  
        The systems also scaled very poorly as the search problem was essentially exponential.

2. Statistical Machine Translation Systems
        Input  
             Source Language $$f$$  
             Target Language $$e$$  
         The Probabilistic Formulation  
       $$ \hat{e} = \mathrm{arg\min}e \ pe \vert f = \mathrm{arg\min}e \ pf \vert e pe$$
        The Translation Model $$pf \vert e$$ trained on parallel corpus
         The Language Model $$pe$$ trained on English only corpus  
        Abundant and free!


4. Deep Learning Naive Approach
       One way we can learn to translate is to learn to translate directly with an RNN. 
        The Model  
             Encoder   
                $$ht = \phih{t 1} xt = fW^{hh}h{t 1} + W^{hx}xt$$
             Decoder   
                $$\begin{align}
                    ht & = \phih{t 1} = fW^{hh}h{t 1} 
                    yt & = \text{softmax}W^{S}ht
                    \end{align}$$  
             Error   
                $$\displaystyle{\max\theta \frac{1}{N} \sum{n=1}^N \log p\thetay^{n}\vert x^{n}}$$  
                Cross Entropy Error.  
             Goal   
                Minimize the Cross Entropy Error for all target words conditioned on source words
        Drawbacks  
            The problem of this approach lies in the fact that the last hidden layer needs to capture the entire sentence to be translated.  
            However since we know that the RNN Gradient basically vanishes as the length of the sequence increases the last hidden layer is usually only capable of capturing upto ~5 words.

5. Naive RNN Translation Model Extension
       1. Train Different RNN weights for encoding and decoding
        2. Compute every hidden state in the decoder from the following concatenated vectors   
             Previous hidden state standard
             Last hidden vector of encoder $$c=hT$$  
             Previous predicted output word $$y{t 1}$$.  
            $$\implies h{D t} = \phiDh{t 1} c y{t 1}$$  
            NOTE Each input of $$\phi$$ has its own linear transformation matrix.  
        3. Train stacked/deep RNNs with multiple layers. 
        4. Potentially train Bidirectional Encoder
        5. Train input sequence in reverser order for simpler optimization problem  
            Instead of $$A\B\C \rightarrow X\Y$$ train with $$ C\B\A \rightarrow X\Y$$  
        6. Better Units Main Improvement  
             Use more complex hidden unit computation in recurrence
             Use GRUs Cho et al. 2014  
             Main Ideas
                 Keep around memories to capture long distance dependencies
                 Allow error messages to flow at different strengths depending on the inputs


GRUs

1. Gated Recurrent Units
       Gated Recurrent Units GRUs are a class of modified Gated RNNs that allow them to combat the vanishing gradient problem by allowing them to capture more information/long range connections about the past memory and decide how strong each signal is.  

2. Main Idea
       Unlike standard RNNs which compute the hidden layer at the next time step directly  GRUs computes two additional layers gates  
        Each with different weights
        Update Gate  
       $$zt = \sigmaW^{z}xt + U^{z}h{t 1}$$  
        Reset Gate  
       $$rt = \sigmaW^{r}xt + U^{r}h{t 1}$$  
       The Update Gate and Reset Gate computed allow us to more directly influence/manipulate what information do we care about and want to store/keep and what content we can ignore.  
        We can view the actions of these gates from their respecting equations as  
        New Memory Content  
            at each hidden layer at a given time step we compute some new memory content  
            if the reset gate $$ = ~0$$ then this ignores previous memory and only stores the new word information.  
       $$ \tilde{h}t = \tanhWxt + rt \odot Uh{t 1}$$
        Final Memory  
            the actual memory at a time step $$t$$ combines the Current and Previous time steps  
            if the update gate $$ = ~0$$ then this again ignores the newly computed memory content and keeps the old memory it possessed.  
       $$ht = zt \odot h{t 1} + 1 zt \odot \tilde{h}t$$  


Long Short Term Memory

1. LSTM
       The Long Short Term Memory LSTM Network is a special case of the Recurrent Neural Network RNN that uses special gated units a.k.a LSTM units as building blocks for the layers of the RNN.  

2. Architecture
       The LSTM usually has four gates  
        Input Gate 
            The input gate determines how much does the current input vector current cell matters      
       $$it = \sigmaW^{i}xt + U^{i}h{t 1}$$ 
        Forget Gate 
            Determines how much of the past memory that we have kept is still needed   
       $$it = \sigmaW^{i}xt + U^{i}h{t 1}$$ 
        Output Gate 
            Determines how much of the current cell matters for our current prediction i.e. passed to the sigmoid
       $$it = \sigmaW^{i}xt + U^{i}h{t 1}$$  
        Memory Cell 
            The memory cell is the cell that contains the short term memory collected from each input
       $$\begin{align}
            \tilde{c}t & = \tanhW^{c}xt + U^{c}h{t 1} & \text{New Memory} 
            ct & = ft \odot c{t 1} + it \odot \tilde{c}t & \text{Final Memory}
        \end{align}$$
       The Final Hidden State is calculated as follows  
       $$ht = ot \odot \sigmact$$
     

3. Properties
        Syntactic Invariance  
            When one projects down the vectors from the last time step hidden layer with PCA one can observe the spatial localization of syntacticly similar sentences  


Neural Machine Translation NMT

1. NMT
       NMT is an approach to machine translation that uses a large artificial neural network to predict the likelihood of a sequence of words typically modeling entire sentences in a single integrated model.

2. Architecture
       The approach uses an Encoder Decoder architecture.
       NMT models can be seen as a special case of language models.   
        In other words they can be seen as Conditional Recurrent Language Model; a language model that has been conditioned on the calculated encoded vector representation of the sentence.

3. Modern Sequence Models for NMT
       

4. Issues of NMT
        Predicting Unseen Words  
            The NMT model is very vulnerable when presented with a word that it has never seen during training e.g. a new name.  
            In fact the model might not even be able to place the translated unseen word correctly in the translated sentence.
         Solution  
             A possible solution is to apply character based translation instead of word based however this approach makes for very long sequences and the computation becomes infeasible.  
             The current proposed approach is to use a Mixture Model of Softmax and Pointers  
                Pointer sentinel Model

5. The Big Wins of NMT
       1. End to End Training All parameters are simultaneously optimized to minimize a loss function on the networks output 
        2. Distributed Representations Share Strength Better exploitation of word and phrase similarities 
        3. Better Exploitation of Context NMT can use a much bigger context  both source and partial target text  to translate more accurately
        4. More Fluent Text Generation Deep Learning text generation is much higher quality

8. GNMT Google's Multilingual Neural Machine Translation System  Zero shot Translation
        Multilingual NMT Approaches  
        Google's Approach  
            Add an Artificial Token at the beginning of the input sentence to indicate the target language.  



 Introduction to NLP  Natural Language Processing


Introduction

0. NLP
       Natural Language Processing is a field at the intersection of computer science artificial intelligence and linguistics concerned with the interactions between computers and human natural languages and in particular concerned with programming computers to fruitfully process large natural language data.

1. Problems in NLP
        Question Answering QA 
         Information Extraction IE    
         Sentiment Analysis  
         Machine Translation MT  
         Spam Detection  
         Parts of Speech POS Tagging  
         Named Entity Recognition NER
         Conference Resolution  
         Word Sense Disambiguation WSD  
         Parsing  
         Paraphrasing  
         Summarization  
         Dialog  
        Fully understanding and representing the meaning of language or even defining it is a difficult goal.
             Perfect language understanding is AI complete  

2. mostly Solved Problems in NLP
        Spam Detection  
         Parts of Speech POS Tagging  
         Named Entity Recognition NER  


3. Within Reach Problems
        Sentiment Analysis  
         Conference Resolution    
         Word Sense Disambiguation WSD  
         Parsing  
         Machine Translation MT  
         Information Extraction IE    


4. Open Problems in NLP
        Question Answering QA   
         Paraphrasing  
         Summarization  
         Dialog  

5. Issues in NLP why nlp is hard?
        Non Standard English "Great Job @ahmedbadary! I luv u 2!! were SOO PROUD of dis."  
         Segmentation Issues "New York New Haven" vs "New York New Haven"  
         Idioms "dark horse" "getting cold feet" "losing face"  
         Neologisms "unfriend" "retweet" "google" "bromance"  
         World Knowledge "Ahmed and Zach are brothers" "Ahmed and Zach are fathers"    
         Tricky Entity Names "Where is Life of Pie playing tonight?" "Let it be was a hit song!"  

6. Tools we need for NLP
        Knowledge about Language.  
         Knowledge about the World.   
         A way to combine knowledge sources.  

7. Methods
       In general we need to construct Probabilistic Models built from language data.    
       We do so by using rough text features.  
        All the names models methods and tools mentioned above will be introduced later as you progress in the text.  

8. NLP in the Industry
        Search
         Online ad Matching
         Automated/Assisted Translation
         Sentiment analysis for marketing   or finance/trading
         Speech recognition
         Chatbots/Dialog Agents
             Automatic Customer Support
             Controlling Devices
             Ordering Goods


NLP and Deep Learning

1. What is Special about Human Language
        Human Language is a system specifically constructed to convey the speaker/writer's meaning.
        It is a deliberate communication not just an environmental signal.  
         Human Language is a discrete/symbolic/categorical signaling system  
         The categorical symbols of a language can be encoded as a signal for communication in several ways  
             Sound
             Gesture 
             Images Writing  
            Yet the symbol is invariant across different encodings!  

         However a brain encoding appears to be a continuous pattern of activation and the symbols are transmitted via continuous signals of sound/vision.  

2. Issues of NLP in Machine Learning
        According to the above passage; we see that although human language is largely symbolic; it is still interpreted by the brain as a continuous signal.  
        This means that we cannot encode this information in a discrete manner; and rather must learn in a sequential continuous way.  
         The large vocabulary symbolic encoding of words creates a problem for machine learning   sparsity!

3. Machine Learning vs Deep Learning
        Most Machine Learning methods work well because of human designed representations and input features.  
            Thus the learning here is done mostly by the people/scientists/engineers who are designing the features and not by the machines.  
            This rendered Machine Learning to become just a numerical optimization method for optimizing weights to best make a final prediction.   
        How does that differ with Deep Learning DL?  
             Representation learning attempts to automatically learn good features or representations
             Deep learning algorithms attempt to learn multiple levels of representation and an output  
             Raw Inputs DL can deal directly with raw inputs e.g. sound characters words  

4. Why Deep Learning?
        Manually designed features are often over specified incomplete and take a long time to design and validate
         Manually designed features are often over specified incomplete and take a long time to design and validate
         Deep learning provides a very flexible almost? universal learnable framework for representing world visual and linguistic information.
         Deep learning can learn unsupervised from raw text and supervised with specific labels like positive/negative
         In ~2010 deep learning techniques started outperforming other machine learning techniques.


5. Why is NLP Hard revisited
        Complexity in representing learning and using linguistic/situational/world/visual knowledge
         Human languages are ambiguous unlike programming and other formal languages
         Human language interpretation depends on real world common sense and contextual knowledge

6. Improvements in Recent Years in NLP
       spanning different  
         Levels speech words syntax semantics  
         Tools POS entities parsing  
         Applications MT sentiment analysis dialogue agents QA      


Representations of NLP Levels

1. Morphology
        Traditionally Words are made of morphemes.  
             uninterested  > un prefix + interest stem + ed suffix
         DL
             Every morpheme is a vectors
             A Neural Network combines two vectors into one vector
             Luong et al. 2013  

2. Semantics
        Traditionally Lambda Calculus  
             Carefully engineered functions
             Take as inputs specific other functions
             No notion of similarity or fuzziness of language  
         DL
             Every word and every phrase and every logical expression is a vector 
             A Neural Network combines two vectors into one vector  
             Bowman et al. 2014  

       

       

       

       

       

       


NLP Tools

1. Parsing for Sentence Structure
       Neural networks can accurately determine the structure of sentences supporting interpretation.  

       

       

       

       

       

       


NLP Applications

1. Sentiment Analysis
        Traditional Curated sentiment dictionaries combined with either bag of words representations ignoring word order or hand designed negation features ain’t gonna capture everything
         DL Same deep learning model that was used for morphology syntax and logical semantics can be used;  
        RecursiveNN.        

2. Question Answering
        Traditional A lot of feature engineering to capture world and other knowledge e.g. regular expressions Berant et al. 2014
         DL Facts are stored in vectors.  FILL IN  
        FILL IN.      


3. Machine Translation
        Traditional Complex approaches with very high error rates.  
         DL Neural Machine Translation.  
            Source sentence is mapped to vector then output sentence generated.  
            Sutskever et al. 2014 Bahdanau et al. 2014 Luong and Manning 2016  
        FILL IN.   

       

       

       


 ASR  Automatic Speech Recognition


Introduction to Speech

1. Probabilistic Speech Recognition
    
       We can view the problem of ASR as a sequence labeling problem and so use statistical models such as HMMs to model the conditional probabilities between the states/words by viewing speech signal as a piecewise stationary signal or a short time stationary signal. 
        Representation we represent the speech signal as an observation sequence $$o = \{ot\}$$  
         Goal find the most likely word sequence $$\hat{w}$$   
         Set Up  
             The system has a set of discrete states
             The transitions from state to state are markovian and are according to the transition probabilities  
                Markovian Memoryless  
             The Acoustic Observations when making a transition are conditioned on the state alone $$Pot \vert ct$$
             The goal is to recover the state sequence and consequently the word sequence  

2. Speech Problems and Considerations
        ASR  
             Spontaneous vs Read speech
             Large vs Small Vocabulary
             Continuous vs Isolated Speech  
                Continuous Speech is harder due to Co Articulation   
             Noisy vs Clear input
             Low vs High Resources 
             Near field vs Far field input
             Accent independence 
             Speaker Adaptive vs Stand Alone speaker independent 
             The cocktail party problem 
         TTS  
             Low Resource
             Realistic prosody
         Speaker Identification
         Speech Enhancement
         Speech Separation       

3. Acoustic Representation
       What is speech?
         Waves of changing air pressure  Longitudinal Waves consisting of compressions and rarefactions
         Realized through excitation from the vocal cords
         Modulated by the vocal tract and the articulators tongue teeth lips 
         Vowels are produced with an open vocal tract stationary
            parametrized by position of tongue
         Consonants are constrictions of vocal tract
         They get converted to Voltage with a microphone
         They are sampled and quantized with an Analogue to Digital Converter 
      Speech as waves  
         Human hearing range is $$~50 HZ 20 kHZ$$
         Human speech range is $$~85 HZ 8 kHZ$$
         Telephone speech sampling is $$8 kHz$$ and a bandwidth range of $$300 Hz 4 kHz$$ 
         1 bit per sample is intelligible
         Contemporary Speech Processing mostly around 16 khz 16 bits/sample  
            A lot of data to handle
      Speech as digits vectors  
         We seek a low dimensional representation to ease the computation  
         The low dimensional representation needs to be invariant to  
             Speaker
             Background noise
             Rate of Speaking
             etc.
         We apply Fourier Analysis to see the energy in different frequency bands which allows analysis and processing
             Specifically we apply windowed short term Fast Fourier Transform FFT  
                e.g. FFT on overlapping $$25ms$$ windows 400 samples taken every $$10ms$$  
        Each frame is around 25ms of speech  
         FFT is still too high dimensional  
             We Downsample by local weighted averages on mel scale non linear spacing an d take a log  
                $$ m = 1127 \ln1+\dfrac{f}{700}$$  
             This results in log mel features $$40+$$ dimensional features per frame    
                Default for NN speech modelling  
      Speech dimensionality for different models  
         Gaussian Mixture Models GMMs 13 MFCCs  
             MFCCs  Mel Frequency Cepstral Coefficients are the discrete cosine transformation DCT of the mel filterbank energies \| Whitened and low dimensional.  
                They are similar to Principle Components of log spectra.  
            GMMs used local differences deltas and  order differences delta deltas to capture the dynamics of the speech $$13 \times 3 \text{ dim}$$
         FC DNN 26 stacked frames of PLP  
             PLP  Perceptual Linear Prediction a common alternative representation using Linear Discriminant Analysis LDA  
                Class aware PCA    
         LSTM/RNN/CNN 8 stacked frames of PLP  
      Speech as Communication      
         Speech Consists of sentences in ASR we usually talk about "utterances"  
         Sentences are composed of words 
         Minimal unit is a "phoneme" Minimal unit that distinguishes one word from another.
             Set of 40 60 distinct sounds.
             Vary per language 
             Universal representations 
                 IPA  international phonetic alphabet
                 X SAMPA  ASCII 
         Homophones  distinct words with the same pronunciation. e.g. "there" vs "their" 
         Prosody  How something is said can convey meaning. e.g. "Yeah!" vs "Yeah?"  

9. Microphones and Speakers
        Microphones  
             Their is a Diaphragm in the Mic
             The Diaphragm vibrates with air pressure
             The diaphragm is connected to a magnet in a coil
             The magnet vibrates with the diaphragm
             The coil has an electric current induced by the magnet based on the vibrations of the magnet
        Speakers  
             The electric current flows from the sound player through a wire into a coil
             The coil has a metal inside it
             The metal becomes magnetic and vibrates inside the coil based on the intensity of the current 
             The magnetized metal is attached to a cone that produces the sound


4. Approximate History of ASR
     1960s Dynamic Time Warping 
     1970s Hidden Markov Models 
     Multi layer perceptron 1986 
     Speech recognition with neural networks 1987 1995 
     Superseded by GMMs 1995 2009 
     Neural network features 2002— 
     Deep networks 2006— Hinton 2002 
     Deep networks for speech recognition
         Good results on TIMIT Mohamed et al. 2009 
         Results on large vocabulary systems 2010 Dahl et al. 2011  Google launches DNN ASR product 2011
         Dominant paradigm for ASR 2012 Hinton et al. 2012 
     Recurrent networks for speech recognition 1990 2012  New models CTC attention LAS neural transducer 

5. Datasets
     TIMIT 
         Hand marked phone boundaries are given 
         630 speakers $$\times$$ 10 utterances 
     Wall Street Journal WSJ 1986 Read speech. WSJO 1991 30k vocab 
     Broadcast News BN 1996 104 hours 
     Switchboard SWB 1992. 2000 hours spontaneous telephone speech   500 speakers 
     Google voice search  anonymized live traffic 3M utterances 2000 hours hand transcribed 4M vocabulary. Constantly refreshed synthetic reverberation + additive noise 
     DeepSpeech 5000h read Lombard speech + SWB with additive noise. 
     YouTube 125000 hours aligned captions Soltau et al. 2016 

5. Development
 

The Methods and Models of Speech Recognition

1. Probabilistic Speech Recognition
    
       We can view the problem of ASR as a sequence labeling problem and so use statistical models such as HMMs to model the conditional probabilities between the states/words by viewing speech signal as a piecewise stationary signal or a short time stationary signal. 
        Representation we represent the speech signal as an observation sequence $$o = \{ot\}$$  
         Goal find the most likely word sequence $$\hat{w}$$   
         Set Up  
             The system has a set of discrete states
             The transitions from state to state are markovian and are according to the transition probabilities  
                Markovian Memoryless  
             The Acoustic Observations when making a transition are conditioned on the state alone $$Pot \vert ct$$
             The goal is to recover the state sequence and consequently the word sequence  
                
2. Fundamental Equation of Speech Recognition
       We set the decoders output as the most likely sequence $$\hat{w}$$ from all the possible sequences $$\mathcal{S}$$ for an observation sequence $$o$$  
       $$\begin{align}
            \hat{w} & = \mathrm{arg } \max{w \in \mathcal{S}} Pw \vert o & 1 
            & = \mathrm{arg } \max{w \in \mathcal{S}} Po \vert w Pw & 2
            \end{align}
        $$  
       The Conditional Probability of a sequence of observations given a sequence of predicted word is a product or sum of logs of an Acoustic Model $$po \vert w$$  and a Language Model $$pw$$  scores.
       The Acoustic Model can be written as the following product    
       $$Po \vert w = \sum{dcp} Po \vert c Pc \vert p Pp \vert w$$ 
       where $$p$$ is the phone sequence and $$c$$ is the state sequence.  

3. Speech Recognition as Transduction
       The problem of speech recognition can be seen as a transduction problem  mapping different forms of energy to other forms representations.  
        Basically we are going from Signal to Language.  

4. Gaussian Mixture Models
        Dominant paradigm for ASR from 1990 to 2010 
         Model the probability distribution of the acoustic features for each state.  
            $$Pot \vert ci = \sumj w{ij} Not; \mu{ij} \sigma{ij}$$   
         Often use diagonal covariance Gaussians to keep number of parameters under control. 
         Train by the E M Expectation Maximization algorithm Dempster et al. 1977 alternating  
             M forced alignment computing the maximum likelihood state sequence for each utterance 
             E parameter $$\mu  \sigma$$ estimation  
         Complex training procedures to incrementally fit increasing numbers of components per mixture  
             More components better fit  79 parameters component. 
         Given an alignment mapping audio frames to states this is parallelizable by state.   
         Hard to share parameters/data across states.  
       Forced Alignment  
         Forced alignment uses a model to compute the maximum likelihood alignment between speech features and phonetic states. 
         For each training utterance construct the set of phonetic states for the ground truth transcription. 
         Use Viterbi algorithm to find ML monotonic state sequence 
         Under constraints such as at least one frame per state. 
         Results in a phonetic label for each frame. 
         Can give hard or soft segmentation.  
     
    
     Decoding   
         Speech recognition Unfolds in much the same way.
          Now we have a graph instead of a straight through path.
          Optional silences between words Alternative pronunciation paths.
          Typically use max probability and work in the log domain.
          Hypothesis space is huge so we only keep a "beam" of the best paths and can lose what would end up being the true best path.   

5. Neural Networks in ASR
        Two Paradigms of Neural Networks for Speech  
             Use neural networks to compute nonlinear feature representations      
                 "Bottleneck" or "tandem" features Hermansky et al. 2000
                 Low dimensional representation is modelled conventionally with GMMs.
                 Allows all the GMM machinery and tricks to be exploited. 
                 Bottleneck features outperform Posterior features Grezl et al. 2017
                 Generally DNN features + GMMs reach the same performance as hybrid DNN HMM systems but are much more complex
             Use neural networks to estimate phonetic unit probabilities  

6. Hybrid Networks
        Train the network as a classifier with a softmax across the phonetic units  
         Train with cross entropy
         Softmax   
       $$yi = \dfrac{e^{\psii \theta}}{\sum{j=1}^N e^{\psij \theta}}$$ 
        We converge to/learn the posterior probability across phonetic states  
       $$Pci \vert ot$$   
        We then model $$Po \vert c$$ with a Neural Net instead of a GMM   
            We can ignore $$Pot$$ since it is the same for all decoding paths   
       $$\begin{align}
            Po \vert c & = \prodt Pot \vert ct & 3 
            Pot \vert ct & = \dfrac{Pct \vert ot Pot}{Pct} & 4 
            & \propto \dfrac{Pct \vert ot}{Pct} & 5 
            \end{align}
        $$  
        The log scaled posterior  from the last term  
       $$\log Pot \vert ct = \log Pct \vert ot  \alpha \log Pct$$ 
        Empirically a prior smoothing on $$\alpha$$ $$\alpha \approx 0.8$$ works better 
        Input Features  
             NN can handle high dimensional correlated features
             Use 26 stacked filterbank inputs 40 dim mel spaced filterbanks
        NN Architectures for ASR  
             Fully Connected DNN  
             CNNs 
                 Time delay neural networks 
                     Waibel et al. 1989 
                     Dilated convolutions Peddinti et al. 2015  
                        Pooling in time results in a loss of information.  
                        Pooling in frequency domain is more tolerable  
                 CNNs in time or frequency domain
                     Abdel Hamid et al. 2014
                     Sainath et al. 2013 
                 Wavenet van den Oord et al. 2016 
             RNNs   
                 RNN Robinson and Fallside 1991 
                 LSTM Graves et al. 2013
                 Deep LSTM P Sak et al. 2014b
                 CLDNN Sainath et al  2015a
                 GRU. DeepSpeech 1/2 Amodei et al. 2015

                 Tips 
                     Bidirectional Schuster and Paliwal 1997 helps but introduces latency. 
                     Dependencies not long at speech frame rates 100Hz.
                     Frame stacking and down sampling help. 

7. Sequence Discriminative Training
     Conventional training uses Cross Entropy loss — Tries to maximize probability of the true state sequence given the data. 
     We care about Word Error Rate of the complete system. 
     Design a loss that's differentiable and closer to what we care about. 
     Applied to neural networks Kingsbury 2009 
     Posterior scaling gets learnt by the network. 
     Improves conventional training and CTC by $$\approx 15%$$ relative. 
     bMMI sMBRPovey et al. 2008  


       


Transitioning into Deep Learning

1. Classical Approach
       Classically Speech Recognition was developed as a big machine incorporating different models from different fields.  
        The models were statistical and they started from text sequences to audio features.  
        Typically a generative language model is trained on the sentences for the intended language then to make the features pronunciation models acoustic models and speech processing models had to be developed. Those required a lot of feature engineering and a lot of human intervention and expertise and were very fragile.
       Recognition was done through Inference Given audio features $$\mathbf{X}=x1x2...xt$$ infer the most likely tedxt sequence $$\mathbf{Y}^\ast=y1y2...yk$$ that caused the audio features.
       $$\displaystyle{\mathbf{Y}^\ast =\mathrm{arg\min}{\mathbf{Y}} p\mathbf{X} \vert \mathbf{Y} p\mathbf{Y}}$$

2. The Neural Network Age
       Researchers realized that each of the independent components/models that make up the ASR can be improved if it were replaced by a Neural Network Based Model.  

3. The Problem with the component based System
        Each component/model is trained independently with a different objective  
         Errors in one component may not behave well with errors in another component

4. Solution to the Component Based System
       We aim to train models that encompass all of these components together i.e. End to End Model  
         Connectionist Temporal Classification CTC
         Sequence to Sequence Listen Attend and Spell LAS
                    
5. End to End Speech Recognition
       We treat End to End Speech Recognition as a modeling task.
       Given Audio $$\mathbf{X}=x1x2...xt$$ audio/processed spectogram and corresponding output text $$\mathbf{Y}=y1y2...yk$$  transcript we want to learn a Probabilistic Model $$p\mathbf{Y} \vert \mathbf{X}$$ 

6. Deep Learning  What's new?
         Algorithms  
             Direct modeling of context dependent tied triphone states through the DNN  
             Unsupervised Pre training
             Deeper Networks
             Better Architectures  
         Data  
             Larger Data
         Computation  
                 GPUs
                 TPUs
         Training Criterion  
             Cross Entropy  > MMI Sequence  level
         Features  
             Mel Frequency Cepstral Coefficients MFCC  > FilterBanks
         Training and Regularization  
             Batch Norm
             Distributed SGD
             Dropout
         Acoustic Modelling  
             CNN
             CTC
             CLDNN
         Language Modelling  
             RNNs
             LSTM
         DATA  
             More diverse  Noisy Accents etc.  


Connectionist Temporal Classification

1. Motivation
        RNNs require a target output at each time step 
         Thus to train an RNN we need to segment the training output i.e. tell the network which label should be output at which time step 
         This problem usually arises when the timing of the input is variable/inconsistent e.g. people speaking at different rates/speeds

2. Connectionist Temporal Classification CTC
       CTC is a type of neural network output and associated scoring function for training recurrent neural networks RNNs such as LSTM networks to tackle sequence problems where the timing is variable.  
       Due to time variability we don't know the alignment of the input with the output.  
        Thus CTC considers all possible alignments.  
        Then it gets a closed formula for the probability of all these possible alignments and maximizes it.

8. Structure
     Input  
        A sequence of observations
     Output  
        A sequence of labels


3. Algorithm
    1. Extract the LOG MEL Spectrogram from the input  
        Use raw audio iff there are multiple microphones
    2. Feed the Spectogram into a bi directional RNN
    3. At each frame we apply a softmax over the entire vocabulary that we are interested in plus a blank token producing a prediction log probability called the score for a different token class at that time step.   
         Repeated Tokens are duplicated
         Any original transcript is mapped to by all the possible paths in the duplicated space
         The Score log probability of any path is the sum of the scores of individual categories at the different time steps
         The probability of any transcript is the sum of probabilities of all paths that correspond to that transcript
         Dynamic Programming allopws is to compute the log probability $$p\mathbf{Y} \vert \mathbf{X}$$ and its gradient exactly.  

10.The Math
       Given a length $$T$$ input sequence $$x$$ the output vectors $$yt$$ are normalized with the Softmax function then interpreted as the probability of emitting the label or blank with index $$k$$ at time $$t$$ 
       $$Pk t \vert x = \dfrac{e^{yt^k}}{\sum{k'} e^{yt^{k'}}}$$ 
       where $$yt^k$$ is element $$k$$ of $$yt$$.  
       A CTC alignment $$a$$ is a length $$T$$ sequence of blank and label indices.  
        The probability $$Pa \vert x$$ of 
        $$a$$ is the product of the emission probabilities at every time step  
       $$Pa \vert x = \prod{t=1}^T Pat t \vert x$$ 
       Denoting by $$\mathcal{B}$$ an operator that removes  the repeated labels then the blanks from alignments and observing that the total probability of an output transcription $$y$$ is equal to the sum of the probabilities of the alignments corresponding to it we can write  
       $$Py \vert x = \sum{a \in \mathcal{B}^{ 1}y} Pa \vert x\\\\\\\\\\\\\\\\$$ 
       Given a target transcription $$y^\ast$$ the network can then be trained to minimise the CTC objective function  
       $$\text{CTC}x =  \log Py^\ast \vert x$$ 


11.Intuition
       The above 'integrating out' over possible alignments eq. $$$$ is what allows the network to be trained with unsegmented data.   
        The intuition is that because we don’t know where the labels within a particular transcription will occur we sum over all the places where they could occur can be efficiently evaluated and differentiated using a dynamic programming algorithm.


5. Analysis
       The ASR model consists of an RNN plus a CTC layer.    
        Jointly the model learns the pronunciation and acoustic model together.  
        However a language model is not learned because the RNN CTC model makes strong conditional independence assumptions similar to HMMs.  
        Thus the RNN CTC model is capable of mapping speech acoustics to English characters but it makes many spelling and grammatical mistakes.  
        Thus the bottleneck in the model is the assumption that the network outputs at different times are conditionally independent given the internal state of the network. 

4. Improvements
        Add a language model to CTC during training time for rescoring.
           This allows the model to correct spelling and grammar.
         Use word targets of a certain vocabulary instead of characters 

7. Applications
        on line Handwriting Recognition
         Recognizing phonemes in speech audio  
         ASR

9. Tips
        Continuous realignment  no need for a bootstrap model
         Always use soft targets
         Don't scale by the posterior
         Produces similar results to conventional training
         Simple to implement in the FST framework 
         CTC could learn to delay output on its own in order to improve accuracy  
             In practice tends to align transcription closely
             This is especially problematic for English letters spelling
             Sol  
                bake limited context into model structure; s.t. the model at time step $$T$$ can see only some future frames. 
                 Caveat may need to compute upper layers quickly after sufficient context arrives.  
                Can be easier if context is near top.   


LAS  Seq2Seq with Attention

1. Motivation
       The CTC model can only make predictions based on the data; once it has made a prediction for a given frame it cannot re adjust the prediction.  
       Moreover the strong independence assumptions that the CTC model makes doesn't allow it to learn a language model.   

2. Listen Attend and Spell LAS
       LAS is a neural network that learns to transcribe speech utterances to characters.  
        In particular it learns all the components of a speech recognizer jointly.
       The model is a seq2seq model; it learns a conditional probability of the next label/character given the input and previous predictions $$py{i+1} \vert y{1..i} x$$.  
       The approach that LAS takes is similar to that of NMT.     
        Where in translation the input would be the source sentence but in ASR the input is the audio sequence.  
       Attention is needed because in speech recognition tasks the length of the input sequence is very large; for a 10 s sample there will be ~10000 frames to go through.      

3. Structure
       The model has two components  
         A listener a pyramidal RNN encoder that accepts filter bank spectra as inputs
         A Speller an attention based RNN decoder that emits characters as outputs 
        Input  
            
         Output 

6. Limitations
        Not an online model  input must all be received before transcripts can be produced
         Attention is a computational bottleneck since every output token pays attention to every input time step
         Length of input has a big impact on accuracy


Online Seq2Seq Models

1. Motivation
        Overcome limitations of seq2seq  
             No need to wait for the entire input sequence to arrive
             Avoids the computational bottleneck of Attention over the entire sequence
         Produce outputs as inputs arrive  
             Solves this problem When has enough information arrived that the model is confident enough to output symbols 

2. A Neural Transducer
        Neural Transducer is a more general class of seq2seq learning models. It avoids the problems of offline seq2seq models by operating on local chunks of data instead of the whole input at once. It is able to make predictions conditioned on partially observed data and partially made predictions.    

 

Real World Applications

1. Siri
        Siri Architecture  
             Start with a Wave Form
             Pass the wave form through an ASR system
             Then use a Natural Language Model to re adjust the labels
             Output Words
             Based on the output do some action or save the output etc.
        "Hey Siri" DNN  
             Much smaller DNN than for the full Vocab. ASR
             Does Binary Classification  Did the speaker say "hey Siri" or not?  
             Consists of 5 Layers
             The layers have few parameters
             It has a Threshold at the end  
             So fast 
             Capable of running on the Apple Watch!
        Two Pass Detection  
             Problem  
                    A big problem that arises in the always on voice is that it needs to run 24/7. 
             Solution  
                We use a Two Pass Detection system  
                 There are two processors implemented in the phone  
                    Low Compute Processor  
                         Always ON
                         Given a threshold value of confidence over binary probabilities the Processor makes the following decision "Should I wake up the Main Processor"  
                         Low power consumption
                    Main Processor  
                         Only ON if woken up by the low compute processor 
                         Runs a much larger DNN   
                         High power consumption
        Computation for DL   


Building ASR Systems

1. Pre Processing
       A Spectrogram is a visual representation of the spectrum of frequencies of sound or other signal as they vary with time.
         Take a small window ~20 ms of waveform  
         Compute FFT and take magnitude i.e. prower  
            Describes Frequency content in local window  
        Concatenate frames from adjacent windows to form the "spectrogram"  

2. Acoustic Model
       An Acoustic Model is used in automatic speech recognition to represent the relationship between an audio signal and the phonemes or other linguistic units that make up speech.  
       Goal create a neural network DNN/RNN from which we can extract transcriptions $$y$$  by training on labeled pairs $$x y^\ast$$.  

3. Network example Architecture
       RNN to predict graphemes 26 chars + space + blank  
         Spectrograms as inputs
         1 Layer of Convolutional Filters
         3 Layers of Gated Recurrent Units
             1000 Neurons per Layer
         1 Fully Connected Layer to predict $$c$$
         Batch Normalization
         CTC Loss Function  Warp CTC 
         SGD+Nesterov Momentum Optimization/Training

4. Incorporating a Language Model
       Incorporating a Language Model helps the model learn  
         Spelling 
         Grammar
         Expand Vocabulary
       Two Ways  
        1. Fuse the Acoustic Model with the language model $$py$$  
        2. Incorporate linguistic data 
             Predict Phonemes + Pronunciation Lexicon + LM  

5. Decoding with Language Models
        Given a word based LM of form $$pw{t+1} \vert w{1t}$$  
         Use Beam Search to maximize Hannun et al. 2014  
       $$\mathrm{arg } \max{w} pw \vert x\ pw^\alpha \ \text{length}w^\beta$$  
     $$pw \vert x = py \vert x$$ for characters that make up $$w$$.  
         We tend to penalize long transcriptions due to the multiplicative nature of the objective so we trade off re weight with $$\alpha  \beta$$  
        Start with a set of candidate transcript prefixes $$A = {}$$  
         For $$t = 1 \ldots T$$   
             For Each Candidate in $$A$$ consider  
                1. Add blank; don't change prefix; update probability using the AM. 
                2. Add space to prefix; update probability using LM. 
                3. Add a character to prefix; update probability using AM. Add new candidates with updated probabilities to $$A{\text{new}}$$   
             $$A = K$$ most probable prefixes in $$A{\text{new}}$$  
       
    

6. Rescoring with Neural LM
       The output from the RNN described above consists of a big list of the top $$k$$ transcriptions in terms of probability.  
        We want to re score these probabilities based on a strong LM.   
         It is Cheap to evaluate $$pwk \vert w{k 1} w{k 2} \ldots w1$$ NLM on many sentences  
         In practice often combine with N gram trained from big corpora  

7. Scaling Up
        Data  
             Transcribing speech data isn't cheap but not prohibitive  
                 Roughly 50¢ to $1 per minute
             Typical speech benchmarks offer 100s to few 1000s of hours  
                 LibriSpeech audiobooks  
                 LDC corpora Fisher Switchboard WSJ \$\$  
                 VoxForge  
             Data is very Application/Problem dependent and should be chosen with respect to the problem to be solved
             Data can be collected as "read" data for <$10   Make sure the data to be read is scripts/plays to get a conversationalist response
             Noise is additive and can be incorporated  
        Computation  
             How big is 1 experiment?
                $$\geq \# \text{Connections} \cdot \# \text{Frames} \cdot \# \text{Utterances} \cdot \# \text{Epochs} \cdot 3 \cdot 2 \\text{ FLOPs}$$   
                E.g. for DS2 with 10k hours of data  
                $$10010^6  100  10^620  3  2 = 1.210^{19} \\text{ FLOPs}$$  
                ~30 days with well optimized code on Titan X  
             Work arounds and solutions
                 More GPUs with data parallelism  
                     Minibatches up to 1024 
                     Aim for $$\geq 64$$ utterances per GPU 
                ~$$< 1$$ wk training time ~8 Titans
             How to use more GPUs?

                 Synch. SGD
                 Synch SGD w/ backup workers
             Tips and Tricks  
                 Make sure the code is optimized single GPU.  
                    A lot of off the shelf code has inefficiencies.  
                    E.g. Watch for bad GEMM sizes.
                 Keep similar length utterances together  
                    The input must be block sized and will be padded; thus keeping similar lengths together reduces unnecessary padding.
        Throughput  
             Large DNN/RNN models run well on GPUs ONLY if the batch size is high enough.  
                Processing 1 audio stream at a time is inefficient.  
                Performance for K1200 GPU  
                | Batch Size | FLOPs | Throughput |
                | 1 | 0.065 TFLOPs | 1x | 
                | 10 | 0.31 TFLOPs | 5x | 
                | 32 | 0.92 TFLOPs | 14x |  
             Batch packets together as data comes in  
                 Each packet Arrow of speech data ~ 100ms  
                 Process packets that arrive at similar times in parallel from    multiple users  


 Deep Learning  Research Papers


Sequence to Sequence Learning with Neural Network

1. Introduction
    This paper presents a general end to end approach to sequence learning that makes minimal assumptions Domain Independent on the sequence structure.  
    It introduces Seq2Seq. 

2. Structure
     Input sequence of input vectors  
     Output sequence of output labels
                
3. Strategy
    The idea is to use one LSTM to read the input sequence one time step at a time to obtain large fixed dimensional vector representation and then to use another LSTM to extract the output sequence from that vector.  
    The  LSTM is essentially a recurrent neural network language model except that it is conditioned on the input sequence.

4. Solves
     Despite their flexibility and power DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality. It is a significant limitation since many important problems are best expressed with sequences whose lengths are not known a priori.  
        The RNN can easily map sequences to sequences whenever the alignment between the inputs the outputs is known ahead of time. However it is not clear how to apply an RNN to problems whose input and the output sequences have different lengths with complicated and non monotonic relationship.  


5. Key Insights
     Uses LSTMs to capture the information present in a sequence of inputs into one vector of features that can then be used to decode a sequence of output features  
     Uses two different LSTM for the encoder and the decoder respectively  
     Reverses the words in the source sentence to make use of short term dependencies in translation that led to better training and convergence 

6. Preparing Data Pre Processing
       
                    

7. Architecture
        Encoder  
             LSTM 
                 4 Layers    
                     1000 Dimensions per layer
                     1000 dimensional word embeddings
         Decoder  
             LSTM 
                 4 Layers    
                     1000 Dimensions per layer
                     1000 dimensional word embeddings
         An Output layer made of a standard softmax function  
            over 80000 words  
         Objective Function  
            $$\dfrac{1}{\vert \mathbb{S} \vert} \sum{TS \in \mathbb{S}} \log pT \vert S
            $$  
            where $$\mathbb{S}$$ is the training set.  
                
8. Algorithm
    Train a large deep LSTM 
     Train by maximizing the log probability of a correct translation $$T$$  given the source sentence $$S$$  
     Produce translations by finding the most likely translation according to the LSTM   
        $$\hat{T} = \mathrm{arg } \max{T} pT \vert S$$
     Search for the most likely translation using a simple left to right beam search decoder which maintains a small number B of partial hypotheses  
        A partial hypothesis is a prefix of some translation  
     At each time step we extend each partial hypothesis in the beam with every possible word in the vocabulary  
        This greatly increases the number of the hypotheses so we discard all but the $$B$$  most likely hypotheses according to the model’s log probability  
     As soon as the “<EOS>” symbol is appended to a hypothesis it is removed from the beam and is added to the set of complete hypotheses  
    

9. Training
        SGD
         Momentum 
         Half the learning rate every half epoch after the 5th epoch
         Gradient Clipping  
            enforce a hard constraint on the norm of the gradient
         Sorting input

10.Parameters
        Initialization of all the LSTM params with uniform distribution $$\in  0.08 0.08$$  
         Learning Rate $$0.7$$ 
         Batches $$28$$ sequences
         Clipping 
       $$g = 5g/\|g\|2 \text{ if } \|g\|2 > 5 \text{ else } g$$ 
                  

11.Issues/The Bottleneck
        The decoder is approximate  
         The system puts too much pressure on the last encoded vector to capture all the long term dependencies

12.Results
       

13.Discussion
        Sequence to sequence learning is a framework that attempts to address the problem of learning variable length input and output sequences. It uses an encoder RNN to map the sequential variable length input into a fixed length vector. A decoder RNN then uses this vector to produce the variable length output sequence one token at a time. During training the model feeds the groundtruth labels as inputs to the decoder. During inference the model performs a beam search to generate suitable candidates for next step predictions.

14.Further Development
       


Towards End to End Speech Recognition with Recurrent Neural Networks

1. Introduction
       This paper presents an ASR system that directly transcribes audio data with text without requiring an intermediate phonetic representation.

2. Structure
        Input 
         Output  
                

3. Strategy
       The goal of this paper is a system where as much of the speech pipeline as possible is replaced by a single recurrent neural network RNN architecture.  
        The language model however will be lacking due to the limitation of the audio data to learn a strong LM. 

4. Solves
         attempts used RNNs or standard LSTMs. These models lacked the complexity that was needed to capture all the models required for ASR. 

5. Key Insights
        The model uses Bidirectional LSTMs to capture the nuances of the problem.  
         The system uses a new objective function that trains the network to directly optimize the WER.  

6. Preparing the Data Pre Processing
       The paper uses spectrograms as a minimal preprocessing scheme.  

7. Architecture
       The system is composed of  
         A Bi LSTM  
         A CTC output layer  
         A combined objective function  
            The new objective function at allows an RNN to be trained to optimize the expected value of an arbitrary loss function defined over output transcriptions such as WER.  
            Given input sequence $$x$$ the distribution $$Py\vert x$$ over transcriptions sequences $$y$$ defined by CTC and a real valued transcription loss function $$\mathcal{L}x y$$ the expected transcription loss $$\mathcal{L}x$$ is defined  
            $$\begin{align}
                \mathcal{L}x &= \sumy Py \vert x\mathcal{L}xy  
                &= \sumy \sum{a \in \mathcal{B}^{ 1}y} Pa \vert x\mathcal{L}xy 
                &= \suma Pa \vert x\mathcal{L}x\mathcal{B}a
                \end{align}$$  
        


8. Algorithm
       

9. Issues/The Bottleneck
       

10.Results
        WSJC 
    WER 
             Standard $$27.3\%$$  
             w/Lexicon of allowed words $$21.9\%$$ 
             Trigram LM $$8.2\%$$ 
             w/Baseline system $$6.7\%$$


Attention Based Models for Speech Recognition

1. Introduction
       This paper introduces and extends the attention mechanism with features needed for ASR. It adds location awareness to the attention mechanism to add robustness against different lengths of utterances.  

2. Motivation
       Learning to recognize speech can be viewed as learning to generate a sequence transcription given another sequence speech.  
        From this perspective it is similar to machine translation and handwriting synthesis tasks for which attention based methods have been found suitable. 
       How ASR differs  
        Compared to Machine Translation speech recognition differs by requesting much longer input sequences which introduces a challenge of distinguishing similar speech fragments in a single utterance.  
        thousands of frames instead of dozens of words   
       It is different from Handwriting Synthesis since the input sequence is much noisier and does not have a clear structure.  

2. Structure
        Input $$x=x1 \ldots x{L'}$$ is a sequence of feature vectors  
             Each feature vector is extracted from a small overlapping window of audio frames
         Output $$y$$ a sequence of phonemes   

3. Strategy
       The goal of this paper is a system that uses attention mechanism with location awareness whose performance is comparable to that of the conventional approaches.   
        For each generated phoneme an attention mechanism selects or weighs the signals produced by a trained feature extraction mechanism at potentially all of the time steps in the input sequence speech frames.  
         The weighted feature vector then helps to condition the generation of the next element of the output sequence.  
         Since the utterances in this dataset are rather short mostly under 5 s we measure the ability of the considered models in recognizing much longer utterances which were created by artificially concatenating the existing utterances.

4. Solves
        Problem  

            The paper argues that  this model adapted to track the absolute location in the input sequence of the content it is recognizing a strategy feasible for short utterances from the original test set but inherently unscalable.  
         Solution  
            The attention mechanism is modified to take into account the location of the focus from the previous step and the features of the input sequence by adding as inputs to the attention mechanism auxiliary Convolutional Features which are extracted by convolving the attention weights from the previous step with trainable filters.  

5. Key Insights
        Introduces attention mechanism to ASR
         The attention mechanism is modified to take into account  
             location of the focus from the previous step  
             features of the input sequence
         Proposes a generic method of adding location awareness to the attention mechanism
         Introduce a modification of the attention mechanism to avoid concentrating the attention on a single frame  

7. Attention based Recurrent Sequence Generator ARSG
       is a recurrent neural network that stochastically generates an output sequence $$y1 \ldots yT$$ from an input $$x$$.  
    In practice $$x$$ is often processed by an encoder which outputs a sequential input representation $$h = h1 \ldots hL$$ more suitable for the attention mechanism to work with.  
       The Encoder a deep bidirectional recurrent network.  
        It forms a sequential representation h of length $$L = L'$$.  
      Structure    
         Input $$x = x1 \ldots x{L'}$$ is a sequence of feature vectors   
            Each feature vector is extracted from a small overlapping window of audio frames.  
         Output $$y$$ is a sequence of phonemes
      Strategy    
        At the $$i$$ th step an ARSG generates an output $$yi$$ by focusing on the relevant elements of $$h$$  
       $$\begin{align}
        \alphai &= \text{Attend}s{i 1} \alpha {i 1} h & 1 
        gi &= \sum{j=1}^L \alpha{ij} hj & 2 //
        yi &\sim \text{Generate}s{i 1} gi & 3  
        \end{align}$$
       where $$s{i−1}$$ is the $$i − 1$$ th state of the recurrent neural network to which we refer as the generator $$\alphai \in \mathbb{R}^L$$ is a vector of the attention weights also often called the alignment; and $$gi$$ is the glimpse.  
        The step is completed by computing a new generator state  
       $$si = \text{Recurrency}s{i 1} gi yi$$  
       where the Recurrency is an RNN.  

12.Attention mechanism Types and Speech Recognition
      Types of Attention      
         Generic Hybrid Attention $$\alphai = \text{Attend}s{i 1} \alpha{i 1} h$$  
         Content based Attention $$\alphai = \text{Attend}s{i 1} h$$   
            In this case Attend is often implemented by scoring each element in h separately and normalizing the scores  
            $$e{ij} = \text{Score}s{i 1} hj $$ 
              $$\alpha{ij} = \dfrac{\text{exp} e{ij} }{\sum{j=1}^L \text{exp}e{ij}}$$  
             Limitations  
                The main limitation of such scheme is that identical or very similar elements of $$h$$ are scored equally regardless of their position in the sequence.  
                Often this issue is partially alleviated by an encoder such as e.g. a BiRNN or a deep convolutional network that encode contextual information into every element of h . However capacity of h elements is always limited and thus disambiguation by context is only possible to a limited extent.  
         Location based Attention $$\alphai = \text{Attend}s{i 1} \alpha{i 1}$$   
            a location based attention mechanism computes the alignment from the generator state and the previous alignment only.  
             Limitations  
                the model would have to predict the distance between consequent phonemes using $$s{i−1}$$ only which we expect to be hard due to large variance of this quantity.  
       Thus we conclude that the Hybrid Attention mechanism is a suitable candidate.  
        Ideally we need an attention model that uses the previous alignment $$\alpha{i 1}$$ to select a short list of elements from $$h$$ from which the content based attention will select the relevant ones without confusion.  

6. Preparing the Data Pre Processing
       The paper uses spectrograms as a minimal preprocessing scheme.  

7. Architecture
       Start with the ARSG based model  
         Encoder is a Bi RNN  
        $$e{ij} = w^T \tanh Ws{i 1} + Vhj + b$$
         Attention Content Based Attention extended for location awareness  
            $$e{ij} = w^T \tanh Ws{i 1} + Vhj + Uf{ij} + b$$
       Extending the Attention Mechanism  
        Content Based Attention extended for location awareness by making it take into account the alignment produced at the previous step.  
          we extract $$k$$ vectors $$f{ij} \in \mathbb{R}^k$$ for every position $$j$$ of the previous alignment $$\alpha{i−1}$$ by convolving it with a matrix $$F \in \mathbb{R}^{k\times r}$$  
            $$fi = F  \alpha{i 1}$$
         These additional vectors $$f{ij} $$ are then used by the scoring mechanism $$e{ij}$$  
            $$e{ij} = w^T \tanh Ws{i 1} + Vhj + Uf{ij} + b$$  

                
            

8. Algorithm
       

9. Issues/The Bottleneck
       

       


Attention Is All You Need

1. Introduction
    This paper introduces the Transformer network architecture.  
    The model relies completely on Attention and disregards recurrence/convolutions completely.

2. Motivation
    Motivation for dropping
     Recurrent Connections  
        Complex tricky to train and regularize capturing long term dependencies is limited and hard to parallelize. Sequence aligned states in RNN are wasteful. Hard to model hierarchical like domains such as languages.  
     Convolutional Connections  
        Convolutional approaches are sometimes effective more on this but they tend to be memory intensive. Path length between positions can be logarithmic when using dilated convolutions left padding for text. autoregressive CNNs WaveNet ByteNET

    Motivation for Transformer
     It gives us the shortest possible path through the network between any two input output locations.  

    Motivation in NLP  
     The following quote  
        “You can’t cram the meaning of a whole %&!$# sentence into a single $&!# vector!”  ACL 2014

3. From Attention to Self Attention
    The Encoder Decoder Architecture  
    For a fixed target output $$tj$$ all hidden state source inputs are taken into account to compute the cosine similarity with the source inputs $$si$$ to generate the $$\thetai$$’s attention weights for every source input $$si$$.  

4. Strategy
    The idea here is to learn a context vector say $$U$$ which gives us a global level information on all the inputs and tells us about the most important information.  
    E.g. This could be done by taking a cosine similarity of this context vector $$U$$  w.r.t the input hidden states from the fully connected layer. We do this for each input $$xi$$ and thus obtain a $$\thetai$$ attention weights.  

    The Goals  
     Parallelization of Seq2Seq RNN/CNN handle sequences word by word sequentially which is an obstacle to parallelize. Transformer achieves parallelization by replacing recurrence with attention and encoding the symbol position in the sequence. This in turn leads to a significantly shorter training time.  
     Reduce sequential computation Constant $$\mathcal{O}1$$ number of operations to learn dependency between two symbols independently of their position distance in sequence.  

    The Transformer reduces the number of sequential operations to relate two symbols from input/output sequences to a constant $$\mathcal{O}1$$ number of operations. Transformer achieves this with the multi head attention mechanism that allows to model dependencies regardless of their distance in input or output sentence by counteracting reduced effective resolution due to averaging the attention weighted positions.  

6. Architecture
    The Transformer follows a Encoder Decoder architecture using stacked self attention and point wise fully connected layers for both the encoder and decoder shown in the left and right halves of Figure 1 respectively  

    Encoder  
    The encoder is composed of a stack of $$N = 6$$ identical layers. Each layer has two sub layers. The  is a multi head self attention mechanism and the  is a simple positionwise fully connected feed forward network. We employ a residual connection around each of the two sub layers followed by layer normalization. That is the output of each sub layer is $$\text{LayerNorm}x + \text{Sublayer}x$$ where $$\text{Sublayer}x$$ is the function implemented by the sub layer itself. To facilitate these residual connections all sub layers in the model as well as the embedding layers produce outputs of dimension $$d{\text{model}} = 512$$.  
    
    Decoder  
    The decoder is also composed of a stack of $$N = 6$$ identical layers. In addition to the two sub layers in each encoder layer the decoder inserts a  sub layer which performs multi head attention over the output of the encoder stack. Similar to the encoder we employ residual connections around each of the sub layers followed by layer normalization. We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions. This masking combined with fact that the output embeddings are offset by one position ensures that the predictions for position $$i$$ can depend only on the known outputs at positions less than $$i$$.  
    
    The Encoder maps an input sequence of symbol representations $$x1 \ldots xn$$ to a sequence of continuous representations $$z = z1 ... zn$$.  
    Given $$z$$ the decoder then generates an output sequence $$y1 ... ym$$ of symbols one element at a time. At each step the model is auto regressive consuming the previously generated symbols as additional input when generating the next.  

7. The Model  Attention
    Formulation
    Standard attention with queries and key value pairs.  
     Scaled Dot Product Attention  
        Given 1 Queries $$\vec{q} \in \mathbb{R}^{dk}$$  2 Keys $$\vec{k} \in \mathbb{R}^{dk}$$  3 Values $$\vec{v} \in \mathbb{R}^{dv}$$  
        Computes the dot products of the queries with all keys; scales each by $$\sqrt{dk}$$; and normalizes with a softmax to obtain the weights $$\thetai$$s on the values.  
        For a given query vector $$\vec{q} = \vec{q}j$$ for some $$j$$  
        $${\displaystyle \vec{o} = \sum{i=0}^{dk} \text{softmax} \dfrac{\vec{q}^T \ \vec{k}i}{\sqrt{dk}} \vec{v}i
        = \sum{i=0}^{dk} \thetai \vec{v}i}$$  
        In practice we compute the attention function on a set of queries simultaneously in matrix form stacked row wise  
        $${\displaystyle \text{Attention}Q K V = O = \text{softmax} \dfrac{QK^T}{\sqrt{dk}} V } \tag{1}$$  
        Motivation We suspect that for large values of $$dk$$ the dot products grow large in magnitude pushing the softmax function into regions where it has extremely small gradients. To counteract this effect we scale the dot products by $$\sqrt{\dfrac{1}{dk}}$$.  
     Multi Head Attention  
        Instead of performing a single attention function with $$d{\text{model}}$$ dimensional keys values and queries; linearly project the queries keys and values h times with different learned linear projections to $$dk dk$$ and $$dv$$ dimensions respectively. Then attend apply $$\text{Attention}$$ function on each of the projected versions in parallel yielding $$dv$$ dimensional output values. The final values are obtained by concatenating and projecting the $$dv$$ dimensional output values from each of the attention heads.  
        $$\begin{aligned} \text { MultiHead }Q K V &=\text { Concat }\left\text { head}{1} \ldots \text { head}{h}\right W^{O}  \text { where head}{i} &=\text { Attention }\leftQ W{i}^{Q} K W{i}^{K} V W{i}^{V}\right \end{aligned}$$  
        Where the projections are parameter matrices $$ W{i}^{Q} \in \mathbb{R}^{d{\text {model }} \times d{k}} W{i}^{K} \in $$ $$\mathbb{R}^{d{\text {model }} \times d{k}}$$ $$W{i}^{V} \in \mathbb{R}^{d{\text {model }} \times d{v}} $$ and $$W^O \in \mathbb{R}^{hdv \times d{\text {model }}}$$.  
        This paper choses $$h = 8$$ parallel attention layers/heads. For each they use $$dk=dv=d{\text{model}}/h = 64$$. The reduced dimensionality of each head allows the total computation cost to be similar to that of a single head w/ full dimensionality.   

        Motivation Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head averaging inhibits this.  
    

    Applications of Attention in the Model  
    The Transformer uses multi head attention in three different ways  
     Encode Decoder Attention Layer standard layer  
         The queries come from the previous decoder layer
         The memory keys and values come from the output of the encoder   

        This allows every position in the decoder to attend over all positions in the input sequence.  
     Encoder Self Attention  
        The encoder contains self attention layers.  
         Both The queries and keys and values come from the encoders output of previous layer  

        Each position in the encoder can attend to all positions in the previous layer of the encoder.
     Decoder Self Attention  
        The decoder also contains self attention layers.  
         Both The queries and keys and values come from the decoders output of previous layer  

        However self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. Since we need to prevent leftward information flow in the decoder to preserve the auto regressive property.  
        This is implemented inside of scaled dot product attention by masking out setting to $$ \infty$$  all values in the input of the softmax which correspond to illegal connections.  


8. The Model  Position wise Feed Forward Network FFN
    In addition to attention sub layers each of the layers in our encoder and decoder contains a fully connected feed forward network which is applied to each position separately and identically.  
    It consists of two linear transformations with a ReLU activation in between  
    $$\text{FFN}x = \max0 xW1 + b1W2 + b2 \tag{2}$$  
    While the linear transformations are the same across different positions they use different parameters from layer to layer.  
    Equivalently we can describe this as two convolutions with kernel size $$= 1$$  

    Dimensional Analysis  
     Input/Output $$\in \mathbb{R}^{d\text{model} = 512} $$  
     Inner Layer $$\in \mathbb{R}^{d{ff} = 2048} $$  


9. The Model  Embeddings and Softmax
    Use learned embeddings to convert the input tokens and output tokens to vectors $$\in \mathbb{R}^d{\text{model}}$$.  
    Use the usual learned linear transformation and softmax to convert decoder output to predicted next token probabilities.  

    The model shares the same weight matrix between the two embedding layers and the pre softmax linear transformation.  
    In the embedding layers multiply those weights by $$\sqrt{d{\text{model}}}$$.  

10.The Model  Positional Encoding
    Motivation Since the model contains no recurrence and no convolution in order for the model to make use of the order of the sequence we must inject some information about the relative or absolute position of the tokens in the sequence.  

    Positional Encoding  
    A way to add positional information to an embedding.  
    There are many choices of positional encodings learned and fixed. Gehring et al. 2017  
    The positional encodings have the same dimension $$d{\text{model}}$$ as the embeddings so that the two can be summed.  
    Approach  
    Add "positional encodings" to the input embeddings at the bottoms of the encoder and decoder stacks.  
    Use sine and cosine functions of different frequencies  
    $$ \begin{aligned} P E{p o s 2 i} &=\sin \leftp o s / 10000^{2 i / d{\mathrm{model}}}\right  P E{p o s 2 i+1} &=\cos \leftp o s / 10000^{2 i / d{\mathrm{model}}}\right \end{aligned} $$  
    where $$p o s$$ is the position and $$i$$ is the dimension.  
    That is each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $$2\pi$$ to $$10000 \cdot 2\pi$$.  

    Motivation  
    We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions since for any fixed offset $$k$$ $$PE{pos + k}$$ can be represented as a linear function of $$P E{pos}$$. 

    Sinusoidal VS Learned We chose the sinusoidal version instead of learned positional embeddings with similar results because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.  


11.Training Tips & Tricks
     Layer Normalization Help ensure that layers remain in reasonable range  
     Specialized Training Schedule Adjust default learning rate of the Adam optimizer  
     Label Smoothing Insert some uncertainty in the training process  
     Masking for decoder attention for Efficient Training using matrix operations  


12.Why Self Attention? as opposed to Conv/Recur. layers
    Total Computational Complexity per Layer    
     Self Attention layers are faster than recurrent layers when the sequence length $$n$$ is smaller than the representation dimensionality $$d$$.  
        Which is most often the case with sentence representations used by state of the art models in machine translations such as word piece and byte pair representations.  

     To improve computational performance for tasks involving very long sequences self attention could be restricted to considering only a neighborhood of size $$r$$ in the input sequence centered around the respective output position.  
        This would increase the maximum path length to $$\mathcal{O}n/r$$.  

    Parallelizable Computations measured by the minimum number of sequential ops required  
     Self Attention layers connect all positions with a constant number of sequentially executed operations whereas a recurrent layer requires $$\mathcal{O}n$$ sequential operations.  


    Path Length between Positions Long Range Dependencies  
     Convolutional Layers A single convolutional layer with kernel width $$k < n$$ does not connect all pairs of input and output positions.  
        Doing so requires  
         Contiguous Kernels valid a stack of $$\mathcal{O}n/k$$ convolutional layers
         Dilated Kernels $$\mathcal{O}\logkn$$  
            increasing the length of the longest paths between any two positions in the network.  
         Separable Kernels decrease the complexity considerably to $$\mathcal{O}\leftk \cdot n \cdot d+n \cdot d^{2}\right$$  
        
        Convolutional layers are generally more expensive than recurrent layers by a factor of $$k$$.  

     Self Attention  
        Even with $$k = n$$ the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer the approach taken in this model.  

     Interpretability  
        As side benefit self attention could yield more interpretable models.  
        Not only do individual attention heads clearly learn to perform different tasks many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.              

19.Results
     Attention Types  
        For small values of $$dk$$ the two mechanisms perform similarly additive attention outperforms dot product attention without scaling for larger values of $$dk$$.  
     Positional Encodings  
        We also experimented with using learned positional embeddings instead and found that the two versions produced nearly identical results.  
            


 
 





 Language Modeling   Recurrent Neural Networks RNNs



> Note 2500 important problem not captured w/ newer models about smoothing and language distribution as Heaps law  






Introduction to and History of Language Models 

1. Language Models
    A Language Model is a statistical model that computes a probability distribution over sequences of words.  

    It is a time series prediction problem in which we must be very careful to train on the past and test on the future.   


2. Applications
        Machine Translation MT   
             Word Ordering  
                p"the cat is small" > p"small the cat is"  
             Word Choice  
                p"walking home after school" > p"walking house after school"
         Speech Recognition     
             Word Disambiguation  
                p"The listeners recognize speech" > p"The listeners wreck a nice beach"  
         Information Retrieval 
             Used in query likelihood model
            
3. Traditional Language Models
     Most language models employ the chain rule to decompose the joint probability into a sequence of conditional probabilities  
        $$\begin{array}{c}{P\leftw{1} w{2} w{3} \ldots w{N}\right=}  {P\leftw{1}\right P\leftw{2} | w{1}\right P\leftw{3} | w{1} w{2}\right \times \ldots \times P\leftw{N} | w{1} w{2} \ldots w{N 1}\right}\end{array}$$  
        Note that this decomposition is exact and allows us to model complex joint distributions by learning conditional distributions over the next word $$wn$$ given the history of words observed $$\leftw{1} \dots w{n 1}\right$$.   
        Thus the Goal of the LM Task is to find good conditional distributions that we can multiply to get the Joint Distribution.  
        Allows you to predict the  word then the  word given the  word then the  given the  two etc..  

     The Probability is usually conditioned on window of $$n$$ previous words  
         An incorrect but necessary Markovian assumption  
            $$Pw1 \ldots wm = \prod{i=1}^m Pwi | w1 \ldots w{i 1} \approx \prod{i=1}^m Pwi | w{i n 1} \ldots w{i 1}$$  
             Only previous history matters
             Limited Memory only last $$n 1$$ words are included in history  
        E.g. $$2 $$gram LM only looks at the previous word  
            $$\begin{aligned} p\leftw{1} w{2} w{3}\right.& \ldots & w{n}   &=p\leftw{1}\right p\leftw{2} | w{1}\right p\leftw{3} | w{1} w{2}\right \times \ldots  & \times p\leftw{n} | w{1} w{2} \ldots w{n 1}\right  & \approx p\leftw{1}\right p\leftw{2} | w{1}\right p\leftw{3} | w{2}\right \times \ldots \times p\leftw{n} | w{n 1}\right \end{aligned}$$   
        The conditioning context $$w{i 1}$$ is called the history.  
     The MLE estimate for probabilities compute for  
         Bi grams  
            $$Pw2 \| w1 = \dfrac{\text{count}w1 w2}{\text{count}w1}$$  
         Tri grams  
            $$Pw3 \| w1 w2 = \dfrac{\text{count}w1 w2 w3}{\text{count}w1 w2}$$  

4. Issues with the Traditional Approaches
    To improve performance we need to  
     Keep higher n gram counts  
     Use Smoothing  
     Use Backoff trying n gram n 1 gram n 2 grams ect.  
        When? If you never saw a 3 gram b4 try 2 gram 1 gram etc.  
    However 
     There are A LOT of n grams
         $$\implies$$ Gigantic RAM requirements  

5. NLP Tasks as LM Tasks
    Much of Natural Language Processing can be structured as conditional Language Modeling  
     Translation  
        $$p{\mathrm{LM}}\text { Les chiens aiment les os }\| \| \text { Dogs love bones }$$  
     QA  
        $$p{\mathrm{LM}}\text { What do dogs love? }\| \| \text { bones } . | \beta$$
     Dialog  
        $$p{\mathrm{LM}}\text { How are you? }\| \| \text { Fine thanks. And you? } | \beta$$    
    where $$\| \|$$ means "concatenation" and $$\beta$$ is an observed data e.g. news article to be conditioned on.  

            
6. Analyzing the LM Tasks
    The simple objective of modeling the next word given observed history contains much of the complexity of natural language understanding NLU e.g. reasoning intelligence etc..  

    Consider predicting the extension of the utterance  
    $$p\cdot | \text { There she built a }$$  
    The distribution of what word to predict right now is quite flat; you dont know where "there" is you dont know who "she" is you dont know what she would want to "build".    

    However With more context we are able to use our knowledge of both language and the world to heavily constrain the distribution over the next word.  
    $$p\cdot | \color{red} {\text { Alice }} \text {went to the} \color{blue} {\text { beach. } } \color{blue} {\text {There}} \color{red} {\text { she}} \text { built a}$$  
    At this point your distributions getting very peaked about what could come next and the reason is because you understand language you understand that in the  utterance "she" is "Alice" and "There" is "Beach" so you've resolved those Co references and you can do that because you understand the syntactic structure of the  utterance; you understand we have a subject and object where the verb phrase is all of these things you do automatically and then using the semantics that "at a beach you build things like sandcastles or boats" and so you can constrict your distribution.  

    If we can get a automatically trained machine to do that then we've come a long way to solving AI.  
    "The diversity of tasks the model is able to perform in a zero shot setting suggests that high capacity models trained to maximize the likelihood of a sufficiently varied text corpus begin to learn how to perform a surprising amount of tasks without the need for explicit supervision"  GPT 2 


7. Evaluating a Language Model \| The Loss
    For a probabilistic model it makes sense to evaluate how well the "learned" distribution matches the real distribution of the data of real utterances. A good model assigns real utterances $$w{1}^{N}$$  from a language a high probability. This can be measured with Cross Entropy  
    $$H\leftw{1}^{N}\right= \frac{1}{N} \log {2} p\leftw{1}^{N}\right$$  
    Why Cross Entropy It is a measure of how many bits are need to encode text with our model bits you would need to represent the distribution.^1  
    Commonly used for character level.  

    Alternatively people tend to use Perplexity  
    $$\text { perplexity }\leftw{1}^{N}\right=2^{H\leftw{1}^{N}\right}$$  
    Why Perplexity  It is a measure of how surprised our model is on seeing each word.     
    If no surprise the perplexity $$ = 1$$.    
    Commonly used for word level.  
    

8. Language Modeling Data
    Language modelling is a time series prediction problem in which we must be careful to train on the past and test on the future.  
    If the corpus is composed of articles it is best to ensure the test data is drawn from a disjoint set of articles to the training data.  

    Two popular data sets for language modeling evaluation are a preprocessed version of the Penn Treebank1 and the Billion Word Corpus.2 Both are flawed     
     The PTB is very small and has been heavily processed. As such it is not representative of natural language.  
     The Billion Word corpus was extracted by  randomly permuting sentences in news articles and then splitting into training and test sets. As such train and test sentences come from the same articles and overlap in time
    
    The recently introduced WikiText datasets are a better option.  

9. Three Approaches to Parameterizing Language Models
    1. Count Based N gram models we approximate the history of observed words with just the previous $$n$$ words.  
        They capture Multinomial distributions.   
    2. Neural N gram models embed the same fixed n gram history in a continuous space and thus better capture correlations between histories.  
        Replace the Multinomial distributions with an FFN.  
    3. RNNs drop the fixed n gram history and compress the entire history in a fixed length vector enabling long range correlations to be captured.   
        Replace the finite history captured by the conditioning context $$w{i 1}$$ with an infinite history captured by the previous hidden state $$h{n 1}$$ but also $$w{n=1}$$.  
    

10.Bias vs Variance in LM Approximations
    The main issue in language modeling is compressing the history a string. This is useful beyond language modeling in classification and representation tasks.  
     With n gram models we approximate the history with only the last n words  
     With recurrent models RNNs next we compress the unbounded history into a fixed sized vector  

    We can view this progression as the classic Bias vs. Variance tradeoff in ML  
     N gram models are biased but low variance.  
        No matter how much data infinite they will always be wrong/biased.  
     RNNs decrease the bias considerably hopefully at a small cost to variance.  

    Consider predicting the probability of a sentence by how many times you have seen it before. This is an unbiased estimator with extremely high variance.  
     In the limit of infinite data gives true distribution.  
    

11.Scaling Language Models Large Vocabularies
    Bottleneck  
    Much of the computational cost of a Neural LM is a function of the size of the vocabulary and is dominated by calculating the softmax  
    $$\hat{p}{n}=\operatorname{softmax}\leftW h{n}+b\right$$  

    Solutions  
     Short Lists use the neural LM for the most frequent words and a traditional n gram LM for the rest.  
        While easy to implement this nullifies the Neural LMs main advantage i.e. generalization to rare events.  
     Batch local short lists approximate the full partition function for data instances from a segment for the data with a subset of vocabulary chosen for that segment.  
     Approximate the gradient/change the objective  if we did not have to sum over the vocabulary to normalize during training it would be much faster. It is tempting to consider maximizing likelihood by making the log partition function an independent parameter $$c$$ but this leads to an ill defined objective  
        $$\hat{p}{n} \equiv \exp \leftW h{n}+b\right \times \exp c$$  
        What does the Softmax layer do?  
        The idea of the Softmax is to say at each time step look at the word we want to predict and the whole vocab; where we try to maximize the probability of the word we want to predict and minimize the probability of ALL THE OTHER WORDS.  

        So The better solution is to try to approximate what the softmax does using 
         Noise Contrastive Estimation NCE this amounts to learning a binary classifier to distinguish data samples from $$k$$ samples from a noise distribution a unigram is a good choice  
        $$p\left\text { Data }=1 | \hat{p}{n}\right=\frac{\hat{p}{n}}{\hat{p}{n}+k p{\text { noise }}\leftw{n}\right}$$   
        Now parametrizing the log partition function as $$c$$ does not degenerate. This is very effective for speeding up training but has no effect on testing.   
         Importance Sampling IS similar to NCE but defines a multiclass classification problem between the true word and noise samples with a Softmax and cross entropy loss.   


              
     Factorize the output vocabulary the idea is to decompose the one big softmax into a series of softmaxes 2 in this case. We map words to a set of classes then we  predict which class the word is in and then we predict the right word from the words in that class.  
        One level factorization works well Brown clustering is a good choice frequency binning is not  
        $$p\leftw{n} | \hat{p}{n}^{\text { class }} \hat{p}{n}^{\text { word }}\right=p\left\operatorname{class}\leftw{n}\right | \hat{p}{n}^{\text { class }}\right \times p\leftw{n} | \operatorname{class}\leftw{n}\right \hat{p} {n}^{\text { word }}\right$$  
        where the function $$\text{ class}\cdot$$ maps each word to one class. Assuming balanced classes this gives a quadratic $$\root{V}$$ speedup.  


             

    Complexity Comparison of the different solutions  
    
        
12.Sub Word Level Language Models
    Could be viewed as an alternative to changing the softmax by changing the input granularity and model text at the morpheme or character level.  
    This results in a much smaller softmax and no unknown words but the downsides are longer sequences and longer dependencies; moreover a lot of the structure in a language is in the words and we want to learn correlations amongst the words but since the model doesn't get the words as a unit it will have to learn what/where a is before it can learn its correlation with other sequences; which effecitely means that we made the learning problem harder and more non linear     
    This also allows the model to capture subword structure and morphology e.g. "disunited" < > "disinherited" < > "disinterested".  
    Character LMs lag behind word based models in perplexity but are clearly the future of language modeling.  

13.Conditional Language Models
    A Conditional LM assigns probabilities to sequences of words given some conditioning context $$x$$. It models "What is the probability of the next word given the history of previously generated words AND conditioning context $$x$$?".  
    The probability decomposed w/ chain rule  
    $$p\boldsymbol{w} | \boldsymbol{x}=\prod{t=1}^{\ell} p\leftw{t} | \boldsymbol{x} w{1} w{2} \ldots w{t 1}\right$$  
     


^1 the problem of assigning a probability to a string and text compression is exactly the same problem so if you have a good language model you also have a good text compression algorithm and both we think of it in terms of the number of bits we can compress our sequence into.  



Recurrent Neural Networks

1. Recurrent Neural Networks
       An RNN is a class of artificial neural network where connections between units form a directed cycle allowing it to exhibit dynamic temporal behavior.
       The standard RNN is a nonlinear dynamical system that maps sequences to sequences.  

2. The Structure of an RNN
       The RNN is parameterized with three weight matrices and three bias vectors  
       $$ \theta = W{hv} W{hh} W{oh} bh bo h0 $$
       These parameter completely describe the RNN.  

3. The Algorithm
       Given an input sequence $$\hat{x} = x1 \ldots xT$$ the RNN computes a sequence of hidden states $$h1^T$$ and a sequence of outputs $$y1^T$$ in the following way  
        for $$t$$ in $$1 ... T$$ do  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$ut \leftarrow W{hv}xt + W{hh}h{t 1} + bh$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$ht \leftarrow ghut$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$ot \leftarrow W{oh}h{t} + bo$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$yt \leftarrow gyot$$   

4. The Loss
       The loss of an RNN is commonly a sum of per time losses  
       $$Ly z = \sum{t=1}^TLyt zt$$
        Language Modelling 
            We use the Cross Entropy Loss function but predicting words instead of classes
       $$ J^{t}\theta =  \sum{j=1}^{\vert V \vert} y{t j} \log \hat{y{t j}}$$
       $$\implies$$
       $$Lyz = J =  \dfrac{1}{T} \sum{t=1}^{T} \sum{j=1}^{\vert V \vert} y{t j} \log \hat{y{t j}}$$
       To Evaluate the model we use Preplexity  
       $$ 2^J$$
    Lower Preplexity is better

5. Analyzing the Gradient
       Assuming the following formulation of an RNN  
       $$ht = Wfh{t 1} + W^{hx}x{t} 
        \hat{yt} = W^{S}fht$$
        The Total Error is the sum of each error at each time step $$t$$  
       $$ \dfrac{\partial E}{\partial W} = \sum{t=1}^{T} \dfrac{\partial Et}{\partial W}$$
        The local Error at a time step $$t$$  
       $$\dfrac{\partial Et}{\partial W} = \sum{k=1}^{t} \dfrac{\partial Et}{\partial yt} \dfrac{\partial yt}{\partial ht} \dfrac{\partial ht}{\partial hk} \dfrac{\partial hk}{\partial W}$$
        To compute the local derivative we need to compute  
       $$\dfrac{\partial ht}{\partial hk}$$
       
       $$\begin{align}
        \dfrac{\partial ht}{\partial hk} &= \prod{j=k+1}^t \dfrac{\partial hj}{\partial h{j 1}} 
        &= \prod{j=k+1}^t J{j j 1}
        \end{align}$$
       $$\\\\\\\\$$ where each $$J{j j 1}$$ is the jacobina matrix of the partial derivatives of each respective  
        $$\\\\\\\\$$ hidden layer.

9. The Vanishing Gradient Problem
        Analyzing the Norms of the Jacobians of each partial  
       $$\| \dfrac{\partial hj}{\partial h{j 1}} \| \leq \| W^T \| \cdot \| \text{ diag}f'h{j 1} \| \leq \betaW \betah$$
       $$\\\\\\\$$ where we defined the $$\beta$$s as upper bounds of the norms.        
        The Gradient is the product of these Jacobian Matrices each associated with a step in the forward computation  
       $$ \| \dfrac{\partial ht}{\partial hk} \| = \| \prod{j=k+1}^t \dfrac{\partial hj}{\partial h{j 1}} \| \leq \betaW \betah^{t k}$$
        Conclusion  
            Now as the exponential $$t k \rightarrow \infty$$  
             If $$\betaW \betah < 1$$   
                $$\betaW \betah^{t k} \rightarrow 0$$.  
                known as Vanishing Gradient. 
             If $$\betaW \betah > 1$$  
                $$\betaW \betah^{t k} \rightarrow \infty$$.  
                known as Exploding Gradient. 
       As the bound can become very small or very large quickly the locality assumption of gradient descent breaks down.

                    
            

                


6. BPTT
       for $$t$$ from $$T$$ to $$1$$ do  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$dot \leftarrow gy'ot · dLyt ; zt/dyt$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$dbo \leftarrow dbo + dot$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$dW{oh} \leftarrow dW{oh} + dotht^T$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$dht \leftarrow dht + W{oh}^T dot$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$dyt \leftarrow gh'yt · dht$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$dW{hv} \leftarrow dW{hv} + dytxt^T$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$dbh \leftarrow dbh + dyt$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$dW{hh} \leftarrow dW{hh} + dyth{t 1}^T$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$dh{t 1} \leftarrow W{hh}^T dyt$$  
        Return $$\\\\ d\theta = dW{hv} dW{hh} dW{oh} dbh dbo dh0$$


7. Backpropagation Through Time
     We can think of the recurrent net as a layered feed forward net with shared weights and then train the feed forward net with linear weight constraints.
     We can also think of this training algorithm in the time domain
         The forward pass builds up a stack of the activities of all the units at each time step
         The backward pass peels activities off the stack to compute the error derivatives at each time step
         After the backward pass we add together the derivatives at all the different times for each weight.  

    Complexity  
    Linear in the length of the longest sequence.  
    Minibatching can be inefficient as the sequences in a batch may have different lengths.  
    Can be alleviated w/ padding.  

8. Truncated BPTT
    Same as BPTT but tries to avoid the problem of long sequences as inputs. It does that by "breaking" the gradient flow every nth time step if input is article then n could be average length of a sentence thus avoiding problems of 1 Memory 2 Exploding Gradient.  

    Downsides  
    If there are dependencies between the segments where BPTT was truncated they will not be learned because the gradient doesn't flow back to teach the hidden representation about what information was useful.  

    Complexity  
    Constant in the truncation length $$T$$.  
    Minibatching is efficient as all sequences have length $$T$$.  

    Notes  
     In TBPTT we Forward Propagate through the break/between segments normally through the entire comp graph. Only the back propagation is truncated.  
     Mini batching on a GPU is an effective way of speeding up big matrix vector products ^2. RNNLMs have two such products that dominate their computation the recurrent matrix $$V$$ and the softmax matrix $$W$$.  



^2 By making them Matrix Matrix products instead.  


LSTMS  
 The core of the history/memory is captured in the cell state $$c{n}$$ instead of the hidden state $$h{n}$$.  
 & Key Idea The update to the cell state $$c{n}=c{n 1}+\operatorname{stanh}\leftV\leftw{n 1} ; h{n 1}\right+b{c}\right$$  here are additive. differentiating a sum gives the identity Making the gradient flow nicely through the sum. As opposed to the multiplicative updates to $$hn$$ in vanilla RNNs.  
    There is non linear funcs applied to the history/context cell state. It is composed of linear functions. Thus avoids gradient shrinking.  

 In the recurrency of the LSTM the activation function is the identity function with a derivative of 1.0. So the backpropagated gradient neither vanishes or explodes when passing through but remains constant.
 By the selective read write and forget mechanism using the gating architecture of LSTM there exist at least one path through which gradient can flow effectively from $$L$$  to $$\theta$$. Hence no vanishing gradient.   
 However one must remember that this is not the case for exploding gradient. It can be proved that there can exist at least one path thorough which gradient can explode.  
 LSTM decouples cell state typically denoted by c and hidden layer/output typically denoted by h and only do additive updates to c which makes memories in c more stable. Thus the gradient flows through c is kept and hard to vanish therefore the overall gradient is hard to vanish. However other paths may cause gradient explosion.  
 The Vanishing gradient solution for LSTM is known as Constant Error Carousel.  


      




      


Important Links  










RNNs Extra!


5. Vanishing/Exploding Gradients
        Exploding Gradients  
             Truncated BPTT 
             Clip gradients at threshold 
             RMSprop to adjust learning rate 
         Vanishing Gradient   
             Harder to detect 
             Weight initialization 
             ReLu activation functions 
             RMSprop 
             LSTM GRUs 

1. Applications
        NER  
         Entity Level Sentiment in context  
         Opinionated Expressions

2. Bidirectional RNNs
        Motivation  
            For classification we need to incorporate information from words both preceding and following the word being processed
       $$\\\\$$ Here $$h = \overrightarrow{h};\overleftarrow{h}$$ represents summarizes the past and the future around a single token.
        Deep Bidirectional RNNs  
       $$\\\\\$$ Each memory layer passes an intermediate sequential representation to the next.

3. Math to Code
       The Parameters $$\{W{hx} W{hh} W{oh} ; bh bo ho\}$$   
       $$\begin{align}
        ht &= \phiW{hx}xt + W{hh}h{t 1} + bh 
        ht &= \phi\begin{bmatrix}
    W{hx} & ; & W{hh}
\end{bmatrix}   
        \begin{bmatrix} xt   ;    h{t 1} \end{bmatrix} + bh
        \end{align}
        $$ 
       $$yt = \phi'W{oh}ht + bo$$ 

4. Initial States

6. Specifying the Initial States

7. Teaching Signals


5. Vanishing/Exploding Gradients
        Exploding Gradients  
             Truncated BPTT 
             Clip gradients at threshold 
             RMSprop to adjust learning rate 
         Vanishing Gradient   
             Harder to detect 
             Weight initialization 
             ReLu activation functions 
             RMSprop 
             LSTM GRUs 

9. Rectifying the Vanishing/Exploding Gradient Problem

8. Linearity of BackProp
       The derivative update are also Correlated which is bad for SGD.  


 Attention Mechanism for DNNs 















Introduction

1. Motivation
    In Vanilla Seq2Seq models the only representation of the input is the fixed dimensional vector representation $$y$$ that we need to carry through the entire decoding process.   

    This presents a bottleneck in condensing all of the information of the entire input sequence into just one fixed length vector representation.  

2. Attention
    Attention is a mechanism that allows DNNs to focus on view certain local or global features of the input sequence as a whole or in part.     

    Attention involves focus on certain parts of the input while having a low resolution view of the rest of the input   similar to human attention in vision/audio.  

    An Attention Unit considers all sub regions and contexts as its input and it outputs the weighted arithmetic mean of these regions.  
    The arithmetic mean is the inner product of actual values and their probabilities.  
    $$mi = \tanh xiW{xi} + CWC$$   
    These probabilities are calculated using the context.  
    The Context $$C$$ represents everything the RNN has outputted until now.  

    The difference between using the hyperbolic tanh and a dot product is the granularity of the output regions of interest  tanh is more fine grained with less choppy and smoother sub regions chosen.  

    The probabilities are interpreted as corresponding to the relevance of the sub region $$xi$$ given context $$C$$.  
    

4. Types of Attention
     Soft Attention we consider different parts of different subregions   
         Soft Attention is deterministic 
     Hard Attention we consider only one subregion  
         Hard Attention is a stochastic process 
    

5. Strategy
     Encode each word in the sentence into a vector representation
     When decoding perform a linear combination of these vectors weighted by attention weights 
     Use this combination in picking the next word subregion  
    

6. Calculating Attention
    An attention function can be described as mapping a query and a set of key value pairs to an output where the query keys values and output are all vectors. The output is computed as a weighted sum of the values where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.  
     Use query vector decoder state and key vectors all encoder states
     For each query key pair calculate weight 
     Normalize to add to one using softmax 
     Combine together value vectors usually encoder states like key vectors by taking the weighted sum
     Use this in any part of the model  
    

7. Attention Score Functions
    $$q$$ is the query $$k$$ is the key  
     Multi Layer Perceptron Bahdanau et al. 2015  
         Flexible often very good with large data   
        $$aqk = w2^T \tanh W1q;k$$   
     Bilinear luong et al. 2015  
         Not used widely in Seq2Seq models
         Results are inconsistent
        $$aqk = q^TWk$$  
     Dot Product luong et al. 2015  
         No parameters
         Requires the sizes to be the same
        $$aqk = q^Tk$$  
     Scaled Dot Product Vaswani et al. 2017  
         Solves the scale problem of the dot product the scale of the dot product increases as dimensions get larger
        $$aqk = \dfrac{q^Tk}{\sqrt{\vert k \vert}}$$  

    


    

     
            
                
8. What to Attend to?
     Input Sentence  
         A previous word for translation  Neural Machine Translation/  
         Copying Mechanism  Gu et al. 2016/  
         Lexicon bias Arthur et al. 2016/  
     Previously Generated Things  
         In language modeling attend to the previous words  Merity et al. 2016/   
            Attend to the previous words that you generated and decide whether to use them again copy  
         In translation attend to either input or previous output  Vaswani et al. 2017/  
    

9. Modalities
     Images Xu et al. 2015    
     Speech Chan et al. 2015  
     Hierarchical Structures Yang et al. 2016  
         Encode with attention over each sentence then attention over each sentence in the document  
     Multiple Sources    
         Attend to multiple sentences in different languages to be translated to one target language Zoph et al. 2015  
         Attend to a sentence and an image Huang et al. 2016  
    
                
10.Intra Attention/Self Attention
    Each element in the sentence attends to other elements   context sensitive encodings.  

    It behaves similar to a Bi LSTM in that it tries to encode information about the context words around the current input into the representation of the word.  
    It differs however  
    1. Intra Attention is much more direct as it takes the context directly without being influenced by many steps inside the RNN 
    2. It is much faster as it is only a dot/matrix product  
    

11.Improvement to Attention
    The Coverage Problem  Neural models tend to drop or repeat content when tested on data not very similar to the training set  
    Solution Model how many times words have been covered  
         Impose a penalty if attention is not $$\approx 1$$ for each word Cohn et al. 2015   
            It forces the system to translate each word at least once.  
         Add embeddings indicating coverage Mi.. et al. 2016  
         Incorporating Markov Properties Cohn et al. 2015  
             Intuition attention from last time tends to be correlated with attention this time
             Strategy Add information about the last attention when making the next decision
         Bidirectional Training Cohn et al. 2015 
             Intuition Our attention should be roughly similar in forward and backward directions
             Method Train so that we get a bonus based on the trace of the matrix product for training in both directions  
                $$\mathrm{Tr} A{X \rightarrow Y}A^T{Y \rightarrow X}$$  
         Supervised Training Mi et al. 2016   
             Sometimes we can get "gold standard" alignments a priori  
                 Manual alignments
                 Pre trained with strong alignment model
             Train the model to match these strong alignments bias the model  
    


12.Attention is not Alignment Koehn and Knowles 2017
     Attention is often blurred
     Attention is often off by one  
        Since the DNN has already seen parts of the information required to generate previous outputs it might not need all of the information from the word that is actually matched with its current output.  

    Thus even if Supervised training is used to increase alignment accuracy the overall error rate of the task might not actually decrease. 


Specialized Attention Varieties

1. Hard Attention Xu et al. 2015
     Instead of a soft interpolation make a Zero One decision about where to attend Xu et al. 2015
         Harder to train  requires reinforcement learning methods
     It helps interpretability Lei et al. 2016   
    

2. Monotonic Attention Yu et al. 2016
     In some cases we might know the output will be the same order as the input  
         Speech Recognition
         Incremental Translation
         Morphological Inflection  sometimes
         Summarization  sometimes
     Hard decisions about whether to read more
    

3. Convolutional Attention Allamanis et al. 2016
     Intuition we might want to be able to attend to "the word after 'Mr.'"  
    

4. Multi headed Attention
     Idea multiple attention heads focus on different parts of the sentence
     Different heads for "copy" vs regular Allamanis et al. 2016   
     Multiple independently learned heads Vaswani et al. 2017
    

5. Tips
     Don't use attention with very long sequences  especially those you want to summarize and process efficiently 
     Fertility we impose the following heuristic "It is bad to pay attention to the same subregion many times" 
    

6. Notes
     Attention is a mean field approximation of sampling from a categorical distribution over source word embeddings or the rnn state aligned with a source word etc  
     Additive VS Multiplicative Attention  
        Additive attention computes the compatibility function using a feed forward network with a single hidden layer.  
        Multiplicative attention uses the dot product.  
        While the two are similar in theoretical complexity dot product attention is much faster and more space efficient in practice since it can be implemented using highly optimized matrix multiplication code.  



Representing Sentences Solving the Vector Problem

1. The Problem Conditioning with Vectors
    The Problem We are compressing a lot of information into a finite sized vector.  
    Moreover gradients flow a very long time/distance; making even LSTMs forget.  

    Sentences are of different sizes but vectors are of the same size; making the compression inherently very lossy.  
    

2. The Solution Representing Sentences as Matrices
    We represent a source sentence as a matrix and generate the target sentence from a matrix  
     Fixed number of rows but number of columns depends on the number of words.  

    This will  
     Solve the capacity problem  
     Solve the gradient flow problem 
    

3. How to build the Matrices?
    1. Concatenation  
         Each word type is represented by an n dimensional vector.  
         Take all the vectors for the sentence and concatenate them into a matrix
         This is the simplest possible model that there are no published results on it...
    2. Convolutional Networks  
         Apply CNNs to transform the naive concatenated matrix to obtain a context dependent matrix  
         Remove the pooling layer at the end to ensure variable sized output  
    3. BiRNNs  
         Most widely used in NMT Bahdanau et al 2015  
         One column per word
         Each column word has two halves concatenated together  
             A "forward representation" word and its LEFT context  
             A "reverse representation" word and its RIGHT context  



 CNNs in NLP 




1. Motivation
    Combination consecutively of words are hard to capture/model/detect.  

2. Padding
Padding  
 After convolution the rows and columns of the output tensor are either  
     Equal to rows/columns of input tensor "same" convolution  
        Keeps the output dimensionality intact.  

     Equal to rows/columns of input tensor minus the size of the filter plus one "valid" or "narrow'  
     Equal to rows/columns of input tensor plus filter minus one "wide"  

Striding  
Skip some of the outputs to reduce length of extracted feature vector  

Pooling  
Pooling is like convolution but calculates some reduction function feature wise.  
 Types  
     Max Pooling "Did you see the feature anywhere in the range?"  
     Average pooling "How prevalent is this feature over the entire range?"  
     k Max pooling "Did you see this feature up to k times?"  
     Dynamic pooling "Did you see this feature in the beginning? In the middle? In the end?"  

Stacking  Stacked Convolution  
 Feeding in convolution from previous layer results in larger are of focus for each feature  
 The increase in the number of words that are covered by stacked convolution e.g. n grams is exponential in the number of layers  

Dilation  Dilated Convolution  
Gradually increase stride every time step no reduction in length.  
One can use the final output vector for next target output prediction. Very useful if the problem we are modeling requires a fixed size output e.g. auto regressive models.  
 Why Dilated Convolution for Modeling Sentences?  
     In contrast to recurrent neural networks
         + Fewer steps from each word to the final representation RNN $$ON$$ Dilated CNN $$0\log{N}$$ 
         + Easier to parallelize on GPU 
          Slightly less natural for arbitrary length dependencies 
          A bit slower on CPU?  
 Interesting Work  
    "Iterated Dilated Convolution Strubell 2017"  
     A method for sequence labeling  
        Multiple Iterations of the same stack of dilated convolutions with different widths to calculate context  
     Results
         Wider context 
         Shared parameters i.e. more parameter efficient  

Structured Convolution  
 Why?  
    Language has structure would like it to localize features.  
    e.g. noun verb pairs very informative but not captured by normal CNNs   

 Examples   
     Tree Structured Convolution Ma et al. 2015  
     Graph Convolution Marcheggiani et al. 2017 
        































 TensorFlow 



Tips and Tricks


1. Saving the model
    After the model is run it uses the most recent checkpoint
     To run a different model with different architecture use a different branch  

       

       

       

       

       

       

       



 Text Classification








Introduction

1. Text Classification Breakdown
    We can think of text classification as being broken down into a two stage process  
    1. Representation Process text into some fixed representation  > How to learn $$\mathbf{x}'$$.  
    2. Classification Classify document given that representation $$\mathbf{x}'$$  > How to learn $$pc\vert x'$$.  


2. Representation
    Bag of Words BOW  
     Pros  
         Easy no effort
     Cons  
         Variable size ignores sentential structure sparse representations  

    Continuous BOW  
     Pros  
         Continuous Repr.
     Cons  
         Ignores word ordering  

    Deep CBOW  
     Pros  
         Can learn feature combinations e.g. "not" AND "hate"  
     Cons  
         Cannot learn word ordering positional info directly e.g. "not hate"  

    Bag of n grams  
     Pros  
         Captures some combination features and word ordering e.g. "not hate" works well  
     Cons  
         Parameter Explosion no sharing between similar words/n grams


3. CNNs for Text
    Two main paradigms  
    1. Context window modeling for tagging etc. get the surrounding context before tagging.  
    2. Sentence modeling do convolution to extract n grams pooling to combine over whole sentence.  




 Articulated Body Pose Estimation  Human Pose Estimation


Introduction
 

DeepPose 



1. Main Idea
       Pose Estimation is formulated as a DNN based regression problem towards body joints.  
        The DNN regressors are presented as a cascade for higher precision in pose estimates.    

2. Structure
        Input 
             Full Image
             7 layered generic Convolutional DNN    
        Each Joint Regressor uses the full image as a signal.   


3. Key Insights
        Replace the explicitly designed feature representations and detectors for the parts the model topology and the interactions between joints by a learned representation through a ConvNet  
         The DNN based Pose Predictors are presented as a cascade to increase the precision of joint localization  
         Although the regression loss does not model explicit interactions between joints such are implicitly captured by all of the 7 hidden layers   all the internal features are shared by all joint regressors

4. Method
        Start with an initial pose estimation based on the full image
         Learn DNN based regressors which refine the joint predictions by using higher resolution sub images


5. Notation
        Pose Vector = $$\mathbf{y} = \left\ldots \mathbf{y}i^T \ldots\right^T \ i \in \{1 \ldots k\}$$  
         Joint Co ordinates = $$\mathbf{y}i^T = xi yi$$ of the $$i$$ th joint  
         Labeled Image = $$x \mathbf{y}$$  
             $$x = $$ Image Data  
             $$\mathbf{y} = $$ Ground Truth Pose Vector
         Bounding Box = $$b$$ a box bounding the human body or parts of it   
         Normalization Function $$= N\mathbf{y}i; b$$ normalizes the joint coordinates w.r.t a bounding box $$b$$  
            Since the joint coordinates are in absolute image coordinates and poses vary in size from image to image   

             Translate by box center
             Scale by box size  
         Normalized pose vector = $$N\mathbf{y}; b = \left\ldots N\mathbf{y}i; b^T \ldots\right^T$$  
         A crop of image $$x$$ by bounding box $$b$$ = $$Nx; b$$
         Learned Function = $$\psix;\theta \in \mathbb{R}^2k$$ is a functions that regresses to normalized pose vector given an image  
             Input image $$x$$
             Output Normalized pose vector $$N\mathbf{y}$$                 
         Pose Prediction   
           $$y^\ast = N^{ 1}\psiNx;\theta$$   

7. Architecture
        Problem Regression Problem
         Goal Learn a function $$\psix;\theta$$ that is trained and used to regress to a pose vector.   
         Estimation $$\psi$$ is based on learned through Deep Neural Net
         Deep Neural Net is a Convolutional Neural Network; namely AlexNet  
             Input image with pre defined size $$ = \$$ # pixels $$\times 3$$ color channels  
                $$220 \times 220$$ with a stride of $$4$$  
             Output target value of the regression$$ = 2k$$ joint coordinates  
    Denote by $$\mathbf{C}$$ a convolutional layer by $$\mathbf{LRN}$$ a local response normalization layer $$\mathbf{P}$$ a pooling layer and by $$\mathbf{F}$$ a fully connected layer  
    For $$\mathbf{C}$$ layers the size is defined as width $$\times$$ height $$\times$$ depth where the  two dimensions have a spatial meaning while the depth defines the number of filters.  
        Alex Net 
             Architecture     $$\mathbf{C}55 \times 55 \times 96 − \mathbf{LRN} − \mathbf{P} − \mathbf{C}27 \times 27 \times 256 − \mathbf{LRN} − \mathbf{P} − \mathbf{C}13 \times 13 \times 384 − \mathbf{C}13 \times 13 \times 384 − \mathbf{C}13 \times 13 \times 256 − \mathbf{P} − \mathbf{F}4096 − \mathbf{F}4096$$   
             Filters  
                 $$\mathbf{C}{1} = 11 \times 11$$  
                 $$\mathbf{C}{2} = 5 \times 5$$  
                 $$\mathbf{C}{3 5} = 3 \times 3$$.
             Total Number of Parameters $$ = 40$$M   
             Training Dataset  
                Denote by $$D$$ the training set and $$DN$$ the normalized training set   
                $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  $$DN = \{NxN\mathbf{y}\vert x\mathbf{y} \in D\}$$   
             Loss the Loss is modified; instead of a classification loss we train a linear regression on top of the last network layer to predict a pose vector by minimizing $$L2$$ distance between the prediction and the true pose vector  
       $$\arg \min\theta \sum{xy \in DN} \sum{i=1}^k \|\mathbf{y}i  \psiix;\theta\|2^2$$  
        Optimization  
             BackPropagation in a distributed online implementation
             Adaptive Gradient Updates
             Learning Rate $$ = 0.0005 = 5\times 10^{ 4}$$
             Data Augmentation randomly translated image crops left/right flips
             DropOut Regularization for the $$\mathbf{F}$$ layers $$ = 0.6$$

9. Architecture
        Motivation   
            Although the pose formulation of the DNN has the advantage that the joint estimation is based on the full image and thus relies on context due its fixed input size of $$220 \times 220$$ the network has limited capacity to look at detail  it learns filters capturing pose properties at coarse scale.  
            The pose properties are necessary to estimate rough pose but insufficient to always precisely localize the body joints.  
            Increasing the input size is infeasible since it will increase the already large number of parameters.  
            Thus a cascade of pose regressors is used to achieve better precision.  
         Structure and Training   
            At the  stage 
             The cascade starts off by estimating an initial pose as outlined in the previous section.  
            At subsequent stages  
             Additional DNN regressors are trained to predict a displacement of the joint locations from previous stage to the true location.  
                Thus each subsequent stage can be thought of as a refinement of the currently predicted pose.   
             Each subsequent stage uses the predicted joint locations to focus on the relevant parts of the image   subimages are cropped around the predicted joint location from previous stage and the pose displacement regressor for this joint is applied on this sub image.  
                Thus subsequent pose regressors see higher resolution images and thus learn features for finer scales which ultimately leads to higher precision  
         Method and Architecture  
             The same network architecture is used for all stages of the cascade but learn different parameters.   
             Start with a bounding box $$b^0$$ which either encloses the full image or is obtained by a person detector
             Obtain an initial pose  
                Stage 1 $$\mathbf{y}^1 \leftarrow N^{ 1}\psiNx;b^0;\theta1;b^0$$  
             At stages $$s \geq 2$$ for all joints
                 Regress  towards a refinement displacement $$\mathbf{y}i^s  \mathbf{y}i^{s 1}$$ by applying a regressor on the sub image defined by $$bi^{s 1}$$ 
                 Estimate new joint boxes $$bi^s$$  
                Stage $$s$$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{y}i^s \leftarrow \mathbf{y}i^{2 1} + N^{ 1}\psiNx;b^0;\thetas;b  \\ 6  
 \ \ \ \ \ \ \ \ \ \ \ \ \ \                  \\\\ \text{for } b = bi^s 1 
  \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
                bi^s \leftarrow \mathbf{y}i^s \sigma diam\mathbf{y}^s \sigma diam\mathbf{y}^s \\ 7$$  
                where we considered a joint bounding box $$bi$$ capturing the sub image around $$\mathbf{y}i bi\mathbf{y}; \sigma = \mathbf{y}i \sigma diam\mathbf{y} \sigma diam\mathbf{y}$$ having as center the i th joint and as dimension the pose diameter scaled by $$\sigma$$ to refine a given joint location $$\mathbf{y}i$$.    
             Apply the cascade for a fixed number of stages $$ = S$$  
         Loss at each stage $$s$$   
      $$\thetas = \arg \min\theta \sum{x\mathbf{y}i \in DA^s} \|\mathbf{y}i  \psiix;\theta\|2^2 \\\\\ 8$$


6. Advantages
        The DNN is capable of capturing the full context of each body joint  
         The approach is simpler to formulate than graphical models methods  no need to explicitly design feature representations and detectors for parts or to explicitly design a model topology and interactions between joints.   
            Instead a generic ConvNet learns these representations

8. Notes
        The use of a generic DNN architecture is motivated by its outstanding results on both classification and localization problems and translates well to pose estimation  
         Such a model is a truly holistic one — the final joint location estimate is based on a complex nonlinear transformation of the full image  
         The use of a DNN obviates the need to design a domain specific pose model
         Although the regression loss does not model explicit interactions between joints such are implicitly captured by all of the 7 hidden layers   all the internal features are shared by all joint regressors  




       


       


       


       


       


       


       


       



FOURTH

       


       


       


       


       


       



 Generative Models  Unsupervised Learning










Unsupervised Learning

1. Unsupervised Learning
    Data $$x$$ Just data no labels!   
    Goal Learn some underlying hidden structure of the data  
    Examples Clustering dimensionality reduction feature learning density estimation etc.  
    


Generative Models

Given some data $$\{dc\}$$ of paired observations $$d$$ and hidden classes $$c$$  

1. Generative Joint Models
    Generative Models are Joint Models.  
    Joint Models place probabilities $$\leftPcd\right$$ over both the observed data and the "target" hidden variables that can only be computed from those observed.  
    
    Generative models are typically probabilistic specifying a joint probability distribution $$Pdc$$ over observation and target label values and tries to Maximize this joint Likelihood.  
    Choosing weights turn out to be trivial chosen as the relative frequencies.  

    They address the problem of density estimation a core problem in unsupervised learning.  

    Examples  
    
     Gaussian Mixture Model
     Naive Bayes Classifiers  
     Hidden Markov Models HMMs
     Restricted Boltzmann Machines RBMs
     AutoEncoders
     Generative Adversarial Networks GANs


2. Discriminative Conditional Models
       Discriminative Models are Conditional Models.  
       Conditional Models provide a model only for the "target" hidden variabless.  
        They take the data as given and put a probability $$\leftPc \vert d\right$$ over the "target" hidden structures given the data.  
       Conditional Models seek to Maximize the Conditional Likelihood.  
        This maximization task is usually harder to do.  
       Examples  
         Logistic Regression
         Conditional LogLinear/Maximum Entropy Models  
         Condtional Random Fields  
         SVMs  
         Perceptrons  
         Neural Networks

3. Generative VS Discriminative Models
    Basically Discriminative Models infer outputs based on inputs  
    while Generative Models generate both inputs and outputs typically given some hidden paramters.  
    
    However notice that the two models are usually viewed as complementary procedures.  
    One does not necessarily outperform the other in either classificaiton or regression tasks.   

4. Example Uses of Generative Models
        Clustering
         Dimensionality Reduction
         Feature Learning
         Density Estimation

5. Density Estimation
       Generative Models given training data will generate new samples from the same distribution.   
       They address the Density Estimation problem a core problem in unsupervised learning.  
        Types of Density Estimation  
             Explicit Explicitly define and solve for $$p\text{model}x$$  
             Implicit Learn model that can sample from $$p\text{model}x$$ without explicitly defining it     

6. Applications of Generative Models
        Realistic samples for artwork
         Super Resolution
         Colorization
         Generative models of time series data can be used for simulation and planning  
            reinforcement learning applications  
         Inference of Latent Representations that can be useful as general feature descriptors 

7. Taxonomy of Generative Models


AutoRegressive Models  PixelRNN and PixelCNN



1. Fully Visible Deep Belief Networks
    Deep Belief Network DBNs are generative graphical models or alternatively a class of deep neural networks composed of multiple layers of latent variables "hidden units" with connections between the layers but not between units within each layer.  
    
    DBNs undergo unsupervised training to learn to probabilistically reconstruct the inputs.  

    They generate an Explicit Density Model.  

    They use the chain rule to decompose the likelihood of an image $$x$$ into products of 1 d distributions  
    then they Maximize the Likelihood of the training data.  

    The conditional distributions over pixels are very complex.  
    We model them using a neural network. 

2. PixelRNN
       is a proposed architecture part of the class of Auto Regressive models to model an explicit distribution of natural images in an expressive tractable and scalable way.  
       It sequentially predicts the pixels in an image along two spatial dimensions.  
       The Method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in an image.  
       The approach is to use probabilistic density models like Gaussian or Normal distribution to quantify the pixels of an image as a product of conditional distributions.  
        This approach turns the modeling problem into a sequence problem where the next pixel value is determined by all the previously generated pixel values.  
        Key Insights  
             Generate image pixels starting from corner  
             Dependency on previous pixels is modeled using an LSTM  
        The Model  
             Scan the image one row at a time and one pixel at a time within each row
             Given the scanned content predict the distribution over the possible values for the next pixel
             Joint distribution over the pixel values is factorized into a product of conditional distributions thus causing the problem as a sequence problem
             Parameters used in prediction are shared across all the pixel positions
             Since each pixel is jointly determined by 3 values 3 colour channels each channel may be conditioned on other channels as well
        Drawbacks   
             Sequential training is slow
             Sequential generation is slow   


3. PixelCNN
       Similar to the PixelRNN model the PixelCNN models  the Pixel Distribution $$p\vec{x}$$ where $$\vec{x} = x0 \ldots xn$$ is the vector of pixel values of a given image.  
       Similarly we use the chain rule for join distribution $$px = px0 \prod1^n pxi | x{i<}$$.  
        such that the  pixel is independent the  depends on the  and the  depends on both the  and  etc.  
        Key Insights  
             Still generate image pixels starting from corner
             Dependency on previous pixels now modeled using a CNN over context region
             Training maximize likelihood of training images  
        Upsides  
             Training is faster than PixelRNN since we can parallelize the convolutions because the context region values are known from the training images.  
        Issues  
             Generation is still sequential thus slow.



4. Improving PixelCNN Performance
     Gated Convolutional Layers 
     Short cut connections
     Discretized logistic loss
     Multi scale
     Training tricks

    Further Reading  
    
     PixelCNN++ \| Salimans et al. 2017    
     Van der Oord et al. NIPS 2016
     Pixel Snail  

5. Pros and Cons of Auto Regressive Models
     Pros   
         Can explicitly compute likelihood $$px$$
         Explicit likelihood of training data gives good evaluation metric
         Good Samples
     Cons  
         Sequential Generation is Slow  


Variational Auto Encoders


Auto Encoders generate Features that capture factors of variation in the training data.

0. Auto Regressive Models VS Variational Auto Encoders
       Auto Regressive Models defined a tractable discrete density function and then optimized the likelihood of training data   
       $$p\thetax = px0 \prod1^n pxi | x{i<}$$  
       On the other hand VAEs defines an intractable continuous density function with latent variable $$z$$  
       $$p\thetax = \int p\thetaz p\thetax|z dz$$
       but cannot optimize directly; instead derive and optimiz a lower bound on likelihood instead.  

1. Variational Auto Encoders VAEs
       Variational Autoencoder models inherit the autoencoder architecture but make strong assumptions concerning the distribution of latent variables.  
       They use variational approach for latent representation learning which results in an additional loss component and specific training algorithm called Stochastic Gradient Variational Bayes SGVB.  

2. Assumptions
       VAEs assume that 
         The data is generated by a directed graphical model $$px\vert z$$ 
         The encoder is learning an approximation $$q\phiz|x$$ to the posterior distribution $$p\thetaz|x$$  
            where $${\displaystyle \mathbf {\phi } }$$ and $${\displaystyle \mathbf {\theta } }$$ denote the parameters of the encoder recognition model and decoder generative model respectively.  
         The training data $$\left\{x^{i}\right\}{i=1}^N$$ is generated from underlying unobserved latent representation $$\mathbf{z}$$

3. The Objective Function
       $${\displaystyle {\mathcal {L}}\mathbf {\phi } \mathbf {\theta } \mathbf {x} =D{KL}q{\phi }\mathbf {z} |\mathbf {x} ||p{\theta }\mathbf {z}  \mathbb {E} {q{\phi }\mathbf {z} |\mathbf {x} }{\big }\log p{\theta }\mathbf {x} |\mathbf {z} {\big }}$$
       where $${\displaystyle D{KL}}$$ is the Kullback Leibler divergence KL Div.  

4. The Generation Process
       

5. The Goal
       The goal is to estimate the true parameters $$\theta^\ast$$ of this generative model.

6. Representing the Model
        To represent the prior $$pz$$ we choose it to be simple usually Gaussian  
         To represent the conditional which is very complex we use a neural network  

7. Intractability
       The Data Likelihood  
       $$p\thetax = \int p\thetaz p\thetax|z dz$$
       is intractable to compute for every $$z$$.  
       Thus the Posterior Density  
       $$p\thetaz|x = \dfrac{p\thetax|z p\thetaz}{p\thetax} = \dfrac{p\thetax|z p\thetaz}{\int p\thetaz p\thetax|z dz}$$ 
       is also intractable

8. Dealing with Intractability
       In addition to decoder network modeling $$p\thetax\vert z$$ define additional encoder network $$q\phiz\vert x$$ that approximates $$p\thetaz\vert x$$
       This allows us to derive a lower bound on the data likelihood that is tractable which we can optimize.  

9. The Model
        The Encoder recognition/inference and Decoder generation networks are probabilistic and output means and variances of each the conditionals respectively  
         The generation forward pass is done via sampling as follows  

10.The Log Likelihood of Data
        Deriving the Log Likelihood  

11.Training the Model
       

12.Pros Cons and Research
        Pros 
             Principled approach to generative models
             Allows inference of $$qz\vert x$$ can be useful feature representation for other tasks  
        Cons 
             Maximizing the lower bound of likelihood is okay but not as good for evaluation as Auto regressive models
             Samples blurrier and lower quality compared to state of the art GANs
        Active areas of research   
             More flexible approximations e.g. richer approximate posterior instead of diagonal Gaussian
             Incorporating structure in latent variables



Generative Adversarial Networks GANs

0. Auto Regressive Models VS Variational Auto Encoders VS GANs
       Auto Regressive Models defined a tractable discrete density function and then optimized the likelihood of training data   
       $$p\thetax = px0 \prod1^n pxi | x{i<}$$  
       While VAEs defined an intractable continuous density function with latent variable $$z$$  
       $$p\thetax = \int p\thetaz p\thetax|z dz$$
       but cannot optimize directly; instead derive and optimize a lower bound on likelihood instead.  
       On the other hand GANs rejects explicitly defining a probability density function in favor of only being able to sample.     

1. Generative Adversarial Networks
       are a class of AI algorithms used in unsupervised machine learning implemented by a system of two neural networks contesting with each other in a zero sum game framework.

2. Motivation
        Problem we want to sample from complex high dimensional training distribution; there is no direct way of doing this.  
         Solution we sample from a simple distribution e.g. random noise and learn a transformation that maps to the training distribution by using a neural network.  
        Generative VS Discriminative discriminative models had much more success because deep generative models suffered due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies and due to difficulty of leveraging the benefits of piecewise linear units in the generative context.  
        GANs propose a new framework for generative model estimation that sidesteps these difficulties.      

3. Structure
        Goal estimating generative models that capture the training data distribution  
         Framework an adversarial process in which two models are simultaneously trained a generative model $$G$$ that captures the data distribution and a discriminative model $$D$$ that estimates the probability that a sample came from the training data rather than $$G$$.  
         Training  
             $$G$$ maximizes the probability of $$D$$ making a mistake       



 CNNs  Convolutional Neural Networks


Introduction

1. CNNs
       In machine learning a convolutional neural network CNN or ConvNet is a class of deep feed forward artificial neural networks that has successfully been applied to analyzing visual imagery.

2. The Big Idea
       CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing.

3. Inspiration Model
       Convolutional networks were inspired by biological processes in which the connectivity pattern between neurons is inspired by the organization of the animal visual cortex.  
    Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.

4. Design
       A CNN consists of an input and an output layer as well as multiple hidden layers.  
        The hidden layers of a CNN typically consist of convolutional layers pooling layers fully connected layers and normalization layers.


Architecture and Design


1. Volumes of Neurons
       Unlike neurons in traditional Feed Forward networks the layers of a ConvNet have neurons arranged in 3 dimensions width height depth.  
    Note Depth here refers to the  dimension of an activation volume not to the depth of a full Neural Network which can refer to the total number of layers in a network.  

2. Connectivity
       The neurons in a layer will only be connected to a small region of the layer before it instead of all of the neurons in a fully connected manner.

3. Functionality
       A ConvNet is made up of Layers. 
    Every Layer has a simple API It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.  

    
    
4. Layers
       We use three main types of layers to build ConvNet architectures 
        Convolutional Layer  
         Pooling Layer  
         Fully Connected Layer

41.Process
       ConvNets transform the original image layer by layer from the original pixel values to the final class scores. 

5. Example Architecture CIFAR 10
       Model INPUT  CONV  RELU  POOL  FC
        INPUT 32x32x3 will hold the raw pixel values of the image in this case an image of width 32 height 32 and with three color channels RGB.   
         CONV Layer will compute the output of neurons that are connected to local regions in the input each computing a dot product between their weights and a small region they are connected to in the input volume.    
        This may result in volume such as $$32\times32\times12$$ if we decided to use 12 filters.  
         RELU Layer  will apply an element wise activation function thresholding at zero. This leaves the size of the volume unchanged $$32\times32\times12$$.  
         POOL Layer will perform a down sampling operation along the spatial dimensions width height resulting in volume such as $$16\times16\times12$$.  
         Fully Connected will compute the class scores resulting in volume of size $$1\times1\times10$$ where each of the 10 numbers correspond to a class score such as among the 10 categories of CIFAR 10.  
        As with ordinary Neural Networks and as the name implies each neuron in this layer will be connected to all the numbers in the previous volume.

6. Fixed Functions VS Hyper Parameters
       Some layers contain parameters and other don’t.
        CONV/FC layers perform transformations that are a function of not only the activations in the input volume but also of the parameters the weights and biases of the neurons.
        RELU/POOL layers will implement a fixed function. 
    The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image.  


     A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume e.g. holding the class scores  
     There are a few distinct types of Layers e.g. CONV/FC/RELU/POOL are by far the most popular  
     Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function  
     Each Layer may or may not have parameters e.g. CONV/FC do RELU/POOL don’t  
     Each Layer may or may not have additional hyperparameters e.g. CONV/FC/POOL do RELU doesn’t  





Convolutional Layers

1. Convolutions
       A Convolution is a mathematical operation on two functions f and g to produce a  function that is typically viewed as a modified version of one of the original functions giving the integral of the point wise multiplication of the two functions as a function of the amount that one of the original functions is translated.
       The convolution of the continous functions f and g  
       $${\displaystyle {\begin{aligned}fgt&\{\stackrel {\mathrm {def} }{=}}\ \int { \infty }^{\infty }f\tau gt \tau \d\tau &=\int { \infty }^{\infty }ft \tau g\tau \d\tau .\end{aligned}}}$$
       The convolution of the discreet functions f and g 
       $${\displaystyle {\begin{aligned}fgn&=\sum {m= \infty }^{\infty }fmgn m&=\sum {m= \infty }^{\infty }fn mgm.\end{aligned}}} commutativity$$

2. Cross Correlation
       Cross Correlation is a measure of similarity of two series as a function of the displacement of one relative to the other.
       The continuous cross correlation on continuous functions f and g  
       $$f\star g\tau \ {\stackrel {\mathrm {def} }{=}}\int { \infty }^{\infty }f^{}t\ gt+\tau \dt$$
       The discrete cross correlation on discreet functions f and g  
       $$f\star gn\ {\stackrel {\mathrm {def} }{=}}\sum {m= \infty }^{\infty }f^{}m\ gm+n.$$

3. Convolutions and Cross Correlation
        Convolution is similar to cross correlation.  
         For discrete real valued signals they differ only in a time reversal in one of the signals.  
         For continuous signals the cross correlation operator is the adjoint operator of the convolution operator.

4. CNNs Convolutions and Cross Correlation
       The term Convolution in the name "Convolution Neural Network" is unfortunately a misnomer.  
        CNNs actually use Cross Correlation instead as their similarity operator.  
        The term 'convolution' has stuck in the name by convention.

5. The Mathematics
        The CONV layer’s parameters consist of a set of learnable filters.  
             Every filter is small spatially along width and height but extends through the full depth of the input volume.  
             For example a typical filter on a  layer of a ConvNet might have size 5x5x3 i.e. 5 pixels width and height and 3 because images have depth 3 the color channels.  
         In the forward pass we slide convolve each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position.  
             As we slide the filter over the width and height of the input volume we will produce a 2 dimensional activation map that gives the responses of that filter at every spatial position.  
            Intuitively the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the  layer or eventually entire honeycomb or wheel like patterns on higher layers of the network. 
             Now we will have an entire set of filters in each CONV layer e.g. 12 filters and each of them will produce a separate 2 dimensional activation map.   
         We will stack these activation maps along the depth dimension and produce the output volume.  
    As a result the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.     
    
6. The Brain Perspective
       Every entry in the 3D output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially.  

7. Local Connectivity
        Convolutional networks exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers 
             Each neuron is connected to only a small region of the input volume.
         The Receptive Field of the neuron defines the extent of this connectivity as a hyperparameter.  
         For example suppose the input volume has size $$32x32x3$$ and the receptive field or the filter size is $$5x5$$ then each neuron in the Conv Layer will have weights to a $$5x5x3$$ region in the input volume for a total of $$553 = 75$$ weights and $$+1$$ bias parameter.  
    Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.

8. Spatial Arrangement
       There are three hyperparameters control the size of the output volume  
           1. The Depth of the output volume is a hyperparameter that corresponds to the number of filters we would like to use each learning to look for something different in the input.  
            2. The Stride controls how depth columns around the spatial dimensions width and height are allocated.  
                e.g. When the stride is 1 then we move the filters one pixel at a time.  

                The Smaller the stride the more overlapping regions exist and the bigger the volume.  
                The bigger the stride the less overlapping regions exist and the smaller the volume.  
            3. The Padding is a hyperparameter whereby we pad the input the input volume with zeros around the border.  
                This allows to control the spatial size of the output volumes.  


9. The Spatial Size of the Output Volume
       We compute the spatial size of the output volume as a function of  
            $$W$$ The input volume size.  
             $$F$$ $$\\$$The receptive field size of the Conv Layer neurons.  
             $$S$$ The stride with which they are applied.  
             $$P$$ The amount of zero padding used on the border.  
       Thus the Total Size of the Output  
       $$\dfrac{W−F+2P}{S} + 1$$  
        Potential Issue If this number is not an integer then the strides are set incorrectly and the neurons cannot be tiled to fit across the input volume in a symmetric way.  
        Fix In general setting zero padding to be $${\displaystyle P = \dfrac{K 1}{2}}$$ when the stride is $${\displaystyle S = 1}$$ ensures that the input volume and output volume will have the same size spatially.  


0. The Convolution Layer
       



Layers

1. Convolution Layer
       One image becomes a stack of filtered images.


Distinguishing features


2. Image Features
       are certain quantities that are calculated from the image to better describe the information in the image and to reduce the size of the input vectors. 
        Examples  
             Color Histogram Compute a bucket based vector of colors with their respective amounts in the image.  
             Histogram of Oriented Gradients HOG we count the occurrences of gradient orientation in localized portions of the image.   
             Bag of Words a bag of visual words is a vector of occurrence counts of a vocabulary of local image features.  
                The visual words can be extracted using a clustering algorithm; K Means.  



 Deep Dream  Visualizing Features in ConvNets


Deep Dream Deep Feature Visualization

0. DeepDream
       DeepDream is a computer vision program created by Google which uses a ConvNet to find and enhance patterns in images via algorithmic pareidolia thus creating a dream like hallucinogenic appearance in the image.  

1. The General Idea
       Rather than synthesizing an image to maximize a specific neuron instead try to amplify the neuron activations at some layer in the network

2. The Process
       1. Start with an input image
        2. Run it through a ConvNet up to some layer
        3. Forward compute activations at chosen layer
        4. Set gradient of chosen layer equal to its activation
        5. Backward Compute gradient on image
        6. Update Image 
        7. Repeat


Feature Inversion

       

       

       

       

       

       

       

       




       

       

       

       

       

       

       

       


FOURTH

       

       







 Recurrent Neural Networks  Applications in Computer Vision


RNNs

### Refer to this section on RNNsahmedbadary.ml/workfiles/research/dl/nlp/rnns

1. Process Sequences
        One to One  
         One to Many 
            Image Captioning image  > seq of words
         Many to One 
            Sentiment Classification seq of words  > Sentiment
         Many to Many   
            Machine Translation seq of words  > seq of words
         Discrete Many to Many  
            Frame Level Video Classification seq. of frames  > seq of classes per frame  


2. RNN Structure
       We can process a sequence of vectors $$\vec{x}$$ by applying a recurrence formula at every time step  
       $$ht = fWh{t 1} xt$$
       where $$ht$$ is the new state $$fW$$ is some function with weights $$W$$ $$h{t 1}$$ is the old state and $$xt$$ is the input vector at some time step $$t$$.   
        The same function and set of parameters weights are used at every time step.  

3. A Vanilla Architecture of an RNN
       $$
        \begin{align}
        ht &= fWh{t 1} xt
        ht &= tanhW{hh}ht{t 1} + W{xh}xt  
        yt &= W{hy}ht
        \end{align}
        $$

4. The RNN Computational Graph
        Many to Many       
        One to Many  
        Seq to Seq   

5. Example Architecture Character Level Language Model

6. The Functional Form of a Vanilla RNN Gradient Flow


Applications in CV

### Coming Soon!


Implementations and Training LSTMs and GRUs

### Coming Soon!


 Image Segmentation  with Deep Learning


Semantic Segmentation

1. Semantic Segmentation
       Semantic Segmentation is the task of understanding an image at the pixel level. It seeks to assign an object class to each pixel in the image.  


2. The Structure
        Input Image  
         Output A class for each pixel in the image.  

3. Properties
       In Semantic Segmentation we don't differentiate among the instances instead we only care about the pixels.


Approaches The Pre DeepLearning Era

1. Semantic Texton Forests
       This approach consists of ensembles of decision trees that act directly on image pixels.  
       Semantic Texton Forests STFs 
are randomized decision forests that use only simple pixel comparisons on local image patches performing both an
implicit hierarchical clustering into semantic textons and an explicit local classification of the patch category.  
       STFs allow us to build powerful texton codebooks without computing expensive filter banks or descriptors and without performing costly k means clustering and nearest neighbor assignment.
       Semantic Texton Forests for Image Categorization and Segmentation Shawton et al. 2008

2. Random Forest based Classifiers
       Random Forests have also been used to perform semantic segmentation for a variety of tasks.

3. Conditional Random Fields
       CRFs provide a probabilistic framework for labeling and segmenting structured data.  
       They try to model the relationship between pixels e.g.
        1. nearby pixels more likely to have same label
        2. pixels with similar color more likely to have same label
        3. the pixels above the pixels "chair" more likely to be "person" instead of "plane"
        4. refine results by iterations
       W. Wu A. Y. C. Chen L. Zhao and J. J. Corso 2014 "Brain Tumor detection and segmentation in a CRF framework with pixel pairwise affinity and super pixel level features"
       Plath et al. 2009 "Multi class image segmentation using conditional random fields and global classification"

4. SuperPixel Segmentation
       The concept of superpixels was  introduced by Xiaofeng Ren and Jitendra Malik in 2003.  
       Superpixel is a group of connected pixels with similar colors or gray levels.  
        They produce an image patch which is better aligned with intensity edges than a rectangular patch.  
       Superpixel segmentation is the idea of dividing an image into hundreds of non overlapping superpixels.  
        Then these can be fed into a segmentation algorithm such as Conditional Random Fields or Graph Cuts for the purpose of segmentation.  
       Efficient graph based image segmentation Felzenszwalb P.F. and Huttenlocher D.P. International Journal of Computer Vision 2004
       Quick shift and kernel methods for mode seeking Vedaldi A. and Soatto S. European Conference on Computer Vision 2008
       Peer Neubert & Peter Protzel 2014. Compact Watershed and Preemptive  


Approaches The Deep Learning Era

1. The Sliding Window Approach
       We utilize classification for segmentation purposes.  
        Algorithm    
             We break up the input image into tiny "crops" of the input image.  
             Use Classification to find the class of the center pixel of the crop.  
                Using the same machinery for classification.
       Basically we do classification on each crop of the image.
        DrawBacks  
             Very Inefficient and Expensive  
                To label every pixel in the image we need a separate "crop" for each pixel in the image which would be quite a huge number.  
             Disregarding Localized Information  
                This approach does not make use of the shared features between overlapping patches in the image.  
                Further it does not make use of the spatial information between the pixels.  
       Farabet et al “Learning Hierarchical Features for Scene Labeling” TPAMI 2013
       Pinheiro and Collobert “Recurrent Convolutional Neural Networks for Scene Labeling” ICML 2014

2. Fully Convolutional Networks
       We make use of convolutional networks by themselves trained end to end pixels to pixels.  
        Structure  
             Input Image vector  
             Output A Tensor $$C \times H \times W$$ where $$C$$ is the number of classes.  
       The key observation is that one can view Fully Connected Layers as Convolutions over the entire image.  
        Thus the structure of the ConvNet is just a stacked number of convolutional layers that preserve the size of the image.  
         Issue with the Architecture   
            The proposed approach of preserving the size of the input image leads to an exploding number of hyperparamters.  
            This makes training the network very tedious and it almost never converges.  
         Solution  
            We allow the network to perform an encoding of the image by   
             Downsampling the image  
            then Upsampling the image back inside the network.  
            The Upsampling is not done via bicubic interpolation instead we use Deconvolutional layers Unpooling for learning the upsampling.   
            However even learnableupsampling produces coarse segmentation maps because of loss of information during pooling. Therefore shortcut/skip connections are introduced from higher resolution feature maps. 
       Long et. al 2014  


Methods Approaches and Algorithms in Training DL Models

1. Upsampling
       Also known as "Unpooling".  
        Nearest Neighbor fill each region with the corresponding pixel value in the original image.  
        Bed of Nails put each corresponding pixel value in the original image into the upper left corner in each new sub region and fill the rest with zeros.   
        Max Unpooling The same idea as Bed of Nails however we re place the pixel values from the original image into their original values that they were extracted from in the Max Pooling step.  

2. Learnable Upsampling Deconvolutional Layers Transpose Convolution
        Transpose Convolution is a convolution performed on a an input of a small size each element in the input acts a scalar that gets multiplied by the filter and then gets placed on a larger output matrix where the regions of overlap get summed.   
       Also known as  
         Deconvolution
         UpConvolution
         Fractionally Strided Convolution  
            Reason if you think of the stride as the ratio in step between the input and the output; this is equivalent to a stride one half convolution because of the ratio of 1 to 2 between the input and the output.  
         Backward Strided Convolution
            Reason The forward pass of a Transpose Convolution is the same mathematical operation as the backward pass of a normal convolution.   
        1 D Example  
        Convolution as Tensor Multiplication All Convolutions with stride and padding can be framed as a Tensor Product by placing the filters intelligently in a tensor.  
            The name Transpose Convolution comes from the fact that the Deconvolution operation viewed as a Tensor Product is just the Transpose of the Convolution operation.  
             1 D Example   
             In fact the name Deconvolution is a mis nomer exactly because of this interpretation  
                The Transpose matrix of the Convolution operation is a convolution iff the stride is equal to 1.  
                If the stride>1 then the transpose matrix no longer represents a convolution.  
        Issues with Transpose Convolution    
             Since we sum the values that overlap in the region of the upsampled image the magnitudes in the output will vary depending on the number of receptive fields in the output.  
                This leads to some checkerboard artifacts.  
         Solution 
             Avoid 3x3 stride two deconvolutions.  
             Use 4x4 stride two or 2x2 stride two deconvolutions. 


 Articulated Body Pose Estimation  Human Pose Estimation


Introduction

1. Human Articulated Body Pose Estimation
       Human Pose Estimation is the process of estimating the configuration of the body pose from a single typically monocular image. 
       In computer vision Body Pose Estimation is the study of algorithms and systems that recover the pose of an articulated body which consists of joints and rigid parts using image based observations.

2. Difficulties in Pose Estimation
       Pose estimation is hard due to many reasons including  
         High Degree of Freedom DOF 244 DOF  
         Variability of human visual appearance
         Variability in lighting conditions 
         Variability in human physique
         partial Occlusions
         Complexity of the human physical structure
         high dimensionality of the pose 
         loss of 3d information that results from observing the pose from 2d planar image projections 
         variability in Clothes  

3. Theory
       Human pose estimation is usually formulated Probabilistically to account for the variability and ambiguities that exist in the inference.  
       In Probabilistic approaches we are interested in estimating the posterior distribution $$p\mathbf{x}\vert \mathbf{z}$$ where $$\mathbf{x}$$ is the pose of the body and and $$\mathbf{z}$$ is a feature set derived from the image.  
        The Key Modeling choices that affect the inference are   
             The representation of the pose   $$\mathbf{x}$$
             The nature and encoding of image features   $$\mathbf{z}$$
             The inference framework required to estimate the posterior   $$p\mathbf{x}\vert \mathbf{z}$$


4. Model based Approaches
       The typical body pose estimation system involves a model based approach in which the pose estimation is achieved by maximizing/minimizing a similarity/dissimilarity between an observation input and a template model.   
       Different kinds of sensors have been explored for use in making the observation.  
         Sensors   
             Visible wavelength imagery
             Long wave thermal infrared imagery
             Time of flight imagery
             Laser range scanner imagery
       These sensors produce intermediate representations that are directly used by the model.
         Representations 
             Image appearance
             Voxel volume element reconstruction
             3D point clouds and sum of Gaussian kernels
             3D surface meshes.

5. The Representation
       A Representation is a model to depict the configuration of the human body.  
        The configuration of the human body can be represented in a variety of ways.  
       There are two common representations used for the human body  
         Kinematic Skeleton Tree  
         Part Models  

6. Kinematic Skeleton Tree with Quaternions
       The most direct and common representation is obtained by parameterizing the body as a kinematic tree $$\vec{x} = \{\tau \theta\tau \theta1 \theta2 \ldots \thetaN\}$$ where the pose is encoded using position of the root segment the pelvis is typically used as root to minimize the height of the kinematic tree $$\tau$$ orientation of the root segment in the world $$\theta\tau$$ and a set of relative joint angels $$\{\thetai\}{i=1}^N$$ that represent the orientation of the body parts with respect to their parents along the tree.  
        e.g. the orientation of the thigh with respect to the pelvis shin with respect to the thigh etc.  
       The kinematic skeleton is constructed by a tree structured chain where each rigid body segment has its local coordinate system that can be transformed to the world coordinate system via a 4×4 transformation matrix $${\displaystyle T{l}}$$ 
       $${\displaystyle T{l}=T{\operatorname {par} l}R{l}}$$
       where $${\displaystyle R{l}}$$ denotes the local transformation from body segment $${\displaystyle S{l}}$$ to its parent $${\displaystyle \operatorname {par} S{l}}$$.  
       Kinematic tree representation can be obtained for 2d 2.5d and 3d body models.  
         2 D   
             $$\tau \in \mathcal{R}^2$$ 
             $$\theta\tau \in \mathcal{R}^1$$
             $$\thetai \in \mathcal{R}^1$$   corresponds to pose of the cardboard person in the image plane  
         3 D   
             $$\tau \in \mathcal{R}^3$$ 
             $$\theta\tau \in SO3$$
             $$\thetai \in SO3$$ for spherical joints e.g. neck  
              $$\thetai \in \mathcal{R}^2$$ for saddle joints e.g. wrist  
              $$\thetai \in \mathbb{R}^1$$ for hinge joints e.g. knee    
         2.5 D are extensions of the 2 D representations where the pose $$\mathbf{x}$$ is augmented with discrete  variables encoding the relative depth layering of body parts with respect to one another in the 2 d cardboard model.  
            This representation is not very common.  
       Each joint in the body has 3 degrees of freedom DoF rotation. Given a transformation matrix $${\displaystyle T{l}}$$ the joint position at the T pose can be transferred to its corresponding position in the world coordination.  
        The 3 D joint rotation is usually expressed as a normalized quaternion $$x y z w$$ due to its continuity that can facilitate gradient based optimization in the parameters estimation.  
       In all dimensionality cases kinematic tree representation results in a high dimensional pose vector $$\mathbf{x}$$ in $$\mathbb{R}^{30}  \mathbb{R}^{70}$$ depending on the fidelity and exact parameterization of the skeleton and joints.  
       Another parameterization uses the 2d or 3d locations of the major joints in the world.   
        However this parametrization is not invariant to the morphology body segment lengths of a given individual.   

7. Part based Models
       The body is modeled as a set of parts $$\mathbf{x} = \{\mathbf{x}1 \mathbf{x}2 \ldots \mathbf{x}M\}$$ each with its own position and orientation in space $$\mathbf{x}i = \{\taui \thetai\}$$ that are connected by a set of statistical or physical constraints that enforce skeletal and sometimes image consistency.  
       The part model is motivated by the human skeleton since any object having the property of articulation can be broken down into smaller parts wherein each part can take different orientations resulting in different articulations of the same object.   
    Different scales and orientations of the main object can be articulated to scales and orientations of the corresponding parts.
       Mathematically the parts are connected by springs; the model is also known as a spring model.  
        The degree of closeness between each part is accounted for by the compression and expansion of the springs. There is geometric constraint on the orientation of springs. For example limbs of legs cannot move 360 degrees. Hence parts cannot have that extreme orientation.  This reduces the possible permutations.  
       The model can be formulated in 2 D or in 3 D.  
        The 2 D parameterizations are much more common.  
        In 2 D each part’s representation is often augmented with an additional variable $$si$$ that accounts for uniform scaling of the body part in the image i.e. $$\mathbf{x}i = \{\taui \thetai si\}$$ with $$\taui \in \mathbb{R}^2 \thetai \in \mathbb{R}^1$$ and $$si \in \mathbb{R}^1$$.  
       The model results in very high dimensional vectors even higher than that of kinematic trees.  

8. Applications
        Markerless motion capture for human computer interfaces
         Physiotherapy 
         3D animation 
         Ergonomics studies 
         Robot control  and
         Visual surveillance
         Human robot interaction
         Gaming
         Sports performance analysis


9. Image Features
       In many of the classical approaches image features that represent the salient parts of the image with respect to the human pose play a huge rule in the performance of any pose estimation approach.   
        The most common features 
             Silhouettes for effectively separating the person from background in static scenes  
             Color for modeling un occluded skin or clothing
             Edges for modeling external and internal contours of the body    
             Gradients for modeling the texture over the body parts  
        Other less common features include Shading and Focus.  
       To reduce dimensionality and increase robustness to noise these raw features are often encapsulated in image descriptors such as shape context SIFT and histogram of oriented gradients HoG.  
        Alternatively hierarchical multi level image encodings can be used such as HMAX spatial pyramids and vocabulary trees.   

DeepPose 



1. Main Idea
       Pose Estimation is formulated as a DNN based regression problem towards body joints.  
        The DNN regressors are presented as a cascade for higher precision in pose estimates.    

2. Structure
        Input 
             Full Image
             7 layered generic Convolutional DNN    
        Each Joint Regressor uses the full image as a signal.   


3. Key Insights
        Replace the explicitly designed feature representations and detectors for the parts the model topology and the interactions between joints by a learned representation through a ConvNet  
         The DNN based Pose Predictors are presented as a cascade to increase the precision of joint localization  
         Although the regression loss does not model explicit interactions between joints such are implicitly captured by all of the 7 hidden layers   all the internal features are shared by all joint regressors

4. Method
        Start with an initial pose estimation based on the full image
         Learn DNN based regressors which refine the joint predictions by using higher resolution sub images


5. Notation
        Pose Vector = $$\mathbf{y} = \left\ldots \mathbf{y}i^T \ldots\right^T \ i \in \{1 \ldots k\}$$  
         Joint Co ordinates = $$\mathbf{y}i^T = xi yi$$ of the $$i$$ th joint  
         Labeled Image = $$x \mathbf{y}$$  
             $$x = $$ Image Data  
             $$\mathbf{y} = $$ Ground Truth Pose Vector
         Bounding Box = $$b$$ a box bounding the human body or parts of it   
         Normalization Function $$= N\mathbf{y}i; b$$ normalizes the joint coordinates w.r.t a bounding box $$b$$  
            Since the joint coordinates are in absolute image coordinates and poses vary in size from image to image   

             Translate by box center
             Scale by box size  
         Normalized pose vector = $$N\mathbf{y}; b = \left\ldots N\mathbf{y}i; b^T \ldots\right^T$$  
         A crop of image $$x$$ by bounding box $$b$$ = $$Nx; b$$
         Learned Function = $$\psix;\theta \in \mathbb{R}^2k$$ is a functions that regresses to normalized pose vector given an image  
             Input image $$x$$
             Output Normalized pose vector $$N\mathbf{y}$$                 
         Pose Prediction   
           $$y^\ast = N^{ 1}\psiNx;\theta$$   

7. Architecture
        Problem Regression Problem
         Goal Learn a function $$\psix;\theta$$ that is trained and used to regress to a pose vector.   
         Estimation $$\psi$$ is based on learned through Deep Neural Net
         Deep Neural Net is a Convolutional Neural Network; namely AlexNet  
             Input image with pre defined size $$ = \$$ # pixels $$\times 3$$ color channels  
                $$220 \times 220$$ with a stride of $$4$$  
             Output target value of the regression$$ = 2k$$ joint coordinates  
    Denote by $$\mathbf{C}$$ a convolutional layer by $$\mathbf{LRN}$$ a local response normalization layer $$\mathbf{P}$$ a pooling layer and by $$\mathbf{F}$$ a fully connected layer  
    For $$\mathbf{C}$$ layers the size is defined as width $$\times$$ height $$\times$$ depth where the  two dimensions have a spatial meaning while the depth defines the number of filters.  
        Alex Net 
             Architecture     $$\mathbf{C}55 \times 55 \times 96 − \mathbf{LRN} − \mathbf{P} − \mathbf{C}27 \times 27 \times 256 − \mathbf{LRN} − \mathbf{P} − \mathbf{C}13 \times 13 \times 384 − \mathbf{C}13 \times 13 \times 384 − \mathbf{C}13 \times 13 \times 256 − \mathbf{P} − \mathbf{F}4096 − \mathbf{F}4096$$   
             Filters  
                 $$\mathbf{C}{1} = 11 \times 11$$  
                 $$\mathbf{C}{2} = 5 \times 5$$  
                 $$\mathbf{C}{3 5} = 3 \times 3$$.
             Total Number of Parameters $$ = 40$$M   
             Training Dataset  
                Denote by $$D$$ the training set and $$DN$$ the normalized training set   
                $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  $$DN = \{NxN\mathbf{y}\vert x\mathbf{y} \in D\}$$   
             Loss the Loss is modified; instead of a classification loss we train a linear regression on top of the last network layer to predict a pose vector by minimizing $$L2$$ distance between the prediction and the true pose vector  
       $$\arg \min\theta \sum{xy \in DN} \sum{i=1}^k \|\mathbf{y}i  \psiix;\theta\|2^2$$  
        Optimization  
             BackPropagation in a distributed online implementation
             Adaptive Gradient Updates
             Learning Rate $$ = 0.0005 = 5\times 10^{ 4}$$
             Data Augmentation randomly translated image crops left/right flips
             DropOut Regularization for the $$\mathbf{F}$$ layers $$ = 0.6$$

9. Architecture
        Motivation   
            Although the pose formulation of the DNN has the advantage that the joint estimation is based on the full image and thus relies on context due its fixed input size of $$220 \times 220$$ the network has limited capacity to look at detail  it learns filters capturing pose properties at coarse scale.  
            The pose properties are necessary to estimate rough pose but insufficient to always precisely localize the body joints.  
            Increasing the input size is infeasible since it will increase the already large number of parameters.  
            Thus a cascade of pose regressors is used to achieve better precision.  
         Structure and Training   
            At the  stage 
             The cascade starts off by estimating an initial pose as outlined in the previous section.  
            At subsequent stages  
             Additional DNN regressors are trained to predict a displacement of the joint locations from previous stage to the true location.  
                Thus each subsequent stage can be thought of as a refinement of the currently predicted pose.   
             Each subsequent stage uses the predicted joint locations to focus on the relevant parts of the image   subimages are cropped around the predicted joint location from previous stage and the pose displacement regressor for this joint is applied on this sub image.  
                Thus subsequent pose regressors see higher resolution images and thus learn features for finer scales which ultimately leads to higher precision  
         Method and Architecture  
             The same network architecture is used for all stages of the cascade but learn different parameters.   
             Start with a bounding box $$b^0$$ which either encloses the full image or is obtained by a person detector
             Obtain an initial pose  
                Stage 1 $$\mathbf{y}^1 \leftarrow N^{ 1}\psiNx;b^0;\theta1;b^0$$  
             At stages $$s \geq 2$$ for all joints
                 Regress  towards a refinement displacement $$\mathbf{y}i^s  \mathbf{y}i^{s 1}$$ by applying a regressor on the sub image defined by $$bi^{s 1}$$ 
                 Estimate new joint boxes $$bi^s$$  
                Stage $$s$$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{y}i^s \leftarrow \mathbf{y}i^{2 1} + N^{ 1}\psiNx;b^0;\thetas;b  \\ 6  
 \ \ \ \ \ \ \ \ \ \ \ \ \ \                  \\\\ \text{for } b = bi^s 1 
  \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
                bi^s \leftarrow \mathbf{y}i^s \sigma diam\mathbf{y}^s \sigma diam\mathbf{y}^s \\ 7$$  
                where we considered a joint bounding box $$bi$$ capturing the sub image around $$\mathbf{y}i bi\mathbf{y}; \sigma = \mathbf{y}i \sigma diam\mathbf{y} \sigma diam\mathbf{y}$$ having as center the i th joint and as dimension the pose diameter scaled by $$\sigma$$ to refine a given joint location $$\mathbf{y}i$$.    
             Apply the cascade for a fixed number of stages $$ = S$$  
         Loss at each stage $$s$$   
      $$\thetas = \arg \min\theta \sum{x\mathbf{y}i \in DA^s} \|\mathbf{y}i  \psiix;\theta\|2^2 \\\\\ 8$$


6. Advantages
        The DNN is capable of capturing the full context of each body joint  
         The approach is simpler to formulate than graphical models methods  no need to explicitly design feature representations and detectors for parts or to explicitly design a model topology and interactions between joints.   
            Instead a generic ConvNet learns these representations

8. Notes
        The use of a generic DNN architecture is motivated by its outstanding results on both classification and localization problems and translates well to pose estimation  
         Such a model is a truly holistic one — the final joint location estimate is based on a complex nonlinear transformation of the full image  
         The use of a DNN obviates the need to design a domain specific pose model
         Although the regression loss does not model explicit interactions between joints such are implicitly captured by all of the 7 hidden layers   all the internal features are shared by all joint regressors  



 Generative Adversarial Networks







Generative Adversarial Networks GANs

0. Auto Regressive Models VS Variational Auto Encoders VS GANs
    Auto Regressive Models defined a tractable discrete density function and then optimized the likelihood of training data   
    $$p\thetax = px0 \prod1^n pxi | x{i<}$$    
    
    While VAEs defined an intractable continuous density function with latent variable $$z$$  
    $$p\thetax = \int p\thetaz p\thetax|z dz$$  
    but cannot optimize directly; instead derive and optimize a lower bound on likelihood instead.  
    
    On the other hand GANs rejects explicitly defining a probability density function in favor of only being able to sample.     
    

1. Generative Adversarial Networks
    GANs are a class of AI algorithms used in unsupervised machine learning implemented by a system of two neural networks contesting with each other in a zero sum game framework.  
    

2. Motivation
     Problem we want to sample from complex high dimensional training distribution; there is no direct way of doing this.  
     Solution we sample from a simple distribution e.g. random noise and learn a transformation that maps to the training distribution by using a neural network.  




     Generative VS Discriminative discriminative models had much more success because deep generative models suffered due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies and due to difficulty of leveraging the benefits of piecewise linear units in the generative context.  
        GANs propose a new framework for generative model estimation that sidesteps these difficulties.      
    

3. Structure
     Goal  
        estimating generative models that capture the training data distribution  
     Framework  
        an adversarial process in which two models are simultaneously trained a generative model $$G$$ that captures the data distribution and a discriminative model $$D$$ that estimates the probability that a sample came from the training data rather than $$G$$.  
     Training  
        $$G$$ maximizes the probability of $$D$$ making a mistake       

4. Training
    Generator network try to fool the discriminator by generating real looking images  
    Discriminator network try to distinguish between real and fake images  


     Train jointly in minimax game.  
         Minimax objective function  
            $$\min {\theta{g}} \max {\theta{d}}\left\mathbb{E}{x \sim p{\text {data }}} \log D{\theta{d}}x+\mathbb{E}{z \sim pz} \log \left1 D{\theta{d}}\leftG{\theta{g}}z\right\right\right$$  
              Discriminator outputs likelihood in $$01$$ of real image  
              $$D{\theta{d}}x$$ Discriminator output for real data $$\boldsymbol{x}$$  
              $$D{\theta{d}}\leftG{\theta{g}}z\right$$ Discriminator output for generated fake data $$Gz$$  
              Discriminator $$\left\theta{d}\right$$ wants to maximize objective such that $$\mathrm{D}\mathrm{x}$$ is close to $$1$$ real and $$\mathrm{D}\mathrm{G}\mathrm{z}$$ is close to $$0$$ fake  
              Generator $$\left\mathrm{f} {\mathrm{g}}\right$$ "wants to minimize objective such that $$\mathrm{D}\mathrm{G}\mathrm{z}$$ is close to $$1$$ discriminator is fooled into thinking generated $$\mathrm{G}\mathrm{z}$$ is real  
     Alternate between\  
        1. Gradient Ascent on Discriminator  
            $$\max {\theta{d}}\left\mathbb{E}{x \sim p{\text {data}}} \log D{\theta{d}}x+\mathbb{E}{z \sim pz} \log \left1 D{\theta{d}}\leftG{\theta{g}}z\right\right\right$$  
        2. Gradient Ascent on Generator different objective  
            $$\max {\theta{g}} \mathbb{E}{z \sim pz} \log \leftD{\theta{d}}\leftG{\theta{g}}z\right\right$$  

    GAN Training Algorithm  
    

      \# of Training steps $$\mathrm{k}$$ some find $$\mathrm{k}=1$$ more stable others use $$\mathrm{k}>1$$ no best rule.  
    
     Recent work e.g. Wasserstein GAN alleviates this problem better stability!  





    Notes  
    
     \ Instead of minimizing likelihood of discriminator being correct now maximize likelihood of discriminator being wrong. Same objective of fooling discriminator but now higher gradient signal for bad samples => works much better! Standard in practice.  
         Previously we used to do gradient descent on generator  
            $$\min {\theta{g}} \mathbb{E}{z \sim pz} \log \left1 D{\theta{d}}\leftG{\theta{g}}z\right\right$$  
            In practice optimizing this generator objective does not work well.  
         Now we are doing gradient ascent on the generator  
            $$\max {\theta{g}} \mathbb{E}{z \sim pz} \log \leftD{\theta{d}}\leftG{\theta{g}}z\right\right$$  
     Jointly training two networks is challenging can be unstable. Choosing objectives with better loss landscapes helps training is an active area of research.  
     The representations have nice structure  
         Average $$\boldsymbol{z}$$ vectors do arithmetic  

            

         Interpolating between random points in latent space is possible  
    

5. Generative Adversarial Nets Convolutional Architectures
      Discriminator is a standard convolutional network.  
      Generator is an upsampling network with fractionally strided convolutions.  


    Architecture guidelines for stable Deep Convolutional GANs  
    
     Replace any pooling layers with strided convolutions discriminator and fractional strided convolutions generator.
     Use batchnorm in both the generator and the discriminator.
     Remove fully connected hidden layers for deeper architectures.
     Use ReLU activation in generator for all layers except for the output which uses Tanh.
     Use LeakyReLU activation in the discriminator for all layers.  
    

6. Pros Cons and Research
     Pros  
         Beautiful state of the art samples!  
     Cons  
         Trickier / more unstable to train
         Can’t solve inference queries such as $$px pz\vert x$$  
     Active areas of research  
         Better loss functions more stable training Wasserstein GAN LSGAN many others
         Conditional GANs GANs for all kinds of applications


Notes  





 CNNs  Convolutional Neural Networks


Introduction

1. CNNs
       In machine learning a convolutional neural network CNN or ConvNet is a class of deep feed forward artificial neural networks that has successfully been applied to analyzing visual imagery.

2. The Big Idea
       CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing.

3. Inspiration Model
       Convolutional networks were inspired by biological processes in which the connectivity pattern between neurons is inspired by the organization of the animal visual cortex.  
    Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.

4. Design
       A CNN consists of an input and an output layer as well as multiple hidden layers.  
        The hidden layers of a CNN typically consist of convolutional layers pooling layers fully connected layers and normalization layers.


Architecture and Design


1. Volumes of Neurons
       Unlike neurons in traditional Feed Forward networks the layers of a ConvNet have neurons arranged in 3 dimensions width height depth.  
    Note Depth here refers to the  dimension of an activation volume not to the depth of a full Neural Network which can refer to the total number of layers in a network.  

2. Connectivity
       The neurons in a layer will only be connected to a small region of the layer before it instead of all of the neurons in a fully connected manner.

3. Functionality
       A ConvNet is made up of Layers. 
    Every Layer has a simple API It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.

    
    
4. Layers
       We use three main types of layers to build ConvNet architectures 
        Convolutional Layer  
         Pooling Layer  
         Fully Connected Layer

41.Process
       ConvNets transform the original image layer by layer from the original pixel values to the final class scores. 

5. Example Architecture CIFAR 10
       Model INPUT  CONV  RELU  POOL  FC
        INPUT 32x32x3 will hold the raw pixel values of the image in this case an image of width 32 height 32 and with three color channels RGB.   
         CONV Layer will compute the output of neurons that are connected to local regions in the input each computing a dot product between their weights and a small region they are connected to in the input volume.    
        This may result in volume such as 32x32x12 if we decided to use 12 filters.  
         RELU Layer  will apply an elementwise activation function thresholding at zero. This leaves the size of the volume unchanged 32x32x12.  
         POOL Layer will perform a downsampling operation along the spatial dimensions width height resulting in volume such as 16x16x12.  
         Fully Connected will compute the class scores resulting in volume of size 1x1x10 where each of the 10 numbers correspond to a class score such as among the 10 categories of CIFAR 10.  
        As with ordinary Neural Networks and as the name implies each neuron in this layer will be connected to all the numbers in the previous volume.

6. Fixed Functions VS Hyper Parameters
       Some layers contain parameters and other don’t.
        CONV/FC layers perform transformations that are a function of not only the activations in the input volume but also of the parameters the weights and biases of the neurons.
        RELU/POOL layers will implement a fixed function. 
    The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image.  


     A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume e.g. holding the class scores  
     There are a few distinct types of Layers e.g. CONV/FC/RELU/POOL are by far the most popular  
     Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function  
     Each Layer may or may not have parameters e.g. CONV/FC do RELU/POOL don’t  
     Each Layer may or may not have additional hyperparameters e.g. CONV/FC/POOL do RELU doesn’t  





Convolutional Layers

1. Convolutions
       A Convolution is a mathematical operation on two functions f and g to produce a  function that is typically viewed as a modified version of one of the original functions giving the integral of the pointwise multiplication of the two functions as a function of the amount that one of the original functions is translated.
       The convolution of the continous functions f and g  
       $${\displaystyle {\begin{aligned}fgt&\{\stackrel {\mathrm {def} }{=}}\ \int { \infty }^{\infty }f\tau gt \tau \d\tau &=\int { \infty }^{\infty }ft \tau g\tau \d\tau .\end{aligned}}}$$
       The convolution of the discreet functions f and g 
       $${\displaystyle {\begin{aligned}fgn&=\sum {m= \infty }^{\infty }fmgn m&=\sum {m= \infty }^{\infty }fn mgm.\end{aligned}}} commutativity$$

2. Cross Correlation
       Cross Correlation is a measure of similarity of two series as a function of the displacement of one relative to the other.
       The continuous cross correlation on continous functions f and g  
       $$f\star g\tau \ {\stackrel {\mathrm {def} }{=}}\int { \infty }^{\infty }f^{}t\ gt+\tau \dt$$
       The discrete cross correlation on discreet functions f and g  
       $$f\star gn\ {\stackrel {\mathrm {def} }{=}}\sum {m= \infty }^{\infty }f^{}m\ gm+n.$$

3. Convolutions and Cross Correlation
        Convolution is similar to cross correlation.  
         For discrete real valued signals they differ only in a time reversal in one of the signals.  
         For continuous signals the cross correlation operator is the adjoint operator of the convolution operator.

4. CNNs Convolutions and Cross Correlation
       The term Convolution in the name "Convolution Neural Network" is unfortunately a misnomer.  
        CNNs actually use Cross Correlation instead as their similarity operator.  
        The term 'convolution' has stuck in the name by convention.

5. The Mathematics
        The CONV layer’s parameters consist of a set of learnable filters.  
             Every filter is small spatially along width and height but extends through the full depth of the input volume.  
             For example a typical filter on a  layer of a ConvNet might have size 5x5x3 i.e. 5 pixels width and height and 3 because images have depth 3 the color channels.  
         In the forward pass we slide convolve each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position.  
             As we slide the filter over the width and height of the input volume we will produce a 2 dimensional activation map that gives the responses of that filter at every spatial position.  
            Intuitively the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the  layer or eventually entire honeycomb or wheel like patterns on higher layers of the network. 
             Now we will have an entire set of filters in each CONV layer e.g. 12 filters and each of them will produce a separate 2 dimensional activation map.   
         We will stack these activation maps along the depth dimension and produce the output volume.  
    As a result the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.     
    
6. The Brain Perspective
       Every entry in the 3D output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially.  

7. Local Connectivity
        Convolutional networks exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers 
             Each neuron is connected to only a small region of the input volume.
         The Receptive Field of the neuron defines the extent of this connectivity as a hyperparameter.  
         For example suppose the input volume has size $$32x32x3$$ and the receptive field or the filter size is $$5x5$$ then each neuron in the Conv Layer will have weights to a $$5x5x3$$ region in the input volume for a total of $$553 = 75$$ weights and $$+1$$ bias parameter.  
    Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.

8. Spatial Arrangement
       There are three hyperparameters control the size of the output volume  
           1. The Depth of the output volume is a hyperparameter that corresponds to the number of filters we would like to use each learning to look for something different in the input.  
            2. The Stride controls how depth columns around the spatial dimensions width and height are allocated.  
                e.g. When the stride is 1 then we move the filters one pixel at a time.  

                The Smaller the stride the more overlapping regions exist and the bigger the volume.  
                The bigger the stride the less overlapping regions exist and the smaller the volume.  
            3. The Padding is a hyperparameter whereby we pad the input the input volume with zeros around the border.  
                This allows to control the spatial size of the output volumes.  


9. The Spatial Size of the Output Volume
       We compute the spatial size of the output volume as a function of  
            $$W$$ The input volume size.  
             $$F$$ $$\\$$The receptive field size of the Conv Layer neurons.  
             $$S$$ The stride with which they are applied.  
             $$P$$ The amount of zero padding used on the border.  
       Thus the Total Size of the Output  
       $$\dfrac{W−F+2P}{S} + 1$$  
        Potential Issue If this number is not an integer then the strides are set incorrectly and the neurons cannot be tiled to fit across the input volume in a symmetric way.  
        Fix In general setting zero padding to be $${\displaystyle P = \dfrac{K 1}{2}}$$ when the stride is $${\displaystyle S = 1}$$ ensures that the input volume and output volume will have the same size spatially.  


0. The Convolution Layer
       



Layers

1. Convolution Layer
       One image becomes a stack of filtered images.








Distinguishing features


2. Image Features
       are certain quantities that are calculated from the image to better describe the information in the image and to reduce the size of the input vectors. 
        Examples  
             Color Histogram Compute a bucked based vector of colors with their respective amounts in the image.  
             Histogram of Oriented Gradients HOG we count the occurrences of gradient orientation in localized portions of the image.   
             Bag of Words a bag of visual words is a vector of occurrence counts of a vocabulary of local image features.  
                The visual words can be extracted using a clustering algorithm; K Means.  









 Object Detection  with Deep Learning


Object Detection

1. Object Detection
       Object Detection is the process of finding multiple instances of real world objects such as faces vehicles and animals in images.  


2. The Structure
        Input Image  
         Output A pair of box co ords class of all the objects in a fixed number of classes that appear in the image.  

3. Properties
       In the problem of object detection we normally do not know the number of objects that we need to detect.  
        This leads to a problem when trying to model the problem as a regression problem due to the undefined number of coordinates of boxes.  
       Thus this problem is mainly modeled as a classification problem.  

4. Applications
        Image Retrieval
         Surveillance 
         Face Detection
         Face Recognition
         Pedestrian Detection
         Self Driving Cars


Approaches The Pre DeepLearning Era

1. Semantic Texton Forests
       This approach consists of ensembles of decision trees that act directly on image pixels.  
       Semantic Texton Forests STFs 
are randomized decision forests that use only simple pixel comparisons on local image patches performing both an
implicit hierarchical clustering into semantic textons and an explicit local classification of the patch category.  
       STFs allow us to build powerful texton codebooks without computing expensive filter banks or descriptors and without performing costly k means clustering and nearest neighbor assignment.
       Semantic Texton Forests for Image Categorization and Segmentation Shawton et al. 2008

2. Region Proposals
       
        Algorithm  
             Find "blobby" image regions that are likely to contain objects  
       These are relatively fast algorithms.   
       Alexe et al “Measuring the objectness of image windows” TPAMI 2012  
        Uijlings et al “Selective Search for Object Recognition” IJCV 2013  
        Cheng et al “BING Binarized normed gradients for objectness estimation at 300fps” CVPR 2014  
        Zitnick and Dollar “Edge boxes Locating object proposals from edges” ECCV 2014

3. Conditional Random Fields
       CRFs provide a probabilistic framework for labeling and segmenting structured data.  
       They try to model the relationship between pixels e.g.
        1. nearby pixels more likely to have same label
        2. pixels with similar color more likely to have same label
        3. the pixels above the pixels "chair" more likely to be "person" instead of "plane"
        4. refine results by iterations
       W. Wu A. Y. C. Chen L. Zhao and J. J. Corso 2014 "Brain Tumor detection and segmentation in a CRF framework with pixel pairwise affinity and super pixel level features"
       Plath et al. 2009 "Multi class image segmentation using conditional random fields and global classification"

4. SuperPixel Segmentation
       The concept of superpixels was  introduced by Xiaofeng Ren and Jitendra Malik in 2003.  
       Superpixel is a group of connected pixels with similar colors or gray levels.  
        They produce an image patch which is better aligned with intensity edges than a rectangular patch.  
       Superpixel segmentation is the idea of dividing an image into hundreds of non overlapping superpixels.  
        Then these can be fed into a segmentation algorithm such as Conditional Random Fields or Graph Cuts for the purpose of segmentation.  
       Efficient graph based image segmentation Felzenszwalb P.F. and Huttenlocher D.P. International Journal of Computer Vision 2004
       Quick shift and kernel methods for mode seeking Vedaldi A. and Soatto S. European Conference on Computer Vision 2008
       Peer Neubert & Peter Protzel 2014. Compact Watershed and Preemptive  


Approaches The Deep Learning Era

1. The Sliding Window Approach
       We utilize classification for detection purposes.  
        Algorithm    
             We break up the input image into tiny "crops" of the input image.  
             Use Classification+Localization to find the class of the center pixel of the crop or classify it as background.  
                Using the same machinery for classification+Localization.  
             Slide the window and look at more "crops"
       Basically we do classification+Localization on each crop of the image.
        DrawBacks  
             Very Inefficient and Expensive  
                We need to apply a CNN to a huge number of locations and scales.   
       Sermant et. al 2013 "OverFeat"

2. Region Proposal Networks R CNNs
       A framework for object detection that utilizes Region Proposals Regions of Interest ROIs consisting of three separate architectures.   
        Structure  
             Input Image vector  
             Output A vector of bounding boxes coordinates and a class prediction for each box     
        Strategy  
            Propose a number of "bounding boxes" then check if any of them actually corresponds to an object.  
            The bounding boxes are created using Selective Search.
        Selective Search A method that looks at the image through windows of different sizes and for each size tries to group together adjacent pixels by texture color or intensity to identify objects.    
        Key Insights  
            1. One can apply high capacity convolutional neural networks CNNs to bottom up region proposals in order to localize and segment objects  
            2. When labeled training data is scarce supervised pre training for an auxiliary task followed by domain specific fine tuning yields a significant performance boost.        
        Algorithm   
             Create Region Proposals Regions of Interest ROIs of bounding boxes  
             Warp the regions to a standard square size to fit the "cnn classification models" due to the FCNs    
             Pass the warped images to a modified version of AlexNet to extract image features  
             Pass the image features to an SVM to classify the image regions into a class or background
             Run the bounding box coordinates in a Linear Regression model to "tighten" the bounding boxes
                 Linear Regression 
                     Structure   
                         Input sub regions of the image corresponding to objects  
                         Output New bounding box coordinates for the object in the sub region.
        Issues   
             Ad hoc training objectives  
                 Fine tune network with softmax classifier log loss
                 Train post hoc linear SVMs hinge loss
                 Train post hoc bounding box regressions least squares
             Training is slow 84h takes a lot of disk space
             Inference detection is slow
                 47s / image with VGG16 Simonyan & Zisserman. ICLR15
                 Fixed by SPP net He et al. ECCV14
       R CNN is slow because it performs a ConvNet forward pass for each region proposal without sharing computation.  
       R. Girshick J. Donahue T. Darrell J. Malik. 2014 "Rich feature hierarchies for accurate object detection and semantic segmentation"  

3. Fast R CNNs
       A single end to end architecture for object detection based on R CNNs that vastly improves on its speed and accuracy by utilizing shared computations of features.     
        Structure  
             Input Image vector  
             Output A vector of bounding boxes coordinates and a class prediction for each box     
        Key Insights  
            1. Instead of running the ConvNet on each region proposal separately we run the ConvNet on the entire image.  
            2. Instead of taking crops of the original image we project the regions of interest onto the ConvNet Feature Map corresponding to each RoI and then use the projected regions in the feature map for classification.  
                This allows us to reuse a lot of the expensive computation of the features.  
            3. Jointly train the CNN classifier and bounding box regressor in a single model. Where earlier we had different models to extract image features CNN classify SVM and tighten bounding boxes regressor Fast R CNN instead used a single network to compute all three.
        Algorithm   
             Create Region Proposals Regions of Interest ROIs of bounding boxes       
             Pass the entire image to a modified version of AlexNet to extract image features by creating an image feature map for the entire image.  
             Project each RoI to the feature map and crop each respective projected region
             Apply RoI Pooling to the regions extracted from the feature map to a standard square size to fit the "cnn classification models" due to the FCNs
             Pass the image features to an SVM to classify the image regions into a class or background
             Run the bounding box coordinates in a Linear Regression model to "tighten" the bounding boxes
                 Linear Regression 
                     Structure   
                         Input sub regions of the image corresponding to objects  
                         Output New bounding box coordinates for the object in the sub region.  
        RoI Pooling is a pooling technique aimed to perform max pooling on inputs of nonuniform sizes to obtain fixed size feature maps e.g. 7×7.  
             Structure   
                 Input A fixed size feature map obtained from a deep convolutional network with several convolutions and max pooling layers.  
                 Output An N x 5 matrix of representing a list of regions of interest where N is a number of RoIs. The  column represents the image index and the remaining four are the coordinates of the top left and bottom right corners of the region.  
            For every region of interest from the input list it takes a section of the input feature map that corresponds to it and scales it to some pre defined size.  
             Scaling    
                1. Divide the RoI into equal sized sections the number of which is the same as the dimension of the output  
                2. Find the largest value in each section  
                3. Copy these max values to the output buffer  
            The dimension of the output is determined solely by the number of sections we divide the proposal into.   
        The Bottleneck   
            It appears that Fast R CNNs are capable of object detection at test time in  
             Including RoIs 2.3s
             Excluding RoIs 0.3s  
            Thus the bottleneck for the speed seems to be the method of creating the RoIs Selective Search
       1 Girshick Ross 2015. "Fast R CNN"  

4. Faster R CNNs
       A single end to end architecture for object detection based on Fast R CNNs that tackles the bottleneck in speed i.e. computing RoIs by introducing Region Proposal Networks RPNs to make a CNN predict proposals from features.     
        Region Proposal Networks share full image convolutional features with the detection network thus enabling nearly cost free region proposals.   
        The network is jointly trained with 4 losses
            1. RPN classify object / not object
            2. RPN regress box coordinates
            3. Final classification score object classes
            4. Final box coordinates
        Region Proposal Network RPN is an end to end fully convolutional network that simultaneously predicts object bounds and objectness scores at each position.  
        RPNs work by passing a sliding window over the CNN feature map and at each window outputting k potential bounding boxes and scores for how good each of those boxes is expected to be. 
        Structure  
             Input Image vector  
             Output A vector of bounding boxes coordinates and a class prediction for each box     
        Key Insights  
            1. Replace Selective Search for finding RoIs by a Region Proposal Network that shares the features and thus reduces the computation and time of the pipeline.  
        Algorithm   
             Pass the entire image to a modified version of AlexNet to extract image features by creating an image feature map for the entire image.  
             Pass the CNN Feature Map to the RPN to generate bounding boxes and a score for each bounding box 
             Pass each such bounding box that is likely to be an object into Fast R CNN to generate a classification and tightened bounding boxes. 
       Ren et al 2015. “Faster R CNN Towards Real Time Object Detection with Region Proposal Networks”  


Methods Approaches and Algorithms in Training DL Models



 Image Classification and Localization  with Deep Learning


Image Localization

1. Image Localization
       Localization is the task of finding a single object in an image.  

2. Image Classification+Localization
       Localization can be combined with classification to not only find the location of an object but also to classify it into one of different classes. 

3. Structure
        Input Image  
         Output A vector of 4 coordinates of the bounding box.  

4. Applications
        Smart Cropping
         Regular Object Extraction as a pre processing step  
         Human Pose Estimation Represent pose as a set of 14 joint positions 


Approaches

1. Localization as a Regression Problem
       Since we are concerned with returning real valued numbers the bounding box coordinates we use a method that is suitable for the task Regression.   
        Algorithm    
             Use any classification architecture  
             Attach two Fully Connected Layers one for Classification and one for Localization  
             Backpropagate through the whole network using cross entropy loss and L2 loss respectively.  
        Evaluation Metric Intersection over Union.  


Training Methods Approaches and Algorithms 

### Updated Soon!


 CNN Architectures


LeNet 5 LeCun et al. 1998  
 Architecture CONV POOL CONV POOL FC FC  
 Parameters 
     CONV F=5 S=1
     POOL F=2 S=2

AlexNet Krizhevsky et al. 2012


1. Architecture
       CONV1 MAX POOL1 NORM1 CONV2 MAX POOL2 NORM2 CONV3 CONV4 CONV5 Max POOL3 FC6 FC7 FC8  

2. Parameters
         Layer CONV1 
             F      

3. Key Insights
         use of ReLU
         used Norm layers not common anymore
         heavy data augmentation
         dropout 0.5
         batch size 128
         SGD Momentum 0.9
         Learning rate 1e 2 reduced by 10
        manually when val accuracy plateaus
         L2 weight decay 5e 4
         7 CNN ensemble 18.2%  > 15.4%

4. Results

5. ZFNet Zeiler and Fergus 2013


VGGNet Simonyan and Zisserman 2014


2. Parameters
        CONV F=1 S=1 P=1
         POOL F=2 S=2  
        For all layers   
    Notice  
            Parameters are mostly in the FC Layers  
            Memory mostly in the CONV Layers

3. Key Insights
        Smaller Filters
         Deeper Networks  
         Similar Training as AlexNet
         No LRN Layer
         Both VGG16 and VGG19
         Uses Ensembles for Best Results

4. Smaller Filters Justification
        A Stack of three 3x3 conv stride 1 layers has same effective receptive field as one 7x7 conv layer  
         However now we have deeper nets and more non linearities
         Also fewer parameters  
            3  3^2C^2  vs. 7^2C^2 for C channels per layer

5. Properties
        FC7 Features generalize well to other tasks

6. VGG16 vs VGG19
       VGG19 is only slightly better and uses more memory 

7. Results
       ILSVRC’14 2nd in classification 1st in localization  


GoogLeNet Szegedy et al. 2014


1. Architecture

2. Parameters
       Parameters as specified in the Architecture and the Inception Modules

3. Key Insights
        Even Deeper Networks
         Computationally Efficient
         22 layers
         Efficient “Inception” module
         No FC layers
         Only 5 million parameters 12x less than AlexNet

4. Inception Module
        Idea design a good local network topology network within a network and then stack these modules on top of each other   
        Architecture  
             Apply parallel filter operations on the input from previous layer  
                 Multiple receptive field sizes for convolution 1x1 3x3 5x5   
                 Pooling operation 3x3  
             Concatenate all filter outputs together depth wise  
        Issue Computational Complexity is very high  
         Solution use BottleNeck Layers that use 1x1 convolutions to reduce feature depth   
            preserves spatial dimensions reduces depth!

7. Results
       ILSVRC’14 classification winner 


ResNet He et al. 2015

1. Architecture

3. Key Insights
        Very Deep Network 152 layers
         Uses Residual Connections
         Deep Networks have very bad performance NOT because of overfitting but because of a lack of adequate optimization  

4. Motivation
        Observation Deeper Networks perform badly on the test error but also on the training error  
         Assumption Deep Layers should be able to perform at least as well as the shallower models   
         Hypothesis the problem is an optimization problem deeper models are harder to optimize  
         Solution work around Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping   

5. Residuals

6. BottleNecks

7. Training
       Batch Normalization after every CONV layer
         Xavier/2 initialization from He et al.
         SGD + Momentum 0.9
         Learning rate 0.1 divided by 10 when validation error plateaus
         Mini batch size 256
         Weight decay of 1e 5
         No dropout used

8. Results
        ILSVRC’15 classification winner 3.57% top 5 error  
         Swept all classification and detection competitions in ILSVRC’15 and COCO’15  
         Able to train very deep networks without degrading 152 layers on ImageNet 1202 on Cifar
         Deeper networks now achieve lowing training error as expected


Comparisons

1. Complexity

2. Forward Pass Time and Power Consumption

 

Interesting Architectures

1. Network in Network NiN Lin et al. 2014
        Mlpconv layer with “micronetwork” within each conv layer to compute more abstract features for local patches
         Micronetwork uses multilayer perceptron FC i.e. 1x1 conv layers
         Precursor to GoogLeNet and ResNet “bottleneck” layers
         Philosophical inspiration for GoogLeNet  

2. Identity Mappings in Deep Residual Networks Improved ResNets He et al. 2016
        Improved ResNet block design from creators of ResNet
         Creates a more direct path for propagating information throughout network moves activation to residual mapping pathway
         Gives better performance

3. Wide Residual Networks Improved ResNets Zagoruyko et al. 2016
        Argues that residuals are the important factor not depth
         User wider residual blocks F x k filters instead of F filters in each layer
         50 layer wide ResNet outperforms 152 layer original ResNet
         Increasing width instead of depth more computationally efficient parallelizable

4. Aggregated Residual Transformations for Deep Neural Networks ResNeXt Xie et al. 2016
        Also from creators of ResNet
         Increases width of residual block through multiple parallel pathways “cardinality”
         Parallel pathways similar in spirit to Inception module

5. Deep Networks with Stochastic Depth Improved ResNets Huang et al. 2016
        Motivation reduce vanishing gradients and training time through short networks during training
         Randomly drop a subset of layers during each training pass
         Bypass with identity function
         Use full deep network at test time

#### Beyond ResNets

6. FractalNet Ultra Deep Neural Networks without Residuals Larsson et al. 2017
        Argues that key is transitioning effectively from shallow to deep and residual representations are not necessary
         Fractal architecture with both shallow and deep paths to output
         Trained with dropping out sub paths 
         Full network at test time

7. Densely Connected Convolutional Networks Huang et al. 2017
        Dense blocks where each layer is connected to every other layer in feedforward fashion
         Alleviates vanishing gradient strengthens feature propagation encourages feature reuse

8. SqueezeNet Efficient NetWork Iandola et al. 2017
         AlexNet level Accuracy With 50x Fewer Parameters and <0.5Mb Model Size
         Fire modules consisting of a ‘squeeze’ layer with 1x1 filters feeding an ‘expand’ layer with 1x1 and 3x3 filters
         Can compress to 510x smaller than AlexNet 0.5Mb



 Generative Compression







Introduction
 

5. ML based Compression
       The main idea behind ML based compression is that structure is automatically discovered instead of manually engineered.  
        Examples 
             DjVu employs segmentation and K means clustering to separate foreground from background and analyze the documents contents.     


WaveOne 



WaveOne is a machine learning based approach to lossy image compression.  

1. Main Idea
       An ML based approach to compression that utilizes the older techniques for quantization but with an encoder decoder model that depends on adversarial training for a higher quality reconstruction.   

2. Model
       The model includes three main steps that are layered together in one pipeline  
         Feature Extraction an approach that aims to recognize the different types of structures in an image.  
             Structures 
                 Across input channels
                 Within individual scales
                 Across Scales
             Methods 
                 Pyramidal Decomposition for analyzing individual scales 
                 Interscale Alignment Procedure  for exploiting structure shared across scales   
         Code Computation and Regularization a module responsible for further compressing the extracted features by quantizing the features and encoding them via two methods.  
             Methods 
                 Adaptive Arithmetic Coding Scheme applied on the features binary expansions
                 Adaptive Codelength Regularization  to penalize the entropy of the features to achieve better compression   
         Adversarial Training Discriminator Loss a module responsible for enforcing realistic reconstructions.
             Methods 
                 Adaptive Arithmetic Coding Scheme applied on the features binary expansions
                 Adaptive Codelength Regularization  to penalize the entropy of the features to achieve better compression   


3. Feature Extraction
        Pyramidal Decomposition   
            Inspired by the use of wavelets for multiresolution analysis in which an input is analyzed recursively via feature extraction and downsampling operators the pyramidal decomposition encoder generalizes the wavelet decomposition idea to learn optimal nonlinear extractors individually for each scale.  
            For each input $$\mathbf{x}$$ to the model and a total of $$M$$ scales denote the input to scale $$m$$ by $$\mathbf{x}m$$.   
             Algorithm  
                Set input to  scale $$\mathbf{x}1 = \mathbf{x}$$
                For each scale $$m$$  
                     Extract coefficients $$\mathbf{c}m = \mathbf{f}m\mathbf{x}m \in \mathbb{R}^{Cm \times Hm \times Wm}$$ via some parametrized function $$\mathbf{f}m\dot$$ for output channels $$Cm$$ height $$Hm$$ and width $$Wm$$  
                     Compute the input to the next scale as $$\mathbf{x}{m+1} = \mathbf{D}m\mathbf{x}m$$ where $$\mathbf{D}m\dot$$ is some downsampling operator either fixed or learned

            Typically $$M$$ is chosen to be $$ = 6$$ scales.  
            The feature extractors for the individual scales are composed of a sequence of convolutions with kernels $$3 \times 3$$ or $$1 \times 1$$ and ReLUs with a leak of $$0.2$$.  
            All downsamplers are learned as $$4 \times 4$$ convolutions with a stride of $$2$$. 
        Interscale Alignment   
            Designed to leverage information shared across different scales — a benefit not offered by the classic wavelet analysis.  
             Structure  
                 Input the set of coefficients extracted from the different scales $$\{\mathbf{c}m\}{m=1}^M \subset \mathbb{R}^{Cm \times Hm \times Wm}$$     
                 Output a tensor $$\mathbf{y} \in \mathbb{R}^{C \times H \times W}$$
             Algorithm  
                 Map each input tensor $$\mathbf{c}m$$ to the target dimensionality via some parametrized function $$\mathbf{g}m·$$  this involves ensuring that this function spatially resamples $$\mathbf{c}m$$ to the appropriate output map size $$H \times W$$ and ouputs the appropriate number of channels $$C$$  
                 Sum $$\mathbf{g}m\mathbf{c}m = 1 \ldots M$$ and apply another parameterized non linear transformation $$\mathbf{g}·$$ for joint processing  
        $$\mathbf{g}m·$$ is chosen as a convolution or a deconvolution with an appropriate stride to produce the target spatial map size $$H \times W$$.  
        $$\mathbf{g}·$$ is choses as a sequence of $$3 \times 3$$ convolutions.  


4. Code Computation and Regularization
       Given the output tensor $$\mathbf{y} \in \mathbb{R}^{C \times H \times W}$$ of the feature extraction step namely alignment we proceed to quantize and encode it.  
        Quantization the tensor $$\mathbf{y}$$ is quantized to bit precision $$B$$    
            Given a desired precision of $$B$$ bits we quantize the feature tensor into $$2^B$$ equal sized bins as  
            For the special case $$B = 1$$ this reduces exactly to a binary quantization scheme.  
            In practice $$B = 6$$ is chosen as a smoother quantization method.  
             Reason   
                Mapping the continuous input values representing the image signal to a smaller countable set to achieve a desired precision of $$B$$ bits
        Bitplane Decomposition we transform the quantized tensor $$\mathbf{\hat{y}}$$ into a binary tensor suitable for encoding via a lossless bitplane decomposition  
            $$\mathbf{\hat{y}}$$ is decomposed into bitplanes by a transformation that maps each value $$\hat{y}{chw}$$ into its binary expansion of $$B$$ bits.  
            Hence each of the $$C$$ spatial maps $$\mathbf{\hat{y}}c \in \mathbb{R}^{H \times W}$$ of $$\mathbf{\hat{y}}$$ expands into $$B$$ binary bitplanes.  

             Reason   
                This decomposition enables the entropy coder to exploit structure in the distribution of the activations in $$\mathbf{y}$$ to achieve a compact representation.  
        Adaptive Arithmetic Encoding encodes $$\mathbf{b}$$ into its final variable length binary sequence $$\mathbf{s}$$ of length $$\mathcal{l}\mathbf{s}$$  
             The binary tensor $$\mathbf{b}$$ that is produced by the bitplane decomposition contains significant structure e.g. higher bitplanes are sparser and spatially neighboring bits often have the same value.  
                This structure can be exploited by using Adaptive Arithmetic Encoding.  
             Method  
                 Encoding     
                    Associate each bit location in the binary tensor $$\mathbf{b}$$ with a context which comprises a set of features indicative of the bit value.  
                    The features are based on the position of the bit and the values of neighboring bits.  
                    To predict the value of each bit from its context features we train a classifier and use its output probabilities to compress $$\mathbf{b}$$ via arithmetic coding.   
                 Decoding    
                    At decoding time we perform the inverse operation to decompress the code.  
                    We interleave between  
                     Computing the context of a particular bit using the values of previously decoded bits  
                     Using this context to retrieve the activation probability of the bit and decode it  
                    This operation constrains the context of each bit to only include features composed of bits already decoded   
             Reason   
                We aim to leverage the structure in the data specifically in the binary tensor $$\mathbf{b}$$ produced by the bitplane decomposition which has low entropy 
        Adaptive Codelength Regularization modulates the distribution of the quantized representation $$\mathbf{\hat{y}}$$ to achieve a target expected bit count across inputs  
             Goal regulate the expected codelength $$\mathbb{E}x\mathcal{l}\mathbf{s}$$ to a target value $$\mathcal{l}{\text{target}}$$.
             Method  
                We design a penalty that encourages a structure that the AAC is able to encode.  
                Namely we regularize the quantized tensor $$\mathbf{\hat{y}}$$ with  
                for iteration $$t$$ and difference index set $$S = \{01 10 11  11\}$$.  
                The  term penalizes the magnitude of each tensor element  
                The  Term penalizes deviations between spatial neighbors  

             Reason   
                The Adaptive Codelength Regularization is designed to solve one problem; the non variability of the latent space code which is what controls defines the bitrate.  
                It essentially allows us to have latent space codes with different lengths depending on the complexity of the input by enabling better prediction by the AAX  
            In practice a total to target ratio $$ = BCHW/\mathcal{l}{\text{target}} = 4$$ works well.  

5. Adversarial Train
        GAN Architecture   
             Generator Encoder Decoder Pipeline  
             Discriminator Classification ConvNet 
        Discriminator Design  




7. Architecture
        Problem Regression Problem
         Goal Learn a function $$\psix;\theta$$ that is trained and used to regress to a pose vector.   
         Estimation $$\psi$$ is based on learned through Deep Neural Net
         Deep Neural Net is a Convolutional Neural Network; namely AlexNet  
             Input image with pre defined size $$ = \$$ # pixels $$\times 3$$ color channels  
                $$220 \times 220$$ with a stride of $$4$$  
             Output target value of the regression$$ = 2k$$ joint coordinates  
    Denote by $$\mathbf{C}$$ a convolutional layer by $$\mathbf{LRN}$$ a local response normalization layer $$\mathbf{P}$$ a pooling layer and by $$\mathbf{F}$$ a fully connected layer  
    For $$\mathbf{C}$$ layers the size is defined as width $$\times$$ height $$\times$$ depth where the  two dimensions have a spatial meaning while the depth defines the number of filters.  
        Alex Net 
             Architecture     $$\mathbf{C}55 \times 55 \times 96 − \mathbf{LRN} − \mathbf{P} − \mathbf{C}27 \times 27 \times 256 − \mathbf{LRN} − \mathbf{P} − \mathbf{C}13 \times 13 \times 384 − \mathbf{C}13 \times 13 \times 384 − \mathbf{C}13 \times 13 \times 256 − \mathbf{P} − \mathbf{F}4096 − \mathbf{F}4096$$   
             Filters  
                 $$\mathbf{C}{1} = 11 \times 11$$  
                 $$\mathbf{C}{2} = 5 \times 5$$  
                 $$\mathbf{C}{3 5} = 3 \times 3$$.
             Total Number of Parameters $$ = 40$$M   
             Training Dataset  
                Denote by $$D$$ the training set and $$DN$$ the normalized training set   
                $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  $$DN = \{NxN\mathbf{y}\vert x\mathbf{y} \in D\}$$   
             Loss the Loss is modified; instead of a classification loss we train a linear regression on top of the last network layer to predict a pose vector by minimizing $$L2$$ distance between the prediction and the true pose vector  
       $$\arg \min\theta \sum{xy \in DN} \sum{i=1}^k \|\mathbf{y}i  \psiix;\theta\|2^2$$  
        Optimization  
             BackPropagation in a distributed online implementation
             Adaptive Gradient Updates
             Learning Rate $$ = 0.0005 = 5\times 10^{ 4}$$
             Data Augmentation randomly translated image crops left/right flips
             DropOut Regularization for the $$\mathbf{F}$$ layers $$ = 0.6$$

9. Architecture
        Motivation   
            Although the pose formulation of the DNN has the advantage that the joint estimation is based on the full image and thus relies on context due its fixed input size of $$220 \times 220$$ the network has limited capacity to look at detail  it learns filters capturing pose properties at coarse scale.  
            The pose properties are necessary to estimate rough pose but insufficient to always precisely localize the body joints.  
            Increasing the input size is infeasible since it will increase the already large number of parameters.  
            Thus a cascade of pose regressors is used to achieve better precision.  
         Structure and Training   
            At the  stage 
             The cascade starts off by estimating an initial pose as outlined in the previous section.  
            At subsequent stages  
             Additional DNN regressors are trained to predict a displacement of the joint locations from previous stage to the true location.  
                Thus each subsequent stage can be thought of as a refinement of the currently predicted pose.   
             Each subsequent stage uses the predicted joint locations to focus on the relevant parts of the image   subimages are cropped around the predicted joint location from previous stage and the pose displacement regressor for this joint is applied on this sub image.  
                Thus subsequent pose regressors see higher resolution images and thus learn features for finer scales which ultimately leads to higher precision  
         Method and Architecture  
             The same network architecture is used for all stages of the cascade but learn different parameters.   
             Start with a bounding box $$b^0$$ which either encloses the full image or is obtained by a person detector
             Obtain an initial pose  
                Stage 1 $$\mathbf{y}^1 \leftarrow N^{ 1}\psiNx;b^0;\theta1;b^0$$  
             At stages $$s \geq 2$$ for all joints
                 Regress  towards a refinement displacement $$\mathbf{y}i^s  \mathbf{y}i^{s 1}$$ by applying a regressor on the sub image defined by $$bi^{s 1}$$ 
                 Estimate new joint boxes $$bi^s$$  
                Stage $$s$$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{y}i^s \leftarrow \mathbf{y}i^{2 1} + N^{ 1}\psiNx;b^0;\thetas;b  \\ 6  
 \ \ \ \ \ \ \ \ \ \ \ \ \ \                  \\\\ \text{for } b = bi^s 1 
  \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
                bi^s \leftarrow \mathbf{y}i^s \sigma diam\mathbf{y}^s \sigma diam\mathbf{y}^s \\ 7$$  
                where we considered a joint bounding box $$bi$$ capturing the sub image around $$\mathbf{y}i bi\mathbf{y}; \sigma = \mathbf{y}i \sigma diam\mathbf{y} \sigma diam\mathbf{y}$$ having as center the i th joint and as dimension the pose diameter scaled by $$\sigma$$ to refine a given joint location $$\mathbf{y}i$$.    
             Apply the cascade for a fixed number of stages $$ = S$$  
         Loss at each stage $$s$$   
      $$\thetas = \arg \min\theta \sum{x\mathbf{y}i \in DA^s} \|\mathbf{y}i  \psiix;\theta\|2^2 \\\\\ 8$$


6. Advantages
        The DNN is capable of capturing the full context of each body joint  
         The approach is simpler to formulate than graphical models methods  no need to explicitly design feature representations and detectors for parts or to explicitly design a model topology and interactions between joints.   
            Instead a generic ConvNet learns these representations

8. Notes
        The use of a generic DNN architecture is motivated by its outstanding results on both classification and localization problems and translates well to pose estimation  
         Such a model is a truly holistic one — the final joint location estimate is based on a complex nonlinear transformation of the full image  
         The use of a DNN obviates the need to design a domain specific pose model
         Although the regression loss does not model explicit interactions between joints such are implicitly captured by all of the 7 hidden layers   all the internal features are shared by all joint regressors  


Proposed Changes

1. Gated Matrix Selection for Latent Space dimensionality estimation and Dynamic bit rate modification
       


2. Conditional Generative Adversarial Training with Random Forests for Generalizable domain compression
       


3. Adversarial Feature Learning for Induced Natural Representation and Artifact Removal
       



Papers  
 Generative Compression



 Current Standards
     An overview of the JPEG2000 still image compression standard

         Notes Pretty in depth by Eastman Kodak Company from early 2000s maybe improvements since then?

1. WaveOne



2. Generative Compression   Santurker Budden Shavit MIT

3. Toward Conceptual Compression   DeepMind



 Generative Compression  






 CNNs  Convolutional Neural Networks


Image Classification

1. The Problem
       Assigning a semantic label from a fixed set of categories to a sub grid of an image.
       The problem is often referred to as The Semantic Gap.

2. The Challenges
       1. Viewpoint Variation  
        2. Illumination Conditions  
        3. Deformation  
        4. Occlusion  
        5. Background Clutter  
        6. Intra class variation  
        7. Scale Variation  

3. Attempts
       1. 

4. The Data Driven Approach
       1. Collect a dataset of images and labels.  
        2. Use Machine Learning to train a classifier.  
        3. Evaluate the classifier on new images.  






Classifiers

1. K Nearest Neighbors
       

       Complexity  
            Training $$\\\\\mathcal{O}1$$   
             Predict $$\\\\\mathcal{O}N$$ 









Metrics

1. L1 Distance
       $$d1I1 I2 = \sump{\|I1^p  I2^p\|}$$  
       Pixel wise absolute value differences.  

2. L2 Distance
       $$d2 I1 I2 = \sqrt{\sum{p} \left I^p1  I^p2 \right^2}$$
       

3. L1 vs. L2
       The L2 distance penalizes errors pixel differences much more than the L1 metric does.  
    The L2 distnace will be small iff there are man small differences in the two vectors but will explode if there is even one big difference between them.  
       Another difference we highlight is that the L1 distance is dependent on the corrdinate system frame while the L2 distance is coordinate invariant.








 Auto Encoders


Introduction and Architecture

0. From PCA to Auto Encoders
    High dimensional data can often be represented using a much lower dimensional code.  
    This happens when  the data lies near a linear manifold in the high dimensional space.  
    Thus if we can this linear manifold we can project the data on the manifold and then represent the data by its position on the manifold without losing much information because in the directions orthogonal to the manifold there isn't much variation in the data.  

    Often PCA is used as a method to determine this linear manifold to reduce the dimensionality of the data from $$N$$ dimensions to say $$M$$ dimensions where $$M < N$$.  
    However what if the manifold that the data is close to is non linear?  
    Obviously we need someway to find this non linear manifold.  

    Deep Learning provides us with Deep AutoEncoders.  
    Auto Encoders allows us to deal with curved manifolds un the input space by using deep layers where the code is a non linear function of the input and the reconstruction of the data from the code is also a non linear function of the code.  

    Using Backpropagation to implement PCA inefficiently  
     Try to make the output be the same as the input in a network with a central bottleneck.  

     The activities of the hidden units in the bottleneck form an efficient code.  
     If the hidden and output layers are linear it will learn hidden units that are a linear function of the data and minimize the squared reconstruction error.  
         This is exactly what PCA does.  
     The M hidden units will span the same space as the  $M$ components found by PCA  
         Their weight vectors may not be orthogonal.  
         They might be skews or rotations of the PCs.  
         They will tend to have equal variances.  

    The reason to use backprop to implement PCA is that it allows us to generalize PCA.  
    With non linear layers before and after the code it should be possible to efficiently represent data that lies on or near a non linear manifold.  
    

1. Auto Encoders
       An AutoEncoder is an artificial neural network used for unsupervised learning of efficient codings.   
        It aims to learn a representation encoding for a set of data typically for the purpose of dimensionality reduction.

22.Deep Auto Encoders
    They provide a really nice way to do non linear dimensionality reduction  
     They provide flexible mappings both ways
     The learning time is linear or better in the number of training examples  
     The final encoding model is fairly compact and fast.  
    

33.Advantages of Depth
    Autoencoders are often trained with only a single layer encoder and a single layer decoder but using deep encoders and decoders offers many advantages  
     Depth can exponentially reduce the computational cost of representing some functions
     Depth can exponentially decrease the amount of training data needed to learn some functions
     Experimentally deep Autoencoders yield better compression compared to shallow or linear Autoencoders  
    

44.Learning Deep Autoencoders
    Training Deep Autoencoders is very challenging  
     It is difficult to optimize deep Autoencoders using backpropagation  
     With small initial weights the backpropagated gradient dies  
    
    There are two main methods for training  
     Just initialize the weights carefully as in Echo State Nets. No longer used  
     Use unsupervised layer by layer pre training. Hinton  
        This method involves treating each neighbouring set of two layers as a restricted Boltzmann machine so that the pretraining approximates a good solution then using a backpropagation technique to fine tune the results. This model takes the name of deep belief network.  
     Joint Training most common  
        This method involves training the whole architecture together with a single global reconstruction objective to optimize.  

    A study published in 2015 empirically showed that the joint training method not only learns better data models but also learned more representative features for classification as compared to the layerwise method.  
    The success of joint training however is mostly attributed depends heavily on the regularization strategies adopted in the modern variants of the model.  


    

2. Architecture
       An auto encoder consists of  
         An Encoding Function 
         A Decoding Function 
         A Distance Function  
       We choose the encoder and decoder to be  parametric functions typically neural networks and to be differentiable with respect to the distance function so the parameters of the encoding/decoding functions can be optimize to minimize the reconstruction loss using Stochastic Gradient Descent.  
       The simplest form of an autoencoder is a feedforward neural network similar to the multilayer perceptron MLP   having an input layer an output layer and one or more hidden layers connecting them   but with the output layer having the same number of nodes as the input layer and with the purpose of reconstructing its own inputs instead of predicting the target value $${\displaystyle Y}$$ given inputs $${\displaystyle X}$$.  

3. Structure and Mathematics
       The encoder and the decoder in an auto encoder can be defined as transitions $$\phi$$ and $$ {\displaystyle \psi }$$ such that  
       $$ {\displaystyle \phi {\mathcal {X}}\rightarrow {\mathcal {F}}} 
    {\displaystyle \psi {\mathcal {F}}\rightarrow {\mathcal {X}}} 
    {\displaystyle \phi \psi =\arg \min{\phi \psi }\|X \psi \circ \phi X\|^{2}}$$
       where $${\mathcal {X} = \mathbf{R}^d}$$ is the input space and $${\mathcal {F} = \mathbf{R}^p}$$ is the latent feature space and $$ p < d$$.   
       The encoder takes the input $${\displaystyle \mathbf {x} \in \mathbb {R} ^{d}={\mathcal {X}}}$$ and maps it to $${\displaystyle \mathbf {z} \in \mathbb {R} ^{p}={\mathcal {F}}} $$

       $${\displaystyle \mathbf {z} =\sigma \mathbf {Wx} +\mathbf {b} }$$  
        The image $$\mathbf{z}$$ is referred to as code latent variables or latent representation.  
          $${\displaystyle \sigma }$$ is an element wise activation function such as a sigmoid function or a rectified linear unit.
         $${\displaystyle \mathbf {W} }$$ is a weight matrix
         $${\displaystyle \mathbf {b} }$$ is the bias.
       The Decoder maps  $${\displaystyle \mathbf {z} }$$ to the reconstruction $${\displaystyle \mathbf {x'} } $$  of the same shape as $${\displaystyle \mathbf {x} }$$  
       $${\displaystyle \mathbf {x'} =\sigma '\mathbf {W'z} +\mathbf {b'} }$$
       where $${\displaystyle \mathbf {\sigma '} \mathbf {W'} {\text{ and }}\mathbf {b'} } $$ for the decoder may differ in general from those of the encoder.  
       Autoencoders minimize  reconstruction errors such as the L 2 loss  
       $${\displaystyle {\mathcal {L}}\mathbf {x} \psi  \phi \mathbf {x}    =  {\mathcal {L}}\mathbf {x} \mathbf {x'} =\|\mathbf {x}  \mathbf {x'} \|^{2}=\|\mathbf {x}  \sigma '\mathbf {W'} \sigma \mathbf {Wx} +\mathbf {b} +\mathbf {b'} \|^{2}}$$
       where $${\displaystyle \mathbf {x} }$$ is usually averaged over some input training set.

4. Applications
       The applications of auto encoders have changed overtime.  
        This is due to the advances in the fields that auto encoders were applied in or to the incompetency of the auto encoders.  
       Recently auto encoders are applied to  
         Data Denoising 
         Dimensionality Reduction for data visualization
     With appropriate dimensionality and sparsity constraints autoencoders can learn data projections that are more interesting than PCA or other basic techniques.
    For 2D visualization specifically t SNE is probably the best algorithm around but it typically requires relatively low dimensional data. So a good strategy for visualizing similarity relationships in high dimensional data is to start by using an autoencoder to compress your data into a low dimensional space e.g. 32 dimensional by an auto encoder then use t SNE for mapping the compressed data to a 2D plane.  






5. Types of Auto Encoders
     Vanilla Auto Encoder
     Sparse Auto Encoder
     Denoising Auto Encoder
     Variational Auto Encoder VAE
     Contractive Auto Encoder
    

6. Auto Encoders for initializing Neural Nets
    After training an auto encoder we can use the encoder to compress the input data into it's latent representation which we can view as features and input those to the neural net e.g. a classifier for prediction.  
    

7. Representational Power Layer Size and Depth
    the universal approximator theorem guarantees that a feedforward neural network with at least one hidden layer can represent an approximation of any function within a broad class to an arbitrary degree of accuracy provided that it has enough hidden units. This means that an autoencoder with a single hidden layer is able to represent the identity function along the domain of the data arbitrarily well.  
    However the mapping from input to code is shallow. This means that we are not able to enforce arbitrary constraints such as that the code should be sparse.  
    A deep autoencoder with at least one additional hidden layer inside the encoder itself can approximate any mapping from input to code arbitrarily well given enough hidden units.  
    

Notes  

 Progression of AEs in CV?  
     Originally Linear + nonlinearity sigmoid  
     Later Deep fully connected  
     Later ReLU CNN UpConv  




DL Book  AEs

1. Undercomplete Autoencoders
    An Undercomplete Autoencoder is one whose code dimension is less than the input dimension.  
    Learning an undercomplete representation forces the autoencoder to capture the most salient features of the training data.  
    

2. Challenges
     If an autoencoder succeeds in simply learning to set $$\psi\phi x = x$$ everywhere then it is not especially useful.  
        Instead autoencoders are designed to be unable to learn to copy perfectly. Usually they are restricted in ways that allow them to copy only approximately and to copy only input that resembles the training data.  
     In Undercomplete Autoencoders If the encoder and decoder are allowed too much capacity the autoencoder can learn to perform the copying task without extracting useful information about the distribution of the data.  
        Theoretically one could imagine that an autoencoder with a one dimensional code but a very powerful nonlinear encoder could learn to represent each training example $$x^{i}$$ with the code $$i$$. This specific scenario does not occur in practice but it illustrates clearly that an autoencoder trained to perform the copying task can fail to learn anything useful about the dataset if the capacity of the autoencoder is allowed to become too great.  
     A similar problem occurs in complete AEs  
     As well as in the overcomplete case in which the hidden code has dimension greater than the input.  
        In complete and overcomplete cases even a linear encoder and linear decoder can learn to copy the input to the output without learning anything useful about the data distribution.  
    

3. Regularized AutoEncoders
    To address the challenges in learning useful representations; we introduce Regularized Autoencoders.  

    Regularized Autoencoders allows us to train any architecture of autoencoder successfully choosing the code dimension and the capacity of the encoder and decoder based on the complexity of distribution to be modeled.  

    Rather than limiting the model capacity by keeping the encoder and decoder shallow and the code size small regularized autoencoders use a loss function that encourages the model to have other properties besides the ability to copy its input to its output   
     Sparsity of the representation 
     Smallness of the derivative of the representation
     Robustness to noise or to missing inputs.  

    A regularized autoencoder can be nonlinear and overcomplete but still learn something useful about the data distribution even if the model capacity is great enough to learn a trivial identity function.  
    

4. Generative Models as unregularized Autoencoders
    In addition to the traditional AEs described here nearly any generative model with latent variables and equipped with an inference procedure for computing latent representations given input may be viewed as a particular form of autoencoder; most notably the descendants of the Helmholtz machine Hinton et al. 1995b such as  
     Variational Autoencoders  
     Generative Stochastic Networks  

    These models naturally learn high capacity overcomplete encodings of the input and do NOT require regularization for these encodings to be useful. Their encodings are naturally useful because the models were trained to approximately maximize the probability of the training data rather than to copy the input to the output.  
    

5. Stochastic Encoders and Decoders
    
    



Regularized Autoencoders

1. Sparse Autoencoders
    Sparse Autoencoders are simply autoencoders whose training criterion involves a sparsity penalty $$\Omega\boldsymbol{h}$$ on the code layer $$\boldsymbol{h}$$ in addition to the reconstruction error  
    $${\displaystyle {\mathcal {L}}\mathbf {x} \psi  \phi \mathbf {x}    + \Omega\boldsymbol{h}}$$  
    where typically we have $$\boldsymbol{h}=\phi\boldsymbol{x}$$ the encoder output.   


    Regularization Interpretation  
    We can think of the penalty $$\Omega\boldsymbol{h}$$ simply as a regularizer term added to a feedforward network whose primary task is to copy the input to the output unsupervised learning objective and possibly also perform some supervised task with a supervised learning objective that depends on these sparse features.  


    Bayesian Interpretation of Regularization  
    Unlike other regularizers such as weight decay there is not a straightforward Bayesian interpretation to this regularizer.  
    Regularized autoencoders defy such an interpretation because the regularizer depends on the data and is therefore by definition not a prior in the formal sense of the word.  
    We can still think of these regularization terms as implicitly expressing a preference over functions.   


    Latent Variable Interpretation  
    Rather than thinking of the sparsity penalty as a regularizer for the copying task we can think of the entire sparse autoencoder framework as approximating maximum likelihood training of a generative model that has latent variables.  

    Correspondence between Sparsity and a Directed Probabilistic Model  
    Suppose we have a model with visible variables $$\boldsymbol{x}$$ and latent variables $$\boldsymbol{h}$$ with an explicit joint distribution $$p{\text {model }}\boldsymbol{x} \boldsymbol{h}=p{\text {model }}\boldsymbol{h} p{\text {model }}\boldsymbol{x} \vert \boldsymbol{h} .$$ We refer to $$p{\text {model }}\boldsymbol{h}$$ as the model's prior distribution over the latent variables representing the model's beliefs prior to seeing $$\boldsymbol{x}$$^1.  
    The log likelihood can be decomposed as  
    $$\log p{\text {model }}\boldsymbol{x}=\log \sum{\boldsymbol{h}} p{\text {model }}\boldsymbol{h} \boldsymbol{x}$$  
    We can think of the autoencoder as approximating this sum with a point estimate for just one highly likely value for $$\boldsymbol{h}$$.  
    This is similar to the sparse coding generative model section 13.4 but with $$\boldsymbol{h}$$ being the output of the parametric encoder rather than the result of an optimization that infers the most likely $$\boldsymbol{h}$$. From this point of view with this chosen $$\boldsymbol{h}$$ we are maximizing  
    $$\log p{\text {model }}\boldsymbol{h} \boldsymbol{x}=\log p{\text {model }}\boldsymbol{h}+\log p{\text {model }}\boldsymbol{x} \vert \boldsymbol{h}$$  
    The $$\log p{\text {model }}\boldsymbol{h}$$ term can be sparsity inducing. For example the Laplace prior  
    $$p{\text {model }}\lefth{i}\right=\frac{\lambda}{2} e^{ \lambda\left|h{i}\right|}$$  
    corresponds to an absolute value sparsity penalty.  
    Expressing the log prior as an absolute value penalty we obtain  
    $$\begin{aligned} \Omega\boldsymbol{h} &=\lambda \sum{i}\left|h{i}\right|  \log p{\text {model }}\boldsymbol{h} &=\sum{i}\left\lambda\left|h{i}\right| \log \frac{\lambda}{2}\right=\Omega\boldsymbol{h}+\text { const } \end{aligned}$$  
    where the constant term depends only on $$\lambda$$ and not $$\boldsymbol{h} .$$ We typically treat $$\lambda$$ as a hyperparameter and discard the constant term since it does not affect the parameter learning.  
    Other priors such as the Student t prior can also induce sparsity.  
    From this point of view of sparsity as resulting from the effect of $$p{\text {model}}\boldsymbol{h}$$ on approximate maximum likelihood learning the sparsity penalty is not a regularization term at all. It is just a consequence of the model’s distribution over its latent variables. This view provides a different motivation for training an autoencoder it is a way of approximately training a generative model. It also provides a different reason for why the features learned by the autoencoder are useful they describe the latent variables that explain the input.  

    Correspondence between Sparsity and an Undirected Probabilistic Model  
    Early work on sparse autoencoders Ranzato et al. 2007a 2008 explored various forms of sparsity and proposed a connection between the sparsity penalty and the log $$Z$$ term that arises when applying maximum likelihood to an undirected probabilistic model $$p\boldsymbol{x}=\frac{1}{Z} \tilde{p}\boldsymbol{x}$$.  
    The idea is that minimizing $$\log Z$$ prevents a probabilistic model from having high probability everywhere and imposing sparsity on an autoencoder prevents the autoencoder from having low reconstruction error everywhere. In this case the connection is on the level of an intuitive understanding of a general mechanism rather than a mathematical correspondence.  

    The interpretation of the sparsity penalty as corresponding to $$\log p{\text {model }}\boldsymbol{h}$$ in a $$\left.\text { directed model } p{\text {model }}\boldsymbol{h} p{\text {model }} \boldsymbol{x} \vert \boldsymbol{h}\right$$ is more mathematically straightforward.   

    Achieving actual zeros in $$\boldsymbol{h}$$  
    One way to achieve actual zeros in $$\boldsymbol{h}$$ for sparse and denoising autoencoders was introduced in Glorot et al. 2011b. The idea is to use rectified linear units to produce the code layer. With a prior that actually pushes the representations to zero like the absolute value penalty one can thus indirectly control the average number of zeros in the representation.  


    Notes  
    


    


2. Denoising Autoencoders
    Denoising Autoencoders DAEs is an autoencoder that receives a corrupted data point as input and is trained to predict the original uncorrupted data point as its output.  
    It minimizes  
    $$L\boldsymbol{x} gf\tilde{\boldsymbol{x}}$$  
    where $$\tilde{\boldsymbol{x}}$$ is a copy of $$\boldsymbol{x}$$ that has been corrupted by some form of noise.  

    Denoising autoencoders must therefore learn to undo this corruption rather than simply copying their input.  
    Denoising training forces $$\psi$$ and $$\phi$$ to implicitly learn the structure of $$p{\text {data}}\boldsymbol{x}$$ as shown by Alain and Bengio 2013 and Bengio et al. 2013c.  
     Provide yet another example of how useful properties can emerge as a byproduct of minimizing reconstruction error.  
     Also an example of how overcomplete high capacity models may be used as autoencoders so long as care is taken to prevent them from learning the identity function.  

    We introduce a corruption process $$C\tilde{\mathbf{x}} \vert \mathbf{x}$$ which represents a conditional distribution over corrupted samples $$\tilde{\boldsymbol{x}}$$ given a data sample $$\boldsymbol{x}$$.  
    The autoencoder then learns a reconstruction distribution $$p{\text {reconstruct }}\mathrm{x} \vert \tilde{\mathrm{x}}$$ estimated from training pairs $$\boldsymbol{x} \tilde{\boldsymbol{x}}$$ as follows  
    
     Sample a training example $$\boldsymbol{x}$$ from the training data.
     Sample a corrupted version $$\tilde{\boldsymbol{x}}$$ from $$C\tilde{\mathbf{x}} \vert \mathbf{x}=\boldsymbol{x}$$
     Use $$\boldsymbol{x} \tilde{\boldsymbol{x}}$$ as a training example for estimating the autoencoder reconstruction distribution $$p{\text {reconstruct }}\boldsymbol{x} \vert \tilde{\boldsymbol{x}}=p{\text {decoder }}\boldsymbol{x} \vert \boldsymbol{h}$$ with $$\boldsymbol{h}$$ the output of encoder $$f\tilde{\boldsymbol{x}}$$ and $$p{\text {decoder}}$$ typically defined by a decoder $$g\boldsymbol{h}$$.  

    Learning  
    Typically we can simply perform gradient based approximate minimization such as minibatch gradient descent on the negative log likelihood $$ \log p{\text {decoder }}\boldsymbol{x} \vert \boldsymbol{h}$$ So long as the encoder is deterministic the denoising autoencoder is a feedforward network and may be trained with exactly the same techniques as any other FFN.  
    We can therefore view the DAE as performing stochastic gradient descent on the following expectation  
    $$ \mathbb{E}{\mathbf{x} \sim \hat{p}{\text {data }}\mathbf{x}} \mathbb{E}{\tilde{\mathbf{x}} \sim C\tilde{\mathbf{x}} \vert \boldsymbol{x}} \log p{\text {decoder }}\boldsymbol{x} \vert \boldsymbol{h}=f\tilde{\boldsymbol{x}}$$  
    where $$\hat{p} {\text {data}}\mathrm{x}$$ is the training distribution.  


    Score Matching  Estimating the Score{ }  
    Score Matching Hyvärinen 2005 is an alternative to maximum likelihood. It provides a consistent estimator of probability distributions based on encouraging the model to have the same score as the data distribution at every training point $$\boldsymbol{x}$$. In this context the score is a particular gradient field    
    $$\nabla{\boldsymbol{x}} \log p\boldsymbol{x}$$   
    For autoencoders it is sufficient to understand that learning the gradient field of $$\log p{\text {data}}$$ is one way to learn the structure of $$p{\text {data itself}}$$.  

    A very important property of DAEs is that their training criterion with $$\text { conditionally Gaussian } p\boldsymbol{x} \vert \boldsymbol{h}$$ makes the autoencoder learn a vector field $$f\boldsymbol{x} \boldsymbol{x}$$ that estimates the score of the data distribution.  
    

    Continuous Valued $$\boldsymbol{x}$$  
    For continuous valued $$\boldsymbol{x}$$ the denoising criterion with Gaussian corruption and reconstruction distribution yields an estimator of the score that is applicable to general encoder and decoder parametrizations Alain and Bengio 2013.  
    This means a generic encoder decoder architecture may be made to estimate the score by training with the squared error criterion   
    $$\|gf\tilde{x} x\|^{2}$$  
    and corruption  
    $$C\tilde{\mathbf{x}}=\tilde{\boldsymbol{x}} \vert \boldsymbol{x}=\mathcal{N}\left\tilde{\boldsymbol{x}} ; \mu=\boldsymbol{x} \Sigma=\sigma^{2} I\right$$  
    with noise variance $$\sigma^{2}$$.  
    

    Guarantees  
    In general there is no guarantee that the reconstruction $$gf\boldsymbol{x}$$ minus the input $$\boldsymbol{x}$$ corresponds to the gradient of any function let alone to the score. That is why the early results Vincent 2011 are specialized to particular parametrizations where $$gf\boldsymbol{x} \boldsymbol{x}$$ may be obtained by taking the derivative of another function. Kamyshanska and Memisevic $$2015$$ generalized the results of Vincent $$2011$$ by identifying a family of shallow autoencoders such that $$gf\boldsymbol{x} \boldsymbol{x}$$ corresponds to a score for all members of the family.  


    DAEs as representing Probability Distributions and Variational AEs  
    So far we have described only how the DAE learns to represent a probability distribution. More generally one may want to use the autoencoder as a generative model and draw samples from this distribution. This is knows as the Variational Autoencoder.  


    Denoising AutoEncoders and RBMs  
    Denoising training of a specific kind of autoencoder sigmoidal hidden units linear reconstruction units using Gaussian noise and mean squared error as the reconstruction cost is equivalent Vincent 2011 to training a specific kind of undirected probabilistic model called an RBM with Gaussian visible units. This kind of model will be described in detail in section 20.5.1; for the present discussion it suffices to know that it is a model that provides an explicit $$p{\text {model }}\boldsymbol{x} ; \boldsymbol{\theta}$$. When the RBM is trained using denoising score matching Kingma and LeCun 2010 its learning algorithm is equivalent to denoising training in the corresponding autoencoder. With a fixed noise level regularized score matching is not a consistent estimator; it instead recovers a blurred version of the distribution. However if the noise level is chosen to approach $$0$$ when the number of examples approaches infinity then consistency is recovered. Denoising score matching is discussed in more detail in section 18.5.  
    Other connections between autoencoders and RBMs exist. Score matching applied to RBMs yields a cost function that is identical to reconstruction error combined with a regularization term similar to the contractive penalty of the CAE Swersky et al. 2011. Bengio and Delalleau 2009 showed that an autoencoder gradient provides an approximation to contrastive divergence training of RBMs.  


    Historical Perspective  
    

    


3. Regularizing by Penalizing Derivatives
    Another strategy for regularizing an autoencoder is to use a penalty $$\Omega$$ as in sparse autoencoders  
    $$L\boldsymbol{x} gf\boldsymbol{x}+\Omega\boldsymbol{h} \boldsymbol{x}$$  
    but with a different form of $$\Omega$$  
    $$\Omega\boldsymbol{h} \boldsymbol{x}=\lambda \sum{i}\left\|\nabla{\boldsymbol{x}} h{i}\right\|^{2}$$  

    This forces the model to learn a function that does not change much when $$\boldsymbol{x}$$ changes slightly. Because this penalty is applied only at training examples it forces the autoencoder to learn features that capture information about the training distribution.  
    An autoencoder regularized in this way is called a contractive autoencoder CAE. This approach has theoretical connections to denoising autoencoders manifold learning and probabilistic modeling.  
    


4. Contractive Autoencoders
    Contractive Autoencoders CAEs Rifai et al. 2011ab introduces an explicit regularizer on the code $$\boldsymbol{h}=\phi\boldsymbol{x}$$ encouraging the derivatives of $$\phi$$ to be as small as possible  
    $$\Omega\boldsymbol{h}=\lambda\left\|\frac{\partial \phi\boldsymbol{x}}{\partial \boldsymbol{x}}\right\| {F}^{2}$$  
    The penalty $$\Omega\boldsymbol{h}$$ is the squared Frobenius norm  sum of squared elements of the Jacobian matrix of partial derivatives associated with the encoder function.  

    Connection to Denoising Autoencoders  
    There is a connection between the denoising autoencoder and the contractive autoencoder Alain and Bengio 2013 showed that in the limit of small Gaussian input noise the denoising reconstruction error is equivalent to a contractive penalty on the reconstruction function that maps $$\boldsymbol{x}$$ to $$\boldsymbol{r}=\psi\phi\boldsymbol{x}$$.  
    In other words  
      Denoising Autoencoders make the reconstruction function resist small but finite sized perturbations of the input  
      Contractive Autoencoders make the feature extraction function resist infinitesimal perturbations of the input.  
    When using the Jacobian based contractive penalty to pretrain features $$\phi\boldsymbol{x}$$ for use with a classifier the best classification accuracy usually results from applying the contractive penalty to $$\phi\boldsymbol{x}$$ rather than to $$\psi\phi\boldsymbol{x}$$.  
    A contractive penalty on $$\phi\boldsymbol{x}$$ also has close connections to score matching.  

    Contractive  Definition and Analysis  
    The name contractive arises from the way that the CAE warps space. Specifically because the CAE is trained to resist perturbations of its input it is encouraged to map a neighborhood of input points to a smaller neighborhood of output points. We can think of this as contracting the input neighborhood to a smaller output neighborhood.  

    To clarify the CAE is contractive only locally all perturbations of a training point $$\boldsymbol{x}$$ are mapped near to $$\phi\boldsymbol{x}$$. Globally two different points $$\boldsymbol{x}$$ and $$\boldsymbol{x}^{\prime}$$ may be mapped to $$\phi\boldsymbol{x}$$ and $$\psi\left\boldsymbol{x}^{\prime}\right$$ points that are farther apart than the original points.  

    As a linear operator  
    We can think of the Jacobian matrix $$J$$ at a point $$x$$ as approximating the nonlinear encoder $$\phix$$ as being a linear operator. This allows us to use the word "contractive" more formally.  
    In the theory of linear operators a linear operator is said to be contractive if the norm of $$J x$$ remains less than or equal to 1 for all unit norm $$x$$. In other words $$J$$ is contractive if it shrinks the unit sphere.  
    We can think of the CAE as penalizing the Frobenius norm of the local linear approximation of $$\phix$$ at every training point $$x$$ in order to encourage each of these local linear operator to become a contraction.  


    Manifold Learning  
    Regularized autoencoders learn manifolds by balancing two opposing forces.  
    In the case of the CAE these two forces are reconstruction error and the contractive penalty $$\Omega\boldsymbol{h} .$$ Reconstruction error alone would encourage the CAE to learn an identity function. The contractive penalty alone would encourage the CAE to learn features that are constant with respect to $$\boldsymbol{x}$$.  
    The compromise between these two forces yields an autoencoder whose derivatives $$\frac{\partial f\boldsymbol{x}}{\partial \boldsymbol{x}}$$ are mostly tiny. Only a small number of hidden units corresponding to a small number of directions in the input may have significant derivatives.  

    The goal of the CAE is to learn the manifold structure of the data.  
    Directions $$x$$ with large $$J x$$ rapidly change $$h$$ so these are likely to be directions which approximate the tangent planes of the manifold.  
    Experiments by Rifai et al. 2011 ab show that training the CAE results in  
    
    1. Most singular values of $$J$$ dropping below $$1$$ in magnitude and therefore becoming contractive  
    2. However some singular values remain above $$1$$ because the reconstruction error penalty encourages the CAE to encode the directions with the most local variance.  

     The directions corresponding to the largest singular values are interpreted as the tangent directions that the contractive autoencoder has learned.  
     Visualizations of the experimentally obtained singular vectors do seem to correspond to meaningful transformations of the input image.  
        Since Ideally these tangent directions should correspond to real variations in the data.  
        For example a CAE applied to images should learn tangent vectors that show how the image changes as objects in the image gradually change pose.  
        



    Issues with Contractive Penalties  
    Although it is cheap to compute the CAE regularization criterion in the case of a single hidden layer autoencoder it becomes much more expensive in the case of deeper autoencoders.  
    The strategy followed by Rifai et al. 2011a is to  
      Separately train a series of single layer autoencoders each trained to reconstruct the previous autoencoder’s hidden layer.  
      The composition of these autoencoders then forms a deep autoencoder.  
     Because each layer was separately trained to be locally contractive the deep autoencoder is contractive as well.  
     The result is not the same as what would be obtained by jointly training the entire architecture with a penalty on the Jacobian of the deep model but it captures many of the desirable qualitative characteristics.  

    Another issue is that the contractive penalty can obtain useless results if we do not impose some sort of scale on the decoder.  
      For example the encoder could consist of multiplying the input by a small constant $$\epsilon$$ and the decoder could consist of dividing the code by $$\epsilon$$.  
    As $$\epsilon$$ approaches $$0$$ the encoder drives the contractive penalty $$\Omega\boldsymbol{h}$$ to approach $$0$$ without having learned anything about the distribution.  
    Meanwhile the decoder maintains perfect reconstruction.  
      In Rifai et al. 2011a this is prevented by tying the weights of $$\phi$$ and $$\psi$$. Both $$\phi$$ and $$\psi$$ are standard neural network layers consisting of an affine transformation followed by an element wise nonlinearity so it is straightforward to set the weight matrix of $$\psi$$ to be the transpose of the weight matrix of $$\phi$$.  



5. Predictive Sparse Decomposition
    



Learning Manifolds with Autoencoders


     onclick="showTextwithParentPopHideevent;"}





^1 This is different from the way we have previously used the word "prior" to refer to the distribution $$p\boldsymbol{\theta}$$ encoding our beliefs about the model's parameters before we have seen the training data.  


 Latent Variable Models









Latent Variable Models

1. Latent Variable Models
    Latent Variable Models are statistical models that relate a set of observable variables so called manifest variables to a set of latent variables.  

    Core Assumption  Local Independence  
    Local Independence  
    The observed items are conditionally independent of each other given an individual score on the latent variables. This means that the latent variable explains why the observed items are related to another.  

    In other words the targets/labels on the observations are the result of an individual's position on the latent variables and that the observations have nothing in common after controlling for the latent variable.  

    $$pAB\vert z = pA\vert z \times B\vert z$$  


    



    Methods for inferring Latent Variables  
    
     Hidden Markov models HMMs
     Factor analysis
     Principal component analysis PCA
     Partial least squares regression
     Latent semantic analysis and probabilistic latent semantic analysis
     EM algorithms
     Pseudo Marginal Metropolis Hastings algorithm
     Bayesian Methods LDA  




    Notes    
    
     Latent Variables encode  information about the data  
        e.g. in compression a 1 bit latent variable can encode if a face is Male/Female.  
     Data Projection  
        You "hypothesis" how the data might have been generated by LVs.  
        Then the LVs generate the data/observations.  
        



          


          
    




Linear Factor Models

1. Linear Factor Models
    Linear Factor Models are generative models that are the simplest class of latent variable models^1.  
    A linear factor model is defined by the use of a stochastic linear decoder function that generates $$\boldsymbol{x}$$ by adding noise to a linear transformation of $$\boldsymbol{h}$$.  


    Applications/Motivation  
    
     Building blocks of mixture models Hinton et al. 1995a; Ghahramani and Hinton 1996; Roweis et al. 2002   
     Building blocks of larger deep probabilistic models Tang et al. 2012  
     They also show many of the basic approaches necessary to build generative models that the more advanced deep models will extend further.  
     These models are interesting because they allow us to discover explanatory factors that have a simple joint distribution.  
     The simplicity of using a linear decoder made these models some of the  latent variable models to be extensively studied.  

    LFTs as Generative Models  
    Linear factor models are some of the simplest generative models and some of the simplest models that learn a representation of data.  

    Data Generation Process
    A linear factor model describes the data generation process as follows  
    
    1. Sample the explanatory factors $$\boldsymbol{h}$$ from a distribution  
        $$\mathbf{h} \sim p\boldsymbol{h} \tag{1}$$  
        where $$p\boldsymbol{h}$$ is a factorial distribution with $$p\boldsymbol{h}=\prod{i} p\lefth{i}\right$$ so that it is easy to sample from.  
    2. Sample the real valued observable variables given the factors  
        $$\boldsymbol{x}=\boldsymbol{W} \boldsymbol{h}+\boldsymbol{b}+ \text{ noise} \tag{2}$$  
        where the noise is typically Gaussian and diagonal independent across dimensions.  

    


2. Factor Analysis
    Probabilistic PCA principal components analysis Factor Analysis and other linear factor models are special cases of the above equations 1 and 2 and only differ in the choices made for the noise distribution and the model’s prior over latent variables $$\boldsymbol{h}$$ before observing $$\boldsymbol{x}$$.  

    Factor Analysis  
    In factor analysis Bartholomew 1987; Basilevsky 1994 the latent variable prior is just the unit variance Gaussian  
    $$\mathbf{h} \sim \mathcal{N}\boldsymbol{h} ; \mathbf{0} \boldsymbol{I}$$  
    while the observed variables $$xi$$ are assumed to be conditionally independent given $$\boldsymbol{h}$$.  
    Specifically the noise is assumed to be drawn from a diagonal covariance Gaussian distribution with covariance matrix $$\boldsymbol{\psi}=\operatorname{diag}\left\boldsymbol{\sigma}^{2}\right$$ with $$\boldsymbol{\sigma}^{2}=\left\sigma{1}^{2} \sigma{2}^{2} \ldots \sigma{n}^{2}\right^{\top}$$ a vector of per variable variances.  

    The role of the latent variables is thus to capture the dependencies between the different observed variables $$xi$$.  
    Indeed it can easily be shown that $$\boldsymbol{x}$$ is just a multivariate normal random variable with   
    $$\mathbf{x} \sim \mathcal{N}\left\boldsymbol{x} ; \boldsymbol{b} \boldsymbol{W} \boldsymbol{W}^{\top}+\boldsymbol{\psi}\right$$  


3. Probabilistic PCA
    Probabilistic PCA principal components analysis Factor Analysis and other linear factor models are special cases of the above equations 1 and 2 and only differ in the choices made for the noise distribution and the model’s prior over latent variables $$\boldsymbol{h}$$ before observing $$\boldsymbol{x}$$.  

     
         Addresses limitations of regular PCA
         PCA can be used as a general Gaussian density model in addition to reducing dimensions
         Maximum likelihood estimates can be computed for elements associated with principal components
         Captures dominant correlations with few parameters     
         Multiple PCA models can be combined as a probabilistic mixture
         Can be used as a base for Bayesian PCA  


    Probabilistic PCA  
    In order to cast PCA in a probabilistic framework we can make a slight modification to the factor analysis model making the conditional variances $$\sigmai^2$$ equal to each other.  
    In that case the covariance of $$\boldsymbol{x}$$ is just $$\boldsymbol{W} \boldsymbol{W}^{\top}+\sigma^{2} \boldsymbol{I}$$ where $$\sigma^2$$ is now a scalar.  
    This yields the conditional distribution  
    $$\mathbf{x} \sim \mathcal{N}\left\boldsymbol{x} ; \boldsymbol{b} \boldsymbol{W} \boldsymbol{W}^{\top}+\sigma^{2} \boldsymbol{I}\right$$  
    or equivalently  
    $$\mathbf{x}=\boldsymbol{W} \mathbf{h}+\boldsymbol{b}+\sigma \mathbf{z}$$  
    where $$\mathbf{z} \sim \mathcal{N}\boldsymbol{z} ; \mathbf{0} \boldsymbol{I}$$ is Gaussian noise.  

    Notice that $$\boldsymbol{b}$$ is the mean value over all data on the directions that are not captured/represented.  

    This probabilistic PCA model takes advantage of the observation that most variations in the data can be captured by the latent variables $$\boldsymbol{h}$$ up to some small residual reconstruction error $$\sigma^2$$.   
    
    Learning parameter estimation
    Tipping and Bishop 1999 then show an iterative EM algorithm for estimating the parameters $$\boldsymbol{W}$$ and $$\sigma^{2}$$.  

    Relation to PCA  Limit Analysis
    Tipping and Bishop 1999 show that probabilistic PCA becomes $$\mathrm{PCA}$$ as $$\sigma \rightarrow 0$$.  
    In that case the conditional expected value of $$\boldsymbol{h}$$ given $$\boldsymbol{x}$$ becomes an orthogonal projection of $$\boldsymbol{x}  \boldsymbol{b}$$  onto the space spanned by the $$d$$ columns of $$\boldsymbol{W}$$ like in PCA.  

    As $$\sigma \rightarrow 0$$ the density model defined by probabilistic PCA becomes very sharp around these $$d$$ dimensions spanned by the columns of $$\boldsymbol{W}$$.  
    This can make the model assign very low likelihood to the data if the data does not actually cluster near a hyperplane.  


    PPCA vs Factor Analysis  
    
     Covariance
         PPCA & PCA is covariant under rotation of the original data axes
         Factor analysis is covariant under component wise rescaling
     Principal components or factors
         PPCA different principal components axes can be found incrementally
         Factor analysis factors from a two factor model may not correspond to those from a one factor model


    Manifold Interpretation of PCA  
    Linear factor models including PCA and factor analysis can be interpreted as learning a manifold Hinton et al. 1997.  
    We can view PPCA as defining a thin pancake shaped region of high probability—a Gaussian distribution that is very narrow along some axes just as a pancake is very flat along its vertical axis but is elongated along other axes just as a pancake is wide along its horizontal axes.  
    

    PCA can be interpreted as aligning this pancake with a linear manifold in a higher dimensional space.  
    This interpretation applies not just to traditional PCA but also to any linear autoencoder that learns matrices $$\boldsymbol{W}$$ and $$\boldsymbol{V}$$ with the goal of making the reconstruction of $$x$$ lie as close to $$x$$ as possible  
    
     Let the Encoder be  
        $$\boldsymbol{h}=f\boldsymbol{x}=\boldsymbol{W}^{\top}\boldsymbol{x} \boldsymbol{\mu}$$  
        The encoder computes a low dimensional representation of $$h$$.  
     With the autoencoder view we have a decoder computing the reconstruction  
        $$\hat{\boldsymbol{x}}=g\boldsymbol{h}=\boldsymbol{b}+\boldsymbol{V} \boldsymbol{h}$$  
     The choices of linear encoder and decoder that minimize reconstruction error  
        $$\mathbb{E}\left\|\boldsymbol{x} \hat{\boldsymbol{x}}\|^{2}\right$$  
        correspond to $$\boldsymbol{V}=\boldsymbol{W} \boldsymbol{\mu}=\boldsymbol{b}=\mathbb{E}\boldsymbol{x}$$ and the columns of $$\boldsymbol{W}$$ form an orthonormal basis which spans the same subspace as the principal eigenvectors of the covariance matrix  
        $$\boldsymbol{C}=\mathbb{E}\left\boldsymbol{x} \boldsymbol{\mu}\boldsymbol{x} \boldsymbol{\mu}^{\top}\right$$  
     In the case of PCA the columns of $$\boldsymbol{W}$$ are these eigenvectors ordered by the magnitude of the corresponding eigenvalues which are all real and non negative.  
     Variances  
        One can also show that eigenvalue $$\lambda{i}$$ of $$\boldsymbol{C}$$ corresponds to the variance of $$x$$ in the direction of eigenvector $$\boldsymbol{v}^{i}$$.  
     Optimal Reconstruction  
         If $$\boldsymbol{x} \in \mathbb{R}^{D}$$ and $$\boldsymbol{h} \in \mathbb{R}^{d}$$ with $$d<D$$ then the optimal reconstruction error  choosing $$\mu b V$$ and $$W$$ as above is  
            $$\min \mathbb{E}\left\|\boldsymbol{x} \hat{\boldsymbol{x}}\|^{2}\right=\sum{i=d+1}^{D} \lambda{i}$$  
         Hence if the covariance has rank $$d$$ the eigenvalues $$\lambda{d+1}$$ to $$\lambda{D}$$ are $$0$$ and reconstruction error is $$0$.  
         Furthermore one can also show that the above solution can be obtained by maximizing the variances of the elements of $$\boldsymbol{h}$$ under orthogonal $$\boldsymbol{W}$$ instead of minimizing reconstruction error.  



    Notes  
    


     EM Algorithm for PCA is more advantageous than MLE closed form.  
     Mixtures of probabilistic PCAs can be defined and are a combination of local probabilistic PCA models.  
     PCA can be generalized to the nonlinear Autoencoders.  
     ICA can be generalized to a nonlinear generative model in which we use a nonlinear function $$f$$ to generate the observed data.  
    

4. Independent Component Analysis ICA

5. Slow Feature Analysis

6. Sparse Coding
    Sparse Coding Olshausen and Field 1996 is a linear factor model that has been heavily studied as an unsupervised feature learning and feature extraction mechanism.  
    In Sparse Coding the noise distribution is Gaussian noise with isotropic precision $$\beta$$  
    $$p\boldsymbol{x} \vert \boldsymbol{h}=\mathcal{N}\left\boldsymbol{x} ; \boldsymbol{W} \boldsymbol{h}+\boldsymbol{b} \frac{1}{\beta} \boldsymbol{I}\right$$  

    The latent variable prior $$p\boldsymbol{h}$$ is chosen to be one with sharp peaks near $$0$$.  
    Common choices include  
    
     factorized Laplace  
        $$p\lefth{i}\right=$ Laplace $\lefth{i} ; 0 \frac{2}{\lambda}\right=\frac{\lambda}{4} e^{ \frac{1}{2} \lambda\left|h{i}\right|}$$  
     factorized Student t distributions  
        $$p\lefth{i}\right \propto \frac{1}{\left1+\frac{h{i}^{2}}{\nu}\right^{\frac{\nu+1}{2}}}$$  
     Cauchy  

    Learning/Training  
    
     Training sparse coding with maximum likelihood is intractable
     Instead the training alternates between encoding the data and training the decoder to better reconstruct the data given the encoding.  
        This is a principled approximation to Maximum Likelihood/workfiles/research/dl/concepts/inference.  
         Minimization wrt. $$\boldsymbol{h}$$ 
         Minimization wrt. $$\boldsymbol{W}$$ 

    Architecture  
    
     Encoder  
         Non parametric.  
         It is an optimization algorithm that solves an optimization problem in which we seek the single most likely code value  
            $$\boldsymbol{h}^{ }=f\boldsymbol{x}=\underset{\boldsymbol{h}}{\arg \max } p\boldsymbol{h} vert \boldsymbol{x}$$   
             Assuming a Laplace Prior on $$p\boldsymbol{h}$$  
                $$\boldsymbol{h}^{ }=\underset{h}{\arg \min } \lambda\|\boldsymbol{h}\|{1}+\beta\|\boldsymbol{x} \boldsymbol{W h}\|{2}^{2}$$  
                where we have taken a log dropped terms not depending on $$\boldsymbol{h}$$ and divided by positive scaling factors to simplify the equation.  
             Hyperparameters  
                Both $$\beta$$ and $$\lambda$$ are hyperparameters.  
                However $$\beta$$ is usually set to $$1$$ because its role is shared with $$\lambda$$.  
                It could also be treated as a parameter of the model and "learned"^2.  


    Variations  
    Not all approaches to sparse coding explicitly build a $$p\boldsymbol{h}$$ and a $$p\boldsymbol{x} \vert \boldsymbol{h}$$.  
    Often we are just interested in learning a dictionary of features with activation values that will often be zero when extracted using this inference procedure.  


    Sparsity  
    
     Due to the imposition of an $$L^{1}$$ norm on $$\boldsymbol{h}$$ this procedure will yield a sparse $$\boldsymbol{h}^{ }$$.  
     If we sample $$\boldsymbol{h}$$ from a Laplace prior it is in fact a zero probability event for an element of $$\boldsymbol{h}$$ to actually be zero.  
        The generative model itself is not especially sparse only the feature extractor is.  
         Goodfellow et al. 2013d describe approximate inference in a different model family the spike and slab sparse coding model for which samples from the prior usually contain true zeros.  


    Properties  
    
     Advantages  
         The sparse coding approach combined with the use of the non parametric encoder  can in principle minimize the combination of reconstruction error and log prior better than any specific parametric encoder.  
         Another advantage is that there is no generalization error to the encoder.  
            Thus resulting in better generalization when sparse coding is used as a feature extractor for a classifier than when a parametric function is used to predict the code.  
             A parametric encoder must learn how to map $$\boldsymbol{x}$$ to $$\boldsymbol{h}$$ in a way that generalizes. For unusual $$\boldsymbol{x}$$ that do not resemble the training data a learned parametric encoder may fail to find an $$\boldsymbol{h}$$ that results in accurate reconstruction or a sparse code.  
             For the vast majority of formulations of sparse coding models where the inference problem is convex the optimization procedure will always find the optimal code unless degenerate cases such as replicated weight vectors occur.  
             Obviously the sparsity and reconstruction costs can still rise on unfamiliar points but this is due to generalization error in the decoder weights rather than generalization error in the encoder.  
             Thus the lack of generalization error in sparse coding’s optimization based encoding process may result in better generalization when sparse coding is used as a feature extractor for a classifier than when a parametric function is used to predict the code.  
                
                 Coates and Ng 2011 demonstrated that sparse coding features generalize better for object recognition tasks than the features of a related model based on a parametric encoder the linear sigmoid autoencoder.  
                 Goodfellow et al. 2013d showed that a variant of sparse coding generalizes better than other feature extractors in the regime where extremely few labels are available twenty or fewer labels per class.  

     Disadvantages  
         The primary disadvantage of the non parametric encoder is that it requires greater time to compute $$\boldsymbol{h}$$ given $$\boldsymbol{x}$$ because the non parametric approach requires running an iterative algorithm.  
             The parametric autoencoder approach uses only a fixed number of layers often only one.  
         It is not straight forward to back propagate through the non parametric encoder which makes it difficult to pretrain a sparse coding model with an unsupervised criterion and then fine tune it using a supervised criterion.  
             Modified versions of sparse coding that permit approximate derivatives do exist but are not widely used Bagnell and Bradley 2009.  

    Generation Sampling  
    
     Sparse coding like other linear factor models often produces poor samples.  
        

     This happens even when the model is able to reconstruct the data well and provide useful features for a classifier.  
         The reason is that each individual feature may be learned well but the factorial prior on the hidden code results in the model including random subsets of all of the features in each generated sample.  
    Motivating Deep Models  
        This motivates the development of deeper models that can impose a non factorial distribution on the deepest code layer as well as the development of more sophisticated shallow models.  

    Notes  
    

    




^1 Probabilistic Models with latent variables.  
^2 some terms depending on $$\beta$$ omitted from above equation\ which are needed to learn $$\beta$$.  


 Variational Auto Encoders



Resources  










Variational Auto Encoders


Auto Encoders generate Features that capture factors of variation in the training data.

0. Auto Regressive Models VS Variational Auto Encoders
       Auto Regressive Models defined a tractable discrete density function and then optimized the likelihood of training data   
       $$p\thetax = px0 \prod1^n pxi | x{i<}$$  
       On the other hand VAEs defines an intractable continuous density function with latent variable $$z$$  
       $$p\thetax = \int p\thetaz p\thetax|z dz$$
       but cannot optimize directly; instead derive and optimiz a lower bound on likelihood instead.  

1. Variational Auto Encoders VAEs
       Variational Autoencoder models inherit the autoencoder architecture but make strong assumptions concerning the distribution of latent variables.  
       They use variational approach for latent representation learning which results in an additional loss component and specific training algorithm called Stochastic Gradient Variational Bayes SGVB.  

2. Assumptions
       VAEs assume that 
         The data is generated by a directed graphical model $$px\vert z$$ 
         The encoder is learning an approximation $$q\phiz|x$$ to the posterior distribution $$p\thetaz|x$$  
            where $${\displaystyle \mathbf {\phi } }$$ and $${\displaystyle \mathbf {\theta } }$$ denote the parameters of the encoder recognition model and decoder generative model respectively.  
         The training data $$\left\{x^{i}\right\}{i=1}^N$$ is generated from underlying unobserved latent representation $$\mathbf{z}$$

3. The Objective Function
    $${\displaystyle {\mathcal {L}}\mathbf {\phi } \mathbf {\theta } \mathbf {x} =D{KL}q{\phi }\mathbf {z} |\mathbf {x} ||p{\theta }\mathbf {z}  \mathbb {E} {q{\phi }\mathbf {z} |\mathbf {x} }{\big }\log p{\theta }\mathbf {x} |\mathbf {z} {\big }}$$  
    where $${\displaystyle D{KL}}$$ is the Kullback Leibler divergence KL Div.  

    Notes  
    
     $$\boldsymbol{z}$$ is some latent vector representation; where each element is capturing how much of some factor of variation that we have in our training data.  
        e.g. attributes orientations position of certain objects etc.  

4. The Generation Process


5. The Goal
    The goal is to estimate the true parameters $$\theta^\ast$$ of this generative model.  
    

6. Representing the Model
      To represent the prior $$pz$$ we choose it to be simple usually Gaussian  
      To represent the conditional $$p{\theta^{}}\leftx | z^{i}\right$$  which is very complex we use a neural network  
    

7. Intractability
    The Data Likelihood  
    $$p\thetax = \int p\thetaz p\thetax|z dz$$  
    is intractable to compute for every $$z$$.  

    Thus the Posterior Density  
    $$p\thetaz|x = \dfrac{p\thetax|z p\thetaz}{p\thetax} = \dfrac{p\thetax|z p\thetaz}{\int p\thetaz p\thetax|z dz}$$   
    is also intractable.
    

8. Dealing with Intractability
    In addition to decoder network modeling $$p\thetax\vert z$$ define additional encoder network $$q\phiz\vert x$$ that approximates $$p\thetaz\vert x$$.  
    This allows us to derive a lower bound on the data likelihood that is tractable which we can optimize.
    

9. The Model
    The Encoder recognition/inference and Decoder generation networks are probabilistic and output means and variances of each the conditionals respectively  
    
    The generation forward pass is done via sampling as follows  


10.The Log Likelihood of Data
    Deriving the Log Likelihood  

11.Training
    Computing the bound forward pass for a given minibatch of input data  


12.Generation


      Diagonal prior on $$\boldsymbol{z} \implies$$ independent latent variables  
      Different dimensions of $$\boldsymbol{z}$$ encode interpretable factors of variation  
    
     Also good feature representation that can be computed using $$\mathrm{q} {\phi}\mathrm{z} \vert \mathrm{x}$$!  

    Examples  
     
     MNIST  

     CelebA  

13.Pros Cons and Research
     Pros  
         Principled approach to generative models
         Allows inference of $$qz\vert x$$ can be useful feature representation for other tasks  
     Cons  
         Maximizing the lower bound of likelihood is okay but not as good for evaluation as Auto regressive models
         Samples blurrier and lower quality compared to state of the art GANs  
     Active areas of research   
         More flexible approximations e.g. richer approximate posterior instead of diagonal Gaussian
         Incorporating structure in latent variables e.g. Categorical Distributions  


 CNNs  Convolutional Neural Networks



CNNs in CV/workfiles/research/dl/cnnx  
CNNs in NLP/workfiles/research/dl/nlp/cnnsNnlp  
CNNs Architectures/workfiles/research/dl/arcts  





Introduction

1. CNNs
    In machine learning a convolutional neural network CNN or ConvNet is a class of deep feed forward artificial neural networks that has successfully been applied to analyzing visual imagery.  
    In general it works on data that have grid like topology.  
    E.g. Time series data 1 d grid w/ samples at regular time intervals image data 2 d grid of pixels.  

    Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.  


2. The Big Idea
    CNNs use a variation of multilayer Perceptrons designed to require minimal preprocessing. In particular they use the Convolution Operation.   
    The Convolution leverage three important ideas that can help improve a machine learning system  
    1. Sparse Interactions/Connectivity/Weights  
        Unlike FNNs where every input unit is connected to every output unit CNNs have sparse interactions. This is accomplished by making the kernel smaller than the input.  
        Benefits   
         This means that we need to store fewer parameters which both  
             Reduces the memory requirements of the model and  
             Improves its statistical efficiency  
         Also Computing the output requires fewer operations  
         In deep CNNs the units in the deeper layers interact indirectly with large subsets of the input which allows modelling of complex interactions through sparse connections.  

        These improvements in efficiency are usually quite large.  
        If there are $$m$$ inputs and $$n$$ outputs then matrix multiplication requires $$m \times n$$ parameters and the algorithms used in practice have $$\mathcal{O}m \times n$$ runtime per example. If we limit the number of connections each output may have to $$k$$ then the sparsely connected approach requires only $$k \times n$$ parameters and $$\mathcal{O}k \times n$$ runtime.   
        
        
        

    2. Parameter Sharing   
        refers to using the same parameter for more than one function in a model.  

        Benefits  
        
         This means that rather than learning a separate set of parameters for every location we learn only one set of parameters.  
             This does not affect the runtime of forward propagation—it is still $$\mathcal{O}k \times n$$  
             But it does further reduce the storage requirements of the model to $$k$$ parameters $$k$$ is usually several orders of magnitude smaller than $$m$$  

        Convolution is thus dramatically more efficient than dense matrix multiplication in terms of the memory requirements and statistical efficiency.  
        

    3. Equivariant Representations  
        For convolutions the particular form of parameter sharing causes the layer to have a property called equivariance to translation.   
        A function is equivariant means that if the input changes the output changes in the same way.  
            Specifically a function $$fx$$ is equivariant to a function $$g$$ if $$fgx = gfx$$.   

        Thus if we move the object in the input its representation will move the same amount in the output.  
        
        Benefits  
          
         It is most useful when we know that some function of a small number of neighboring pixels is useful when applied to multiple input locations e.g. edge detection  
         Shifting the position of an object in the input doesn't confuse the NN  
         Robustness against translated inputs/images   

        Note Convolution is not naturally equivariant to some other transformations such as changes in the scale or rotation of an image.  


    Finally the convolution provides a means for working with inputs of variable sizes i.e. data that cannot be processed by neural networks defined by matrix multiplication with a fixed shape matrix.  

3. Inspiration Model
    Convolutional networks were inspired by biological processes in which the connectivity pattern between neurons is inspired by the organization of the animal visual cortex.  
    Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field. 



Architecture and Design


0. Design
    A CNN consists of an input and an output layer as well as multiple hidden layers.  
    The hidden layers of a CNN typically consist of convolutional layers pooling layers fully connected layers and normalization layers.  

1. Volumes of Neurons
    Unlike neurons in traditional Feed Forward networks the layers of a ConvNet have neurons arranged in 3 dimensions width height depth.  
    Note Depth here refers to the  dimension of an activation volume not to the depth of a full Neural Network which can refer to the total number of layers in a network.  

2. Connectivity
    The neurons in a layer will only be connected to a small region of the layer before it instead of all of the neurons in a fully connected manner.

3. Functionality
    A ConvNet is made up of Layers.  
    Every Layer has a simple API It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.  
    

    
4. Layers
    We use three main types of layers to build ConvNet architectures  
     Convolutional Layer  
         Convolution Linear Transformation   
         Activation Non Linear Transformation; e.g. ReLU  
            Known as Detector Stage  
     Pooling Layer  
     Fully Connected Layer  

41.Process
       ConvNets transform the original image layer by layer from the original pixel values to the final class scores. 

5. Example Architecture CIFAR 10
       Model INPUT  CONV  RELU  POOL  FC
        INPUT 32x32x3 will hold the raw pixel values of the image in this case an image of width 32 height 32 and with three color channels RGB.   
         CONV Layer will compute the output of neurons that are connected to local regions in the input each computing a dot product between their weights and a small region they are connected to in the input volume.    
        This may result in volume such as $$32\times32\times12$$ if we decided to use 12 filters.  
         RELU Layer  will apply an element wise activation function thresholding at zero. This leaves the size of the volume unchanged $$32\times32\times12$$.  
         POOL Layer will perform a down sampling operation along the spatial dimensions width height resulting in volume such as $$16\times16\times12$$.  
         Fully Connected will compute the class scores resulting in volume of size $$1\times1\times10$$ where each of the 10 numbers correspond to a class score such as among the 10 categories of CIFAR 10.  
        As with ordinary Neural Networks and as the name implies each neuron in this layer will be connected to all the numbers in the previous volume.

6. Fixed Functions VS Hyper Parameters
       Some layers contain parameters and other don’t.
        CONV/FC layers perform transformations that are a function of not only the activations in the input volume but also of the parameters the weights and biases of the neurons.
        RELU/POOL layers will implement a fixed function. 
    The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image.  


     A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume e.g. holding the class scores  
     There are a few distinct types of Layers e.g. CONV/FC/RELU/POOL are by far the most popular  
     Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function  
     Each Layer may or may not have parameters e.g. CONV/FC do RELU/POOL don’t  
     Each Layer may or may not have additional hyperparameters e.g. CONV/FC/POOL do RELU doesn’t  





The Convolutional Layer

1. Convolutions
    In its most general form the convolution is a Linear Operation on two functions of real valued arguments.  

    Mathematically a Convolution is a mathematical operation on two functions $$f$$ and $$g$$ to produce a  function that is typically viewed as a modified version of one of the original functions giving the integral of the point wise multiplication of the two functions as a function of the amount that one of the original functions is translated.  
    The convolution could be thought of as a weighting function e.g. for taking the weighted average of a series of numbers/function outputs.  

    The convolution of the continuous functions $$f$$ and $$g$$  
    $${\displaystyle {\begin{aligned}f  gt&\{\stackrel {\mathrm {def} }{=}}\ \int { \infty }^{\infty }f\tau gt \tau \d\tau &=\int{ \infty }^{\infty }ft \tau g\tau \d\tau .\end{aligned}}}$$  

    The convolution of the discreet functions f and g 
    $${\displaystyle {\begin{aligned}f  gn&=\sum{m= \infty }^{\infty }fmgn m&=\sum{m= \infty }^{\infty }fn mgm.\end{aligned}}} commutativity$$  
    In this notation we refer to  
     The function $$f$$ as the Input  
     The function $$g$$ as the Kernel/Filter  
     The output of the convolution as the Feature Map  

    Commutativity  
    Can be achieved by flipping the kernel with respect to the input; in the sense that as increases the index into the $$m$$ input increases but the index into the kernel decreases.  
    While the commutative property is useful for writing proofs it is not usually an important property of a neural network implementation.  
    Moreover in a CNN the convolution is used simultaneously with other functions and the combination of these functions does not commute regardless of whether the convolution operation flips its kernel or not.  
    Because convolutional networks usually use multichannel convolution the linear operations they are based on are not guaranteed to be commutative even if kernel flipping is used. These multichannel operations are only commutative if each operation has the same number of output channels as input channels.

                

2. Cross Correlation
       Cross Correlation is a measure of similarity of two series as a function of the displacement of one relative to the other.
       The continuous cross correlation on continuous functions f and g  
       $$f\star g\tau \ {\stackrel {\mathrm {def} }{=}}\int { \infty }^{\infty }f^{}t\ gt+\tau \dt$$
       The discrete cross correlation on discreet functions f and g  
       $$f\star gn\ {\stackrel {\mathrm {def} }{=}}\sum {m= \infty }^{\infty }f^{}m\ gm+n.$$  

3. Convolutions and Cross Correlation
     Convolution is similar to cross correlation.  
     For discrete real valued signals they differ only in a time reversal in one of the signals.  
     For continuous signals the cross correlation operator is the adjoint operator of the convolution operator.

4. CNNs Convolutions and Cross Correlation
    The term Convolution in the name "Convolution Neural Network" is unfortunately a misnomer.  
    CNNs actually use Cross Correlation instead as their similarity operator.  
    The term 'convolution' has stuck in the name by convention.  


15.Convolution in DL
    The Convolution operation  
    $$st=x  wt=\sum{a= \infty}^{\infty} xa wt a$$   
    we usually assume that these functions are zero everywhere but in the finite set of points for which we store the values.  


16.Convolution Over Two Axis
    If we use a two dimensional image $$I$$ as our input we probably also want to use a two dimensional kernel $$K$$  
    $$Si j=I  Ki j=\sum{m} \sum{n} Im n Ki m j n$$  

    In practice we use the following formula instead commutativity  
    $$Si j=K  Ii j=\sum{m} \sum{n} Ii m j n Km n$$  
    Usually the latter formula is more straightforward to implement in a machine learning library because there is less variation in the range of valid values of $$m$$ and $$n$$.  


    The Cross Correlation is usually implemented by ML libs  
    $$Si j=K  Ii j=\sum{m} \sum{n} Ii+m j+n Km n$$  


17.The Mathematics of the Convolution Operation
     The operation can be broken into matrix multiplications using the Toeplitz matrix representation for 1D and block circulant matrix for 2D convolution  
         Discrete convolution can be viewed as multiplication by a matrix but the matrix has several entries constrained to be equal to other entries.  
        For example for univariate discrete convolution each row of the matrix is constrained to be equal to the row above shifted by one element. This is known as a Toeplitz matrix.  
            A Toeplitz matrix has the property that values along all diagonals are constant.  

        
         In two dimensions a doubly block circulant matrix corresponds to convolution.  
            A matrix which is circulant with respect to its sub matrices is called a block circulant matrix. If each of the submatrices is itself circulant the matrix is called doubly block circulant matrix.  
                
     Convolution usually corresponds to a very sparse matrix a matrix whose entries are mostly equal to zero.  
        This is because the kernel is usually much smaller than the input image.  
     Any neural network algorithm that works with matrix multiplication and does not depend on specific properties of the matrix structure should work with convolution without requiring any further changes to the neural network.  
     Typical convolutional neural networks do make use of further specializations in order to deal with large inputs efficiently but these are not strictly necessary from a theoretical perspective.  


5. The Convolution operation in a CONV Layer
     The CONV layer’s parameters consist of a set of learnable filters.  
         Every filter is small spatially along width and height but extends through the full depth of the input volume.  
         For example a typical filter on a  layer of a ConvNet might have size 5x5x3 i.e. 5 pixels width and height and 3 because images have depth 3 the color channels.  
     In the forward pass we slide convolve each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position.  
         As we slide the filter over the width and height of the input volume we will produce a 2 dimensional activation map that gives the responses of that filter at every spatial position.  
        Intuitively the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the  layer or eventually entire honeycomb or wheel like patterns on higher layers of the network. 
         Now we will have an entire set of filters in each CONV layer e.g. 12 filters and each of them will produce a separate 2 dimensional activation map.   
     We will stack these activation maps along the depth dimension and produce the output volume.  

    As a result the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.     
    
6. The Brain Perspective
    Every entry in the 3D output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially.  

7. Local Connectivity
     Convolutional networks exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers 
         Each neuron is connected to only a small region of the input volume.
     The Receptive Field of the neuron defines the extent of this connectivity as a hyperparameter.  
     For example suppose the input volume has size $$32\times32\times3$$ and the receptive field or the filter size is $$5\times5$$ then each neuron in the Conv Layer will have weights to a $$5\times5\times3$$ region in the input volume for a total of $$553 = 75$$ weights and $$+1$$ bias parameter.  

    Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.

8. Spatial Arrangement
    There are three hyperparameters control the size of the output volume  
    1. The Depth of the output volume is a hyperparameter that corresponds to the number of filters we would like to use each learning to look for something different in the input.  
    2. The Stride controls how depth columns around the spatial dimensions width and height are allocated.  
        e.g. When the stride is 1 then we move the filters one pixel at a time.  

         The Smaller the stride the more overlapping regions exist and the bigger the volume.  
         The bigger the stride the less overlapping regions exist and the         smaller the volume.  

    3. The Padding is a hyperparameter whereby we pad the input the input volume with zeros around the border.   
        This allows to control the spatial size of the output volumes.  


9. The Spatial Size of the Output Volume
    We compute the spatial size of the output volume as a function of  
     $$W$$ The input volume size.  
     $$F$$ $$\\$$The receptive field size of the Conv Layer neurons.  
     $$S$$ The stride with which they are applied.  
     $$P$$ The amount of zero padding used on the border.  
    Thus the Total Size of the Output  
    $$\dfrac{W−F+2P}{S} + 1$$  

    Potential Issue If this number is not an integer then the strides are set incorrectly and the neurons cannot be tiled to fit across the input volume in a symmetric way.  
     Fix In general setting zero padding to be $${\displaystyle P = \dfrac{K 1}{2}}$$ when the stride is $${\displaystyle S = 1}$$ ensures that the input volume and output volume will have the same size spatially.  

10.Calculating the Number of Parameters
    Given  
     Input Volume  $$32\times32\times3$$  
     Filters  $$10 5\times5$$  
     Stride  $$1$$  
     Pad  $$2$$  
    
    The number of parameters equals the number of parameters in each filter $$ = 553 + 1 = 76$$ +1 for bias times the number of filters $$ 76  10 = 760$$.  
            


11.The Convolution Layer

    The Conv Layer and the Brain  


12.From FC layers to Conv layers


13.$$1\times1$$ Convolutions



0. Notes
     Summary  
         ConvNets stack CONVPOOLFC layers 
         Trend towards smaller filters and deeper architectures 
         Trend towards getting rid of POOL/FC layers just CONV 
         Typical architectures look like CONV RELU  N POOL?  M FC RELU  K SOFTMAX  
            where $$N$$ is usually up to \~5 $$M$$ is large $$0 <= K <= 2$$.  
            But recent advances such as ResNet/GoogLeNet challenge this paradigm 
     Effect of Different Biases  
        Separating the biases may slightly reduce the statistical efficiency of the model but it allows the model to correct for differences in the image statistics at different locations. For example when using implicit zero padding detector units at the edge of the image receive less total input and may need larger biases.  
     In the kinds of architectures typically used for classification of a single object in an image the greatest reduction in the spatial dimensions of the network comes from using pooling layers with large stride.  




The Pooling Layer

1. The Pooling Operation/Function
    The pooling function calculates a summary statistic of the nearby pixels at the point of operation.  
    Some common statistics are max mean weighted average and $$L^2$$ norm of a surrounding rectangular window.  


2. The Key Ideas/Properties
    In all cases pooling helps to make the representation approximately invariant to small translations of the input.   
    Invariance to translation means that if we translate the input by a small amount the values of most of the pooled outputs do not change.  
    Invariance to local translation can be a useful property if we care more about whether some feature is present than exactly where it is.  

    

    Learned Invariance to other transformations  
    Pooling over spatial regions produces invariance to translation but if we pool over the outputs of separately parametrized convolutions the features can learn which transformations to become invariant to.  
    This property has been used in Maxout networks.  
    


    For many tasks pooling is essential for handling inputs of varying size.  
    This is usually accomplished by varying the size of an offset between pooling regions so that the classification layer always receives the same number of summary statistics regardless of the input size. For example the final pooling layer of the network may be defined to output four sets of summary statistics one for each quadrant of an image regardless of the image size.  

    One can use fewer pooling units than detector units since they provide a summary; thus by reporting summary statistics for pooling regions spaced $$k$$ pixels apart rather than $$1$$ pixel apart we can improve the computational efficiency of the network because the next layer has roughly $$k$$ times fewer inputs to process.  
    This reduction in the input size can also result in improved statistical efficiency and reduced memory requirements for storing the parameters.  
    


3. Theoretical Guidelines for choosing the pooling function


4. Variations
    Dynamical Pooling  
    

    Learned Pooling  
    Another approach is to learn a single pooling structure that is then applied to all images Jia et al. 2012.  

5. Pooling and Top Down Architectures
    Pooling can complicate some kinds of neural network architectures that use top down information such as Boltzmann machines and autoencoders.  

6. The Pooling Layer summary





Nes
 Pooling Layer  
     Makes the representations smaller and more manageable
     Operates over each activation map independently i.e. preserves depth  
 You can use the stride instead of the pooling to downsample  


Convolution and Pooling as an Infinitely Strong Prior
            
1. A Prior Probability Distribution
    This is a probability distribution over the parameters of a model that encodes our beliefs about what models are reasonable before we have seen any data.  

2. What is a weight prior?
    Assumptions about the weights before learning in terms of acceptable values and range are encoded into the prior distribution of the weights.  
     A Weak Prior  has a high entropy and thus variance and shows that there is low confidence in the initial value of the weight.  
     A Strong Prior in turn has low entropy/variance and shows a narrow range of values about which we are confident before learning begins.  
     A Infinitely Strong Prior demarkets certain values as forbidden completely assigning them zero probability.  

3. Convolutional Layer as a FC Layer
    If we view the conv layer as a FC layer the  
     Convolution operation imposes an infinitely strong prior by making the following restrictions on the weights  
         Adjacent units must have the same weight but shifted in space.  
         Except for a small spatially connected region all other weights must be zero.  
     Pooling operation imposes an infinitely strong prior by  
         Requiring features to be Translation Invariant.   
        
        

4. Key Insights/Takeaways
     Convolution and pooling can cause underfitting if the priors imposed are not suitable for the task. When a task involves incorporating information from very distant locations in the input then the prior imposed by convolution may be inappropriate.  
    As an example consider this scenario. We may want to learn different features for different parts of an input. But the compulsion to used tied weights enforced by standard convolution on all parts of an image forces us to either compromise or use more kernels extract more features.  

     Convolutional models should only be compared with other convolutional models. This is because other models which are permutation invariant can learn even when input features are permuted thus loosing spatial relationships. Such models need to learn these spatial relationships which are hard coded in CNNs.

 

Variants of the Basic Convolution Function and Structured Outputs

1. Practical Considerations for Implementing the Convolution Function
     In general a convolution layer consists of application of several different kernels to the input. This allows the extraction of several different features at all locations in the input. This means that in each layer a single kernel filter isn’t applied. Multiple kernels filters usually a power of 2 are used as different feature detectors.  
     The input is generally not real valued but instead vector valued e.g. RGB values at each pixel or the feature values computed by the previous layer at each pixel position. Multi channel convolutions are commutative only if number of output and input channels is the same.  
     Strided Convolutions are a means to do DownSampling; they are used to reduce computational cost by calculating features at a coarser level. The effect of strided convolution is the same as that of a convolution followed by a downsampling stage. This can be used to reduce the representation size.  
        $$Z{i j k}=c\mathrm{K} \mathrm{V} s{i j k}=\sum{l m n}\leftV{lj 1 \times s+mk 1 \times s+n} K{i l m n}\right \tag{9.8}$$  
     Zero Padding is used to make output dimensions and kernel size independent i.e. to control the output dimension regardless of the size of the kernel. There are three types  
        1. Valid The output is computed only at places where the entire kernel lies inside the input. Essentially no zero padding is performed. For a kernel of size $$k$$ in any dimension the input shape of $$m$$ in the direction will become $$m k+1$$ in the output. This shrinkage restricts architecture depth.   
        2. Same The input is zero padded such that the spatial size of the input and output is same. Essentially for a dimension where kernel size is $$k$$ the input is padded by $$k 1$$ zeros in that dimension. Since the number of output units connected to border pixels is less than that for center pixels it may under represent border pixels.  
        3. Full The input is padded by enough zeros such that each input pixel is connected to the same number of output units.  
        The optimal amount of Zero Padding usually lies between "valid" and "same" convolution.  

     Locally Connected Layers/Unshared Convolution has the same connectivity graph as a convolution operation but without parameter sharing i.e. each output unit performs a linear operation on its neighbourhood but the parameters are not shared across output units..  
        This allows models to capture local connectivity while allowing different features to be computed at different spatial locations; at the expense of having a lot more parameters.      
        $$Z{i j k}=\sum{l m n}\leftV{l j+m 1 k+n 1} w{i j k l m n}\right \tag{9.9}$$  
        They're useful when we know that each feature should be a function of a small part of space but there is no reason to think that the same feature should occur across all of space.  
        For example if we want to tell if an image is a picture of a face we only need to look for the mouth in the bottom half of the image.  
     Tiled Convolution offers a middle ground between Convolution and locally connected layers. Rather than learning a separate set of weights at every spatial location it learns/uses a set of kernels that are cycled through as we move through space.  
        This means that immediately neighboring locations will have different filters as in a locally connected layer but the memory requirements for storing the parameters will increase only by a factor of the size of this set of kernels rather than by the size of the entire output feature map.  
        $$Z{i j k}=\sum{l m n} V{l j+m 1 k+n 1} K{i l m n j \% t+1 k \% t+1} \tag{9.10}$$  
     Max Pooling and Locally Connected Layers and Tiled Layers When max pooling operation is applied to locally connected layer or tiled convolution the model has the ability to become transformation invariant because adjacent filters have the freedom to learn a transformed version of the same feature.  
    This essentially similar to the property leveraged by pooling over channels rather than spatially.  
     Different Connections Besides locally connected layers and tiled convolution another extension can be to restrict the kernels to operate on certain input channels. One way to implement this is to connect the  m input channels to the  n output channels the next m input channels to the next n output channels and so on. This method decreases the number of parameters in the model without decreasing the number of output units.  
     Other Operations The following three operations—convolution backprop from output to weights and backprop from output to inputs—are sufficient to compute all the gradients needed to train any depth of feedforward convolutional network as well as to train convolutional networks with reconstruction functions based on the transpose of convolution. 
    See Goodfellow 2010 for a full derivation of the equations in the fully general multidimensional multiexample case.  
     Bias  Bias terms can be used in different ways in the convolution stage.  
         For locally connected layer and tiled convolution we can use a bias per output unit and kernel respectively.  
         In case of traditional convolution a single bias term per output channel is used.  
         If the input size is fixed a bias per output unit may be used to counter the effect of regional image statistics and smaller activations at the boundary due to zero padding.  
        
2. Structured Outputs
    Convolutional networks can be trained to output high dimensional structured output rather than just a classification score.  
    A good example is the task of image segmentation where each pixel needs to be associated with an object class.  
    Here the output is the same size spatially as the input. The model outputs a tensor $$S$$  where $$S{i j k}$$ is the probability that pixel $$jk$$ belongs to class $$i$$.  
     Problem One issue that often comes up is that the output plane can be smaller than the input plane.  
     Solutions  
         To produce an output map as the same size as the input map only same padded convolutions can be stacked.  
         Avoid Pooling Completely Jain et al. 2007  
         Emit a lower Resolution grid of labels Pinheiro and Collobert 2014 2015 
         Recurrent Convolutional Models The output of the  labelling stage can be refined successively by another convolutional model. If the models use tied parameters this gives rise to a type of recursive model.  
        

    




Distinguishing features


2. Image Features
       are certain quantities that are calculated from the image to better describe the information in the image and to reduce the size of the input vectors. 
        Examples  
             Color Histogram Compute a bucket based vector of colors with their respective amounts in the image.  
             Histogram of Oriented Gradients HOG we count the occurrences of gradient orientation in localized portions of the image.   
             Bag of Words a bag of visual words is a vector of occurrence counts of a vocabulary of local image features.  
                The visual words can be extracted using a clustering algorithm; K Means.  



 Neural Networks



Resources  








Interpretation of NNs  

 Schmidhuber was a pioneer for the view of "neural networks as programs" which is claimed in his blog post. As opposed to the "representation learning view" by Hinton Bengio and other people which is currently dominant in deep learning.   




Neural Architectures


2. Neural Architectures  Graphical and Probabilistic Properties

    
    
     FFN 
         Directed
         Acyclic
         ?
     MLP 
         Directed
         Acyclic
         Fully Connected Complete?
     CNN 
         Directed
         Acyclic
         ?
     RNN 
         Directed
         Cyclic
         ?
     Hopfield 
         Undirected
         Cyclic
         Complete
     Boltzmann Machine 
         Undirected
         Cyclic
         Complete
     RBM 
         Undirected
         Cyclic
         Bipartite
     Bayesian Networks 
         Directed
         Acyclic
         ?
     HMMs 
         Directed
         Acyclic
         ?
     MRF 
         Undirected
         Cyclic
         ?
     CRF 
         Undirected
         Cyclic
         ?
     DBN 
         Directed
         Acyclic
         ?
     GAN 
         ?
         ?
         Bipartite Complete


    Notes  
    

    

3. Neural Networks and Graphical Models
    Deep NNs as PGMs  
    You can view a deep neural network as a graphical model but here the CPDs are not probabilistic but are deterministic. Consider for example that the input to a neuron is $$\vec{x}$$ and the output of the neuron is $$y .$$ In the CPD for this neuron we have $$p\vec{x} y=1$$ and $$p\vec{x} \hat{y}=0$$ for $$\hat{y} \neq y .$$ Refer to the section 10.2 .3 of Deep Learning Book for more details.  
    

3. Neural Networks as Gaussian Processes


    We can think about the finite model as an approximation to a Gaussian process.  
    When we optimise our objective we minimise some "distance" KL divergence to be more exact between your model and the Gaussian process.  
    
    










 Hopfield Networks




















Hopfield Networks

1. Hopfield Networks
    Hopfield Networks are a form of recurrent artificial neural networks that serve as content addressable "associative" memory systems with binary threshold nodes.  

    They are energy models i.e. their properties derive from a global energy function.  
     A Hopfield net is composed of binary threshold units with recurrent connections between them.  

    Motivation  
    Recurrent networks of non linear units are generally very hard to analyze.  
    They can behave in many different ways  
    
     Settle to a stable state
     Oscillate
     Follow chaotic trajectories that cannot be predicted far into the future  

    Main Idea  
    John Hopfield realized that if the connections are symmetric there is a global energy function  
    
     Each binary "configuration" of the whole network has an energy.  
         Binary Configuration is an assignment of binary values to each neuron in the network.  
            Every neuron has a particular binary value in a configuration.  
     The binary threshold decision rule causes the network to settle to a minimum of this energy function.  
        The rule causes the network to go downhill in energy and by repeatedly applying the rule the network will end up in an energy minimum.  


    The Energy Function  
    
     The global energy is the sum of many contributions  
        $$E= \sum{i} s{i} b{i} \sum{i< j} s{i} s{j} w{i j}$$  
         Each contribution depends on  
             One connection weight $$w{i j}$$  
                A symmetric connection between two neurons; thus have the following restrictions  
                 $$w{i i}=0 \forall i$$ no unit has a connection with itself  
                 $$w{i j}=w{j i} \forall i j$$ connections are symmetric  
            and 
             The binary states of two neurons $$s{i}$$ and $$s{j}$$
                where $$s{j} \in \{ 1 1\}$$ or $$\in \{0 1\}$$ is the state of unit $$j$$ and $$\theta{j}$$ is the threshold of unit $$j$$.  
         To make up the following terms  
             The quadratic term $$s{i} s{j} \in \{ 1 1\}$$ involving the states of two units and  
             The bias term $$si bi \in \{ \theta \theta\}$$ involving the states of individual units.  
     This simple quadratic energy function makes it possible for each unit to compute locally how it's state affects the global energy  
        The Energy Gap is the difference in the global energy of the whole configuration depending on whether $$i$$ is on  
        $$\begin{align}
            \text{Energy Gap} &= \Delta E{i} 
              &= E\lefts{i}=0\right E\lefts{i}=1\right 
              &= b{i}+\sum{j} s{j} w{i j} 
            \end{align}
            $$  
        i.e. the difference between the energy when $$i$$ is on and the energy when $$i$$ is off.  
         The Energy Gap and the Binary Threshold Decision Rule  
             This difference energy gap is exactly what the binary threshold decision rule computes.  
             The Binary Decision Rule is the derivative of the energy gap wrt the state of the $$i$$ th unit $$si$$

    Settling to an Energy Minimum  
    To find an energy minimum in this net  
    
     Start from a random state then  
     Update units one at a time in random order  
         Update each unit to whichever of its two states gives the lowest global energy.  
            i.e. use binary threshold units.  

    A Deeper Energy Minimum  
    The net has two triangles in which the three units mostly support each other.  
    
     Each triangle mostly hates the other triangle.  
    The triangle on the left differs from the one on the right by having a weight of $$2$$ where the other one has a weight of $$3$$.  
     So turning on the units in the triangle on the right gives the deepest minimum.  

    Sequential Updating  Justification  
    
     If units make simultaneous decisions the energy could go up.  
     With simultaneous parallel updating we can get oscillations.  
         They always have a period of $$2$$ bi phasic oscillations.  
     If the updates occur in parallel but with random timing the oscillations are usually destroyed.  

    Using Energy Models with binary threshold rule for Storing Memories  
    
     Hopfield 1982 proposed that memories could be energy minima of a neural net w/ symmetric weights.  
         The binary threshold decision rule can then be used to "clean up" incomplete or corrupted memories.  
            Transforms partial memories to full memories.   
     The idea of memories as energy minima was proposed by I. A. Richards 1924 in "Principles of Literary Criticism".  
     Using energy minima to represent memories gives a content addressable "associative" memory  
         An item can be accessed by just knowing part of its content.  
             This was really amazing in the year 16 BG Before Google.  
         It is robust against hardware damage. 
         It's like reconstructing a dinosaur from a few bones.  
            Because you have an idea about how the bones are meant to fit together.  


    Storing memories in a Hopfield net  
    
     If we use activities of $$1$$ and $$ 1$$ we can store a binary state vector by incrementing the weight between any two units by the product of their activities.  
        $$\Delta w{i j}=s{i} s{j}$$   
         This is a very simple rule that is not error driven i.e. does not learn by correcting errors.  
            That is both its strength and its weakness  
             It is an online rule  
             It is not very efficient to store things  
         We treat biases as weights from a permanently on unit.    
     With states of $$0$$ and $$1$$ the rule is slightly more complicated  
        $$\Delta w{i j}=4\lefts{i} \frac{1}{2}\right\lefts{j} \frac{1}{2}\right$$  



    Summary  Big Ideas of Hopfield Networks  
    
     Idea #1 we can find a local energy minimum by using a network of symmetrically connected binary threshold units.  
     Idea #2 these local energy minima might correspond to memories.  


    Notes  
    
     They were responsible for resurgence of interest in Neural Networks in 1980s
     They can be used to store memories as distributed patterns of activity
     The constraint that weights are symmetric guarantees that the energy function decreases monotonically while following the activation rules  
     The Hopfield Network is a non linear dynamical system that converges to an attractor.  
    


2. Structure
    The Hopfield Network is formally described as a complete Undirected graph
    
     The Units  
        The units in a Hopfield Net are binary threshold units  
        i.e. the units only take on two different values for their states and the value is determined by whether or not the units' input exceeds their threshold.  
     The States  
        The state $$si$$ for unit $$i$$ take on values of $$1$$ or $$ 1$$  
        i.e. $$si \in \{ 1 1\}$$.  
     The Weights  
        Every pair of units $$i$$ and $$j$$ in a Hopfield network has a connection that is described by the connectivity weight $$w{i j}$$.   
         Symmetric Connections weights  
             The connections in a Hopfield net are constrained to be symmetric by making the following restrictions  
                 $$w{i i}=0 \forall i$$ no unit has a connection with itself  
                 $$w{i j}=w{j i} \forall i j$$ connections are symmetric  
             The constraint that weights are symmetric guarantees that the energy function decreases monotonically while following the activation rules.  
                A network with asymmetric weights may exhibit some periodic or chaotic behaviour; however Hopfield found that this behavior is confined to relatively small parts of the phase space and does not impair the network's ability to act as a content addressable associative memory system.  



3. Update Rule
    Updating one unit node in the graph simulating the artificial neuron in the Hopfield network is performed using the following rule  
    $$s{i} \leftarrow\left\{\begin{array}{ll}{+1} & {\text { if } \sum{j} w{i j} s{j} \geq \theta{i}}  { 1} & {\text { otherwise }}\end{array}\right.$$  

    Updates in the Hopfield network can be performed in two different ways  
    
     Synchronous All units are updated at the same time. This requires a central clock to the system in order to maintain synchronization.  
        This method is viewed by some as less realistic based on an absence of observed global clock influencing analogous biological or physical systems of interest.  

    Neural Attraction and Repulsion in state space  
    Neurons "attract or repel each other" in state space.  
    The weight between two units has a powerful impact upon the values of the neurons. Consider the connection weight $$w{ij}$$ between two neurons $$i$$ and $$j$$.  
    If $$w{{ij}}>0$$ the updating rule implies that  
    
     when $$s{j}=1$$ the contribution of $$j$$ in the weighted sum is positive. Thus $$s{i}$$ is pulled by $$j$$ towards its value $$s{i}=1$$
     when $$s{j}= 1$$ the contribution of $$j$$ in the weighted sum is negative. Then again $$s{i}$$ is pushed by $$j$$ towards its value $$s{i}= 1$$  
    
    Thus the values of neurons $$i$$ and $$j$$ will converge if the weight between them is positive.  
    Similarly they will diverge if the weight is negative.  
    

4. Energy
    Hopfield nets have a scalar value associated with each state of the network referred to as the "energy" $$E$$ of the network where  
    $$E= \frac{1}{2} \sum{i j} w{i j} s{i} s{j}+\sum{i} \theta{i} s{i}$$  
    

    This quantity is called "energy" because it either decreases or stays the same upon network units being updated.  
    Furthermore under repeated updating the network will eventually converge to a state which is a local minimum in the energy function.  
    Thus if a state is a local minimum in the energy function it is a stable state for the network.  
    
    Relation to Ising Models  
    

    

44.Initialization and Running
    Initialization of the Hopfield Networks is done by setting the values of the units to the desired start pattern.  
    Repeated updates are then performed until the network converges to an attractor pattern.  
    

55.Convergence
    The Hopfield Network converges to an attractor pattern describing a stable state of the network as a non linear dynamical systems.  
    
    Convergence is generally assured as Hopfield proved that the attractors of this nonlinear dynamical system are stable non periodic and non chaotic as in some other systems.  

    Therefore in the context of Hopfield Networks an attractor pattern is a final stable state a pattern that cannot change any value within it under updating.  
    


66.Training
     Training a Hopfield net involves lowering the energy of states that the net should "remember".  
     This allows the net to serve as a content addressable memory system 
        I.E. the network will converge to a "remembered" state if it is given only part of the state.  
     The net can be used to recover from a distorted input to the trained state that is most similar to that input.  
        This is called associative memory because it recovers memories on the basis of similarity.  
     Thus the network is properly trained when the energy of states which the network should remember are local minima.  
     Note that in contrast to Perceptron training the thresholds of the neurons are never updated.  
    


5. Learning Rules
    There are various different learning rules that can be used to store information in the memory of the Hopfield Network.  

    Desirable Properties  
    
     Local A learning rule is local if each weight is updated using information available to neurons on either side of the connection that is associated with that particular weight.  
     Incremental New patterns can be learned without using information from the old patterns that have been also used for training.  
        That is when a new pattern is used for training the new values for the weights only depend on the old values and on the new pattern.  

    These properties are desirable since a learning rule satisfying them is more biologically plausible.  
    For example since the human brain is always learning new concepts one can reason that human learning is incremental. A learning system that were not incremental would generally be trained only once with a huge batch of training data.  

    Hebbian Learning Rule  
    The Hebbian rule is both local and incremental.  
    For the Hopfield Networks it is implemented in the following manner when learning $$n$$ binary patterns  
    $$w{i j}=\frac{1}{n} \sum{\mu=1}^{n} \epsilon{i}^{\mu} \epsilon{j}^{\mu}$$  
    where $$\epsilon{i}^{\mu}$$ represents bit $$i$$ from pattern $$\mu$$.  

      If the bits corresponding to neurons $$i$$ and $$j$$ are equal in pattern $$\mu$$ then the product $$\epsilon{i}^{\mu} \epsilon{j}^{\mu}$$ will be positive.  
    This would in turn have a positive effect on the weight $$w{i j}$$ and the values of $$i$$ and $$j$$ will tend to become equal.  
      The opposite happens if the bits corresponding to neurons $$i$$ and $$j$$ are different.  

    The Storkey Learning Rule  
    This rule was introduced by Amos Storkey 1997 and is both local and incremental.  
    The weight matrix of an attractor neural network is said to follow the Storkey learning rule if it obeys  
    $$w{i j}^{\nu}=w{i j}^{\nu 1}+\frac{1}{n} \epsilon{i}^{\nu} \epsilon{j}^{\nu} \frac{1}{n} \epsilon{i}^{\nu} h{j i}^{\nu} \frac{1}{n} \epsilon{j}^{\nu} h{i j}^{\nu}$$  
    where $$h{i j}^{\nu}=\sum{k=1}^{n} \sum{i \neq k \neq j}^{n} w{i k}^{\nu 1} \epsilon{k}^{\nu}$$ is a form of local field at neuron $$i$$.  

    This learning rule is local since the synapses take into account only neurons at their sides.  


    Storkey vs Hebbian Learning Rules  
    Storkey showed that a Hopfield network trained using this rule has a greater capacity than a corresponding network trained using the Hebbian rule.  
    The Storkey rule makes use of more information from the patterns and weights than the generalized Hebbian rule due to the effect of the local field.  
    

6. Spurious Patterns
     Patterns that the network uses for training called retrieval states become attractors of the system.  
     Repeated updates would eventually lead to convergence to one of the retrieval states.  
     However sometimes the network will converge to spurious patterns different from the training patterns.  
     Spurious Patterns arise due to spurious minima.  
        The energy in these spurious patterns is also a local minimum  
         For each stored pattern $$x$$ the negation $$ x$$ is also a spurious pattern.  
         A spurious state can also be a linear combination of an odd number of retrieval states. For example when using $$3$$ patterns $$\mu{1} \mu{2} \mu{3}$$ one can get the following spurious state  
        $$\epsilon{i}^{\operatorname{mix}}=\pm \operatorname{sgn}\left \pm \epsilon{i}^{\mu{1}} \pm \epsilon{i}^{\mu{2}} \pm \epsilon{i}^{\mu{3}}\right$$  
     Spurious patterns that have an even number of states cannot exist since they might sum up to zero.  

     Spurious Patterns memories occur when two nearby energy minima combine to make a new minimum in the wrong place.  
     Physicists in trying to increase the capacity of Hopfield nets rediscovered the Perceptron convergence procedure.  
    

7. Capacity
    The Network capacity of the Hopfield network model is determined by neuron amounts and connections within a given network. Therefore the number of memories that are able to be stored is dependent on neurons and connections.  
    
    Capacity  
    
     It was shown that the recall accuracy between vectors and nodes was $$0.138$$ approximately $$138$$ vectors can be recalled from storage for every $$1000$$ nodes Hertz et al. 1991.  
     Using Hopfield's storage rule the capacity of a totally connected net with $$N$$ units is only about $$0.15N$$ memories  
         At $$N$$ bits per memory the total information stored is only $$0.15 N^{2}$$ bits.  
         This does not make efficient use of the bits required to store the weights.  
         It depends on a constant $$0.15$$ 
     Capacity Requirements for Efficient Storage 
         The net has $$N^{2}$$ weights and biases.  
         After storing $$M$$ memories each connection weight has an integer value in the range $$ M M$$.  
         So the number of bits required to Efficiently store the weights and biases is  
            $$ N^{2} \log 2 M+1$$  
         It scales logarithmically with the number of stored memories $$M$$.  

    Effects of Limited Capacity  
    
     Since the capacity of Hopfield Nets is limited to $$\approx 0.15N$$ it is evident that many mistakes will occur if one tries to store a large number of vectors.  
        When the Hopfield model does not recall the right pattern it is possible that an intrusion has taken place since semantically related items tend to confuse the individual and recollection of the wrong pattern occurs.  
        Therefore the Hopfield network model is shown to confuse one stored item with that of another upon retrieval.  
     Perfect recalls and high capacity $$>0.14$$ can be loaded in the network by Storkey learning method.  
        Ulterior models inspired by the Hopfield network were later devised to raise the storage limit and reduce the retrieval error rate with some being capable of one shot learning.  
        

    Spurious Minima Limit the Capacity  
    
     Each time we memorize a configuration we hope to create a new energy minimum.  
     The problem is if two nearby minima merge to create a minimum at an intermediate location^1  

         Then we would get a blend of them rather than individual memories.  
         The Merging of Nearby Minima limits the capacity of a Hopfield Net.  


    Avoiding Spurious Minima by Unlearning  
    Unlearning is a strategy proposed by Hopfield Feinstein and Palmer to avoid spurious minima.  
    It involves applying the opposite of the storage rule of the binary state the network settles to.  
    
    Strategy  
    
     Let the net settle from a random initial state and then do Unlearning.  
        Whatever binary state it settles to apply the opposite of the storage rule.  
        Starting from red merged minimum doing unlearning will produce the two separate minima  

     This will get rid of deep spurious minima and increase memory capacity.    
     The strategy was shown to work but with no good analysis.  

    Unlearning and Biological Dreaming  
    The question of why do we dream/what is the function of dreaming is a long standing question  
    
     When dreaming the state of the brain is extremely similar to the state of the brain when its awake; except its not driven by real input rather its driven by a relay station.  
     We dream for several hours a day yet we actually don't remember most if not all of our dreams at all.  

    Crick and Mitchison proposed unlearning as a model of what dreams are for  
    
     During the day we store a lot of things and get spurious minima.  
     At night we put the network brain in a random state settle to a minimum and then unlearn what we settled to.  
     The function of dreams is to get rid of those spurious minima.  
     That's why we don't remember them even though we dream for many hours unless we wake up during the dream.  
        I.E. We don't store our dreams
    
    Optimal Amount of Unlearning  
    From a mathematical pov we want to derive exactly how much unlearning we need to do.  
    Unlearning is part of the process of fitting a model to data and doing maximum likelihood fitting of that model then unlearning should automatically come out of fitting the model AND the amount of unlearning needed to be done.   
    Thus the solution is to derive unlearning as the right way to minimize some cost function where the cost function is "how well your network models the data that you saw during the day".  


    The Maximal Capacity of a given network architecture over all possible learning rules  
    
    Learning Rules that are able to saturate the Gardner Bound  
    A simple learning rule that is guaranteed to achieve this bound is the Perceptron Learning Algorithm PLA applied to each neuron independently.  
    However unlike the rule used in the Hopfield model PLA is a supervised rule that needs an explicit “error signal” in order to achieve the Gardner bound.  

    Increasing the Capacity of Hopfield Networks  
    Elizabeth Gardner showed that there was a much better storage rule that uses the full capacity of the weights  
    
     Instead of trying to store vectors in one shot cycle through the training set many times.  
     Use the Perceptron Learning Algorithm PLA to train each unit to have the correct state given the states of all the other units in that vector.  
     It loses the online learning property in the interest of more efficient storage.  
     Statisticians call this technique "pseudo likelihood".  
     Procedure Description  
         Set the network to the memory state you want to store
         Take each unit separately and check if this unit adopts the state we want for it given the states of all the other units 
             if it would you leave its incoming weights alone  
             if it wouldn't you change its incoming weights in the way specified by the PLA   
                Notice those will be integer changes to the weights  
         Repeat several times as needed.  
     Convergence  
         If there are too many memories the Perceptron Convergence Procedure won't converge  
         PLA converges only if there is a set of weights that will solve the problem   
            Assuming there is this is a much more efficient way to store memories.  
    

8. Hopfield Networks with Hidden Units
    We add some hidden units to the network with the goal of making the states of those hidden units represent an interpretation of the perceptual input that's shown on the visible units.  
    The idea is that the weights between units represent constraints on good interpretations and by finding a low energy state we find a good interpretation of the input data.  

    Different Computational Role for Hopfield Nets{ }  
    Instead of using the Hopfield Net to store memories we can use it to construct interpretations of sensory input where  
    
     The input is represented by the visible units.  
     The interpretation is represented by the states of the hidden units.  
     The Quality of the interpretations is represented by the negative Energy function.  





    Two Difficult Computational Issues  
    Using the states of the hidden units to represent an interpretation of the input raises two difficult issues  
    
     Search How do we avoid getting trapped in poor local minima of the energy function?  
        Poor minima represent sub optimal interpretations.  
     Learning How do we learn the weights on the connections to the hidden units? and between the hidden units?  
        Notice that there is no supervision in the problem.  


9. Stochastic Units to improve Search
    Adding noise helps systems escape local energy minima.  

    Noisy Networks Find Better Energy Minima  
    
     A Hopfield net always makes decisions that reduce the energy.
         This makes it impossible to escape from local minima.  
     We can use random noise to escape from poor minima.
         Start with a lot of noise so its easy to cross energy barriers.
         Slowly reduce the noise so that the system ends up in a deep minimum.  
            This is "simulated annealing" Kirkpatrick et.al. 1981.  

    Effects of Temperature on the Transition Probabilities  
    The temperature in a physical system or a simulated system with an Energy Function affects the transition probabilities.  
    
     High Temperature System  

         The probability of crossing barriers is high.  
            I.E. The probability of going uphill from $$B$$ to $$A$$ is lower than the probability of going downhill from $$A$$ to $$B$$; but not much lower.  
             In effect the temperature flattens the energy landscape.    
         So the Ratio of the Probabilities is low.  
            Thus  
             It is easy to cross barriers  
             It is hard to stay in a deep minimum once you've got there  
     Low Temperature System  

         The probability of crossing barriers is much smaller.  
            I.E. The probability of  going uphill from $$B$$ to $$A$$ is much lower than the probability of going downhill from $$A$$ to $$B$$.  
         So the Ratio of the Probabilities is much higher.  
            Thus  
             It is harder to cross barriers  
             It is easy to stay in a deep minimum once you've got there  
         Thus if we run the system long enough we expect all the particles to end up in $$B$$.  
            However if we run it at a low temperature it will take a very long time for particles to escape from $$A$$.  
         To increase the speed of convergence starting at a high temperature then gradually decreasing it is a good compromise.  

    Stochastic Binary Units  
    To inject noise in a Hopfield Net we replace the binary threshold units with binary binary stochastic units that make biased random decisions.  
      The Temperature controls the amount of noise.  
      Raising the noise level is equivalent to decreasing all the energy gaps between configurations.  
    $$p\lefts{i}=1\right=\frac{1}{1+e^{ \Delta E{i} / T}}$$  
     This is a normal logistic equation but with the energy gap scaled by a temperature $$T$$  
         High Temperature the exponential will be $$\approx 0$$ and $$p\lefts{i}=1\right= \dfrac{1}{2}$$.  
            I.E. the probability of a unit turning on is about a half.  
            It will be in its on and off states equally often.  
         Low Temperature depending on the sign of $$\Delta E{i}$$ the unit will become more firmly on or more firmly off.  
         Zero Temperature e.g. in Hopfield Nets the sign of $$\Delta E{i}$$ determines whether RHS is $$0$$ or $$1$$.  
            I.E. the unit will behave deterministically; a standard binary threshold unit that will always adopt whichever of the two states gives the lowest energy.  
     Boltzmann Machines use stochastic binary units with temperature $$T=1$$ i.e. standard logistic equation.    


    Thermal Equilibrium at a fixed temperature $$T=1$$  
    
     Thermal Equilibrium does not mean that the system has settled down into the lowest energy configuration.  
        I.E. not the states of the individual units that settle down.  
        The individual units still rattle around at Equilibrium unless the temperature is zero $$T=0$$.  
     What settles down is the probability distribution over configurations.  
         It settles to the stationary distribution.  
             The stationary distribution is determined by the energy function of the system.  
             In the stationary distribution the probability of any configuration is $$\propto e^{ E}$$.  
     Intuitive Interpretation of Thermal Equilibrium  
         Imagine a huge ensemble of systems that all have exactly the same energy function.  
         The probability of a configuration is just the fraction of the systems that have that configuration.  
     Approaching Thermal Equilibrium  
         Start with any distribution we like over all the identical systems.  
             We could start with all the systems in the same configuration Dirac distribution.  
             Or with an equal number of systems in each possible configuration uniform distribution.  
         Then we keep applying our stochastic update rule to pick the next configuration for each individual system.  
         After running the systems stochastically in the right way we may eventually reach a situation where the fraction of systems in each configuration remains constant.  
             This is the stationary distribution that physicists call thermal equilibrium.  
             Any given system keeps changing its configuration but the fraction of systems in each configuration does not change.  
     Analogy  
        
         Imagine a casino in Las Vegas that is full of card dealers we need many more than $$52!$$ of them.  
         We start with all the card packs in standard order and then the dealers all start shuffling their packs.  
             After a few shuffling steps the king of spades. still has a good chance of being next to the queen of spades. The packs have not yet forgotten where they stated.  
             After prolonged shuffling the packs will have forgotten where they! started. There will be an equal number of packs in each of the $$52!$$ possible orders.  
             Once equilibrium has been reached the number of packs that leave a configuration at each time step will be equal to the number that enter the configuration.  
         The only thing wrong with this analogy is that all the configurations have equal energy so they all end up with the same probability.  
             We are generally interested in reaching equilibrium for systems where certain configurations have lower energy than others.  



    As Boltzmann Machines  
    Boltzmann Machines are just Stochastic Hopfield Nets with Hidden Units.  
    





How a Boltzmann machine models a set of binary data vectors  
Why model a set of binary data vectors and what we could do with such a model if we had it
The probabilities assigned to binary data vectors are determined by the weights in a Boltzmann machine

BMs are good at modeling binary data 

Modeling Binary Data  
Given a training set of binary vectors fit a model that will assign a probability to every possible binary vector.  
 This is useful for deciding if other binary vectors come from the same distribution e.g. documents represented by binary features that represents the occurrence of a particular word.  
 It can be used for monitoring complex systems to detect unusual behavior.  
 If we have models of several different distributions it can be used to compute the posterior probability that a particular distribution produced the observed data  
    $$p\text {Model} i | \text { data }=\dfrac{p\text {data} | \text {Model} i}{\sum{j} p\text {data} | \text {Model} j}$$  


Models for Generating Data  
There are different kinds of models to generate data  

 Causal Models  
 Energy based Models  



How a Causal Model Generates Data  

 In a Causal Model we generate data in two sequential steps  
      pick the hidden states from their prior distribution.  
        in causal models often independent in the prior.  
     Then pick the visible states from their conditional distribution given the hidden states.  

 The probability of generating a visible vector $$\mathrm{v}$$ is computed by summing over all possible hidden states. Each hidden state is an 'explanation" of $$\mathrm{v}$$  
    $$p\boldsymbol{v}=\sum{\boldsymbol{h}} p\boldsymbol{h} p\boldsymbol{v} | \boldsymbol{h}$$  

> Generating a binary vector  generate the states of some latent variables and then use the latent variables to generate the binary vector.  


How a Boltzmann Machine Generates Data  

 It is not a causal generative model.  
 Instead everything is defined in terms of the energies of joint configurations of the visible and hidden units. 
 The energies of joint configurations are related to their probabilities in two ways  
     We can simply define the probability to be  
        $$p\boldsymbol{v} \boldsymbol{h} \propto e^{ E\boldsymbol{v} \boldsymbol{h}}$$  
     Alternatively we can define the probability to be the probability of finding the network in that joint configuration after we have updated all of the stochastic binary units many times until thermal equilibrium.  

    These two definitions agree  analysis below.  
 The Energy of a joint configuration  
    $$\begin{align}
        E\boldsymbol{v} \boldsymbol{h} &=  \sum{i \in v{i s}} v{i} b{i} \sum{k \in h{i d}} h{k} b{k} \sum{i< j} v{i} v{j} w{i j} \sum{i k} v{i} h{k} w{i k} \sum{k< l} h{k} h{l} w{k l} 
        &=  \boldsymbol{v}^{\top} \boldsymbol{R} \boldsymbol{v} \boldsymbol{v}^{\top} \boldsymbol{W} \boldsymbol{h} \boldsymbol{h}^{\top} \boldsymbol{S} \boldsymbol{h} \boldsymbol{b}^{\top} \boldsymbol{v} \boldsymbol{c}^{\top} \boldsymbol{h}   
        \end{align}
        $$  
    where $$v{i} b{i}$$ binary state of unit $$i$$ in $$\boldsymbol{v}$$ $$hk bk$$ is the bias of unit $$k$$ $$i< j$$ indexes every non identical pair of $$i$$ and $$j$$ once avoid self interactions and double counting and $$w{i k}$$ is the weight between visible unit $$i$$ and hidden unit $$k$$.  
 Using Energies to define Probabilities  
     The probability of a joint configuration over both visible and hidden units depends on the energy of that joint configuration compared with the energy of all other joint configurations  
        $$p\boldsymbol{v} \boldsymbol{h}=\dfrac{e^{ E\boldsymbol{v} \boldsymbol{h}}}{\sum{\boldsymbol{u} \boldsymbol{g}} e^{ E\boldsymbol{u} \boldsymbol{g}}}$$  
     The probability of a configuration of the visible units is the sum of the probabilities of all the joint configurations that contain it  
        $$p\boldsymbol{v}=\dfrac{\sum{\boldsymbol{h}} e^{ E\boldsymbol{v} \boldsymbol{h}}}{\sum{\boldsymbol{u} \boldsymbol{g}} e^{ E\boldsymbol{u} \boldsymbol{g}}}$$  

    where the denomenators are the partition function $$Z$$.  
     

 Sampling from the Model  
     If there are more than a few hidden units we cannot compute the normalizing term the partition function because it has exponentially many terms i.e. intractable.  
     So we use Markov Chain Monte Carlo to get samples from the model starting from a random global configuration  
         Keep picking units at random and allowing them to stochastically update their states based on their energy gaps.  
     Run the Markov chain until it reaches its stationary distribution thermal equilibrium at a temperature of $$1$$.  
         The probability of a global configuration is then related to its energy by the Boltzmann Distribution   
            $$p\mathbf{v} \mathbf{h} \propto e^{ E\mathbf{v} \mathbf{h}}$$  
 Sampling from the Posterior distribution over hidden configurations for a given Data vector  
     The number of possible hidden configurations is exponential so we need MCMC to sample from the posterior.  
         It is just the same as getting a sample from the model except that we keep the visible units clamped to the given data vector.  
            I.E. Only the hidden units are allowed to change states updated  
     Samples from the posterior are required for learning the weights.  
        Each hidden configuration is an "explanation" of an observed visible configuration.  
        Better explanations have lower energy.  


The Goal of Learning  
We want to maximize the product of the probabilities sum of log probabilities that the Boltzmann Machine assigns to the binary vectors in the training set.  
This is Equivalent to maximizing the probability of obtaining exactly $$N$$ training cases if we ran the BM as follows  

 For $$i$$ in $$1 \ldots N$$  
     Run the network with no external input and let it settle to its stationary distribution    
     Sample the visible vector  

Possible Difficulty in Learning  Global Information  
Consider a chain of units with visible units at the ends  

If the training set is $$10$$ and $$01$$ we want the product of all the weights to be negative.  
So to know how to change w1 or w5 we must know w3.  

Learning with Local Information  
A very surprising fact is the following  
Everything that one weight needs to know about the other weights and the data is contained in the difference of two correlations.  
The derivative of log probability of one training vector wrt. one weight $$w{ij}$$  
$$\dfrac{\partial \log p\mathbf{v}}{\partial w{i j}}=\left\langle s{i} s{j}\right\rangle{\mathbf{v}} \left\langle s{i} s{j}\right\rangle{\text {free}}$$  
where  

 $$\left\langle s{i} s{j}\right\rangle{\mathbf{v}}$$ is the expected value of product of states at thermal equilibrium when the training vector is clamped on the visible units.  
    This is the positive phase
     Effect Raise the weights in proportion to the product of the activities the units have when you are presenting data.  
     Interpretation similar to the storage term for a Hopfield Net.   
        It is a Hebbian Learning Rule
 $$\left\langle s{i} s{j}\right\rangle{\text {free}}$$ is the expected value of product of states at thermal equilibrium when nothing is clamped.  
    This is the negative phase
     Effect Reduce the weights in proportion to "how often the two units are on together when sampling from the models distribution".  
     Interpretation similar to the Unlearning term i.e. the opposite of the storage rule for avoiding getting rid of spurious minima.  
        Moreover this rule specifies the exact amount of unlearning to be applied.  

So the change in the weight is proportional to the expected product of the activities averaged over all visible vectors in the training set that's what we call data MINUS the product of the same two activities when there is no clamping and the network has reached thermal equilibrium with no external interference  
$$\Delta w{i j} \propto\left\langle s{i} s{j}\right\rangle{\text{data}} \left\langle s{i} s{j}\right\rangle{\text{model}}$$  

Thus the learning algorithm only requires local information.  


Effects of the Positive and Negative Phases of Learning  
Given the probability of a training data vector $$\boldsymbol{v}$$  
$$p\boldsymbol{v}=\dfrac{\sum{\boldsymbol{h}} e^{ E\boldsymbol{v} \boldsymbol{h}}}{\sum{\boldsymbol{u}} \sum{\boldsymbol{g}} e^{ E\boldsymbol{u} \boldsymbol{g}}}$$   
and the log probability  
$$\begin{align}
    \log p\boldsymbol{v} &= \log \left\dfrac{\sum{\boldsymbol{h}} e^{ E\boldsymbol{v} \boldsymbol{h}}}{\sum{\boldsymbol{u}} \sum{\boldsymbol{g}} e^{ E\boldsymbol{u} \boldsymbol{g}}}\right 
    &= \log \left\sum{\boldsymbol{h}} e^{ E\boldsymbol{v} \boldsymbol{h}}\right  \log \left\sum{\boldsymbol{u}} \sum{\boldsymbol{g}} e^{ E\boldsymbol{u} \boldsymbol{g}}\right 
    &= \left \sum{\boldsymbol{h}} \log e^{ E\boldsymbol{v} \boldsymbol{h}}\right  \left\sum{\boldsymbol{u}} \sum{\boldsymbol{g}} \log e^{ E\boldsymbol{u} \boldsymbol{g}}\right 
    &= \left\sum{\boldsymbol{h}}  E\boldsymbol{v} \boldsymbol{h}\right  \left\sum{\boldsymbol{u}} \sum{\boldsymbol{g}}  E\boldsymbol{u} \boldsymbol{g}\right
    \end{align}
    $$   

 Positive Phase   
     The  term is decreasing the energy of terms in that sum that are already large. 
         It finds those terms by settling to thermal equilibrium with the vector $$\boldsymbol{v}$$ clamped so they can find an $$\boldsymbol{h}$$ that produces a low energy with $$\boldsymbol{v}$$.  
     Having sampled those vectors $$\boldsymbol{h}$$ it then changes the weights to make that energy even lower.  
     Summary  
        The positive phase finds hidden configurations that work well with $$\boldsymbol{v}$$ and lowers their energies.  
 Negative Phase  
     The  term is doing the same thing but for the partition function.  
        It's finding global configurations combinations of visible and hidden states that give low energy and therefore are large contributors to the partition function.   
     Having found those global configurations $$\boldsymbol{v}' \boldsymbol{h}'$$ it tries to raise their energy so that they contribute less.    
     Summary  
        The negative phase finds the joint configurations that are the best competitors and raises their energies.  

Thus the positive term is making the top term in $$p\boldsymbol{v}$$ bigger and the negative term is making the bottom term in $$p\boldsymbol{v}$$ smaller.  


If we only use the Hebbian rule positive phase without the unlearning term negative phase the synapse strengths will keep getting stronger and stronger the weights will all become very positive and the whole system will blow up.  
The Unlearning counteracts the positive phase's tendency to just add a large constant to the unnormalized probability everywhere.


Learning Rule Justification  Why the Derivative is so simple  

 The probability of a global configuration at thermal equilibrium is an exponential function of its energy.  
     Thus settling to equilibrium makes the log probability a linear function of the energy.  
 The energy is a linear function of the weights and states  
    $$\dfrac{\partial E}{\partial w{i j}}=s{i} s{j}$$  
    It is a log linear model.  
    This an important fact because we are trying to manipulate the log probabilities by manipulating the weights.  
 The process of settling to thermal equilibrium propagates information about the weights.  
     We don't need an explicit back propagation stage.  
     We still need two stages  
        1. Settle with the data
        2. Settle with NO data 
     However the network behaves basically in the same way in the two phases^1; while the forward and backprop stages are very different.  

The Batch Learning Algorithm  An inefficient way to collect the Learning Statistics  

 Positive phase  
     Clamp a data vector on the visible units.
     Let the hidden units reach thermal equilibrium at a temperature of 1 may use annealing to speed this up  
        by updating the hidden units one at a time.  
     Sample $$\left\langle s{i} s{j}\right\rangle$$ for all pairs of units
     Repeat for all data vectors in the training set.
 Negative phase  
     Do not clamp any of the units
     Set all the units to random binary states.
     Let the whole network reach thermal equilibrium at a temperature of 1 by updating all the units one at a time.  
         Difficulty where do we start? 
     Sample $$\left\langle s{i} s{j}\right\rangle$$ for all pairs of units
     Repeat many times to get good estimates  
         Difficulty how many times? especially w/ multiple modes 
 Weight updates  
     Update each weight by an amount proportional to the difference in $$\left\langle s{i} s{j}\right\rangle$$ in the two phases.  




^1 The state space is the corners of a hypercube. Showing it as a $$1 D$$ continuous space is a misrepresentation.  


 Recurrent Neural Networks  Deep Learning Book Ch.10



RNNs in NLP/workfiles/research/dl/nlp/rnns  
RNNs in CV/workfiles/research/dl/rnnscv  






Introduction

1. Recurrent Neural Networks
    Recurrent Neural Networks RNNs are a family of neural networks for processing sequential data.  

    In an RNN the connections between units form a directed cycle allowing it to exhibit dynamic temporal behavior.  

    The standard RNN is a nonlinear dynamical system that maps sequences to sequences.  


2. Big Idea
    RNNs share parameters across different positions/index of time/time steps of the sequence which allows it to generalize well to examples of different sequence length.  
     Such sharing is particularly important when a specific piece of information can occur at multiple positions within the sequence.  

    A related idea is the use of convolution across a 1 D temporal sequence time delay NNs. This convolution operation allows the network to share parameters across time but is shallow.  
    The output of convolution is a sequence where each member of the output is a function of a small number of neighboring members of the input.  


3. Dynamical Systems
    Classical Form of a Dynamical System  
    $$\boldsymbol{s}^{t}=f\left\boldsymbol{s}^{t 1} ; \boldsymbol{\theta}\right \tag{10.1}$$  
    where $$\boldsymbol{s}^{t}$$  is called the state of the system.  


    A Dynamical System driven by an external signal $$\boldsymbol{x}^{t}$$  
    $$\boldsymbol{s}^{t}=f\left\boldsymbol{s}^{t 1} \boldsymbol{x}^{t} ; \boldsymbol{\theta}\right \tag{10.4}$$  
    the state now contains information about the whole past sequence.   

    Basically any function containing recurrence can be considered an RNN.  

    The RNN Equation as a Dynamical System  
    $$\boldsymbol{h}^{t}=f\left\boldsymbol{h}^{t 1} \boldsymbol{x}^{t} ; \boldsymbol{\theta}\right \tag{10.5}$$  
    where the variable $$\mathbf{h}$$ represents the state.  



4. Unfolding the Computation Graph
    Unfolding maps the left to the right in the figure below from figure 10.2 both are computational graphs of a RNN without output $$\mathbf{o}$$  
    where the black square indicates that an interaction takes place with a delay of $$1$$ time step from the state at time $$t$$  to the state at time $$t + 1$$.  

    We can represent the unfolded recurrence after $$t$$ steps with a function $$g^{t}$$  
    $$\begin{aligned} \boldsymbol{h}^{t} &=g^{t}\left\boldsymbol{x}^{t} \boldsymbol{x}^{t 1} \boldsymbol{x}^{t 2} \ldots \boldsymbol{x}^{2} \boldsymbol{x}^{1}\right  &=f\left\boldsymbol{h}^{t 1} \boldsymbol{x}^{t} ; \boldsymbol{\theta}\right \end{aligned}$$  
    The function $$g^{t}$$ takes the whole past sequence $$\left\boldsymbol{x}^{t} \boldsymbol{x}^{t 1} \boldsymbol{x}^{t 2} \ldots \boldsymbol{x}^{2} \boldsymbol{x}^{1}\right$$ as input and produces the current state but the unfolded recurrent structure allows us to factorize $$g^{t}$$ into repeated applications of a function $$f$$.  
    
    The unfolding process thus introduces two major advantages  
    
    1. Regardless of the sequence length the learned model always has the same input size.  
        Because it is specified in terms of transition from one state to another state rather than specified in terms of a variable length history of states.  
    2. It is possible to use the same transition function $$f$$ with the same parameters at every time step.  
    Thus we can learn a single shared model $$f$$ that operates on all time steps and all sequence lengths rather than needing to learn a separate model $$g^{t}$$ for all possible time steps  

    Benefits   
    
     Allows generalization to sequence lengths that did not appear in the training set
     Enables the model to be estimated to be estimated with far fewer training examples than would be required without parameter sharing.  
    

5. The State of the RNN $$\mathbf{h}^{t}$$
    The network typically learns to use $$\mathbf{h}^{t}$$ as a kind of lossy summary of the task relevant aspects of the past sequence of inputs up to $$t$$.  
    This summary is in general necessarily lossy since it maps an arbitrary length sequence $$\left\boldsymbol{x}^{t} \boldsymbol{x}^{t 1} \boldsymbol{x}^{t 2} \ldots \boldsymbol{x}^{2} \boldsymbol{x}^{1}\right$$  to a fixed length vector $$h^{t}$$.  

    The most demanding situation the extreme is when we ask $$h^{t}$$ to be rich enough to allow one to approximately recover/reconstruct the input sequence as in AutoEncoders.  

6. RNN Architectures/Design Patterns
    We will be introducing three variations of the RNN and will be analyzing variation 1 the basic form of the RNN.  
    1. Variation 1; The Standard RNN basic form{ }  
        
         Architecture  
             Produces an output at each time step
             Recurrent connections between hidden units  
         Equations  
            The standard RNN is parametrized with three weight matrices and three bias vectors  
            $$\theta=\leftW{h x} = U W{h h} = W W{o h} = V b{h} b{o} h{0}\right$$  
            Then given an input sequence $$\left\boldsymbol{x}^{t} \boldsymbol{x}^{t 1} \boldsymbol{x}^{t 2} \ldots \boldsymbol{x}^{2} \boldsymbol{x}^{1}\right$$ the RNN performs the following computations for every time step   
            $$\begin{aligned} \boldsymbol{a}^{t} &=\boldsymbol{b}+\boldsymbol{W h}^{t 1}+\boldsymbol{U} \boldsymbol{x}^{t}  \boldsymbol{h}^{t} &=\tanh \left\boldsymbol{a}^{t}\right  \boldsymbol{o}^{t} &=\boldsymbol{c}+\boldsymbol{V} \boldsymbol{h}^{t}  \hat{\boldsymbol{y}}^{t} &=\operatorname{softmax}\left\boldsymbol{o}^{t}\right \end{aligned}$$  
            where the parameters are the bias vectors $$\mathbf{b}$$ and $$\mathbf{c}$$ along with the weight matrices $$\boldsymbol{U}$$ $$\boldsymbol{V}$$ and $$\boldsymbol{W}$$ respectively for input to hidden hidden to output and hidden to hidden connections.  
            We also Assume the hyperbolic tangent activation function and that the output is discrete^1.  
         The Total Loss  
            The Total Loss for a given sequence of $$\mathbf{x}$$ values paired with a sequence of $$\mathbf{y}$$ values is the sum of the losses over all the time steps. Assuming $$L^{t}$$ is the negative log likelihood of $$y^{t}$$ given $$\boldsymbol{x}^{1} \ldots \boldsymbol{x}^{t}$$ then  
            $$\begin{aligned} & L\left\left\{\boldsymbol{x}^{1} \ldots \boldsymbol{x}^{\tau}\right\}\left\{\boldsymbol{y}^{1} \ldots \boldsymbol{y}^{\tau}\right\}\right =& \sum{t} L^{t} =&  \sum{t} \log p{\text { model }}\lefty^{t} |\left\{\boldsymbol{x}^{1} \ldots \boldsymbol{x}^{t}\right\}\right \end{aligned}$$  
            where $$p{\text { model }}\lefty^{t} |\left\{\boldsymbol{x}^{1} \ldots \boldsymbol{x}^{t}\right\}\right$$ is given by reading the entry for $$y^{t}$$ from the model's output vector $$\hat{\boldsymbol{y}}^{t}$$.  
         Complexity  
             Forward Pass  
                The runtime is $$\mathcal{O}\tau$$ and cannot be reduced by parallelization because the forward propagation graph is inherently sequential; each time step may only be computed after the previous one.  
             Backward Pass  
                The standard algorithm used is called Back Propagation Through Time BPTT with a runtime of $$\mathcal{O}\tau$$  
                                 
         Properties  
             The Standard RNN is Universal in the sense that any function computable by a Turing Machine can be computed by such an RNN of a finite size.  
                The functions computable by a Turing machine are discrete so these results regard exact implementation of the function not approximations.  
                The RNN when used as a Turing machine takes a binary sequence as input and its outputs must be discretized to provide a binary output.  

             The output can be read from the RNN after a number of time steps that is asymptotically linear in the number of time steps used by the Turing machine and asymptotically linear in the length of the input Siegelmann and Sontag 1991; Siegelmann 1995; Siegelmann and Sontag 1995; Hyotyniemi 1996.  
             The theoretical RNN used for the proof can simulate an unbounded stack by representing its activations and weights with rational numbers of unbounded precision.  

    2. Variation 2{ }  
        
         Architecture  
             Produces an output at each time step  
             Recurrent connections only from the output at one time step to the hidden units at the next time step  
         Equations  
            $$\begin{aligned} \boldsymbol{a}^{t} &=\boldsymbol{b}+\boldsymbol{W o}^{t 1}+\boldsymbol{U} \boldsymbol{x}^{t} 
            \boldsymbol{h}^{t} &=\tanh \left\boldsymbol{a}^{t}\right 
            \boldsymbol{o}^{t} &=\boldsymbol{c}+\boldsymbol{V} \boldsymbol{h}^{t} 
            \hat{\boldsymbol{y}}^{t} &=\operatorname{softmax}\left\boldsymbol{o}^{t}\right \end{aligned}$$  
         Properties  
             Strictly less powerful because it lacks hidden to hidden recurrent connections.  
                It cannot simulate a universal Turing Machine.  
             It requires that the output units capture all the information about the past that the network will use to predict the future; due to the lack of hidden to hidden recurrence.  
                But since the outputs are trained to match the training set targets they are unlikely to capture the necessary information about the past history.  
             The Advantage of eliminating hidden to hidden recurrence is that all the time steps are de coupled^2. Training can thus be parallelized with the gradient for each step $$t$$ computed in isolation.  
                Thus the model can be trained with Teacher Forcing.  
                

    3. Variation 3{ }  
        
         Architecture  
             Produces a single output after reading entire sequence
             Recurrent connections between hidden units
         Equations  
            $$\begin{aligned} \boldsymbol{a}^{t} &=\boldsymbol{b}+\boldsymbol{W h}^{t 1}+\boldsymbol{U} \boldsymbol{x}^{t} 
            \boldsymbol{h}^{t} &=\tanh \left\boldsymbol{a}^{t}\right 
            \boldsymbol{o} = \boldsymbol{o}^{T} &=\boldsymbol{c}+\boldsymbol{V} \boldsymbol{h}^{T} 
            \hat{\boldsymbol{y}} = \hat{\boldsymbol{y}}^{T} &=\operatorname{softmax}\left\boldsymbol{o}^{T}\right \end{aligned}$$  


7. Teacher Forcing
    Teacher forcing is a procedure that emerges from the maximum likelihood criterion in which during training the model receives the ground truth output $$y^{t}$$ as input at time $$t + 1$$.  
    
    Models that have recurrent connections from their outputs leading back into the model may be trained with teacher forcing.  

    Teacher forcing may still be applied to models that have hidden to hidden connections as long as they have connections from the output at one time step to values computed in the next time step. As soon as the hidden units become a function of earlier time steps however the BPTT algorithm is necessary. Some models may thus be trained with both teacher forcing and BPTT.  

    The disadvantage of strict teacher forcing arises if the network is going to be later used in an closed loop mode with the network outputs or samples from the output distribution fed back as input. In this case the fed back inputs that the network sees during training could be quite different from the kind of inputs that it will see at test time.  
    
    Methods for Mitigation  
    
    1. Train with both teacher forced inputs and free running inputs for example by predicting the correct target a number of steps in the future through the unfolded recurrent output to input paths^3.  
    2. Another approach Bengio et al. 2015b to mitigate the gap between the inputs seen at training time and the inputs seen at test time randomly chooses to use generated values or actual data values as input. This approach exploits a curriculum learning strategy to gradually use more of the generated values as input.

    proof p.377 378  

9. Computing the Gradient in an RNN
    Note The computation is as noted before w.r.t. the standard RNN variation 1  

    Computing the gradient through a recurrent neural network is straightforward. One simply applies the generalized back propagation algorithm of section 6.5.6 to the unrolled computational graph. No specialized algorithms are necessary.  

    
    Once the gradients on the internal nodes of the computational graph are obtained we can obtain the gradients on the parameter nodes which have descendents at all the time steps  
    

    Notes  
    
     We do not need to compute the gradient with respect to $$\mathbf{x}^{t}$$ for training because it does not have any parameters as ancestors in the computational graph defining the loss.  

    
    ^1 A natural way to represent discrete variables is to regard the output $$\mathbf{o}$$ as giving the unnormalized log probabilities of each possible value of the discrete variable. We can then apply the softmax operation as a post processing step to obtain a vector $$\hat{\boldsymbol{y}}$$ of normalized probabilities over the output.  
    ^2 for any loss function based on comparing the prediction at time $$t$$ to the training target at time $$t$$.  
    ^3 In this way the network can learn to take into account input conditions such as those it generates itself in the free running mode not seen during training and how to map the state back toward one that will make the network generate proper outputs after a few steps.  
    

10.Recurrent Networks as Directed Graphical Models
    Since we wish to interpret the output of an RNN as a probability distribution we usually use the cross entropy associated with that distribution to define the loss.  
    E.g. Mean squared error is the cross entropy loss associated with an output distribution that is a unit Gaussian.  

    When we use a predictive log likelihood training objective such as equation 10.12 we train the RNN to estimate the conditional distribution of the next sequence element $$\boldsymbol{y}^{t}$$ given the past inputs. This may mean that we maximize the log likelihood  
    $$\log p\left\boldsymbol{y}^{t} | \boldsymbol{x}^{1} \ldots \boldsymbol{x}^{t}\right \tag{10.29}$$  
    or if the model includes connections from the output at one time step to the nexttime step  
    $$\log p\left\boldsymbol{y}^{t} | \boldsymbol{x}^{1} \ldots \boldsymbol{x}^{t} \boldsymbol{y}^{1} \ldots \boldsymbol{y}^{t 1}\right \tag{10.30}$$  
    Decomposing the joint probability over the sequence of $$\mathbf{y}$$ values as a series of one step probabilistic predictions is one way to capture the full joint distribution across the whole sequence. When we do not feed past $$\mathbf{y}$$ values as inputs that condition the next step prediction the outputs $$\mathbf{y}$$ are conditionally independent given the sequence of $$\mathbf{x}$$ values.  



    Summary  
    This section is useful for understanding RNN from a probabilistic graphical model perspective. The main point is to show that RNN provides a very efficient parametrization of the joint distribution over the observations $$y^{t}$$.  
    The introduction of hidden state and hidden to hidden connections can be motivated as reducing fig 10.7 to fig 10.8; which have $$\mathcal{O}k^{\tau}$$ and $$\mathcal{O}1\times \tau$$ parameters respectively where $$\tau$$ is the length of the sequence.  


18.Backpropagation Through Time
     We can think of the recurrent net as a layered feed forward net with shared weights and then train the feed forward net with linear weight constraints.
     We can also think of this training algorithm in the time domain
         The forward pass builds up a stack of the activities of all the units at each time step
         The backward pass peels activities off the stack to compute the error derivatives at each time step
         After the backward pass we add together the derivatives at all the different times for each weight.  
    


19.Downsides of RNNs
     RNNs are not Inductive They memorize sequences extremely well but they don’t necessarily always show convincing signs of generalizing in the correct way.  
     They unnecessarily couple their representation size to the amount of computation per step if you double the size of the hidden state vector you’d quadruple the amount of FLOPS at each step due to the matrix multiplication.  
        Ideally we’d like to maintain a huge representation/memory e.g. containing all of Wikipedia or many intermediate state variables while maintaining the ability to keep computation per time step fixed.  
    


20.RNNs as a model with Memory \| Comparison with other Memory models
    Modeling Sequences/concepts  


NOTES  

 RNNs may also be applied in two dimensions across spatial data such as images  
 A Deep RNN in vertical dim stacking up hidden layers increases the memory representational ability with linear scaling in computation as opposed to increasing the size of the hidden layer  > quadratic computation.  
 A Deep RNN in time dim add extra pseudo steps for each real step increase ONLY the representational ability efficiency and NOT memory.  
 Dropout in Recurrent Connections dropout is ineffective when applied to recurrent connections as repeated random masks zero all hidden units in the limit. The most common solution is to only apply dropout to non recurrent connections.  
 Different Connections in RNN Architectures  
    1. PeepHole Connection  
        is an addition on the equations of the LSTM as follows  
        $$ \Gammao = \sigmaWoa^{t 1} x^{t} + bo 
        \implies 
        \sigmaWoa^{t 1} x^{t} c^{t 1} + bo$$  
        Thus we add the term $$c^{t 1}$$ to the output gate.  
 Learning Long Range Dependencies in RNNs/sequence models  
    One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences the easier it is to learn long range dependencies.  



      
 Gradient Clipping Intuition  
     The image above is that of the Error Surface of a single hidden unit RNN  
     The observation here is that there exists High Curvature Walls.   
        This Curvature Wall will move the gradient to a very different/far probably less useful area. 
        Thus if we clip the gradients we will avoid the walls and will remain in the more useful area that we were exploring already.   
    Draw a line between the original point on the Error graph and the End optimized point then evaluate the Error on points on that line and look at the changes $$\rightarrow$$ this shows changes in the curvature.  




 PGMs  Probabilistic Graphical Models


Resources  












Graphical Models





0. Motivation
    Machine learning algorithms often involve probability distributions over a very large number of random variables. Often these probability distributions involve direct interactions between relatively few variables. Using a single function to describe the entire joint probability distribution can be very inefficient both computationally and statistically.  

    A description of a probability distribution is exponential in the number of variables it models.  


1. Graphical Model
    A graphical model or probabilistic graphical model PGM or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure factorization of a probability distribution between random variables.  
    Generally this is one of the most common statistical models  

    Properties  
    
     Factorization  
     Independence  

    Graph Structure  
    A PGM uses a graph $$\mathcal{G}$$ in which each node in the graph corresponds to a random variable and an edge connecting two r.vs means that the probability distribution is able to represent interactions between those two r.v.s.  

    
    Types  
    
     Directed  
        Directed models use graphs with directed edges and they represent factorizations into conditional probability distributions.  
        They contain one factor for every random variable $$xi$$ in the distribution and that factor consists of the conditional distribution over $$xi$$ given the parents of $$xi$$.  
     Undirected  
        Undirected models use graphs with undirected edges and they represent factorizations into a set of functions; unlike in the directed case these functions are usually not probability distributions of any kind.  


    Core Idea of Graphical Models  
    The probability distribution factorizes according to the cliques in the graph with the potentials usually being of the exponential family and a graph expresses the conditional dependence structure between random variables.    
    
            
2. Neural Networks and Graphical Models
    Deep NNs as PGMs  
    You can view a deep neural network as a graphical model but here the CPDs are not probabilistic but are deterministic. Consider for example that the input to a neuron is $$\vec{x}$$ and the output of the neuron is $$y .$$ In the CPD for this neuron we have $$p\vec{x} y=1$$ and $$p\vec{x} \hat{y}=0$$ for $$\hat{y} \neq y .$$ Refer to the section 10.2 .3 of Deep Learning Book for more details.  
    
 


Bayesian Network

1. Bayesian Network
    A Bayesian network Bayes network belief network or probabilistic directed acyclic graphical model is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph DAG.  
    E.g. a Bayesian network could represent the probabilistic relationships between diseases and symptoms.  


    Bayes Nets big picture  
    Bayes Nets a technique for describing complex joint distributions models using simple local distributions conditional probabilities.  

    In other words they are a device for describing a complex distribution over a large number of variables that is built up of small pieces local interactions; with the assumptions necessary to conclude that the product of those local interactions describe the whole domain.  

    Formally a Bayes Net consists of  
    
    1. A directed acyclic graph of nodes one per variable $$X$$ 
    2. A conditional distribution for each node $$PX\vert A1\ldots An$$ where $$Ai$$ is the $$i$$th parent of $$X$$ stored as a conditional probability table or CPT.  
        Each CPT has $$n+2$$ columns one for the values of each of the $$n$$ parent variables $$A1 \ldots An$$ one for the values of $$X$$ and one for the conditional probability of $$X$$.  

    Each node in the graph represents a single random variable and each directed edge represents one of the conditional probability distributions we choose to store i.e. an edge from node $$A$$ to node $$B$$ indicates that we store the probability table for $$PB\vert A$$.  
    Each node is conditionally independent of all its ancestor nodes in the graph given all of its parents. Thus if we have a node representing variable $$X$$ we store $$PX\vert A1A2...AN$$ where $$A1\ldotsAN$$ are the parents of $$X$$.   


    The local probability tables of conditional distributions and the DAG together encode enough information to compute any probability distribution that we could have otherwise computed given the entire joint distribution.


    Motivation  
    There are problems with using full join distribution tables as our probabilistic models  
    
     Unless there are only a few variables the joint is WAY too big to represent explicitly
     Hard to learn estimate anything empirically about more than a few variables at a time  


    Examples of Bayes Nets  
    
     Coin Flips  
        
     Traffic  
        
     Traffic II  
        
     Alarm Network  
        img4


    Probabilities in BNs  
    
     Bayes Nets implicitly encode joint distributions  
        Encoded as a product of local conditional distributions  
        $$px1 x2 \ldots xn = \prod{i=1}^{n} pxi \vert \text{parents}Xi \tag{1.1}$$  
     We are guaranteed that $$1.1$$ results in a proper joint distribution  
        1. Chain Rule is valid for all distributions  
            $$px1 x2 \ldots xn = \prod{i=1}^{n} pxi \vert x1 \ldots x{i 1}$$  
        2. Conditional Independences Assumption  
            $$px1 x2 \ldots x{i 1} = \prod{i=1}^{n} pxi \vert \text{parents}Xi$$
            $$\implies$$  
            $$px1 x2 \ldots xn = \prod{i=1}^{n} pxi \vert \text{parents}Xi \tag{1.1}$$  

    Thus from above Not every BN can represent every joint distribution.  
    The topology enforces certain conditional independencies that need to be met.  
    e.g. Only distributions whose variables are absolutely independent can be represented by a Bayes Net with no arcs  


    Causality  
    Although the structure of the BN might be in a way that encodes causality it is not necessary to define the joint distribution graphically. The two definitions below are the same  
    img5  
    To summarize  
    
     When BNs reflect the true causal patterns  
         Often simpler nodes have fewer parents
         Often easier to think about
         Often easier to elicit from experts 
     BNs need NOT be causal  
         Sometimes no causal net exists over the domain especially if variables are missing  
             e.g. consider the variables $$\text{Traffic}$$ and $$\text{Drips}$$  
         Results in arrows that reflect correlation not causation  
     The meaning of the arrows  
         The topology may happen to encode causal structure
         But the topology really encodes conditional independence    
            $$px1 x2 \ldots x{i 1} = \prod{i=1}^{n} pxi \vert \text{parents}Xi$$
    

    Questions we can ask
    Since a BN encodes a joint distribution we can ask any questions a joint distribution can answer  
    
     Inference given a fixed BN what is $$PX \vert \text { e? }$$
     Representation given a BN graph what kinds of distributions can it encode?
     Modeling what BN is most appropriate for a given domain?  


    Size of a BN  
    
     Size of a full joint distribution over $$N$$ boolean variables $$2^N$$  
     Size of an $$N$$ node net with nodes having up to $$k$$ parents $$\mathcal{O}N \times 2^{k+1}$$  


    Advantages of BN over full joint distribution  
    
     Both will model calculate $$pX1 X2 \ldots XN$$  
     BNs give you huge space savings  
     It is easier to elicit local CPTs  
     Is faster to answer queries      
    


    Notes  
    
     The acyclicity gives an order to the order less chain rule of conditional probabilities  
     Think of the conditional distribution for each node as a description of a noisy "causal" process  
     The graph of the BN represents certain independencies directly but also contains extra independence assumptions that can be "inferred" from the shape of the graph  

     There could be extra independence relationships in the distribution that are not represented by the BN graph structure but can be read in the CPT. This happens when the structure of the BN is not "optimal" for the assumptions between the variables.  
    

2. Independence / D Separation
    Goal is to find a graph algorithm that can show independencies between variables in BNs. Steps    
    1. Study independence properties for triples  
    2. Analyze complex cases configurations in terms of member triples  
    3. D separation a condition / algorithm for answering such queries  

    
    Causal Chains  

    A Causal Chain is a configuration of three nodes that expresses the following representation of the joint distribution over $$X$$ $$Y$$ and $$Z$$  
    $$Px y z=Pz \vert y Py \vert x Px$$  

    Let's try to see if we can guarantee independence between $$X$$ and $$Z$$  
     No Observations  

        $$X$$ and $$Z$$ are not guaranteed independent.  
         Proof  
            By Counterexample  
            $$Py \vert x=\left\{\begin{array}{ll}{1} & {\text { if } x=y}  {0} & {\text { else }}\end{array} \quad Pz \vert y=\left\{\begin{array}{ll}{1} & {\text { if } z=y}  {0} & {\text { else }}\end{array}\right.\right.$$  
            $$\text { In this case } Pz \vert x=1 \text { if } x=z \text { and } 0 \text { otherwise so } X \text { and } Z \text { are not independent.}$$  

     $$Y$$ Observed  

        $$X$$ and $$Z$$ are independent given $$Y$$. i.e. $$PX \vert Z Y=PX \vert Y$$  
         Proof  
            $$\begin{aligned} PX \vert Z y &=\frac{PX Z y}{PZ y}=\frac{PZ \vert y Py \vert X PX}{\sum{x} PX y Z}=\frac{PZ \vert y Py \vert X PX}{PZ \vert y \sum{x} Py \vert x Px}  &=\frac{Py \vert X PX}{\sum{x} Py \vert x Px}=\frac{Py \vert X PX}{Py}=PX \vert y \end{aligned}$$  
    An analogous proof can be used to show the same thing for the case where X has multiple parents.  

    To summarize in the causal chain chain configuration $$X \perp Z \vert Y$$ .  

    Evidence along the chain "blocks" the influence.  



          

    
    Common Cause  
    Common Cause is another configuration for a triple. It expresses the following representation  
    $$Px y z=Px \vert y Pz \vert y Py$$  


    Let's try to see if we can guarantee independence between $$X$$ and $$Z$$  
     No Observations  

        $$X$$ and $$Z$$ are not guaranteed independent.  
         Proof  
            By Counterexample  
            $$Px \vert y=\left\{\begin{array}{ll}{1} & {\text { if } x=y}  {0} & {\text { else }}\end{array} \quad Pz \vert y=\left\{\begin{array}{ll}{1} & {\text { if } z=y}  {0} & {\text { else }}\end{array}\right.\right.$$  
            \text { Then } Px \vert z=1 \text { if } x=z \text { and } 0 \text { otherwise so } X \text { and } Z \text { are not independent. }  

     $$Y$$ Observed  

        $$X$$ and $$Z$$ are independent given $$Y$$. i.e. $$PX \vert Z Y=PX \vert Y$$  
         Proof  
            $$PX \vert Z y=\frac{PX Z y}{PZ y}=\frac{PX \vert y PZ \vert y Py}{PZ \vert y Py}=PX \vert y$$  

    Observing the cause blocks the influence.  



          


    Common Effect  
    Common Effect is the last configuration for a triplet. Expressing the representation  
    $$Px y z=Py \vert x z Px Pz$$  


    Let's try to see if we can guarantee independence between $$X$$ and $$Z$$  
     No Observations  

        $$X$$ and $$Z$$ are readily guaranteed to be independent $$X \perp Z$$.  

     $$Y$$ Observed  

        $$X$$ and $$Z$$ are not necessarily independent given $$Y$$. i.e. $$PX \vert Z Y\neqPX \vert Y$$  
         Proof  
            By Counterexample  
            $$\text { Suppose all three are binary variables. } X \text { and } Z \text { are true and false with equal probability }$$  
            $$\begin{array}{l}{PX=\text {true}=PX=\text { false }=0.5}  {PZ=\text {true}=PZ=\text { false }=0.5}\end{array}$$  
            $$ \text { and } Y \text { is determined by whether } X \text { and } Z \text { have the same value } $$  
            $$PY \vert X Z=\left\{\begin{array}{ll}{1} & {\text { if } X=Z \text { and } Y=\text { true }}  {1} & {\text { if } X \neq Z \text { and } Y=\text { false }}  {0} & {\text { else }}\end{array}\right.$$  
            $$ \text { Then } X \text { and } Z \text { are independent if } Y \text { is unobserved. But if } Y \text { is observed then knowing } X \text { will } 
            \text { tell us the value } {\text { of } Z \text { and vice versa. } \text{So } X \text { and } Z \text { are } \text {not} \text { conditionally independent given } Y \text {. }} $$  

    Common Effect can be viewed as "opposite" to Causal Chains and Common Cause $$X$$ and $$Z$$ are guaranteed to be independent if $$Y$$ is not conditioned on. But when conditioned on $$Y X$$ and $$Z$$ may be dependent depending on the specific probability values for $$PY \vert X Z$$.  

    This same logic applies when conditioning on descendants of $$Y$$ in the graph. If one of $$Y$$ 's descendent nodes is observed as in Figure $$7 X$$ and $$Z$$ are not guaranteed to be independent.  


    Observing an effect activates influence between possible causes.  



          
    

    General Case and D Separation  
    We can use the previous three cases as building blocks to help us answer conditional independence questions on an arbitrary Bayes’ Net with more than three nodes and two edges.  We formulate the problem as follows  
    Given a Bayes Net $$G$$ two nodes $$X$$ and $$Y$$ and a possibly empty set of nodes $$\left\{Z{1} \ldots Z{k}\right\}$$ that represent observed variables must the following statement be true $$X \perp Y |\left\{Z{1} \ldots Z{k}\right\} ?$$  

    D Separation directed separation is a property of the structure of the Bayes Net graph that implies this conditional independence relationship and generalizes the cases we’ve seen above. If a set of variables $$Z{1} \cdots Z{k} d $$  separates $$X$$ and $$Y$$ then $$X \perp Y \vert\left\{Z{1} \cdots Z{k}\right\}$$ in all possibutions that can be encoded by the Bayes net.  


    D Separation Algorithm  
    1. Shade all observed nodes $$\left\{Z{1} \ldots Z{k}\right\}$$ in the graph.
    2. Enumerate all undirected paths from $$X$$ to $$Y$$ .
    3. For each path
         Decompose the path into triples segments of 3 nodes.
         If all triples are active this path is active and $$d$$  connects $$X$$ to $$Y$$.
    4. If no path d connects $$X$$ and $$Y$$ and $$Y$$ are d separated so they are conditionally independent
    given $$\left\{Z{1} \ldots Z{k}\right\}$$  

    Any path in a graph from $$X$$ to $$Y$$ can be decomposed into a set of 3 consecutive nodes and 2 edges  each of which is called a triple. A triple is active or inactive depending on whether or not the middle node is observed. If all triples in a path are active then the path is active and $$d$$  connects $$X$$ to $$Y$$ meaning $$X$$ is not guaranteed to be conditionally independent of $$Y$$ given the observed nodes. If all paths from $$X$$ to $$Y$$ are inactive then $$X$$ and $$Y$$ are conditionally independent given the observed nodes.  

    Active Triples We can enumerate all possibilities of active and inactive triples using the three canonical graphs we presented above in Figure 8 and 9.  




          


    Examples  
    
     

     

     



          
      

    Structure Implications  
    Given a Bayes net structure can run d separation algorithm to build a complete list of conditional independences that are necessarily true of the form.  
    $$X{i} \perp X{j} |\left\{X{k{1}} \ldots X{k{n}}\right\}$$  
    This list determines the set of probability distributions that can be represented.  

    Topology Limits Distributions  
    

     Given some graph topology $$G$$ only certain joint distributions can be encoded  
     The graph structure guarantees certain conditional independences
     There might be more independence
     Adding arcs increases the set of distributions but has several costs
     Full conditioning can encode any distribution  
     The more assumptions you make the fewer the number of distributions you can represent  

    Bayes Nets Representation Summary  
    
     Bayes nets compactly encode joint distributions
     Guaranteed independencies of distributions can be deduced from BN graph structure
     D separation gives precise conditional independence guarantees from graph alone
     A Bayes nets joint distribution may have further conditional independence that is not detectable until you inspect its specific distribution

    

3. Inference


    Inference is the process of calculating the joint PDF for some set of query variables based on some set of observed variables.  
    For example  
    
     Posterior Probability inference  
        $$P\leftQ \vert E{1}=e{1} \ldots E{k}=e{k}\right$$  
     Most Likely Explanation inference  
        $$\operatorname{argmax}{q} P\leftQ=q \vert E{1}=e{1} \ldots\right$$  

    Notation  General Case  
    $$ \left.\begin{array}{ll}{\textbf { Evidence variables }} & {E{1} \ldots E{k}=e{1} \ldots e{k}}  {\textbf { Query}^{ } \textbf { variable }} & {Q}  {\textbf { Hidden variables }} & {H{1} \ldots H{r}}\end{array}\right\} \begin{array}{l}{X{1} X{2} \ldots X{n}}  {\text { All variables }}\end{array} $$  


    Inference by Enumeration  
    We can solve this problem naively by forming the joint PDF and using inference by enumeration as described above. This requires the creation of and iteration over an exponentially large table.   
    Algorithm  
     Select the entries consistent with the evidence
     Sum out $$\mathrm{H}$$ to get join of Query and evidence  
     Normalize $$\times \dfrac{1}{Z} = \dfrac{1}{\text{sum of entries}}$$  


          



    Variable Elimination  
    Alternatively we can use Variable Elimination eliminate variables one by one.  
    To eliminate a variable $$X$$ we  
    
    1. Join multiply together all factors involving $$X$$.
    2. Sum out $$X$$.  

    A factor is an unnormalized probability; represented as a multidimensional array  
    
     Joint Distributions $$PXY \in \mathbb{R}^2$$  
         Entries $$Px y$$ for all $$x y$$  
         Sums to $$1$$  
     Selected Joint $$PxY \in \mathbb{R}$$ 
         A slice of the joint distribution
         Entries $${P}{x} {y}$$ for fixed $${x}$$ all $${y}$$  
         Sums to $$Px$$  

     Single Conditional $$PY \vert x$$  
         Entries $${P}{y} \vert {x}$$ for fixed $${x}$$ all $${y}$$  
         Sums to $$1$$ 
     Family of Conditionals $$PY \vert X$$ 
         Multiple Conditionals
         Entries $${P}{y} \vert {x}$$ for all $${x} {y}$$  
         Sums to $$\vert X\vert$$  

     Specified family $$Py \vert X$$
         Entries $$Py \vert x$$ for fixed $$y$$ but for all $$x$$
         Sums to random number; not a distribution  

    For Joint Distributions the \# capital variables dictates the "dimensionality" of the array.  


    At all points during variable elimination each factor will be proportional to the probability it corresponds to but the underlying distribution for each factor won’t necessarily sum to $$1$$ as a probability distribution should.  

    Inference by Enumeration vs. Variable Elimination  
    Inference by Enumeration is very slow You must join up the whole joint distribution before you sum out the hidden variables.  
    Variable Elimination Interleave joining and marginalization.  
    Still NP hard but usually much faster.  

    Notice that $$\sumr Pr Pt \vert r = Pt$$ thus in VE you end up with $$\sum{t} PL \vert t Pt$$.   


    General Variable Elimination  Algorithm  



    VE  Computational and Space Complexity  
    
     The computational and space complexity of variable elimination is determined by the largest factor.  
     The elimination ordering can greatly affect the size of the largest factor.  
     There does NOT always exist an ordering that only results in small factors.  
     VE is NP Hard  
         Proof  
            We can reduce 3 Sat to a BN Inference problem.  
            We can encode a Constrained Satisfiability Problem CSP in a BN and use it to give a solution to the CSP; if the CSP consists of 3 clauses then finding a solution for the CSP via BN Inference is equivalent to solving 3 Sat.  
            

     Thus inference in Bayes’ nets is NP hard
        No known efficient probabilistic inference in general.  

    Polytrees  
    A Polytree is a directed graph with no undirected cycles.  
    For polytrees we can always find an ordering that is efficient.  
     Cut set conditioning for Bayes’ net inference Choose set of variables such that if removed only a polytree remains.   
      


4. Sampling
    Sampling is the process of generating observations/samples from a distribution.  
      Sampling is like doing repeated probabilistic simulation.  
      Sampling could be used for learning e.g. RL. But in the context of BNs it is used for Inference.  
      Sampling provides a way to do efficient inference by presenting us with a tradeoff between accuracy and computation time.  
      The Goal is to prove that as the number of samples you generate $$N$$ goes to $$\infty$$ the approximation converges to the true probability you are trying to compute.  
      Using sampling in a BN from the entire network is necessary because listing all the outcomes is too expensive even if we can create them given infinite time.  

    Idea/Algorithm for Inference  
    
     Draw $$N$$ samples from a sampling distribution $$S$$  
     Compute an approximate posterior probability
     Show this converges to the true probability $$P$$   

    Sampling from a given distribution  
    
     Get sample $$u$$ from uniform distribution over $$01$$  
     Convert this sample $$u$$ into an outcome for the given distribution by having each target outcome associated with a sub interval of $$01$$ with sub interval size equal to probability of the outcome  


    Sampling Algorithms in BNs  
    
     Prior Sampling
     Rejection Sampling
     Likelihood Weighting
     Gibbs Sampling


    Prior Sampling  


    Algorithm  
    
     For $$i=12 \ldots n$$  
         Sample $$x{i}$$ from $$P\leftX{i} \vert \text { Parents }\leftX{i}\right\right$$  
     Return $$\leftx{1} x{2} \ldots x{n}\right$$  

    Notes  
    
     This process generates samples with probability  
        $$
        \begin{aligned} S{P S}\leftx{1} \ldots x{n}\right=\prod{i=1}^{n} P\leftx{i} \vert \text { Parents }\leftX{i}\right\right=P\leftx{1} \ldots x{n}\right  \text { ...i.e. the BN's joint probability } \end{aligned}
        $$  
     Let the number of samples of an event be $$N{P S}\leftx{1} \cdots x{n}\right$$  
     Thus  
        $$\begin{aligned} \lim {N \rightarrow \infty} \widehat{P}\leftx{1} \ldots x{n}\right &=\lim {N \rightarrow \infty} N{P S}\leftx{1} \ldots x{n}\right / N  &=S{P S}\leftx{1} \ldots x{n}\right  &=P\leftx{1} \ldots x{n}\right \end{aligned}$$   
     I.e. the sampling procedure is consistent   


    Rejection Sampling  
    Rejection Sampling  

    It is also consistent.  

    Idea  
    
    Same as Prior Sampling but no point in keeping all of the samples. We just tally the outcomes that match our evidence and reject the rest.  

    Algorithm  
    
     Input evidence instantiation  
     For $$i=12 \ldots n$$  
         Sample $$x{i}$$ from $$P\leftX{i} \vert \text { Parents }\leftX{i}\right\right$$  
         If $x{i}$ not consistent with evidence  
             Reject return  no sample is generated in this cycle  
     Return $$\leftx{1} x{2} \ldots x{n}\right$$   



    Likelihood Weighting  
    Likelihood Weighting  


    Key Ideas  
    
    Fixes a problem with Rejection Sampling  
     If evidence is unlikely rejects lots of samples  
     Evidence not exploited as you sample  
    Idea fix evidence variables and sample the rest.  
     Problem sample distribution not consistent!  
     Solution weight by probability of evidence given parents.  


    Algorithm  
    
     Input evidence instantiation  
     $$w=1.0$$  
     for $$i=12 \dots n$$  
         if $$\mathrm{x} {\mathrm{i}}$$ is an evidence variable  
             $$\mathrm{n} \mathrm{x}   {\mathrm{i}}=$$ observation $$\mathrm{x} {\mathrm{i}}$$ for $$\mathrm{x} {\mathrm{i}}$$  
             $$\operatorname{set} \mathrm{w}=\mathrm{w}  \mathrm{P}\left\mathrm{x} {\mathrm{i}} \vert \text { ParentsX. }\right.$$    
         else  
             Sample $$x i$$ from $$P\leftX  {i} \vert \text { Parents }\leftX  {i}\right\right$$  
     Return $$\left\mathrm{x} {1} \mathrm{x} {2} \ldots \mathrm{x} {\mathrm{n}}\right \mathrm{w}$$  


    Notes  
    
     Sampling distribution if $$z$$ sampled and $$e$$ fixed evidence  
        $$S{W S}\mathbf{z} \mathbf{e}=\prod{i=1}^{l} P\leftz{i} \vert \text { Parents }\leftZ{i}\right\right$$  
     Now samples have weights  
        $$w\mathbf{z} \mathbf{e}=\prod{i=1}^{m} P\lefte{i} \vert \text { Parents }\leftE{i}\right\right$$  
     Together weighted sampling distribution is consistent  
        $$\begin{aligned} S{\mathrm{WS}}z e \cdot wz e &=\prod{i=1}^{l} P\leftz{i} \vert \text { Parents }\leftz{i}\right\right \prod{i=1}^{m} P\lefte{i} \vert \text { Parents }\lefte{i}\right\right  &=P\mathrm{z} \mathrm{e} \end{aligned}$$   
     Likelihood weighting is good
         We have taken evidence into account as we generate the sample  
         E.g. here $$W$$’s value will get picked based on the evidence values of $$S$$ $$R$$  
         More of our samples will reflect the state of the world suggested by the evidence   
     Likelihood weighting doesn’t solve all our problems  
         Evidence influences the choice of downstream variables but not upstream ones C isn’t more likely to get a value matching the evidence  
     We would like to consider evidence when we sample every variable leads to Gibbs sampling   


    Gibbs Sampling  
    Gibbs Sampling  

     Procedure keep track of a full instantiation $$x1 x2 \ldots xn$$. Start with an arbitrary instantiation consistent with the evidence. Sample one variable at a time conditioned on all the rest but keep evidence fixed. Keep repeating this for a long time.
     Property in the limit of repeating this infinitely many times the resulting samples come from the correct distribution i.e. conditioned on evidence.
     Rationale both upstream and downstream variables condition on evidence.
     In contrast likelihood weighting only conditions on upstream evidence and hence weights obtained in likelihood weighting can sometimes be very small. Sum of weights over all samples is indicative of how many “effective” samples were obtained so we want high weight.   

     Gibbs sampling produces sample from the query distribution $$PQ \vert \text { e }$$ in limit of re sampling infinitely often  
     Gibbs sampling is a special case of more general methods called Markov chain Monte Carlo MCMC methods  
         Metropolis Hastings is one of the more famous MCMC methods in fact Gibbs sampling is a special case of Metropolis Hastings  
     Monte Carlo Methods are just sampling  

    Algorithm by Example  
    


    Efficient Resampling of One Variable   
    
     Sample from $$\mathrm{P}\mathrm{S} \vert+\mathrm{c}+\mathrm{r} \mathrm{w}$$  
        $$\begin{aligned} PS \vert+c+r w &=\frac{PS+c+r w}{P+c+r w}  &=\frac{PS+c+r w}{\sum{s} Ps+c+r w}  &=\frac{P+c PS \vert+c P+r \vert+c P w \vert S+r}{\sum{s} P+c Ps \vert+c P+r \vert+c P w \vert s+r}  &=\frac{P+c PS \vert+c P+r \vert+c P w \vert S+r}{P+c P+r \vert+c \sum{s} Ps \vert+c}  &=\frac{PS \vert+c P w \vert S+r}{\sum{s} Ps \vert+c P w \vert s+r} \end{aligned}$$  
     Many things cancel out   only CPTs with $$S$$ remain!  
     More generally only CPTs that have resampled variable need to be considered and joined together  


    Bayes’ Net Sampling Summary  
    

    

5. Decision Networks / VPI Value of Perfect Information

    





Random Field Techniques

1. Random Field
    A Random Field is a random function over an arbitrary domain usually a multi dimensional space such as $$\mathbb{R}^{n}$$ . That is it is a function $$fx$$ that takes on a random value at each point $$x \in \mathbb{R}^{n}$$ or some other domain. It is also sometimes thought of as a synonym for a stochastic process with some restriction on its index set. That is by modern definitions a random field is a generalization of a stochastic process where the underlying parameter need no longer be real or integer valued "but can instead take values that are multidimensional vectors on some manifold.  

    Formally  
    Given a probability space $$\Omega \mathcal{F} P$$ an $$X$$  valued random field is a collection of $$X$$  valued random variables indexed by elements in a topological space $$T$$. That is a random field $$F$$ is a collection  
    $$\left\{F{t}  t \in T\right\}$$  
    where each $$F{t}$$ is an $$X$$ valued random variable.  

    Notes  
    

    





 Deep Generative Models










In situations respecting the following assumptions Semi Supervised Learning should improve performance    

 Semi supervised learning works when $$p\mathbf{y} \vert \mathbf{x}$$ and $$p\mathbf{x}$$ are tied together.  
     This happens when $$\mathbf{y}$$ is closely associated with one of the causal factors of $$\mathbf{x}$$.  
 The best possible model of $$\mathbf{x}$$ wrt. generalization is the one that uncovers the above "true" structure with $$\boldsymbol{h}$$ as a latent variable that explains the observed variations in $$\boldsymbol{x}$$.  
     Since we can write the Marginal Probability of Data as  
        $$p\boldsymbol{x}=\mathbb{E} {\mathbf{h}} p\boldsymbol{x} \vert \boldsymbol{h}$$  
         Because the "true" generative process can be conceived as structured according to this directed graphical model with $$\mathbf{h}$$ as the parent of $$\mathbf{x}$$  
            $$p\mathbf{h} \mathbf{x}=p\mathbf{x} \vert \mathbf{h} p\mathbf{h}$$  
     Thus The "ideal" representation learning discussed above should recover these latent factors.  
 The marginal $$p\mathbf{x}$$ is intimately tied to the conditional $$p\mathbf{y} \vert \mathbf{x}$$ and knowledge of the structure of the former should be helpful to learn the latter.  
     Since the conditional distribution of $$\mathbf{y}$$ given $$\mathbf{x}$$ is tied by Bayes' rule to the components in the above equation  
        $$p\mathbf{y} \vert \mathbf{x}=\frac{p\mathbf{x} \vert \mathbf{y} p\mathbf{y}}{p\mathbf{x}}$$  





Introduction and Preliminaries

1. Unsupervised Learning
    Unsupervised Learning is the task of making inferences by learning a better representation from some datapoints that do not have any labels associated with them.  
    It intends to learn/infer an a priori probability distribution $$p{X}x$$; I.E. it solves a density estimation problem.  
    It is a type of self organized Hebbian learning that helps find previously unknown patterns in data set without pre existing labels.   


2. Density Estimation
    Density Estimation is a problem in Machine Learning that requires learning a function $$p{\text {model}}  \mathbb{R}^{n} \rightarrow \mathbb{R}$$ where $$p{\text {model}}x$$ can be interpreted as a probability  density function if $$x$$ is continuous or a probability mass function if $$x$$ is discrete on the space that the examples were drawn from.  

    To perform such a task well an algorithm needs to learn the structure of the data it has seen. It must know where examples cluster tightly and where they are unlikely to occur.  

     Types of Density Estimation  
         Explicit Explicitly define and solve for $$p\text{model}x$$  
         Implicit Learn model that can sample from $$p\text{model}x$$ without explicitly defining it     

    


3. Generative Models GMs
    A Generative Model is a statistical model of the joint probability distribution on $$X \times Y$$  
    $${\displaystyle PXY}$$   
    where $$X$$ is an observable variable and $$Y$$ is a target variable.  

    In supervised settings a Generative Model is a model of the conditional probability of the observable $$X$$ given a target $$y$$  
    $$PX | Y=y$$   


    Application  Density Estimation  
    Generative Models address the Density Estimation problem a core problem in unsupervised learning since they model   
    Given training data GMs will generate new samples from the same distribution.   
     Types of Density Estimation  
         Explicit Explicitly define and solve for $$p\text{model}x$$  
         Implicit Learn model that can sample from $$p\text{model}x$$ without explicitly defining it     


    Examples of Generative Models  
    
     Gaussian Mixture Model and other types of mixture model
     Hidden Markov Model
     Probabilistic context free grammar
     Bayesian network e.g. Naive Bayes Autoregressive Model
     Averaged one dependence estimators
     Latent Dirichlet allocation LDA
     Boltzmann machine e.g. Restricted Boltzmann machine Deep belief network
     Variational autoencoder
     Generative Adversarial Networks
     Flow based Generative Model

    


    

    Notes  
    
     Generative Models are Joint Models.  
     Latent Variables are Random Variables
    



Deep Generative Models

1. Generative Models GMs
    

2. Deep Generative Models DGMs
    
     DGMs represent probability distributions over multiple variables in some way  
         Some allow the probability distribution function to be evaluated explicitly. 
         Others do not allow the evaluation of the probability distribution function but support operations that implicitly require knowledge of it such as drawing samples from the distribution.  
     Structure/Representation  
         Some of these models are structured probabilistic models described in terms of graphs and factors using the language of probabilistic graphical models.  
         Others cannot be easily described in terms of factors but represent probability distributions nonetheless.  
































 Gated Units  RNN Architectures






GRUs

1. Gated Recurrent Units
       Gated Recurrent Units GRUs are a class of modified Gated RNNs that allow them to combat the vanishing gradient problem by allowing them to capture more information/long range connections about the past memory and decide how strong each signal is.  

2. Main Idea
       Unlike standard RNNs which compute the hidden layer at the next time step directly  GRUs computes two additional layers gates  
        Each with different weights
        Update Gate  
       $$zt = \sigmaW^{z}xt + U^{z}h{t 1}$$  
        Reset Gate  
       $$rt = \sigmaW^{r}xt + U^{r}h{t 1}$$  
       The Update Gate and Reset Gate computed allow us to more directly influence/manipulate what information do we care about and want to store/keep and what content we can ignore.  
        We can view the actions of these gates from their respecting equations as  
        New Memory Content  
            at each hidden layer at a given time step we compute some new memory content  
            if the reset gate $$ = ~0$$ then this ignores previous memory and only stores the new word information.  
       $$ \tilde{h}t = \tanhWxt + rt \odot Uh{t 1}$$
        Final Memory  
            the actual memory at a time step $$t$$ combines the Current and Previous time steps  
            if the update gate $$ = ~0$$ then this again ignores the newly computed memory content and keeps the old memory it possessed.  
       $$ht = zt \odot h{t 1} + 1 zt \odot \tilde{h}t$$  


Long Short Term Memory

1. LSTM
       The Long Short Term Memory LSTM Network is a special case of the Recurrent Neural Network RNN that uses special gated units a.k.a LSTM units as building blocks for the layers of the RNN.  

2. Architecture
       The LSTM usually has four gates  
        Input Gate 
            The input gate determines how much does the current input vector current cell matters      
       $$it = \sigmaW^{i}xt + U^{i}h{t 1}$$ 
        Forget Gate 
            Determines how much of the past memory that we have kept is still needed   
       $$it = \sigmaW^{i}xt + U^{i}h{t 1}$$ 
        Output Gate 
            Determines how much of the current cell matters for our current prediction i.e. passed to the sigmoid
       $$it = \sigmaW^{i}xt + U^{i}h{t 1}$$  
        Memory Cell 
            The memory cell is the cell that contains the short term memory collected from each input
       $$\begin{align}
            \tilde{c}t & = \tanhW^{c}xt + U^{c}h{t 1} & \text{New Memory} 
            ct & = ft \odot c{t 1} + it \odot \tilde{c}t & \text{Final Memory}
        \end{align}$$
       The Final Hidden State is calculated as follows  
       $$ht = ot \odot \sigmact$$
     

3. Properties
        Syntactic Invariance  
            When one projects down the vectors from the last time step hidden layer with PCA one can observe the spatial localization of syntacticly similar sentences  



 Boltzmann Machines


Resources  









Preliminaries



2. The Boltzmann Distribution
    The Boltzmann Distribution is a probability distribution or probability measure that gives the probability that a system will be in a certain state as a function of that state's energy and the temperature of the system  
    $$p{i} = \dfrac{1}{Z} e^{ \frac{\varepsilon{i}}{kB T}}$$  
    where $$p{i}$$ is the probability of the system being in state $$i$$ $$\varepsilon{i}$$ is the energy of that state and a constant $$kB T$$ of the distribution is the product of Boltzmann's constant $$kB$$ and thermodynamic temperature $$T$$ and $$Z$$ is the partition function.    

    The distribution shows that states with lower energy will always have a higher probability of being occupied.  
    The ratio of probabilities of two states AKA Boltzmann factor only depends on the states' energy difference AKA Energy Gap{ }  
    $$\frac{p{i}}{p{j}}=e^{\frac{\varepsilon{j} \varepsilon{i}}{kB T}}$$  

    Derivation  
    The Boltzmann distribution is the distribution that maximizes the entropy  
    $$H\leftp{1} p{2} \cdots p{M}\right= \sum{i=1}^{M} p{i} \log{2} p{i}$$  
    subject to the constraint that $$\sum p{i} \varepsilon{i}$$ equals a particular mean energy value.  

    


    Applications in Different Fields  
    
     Statistical Mechanics
        The canonical ensemble is a probability distribution with the form of the Boltzmann distribution.  
        It gives the probabilities of the various possible states of a closed system of fixed volume in thermal equilibrium with a heat bath.  
     Measure Theory
        The Boltzmann distribution is also known as the Gibbs Measure.  
        The Gibbs Measure is a probability measure which is a generalization of the canonical ensemble to infinite systems.  
     Statistics/Machine Learning
        The Boltzmann distribution is called a log linear model.  
     Probability Theory/Machine Learning
        The Boltzmann distribution is known as the softmax function.  
        The softmax function is used to represent a categorical distribution.  
     Deep Learning




    






Boltzmann Machines

1. Boltzmann Machines BMs
    A Boltzmann Machine BM is a type of stochastic recurrent neural network and Markov Random Field MRF.    

    Goal  What do BMs Learn  
    Boltzmann Machines were originally introduced as a general “connectionist” approach to learning  arbitrary probability distributions over binary vectors.  
    They are capable of learning internal representations of data.  
    They are also able to represent and solve difficult combinatoric problems.  

    Structure  

    
     Input  
        BMs are defined over a $$d$$ dimensional binary random vector $$\mathrm{x} \in\{01\}^{d}$$.  
     Output  
        The units produce binary results.  
     Units  
         Visible Units $$\boldsymbol{v}$$  
         Hidden Units $$\boldsymbol{h}$$  
     Probabilistic Model  
        It is an energy based model; it defines the joint probability distribution using an energy function  
        $$P\boldsymbol{x}=\frac{\exp  E\boldsymbol{x}}{Z}$$    
        where $$E\boldsymbol{x}$$ is the energy function and $$Z$$ is the partition function.  
     The Energy Function  
         With only visible units  
            $$E\boldsymbol{x}= \boldsymbol{x}^{\top} \boldsymbol{U} \boldsymbol{x} \boldsymbol{b}^{\top} \boldsymbol{x}$$  
            where $$U$$ is the "weight" matrix of model parameters and $$\boldsymbol{b}$$ is the vector of bias parameters.  
         With both visible and hidden units  
            $$E\boldsymbol{v} \boldsymbol{h}= \boldsymbol{v}^{\top} \boldsymbol{R} \boldsymbol{v} \boldsymbol{v}^{\top} \boldsymbol{W} \boldsymbol{h} \boldsymbol{h}^{\top} \boldsymbol{S} \boldsymbol{h} \boldsymbol{b}^{\top} \boldsymbol{v} \boldsymbol{c}^{\top} \boldsymbol{h}$$  


    Approximation Capabilities  
    A BM with only visible units is limited to modeling linear relationships between variables as described by the weight matrix^2.  
    A BM with hidden units is a universal approximator of probability mass functions over discrete variables Le Roux and Bengio 2008.  


    Relation to Hopfield Networks  
    A Boltzmann Machine is just a Stochastic Hopfield Network with Hidden Units.  
    BMs can be viewed as the stochastic generative counterpart of Hopfield networks.  

    
    
    It is important to note that although Boltzmann Machines bear a strong resemblance to Hopfield Networks they are actually nothing like them in there functionality.  
    
     Similarities  
         They are both networks of binary units.  
         They both are energy based models with the same energy function 
         They both have the same update rule/condition of estimating a unit’s output by the sum of all weighted inputs.  
     Differences  
         Goal BMs are NOT memory networks. They are not trying to store things. Instead they employ a different computational role/workfiles/research/dl/archits/hopfield; they are trying to learn latent representations of the data.  
            The goal is representation learning.   
         Units BMs have an extra set of units other than the visible units called hidden units. These units represent latent variables that are not observed but learned from the data.  
            These are necessary for representation learning.  
         Objective BMs have a different objective; instead of minimizing the energy function they minimize the error KL Divergence between the "real" distribution over the data and the model distribution over global states marginalized over hidden units.  
            Interpreted as the error between the input data and the reconstruction produced by the hidden units and their weights.  
            This is necessary to capture the training data probability distribution.  
         Energy Minima energy minima were useful for Hopfield Nets and served as storage points for our input data memories. However they are very harmful for BMs since there is a global objective of finding the best distribution that approximates the real distribution.  
            This is necessary to capture the training data probability distribution "well".   
         Activation Functions the activation function for a BM is just a stochastic version of the binary threshold function. The unit would still update to a binary state according to a threshold value but with the update to the unit state being governed by a probability distribution Boltzmann distribution.  
            This is necessary important$$^{  }$$ to escape energy minima.  
    


    Relation to the Ising Model  
    The global energy $$E$$ in a Boltzmann Machine is identical in form to that of the Ising Model.  




    Notes  
    
     Factor Analysis is a Causal Model with continuous variables.  
    


2. Unit State Probability
     The units in a BM are binary units.  
     Thus they have two states $$si \in \{01\}$$ to be in  
        1. On $$si = 1$$   
        2. Off $$si = 0$$  
     The probability that the $$i$$ th unit will be on $$si = 1$$ is  
        $$psi=1=\dfrac{1}{1+ e^{ \Delta E{i}/T}}$$  
        where the scalar $$T$$ is the temperature of the system.  
         The RHS is just the logistic function. Rewriting the probability  
        $$psi=1=\sigma\Delta E{i}/T$$  

        
         Using the Boltzmann Factor ratio of probabilities of states  
            $$\begin{align}
                \dfrac{psi=0}{psi=1} &= e^{\frac{E\lefts{i}=0\right E\lefts{i}=1\right}{k T}} 
                \dfrac{1  psi=1}{psi=1} &= e^{\frac{ E\lefts{i}=1\right E\lefts{i}=0\right}{k T}} 
                \dfrac{1}{psi=1}  1 &= e^{\frac{ \Delta Ei}{k T}}  
                psi=1 &= \dfrac{1}{1 + e^{ \Delta Ei/T}} 
                \end{align}
                $$ 
            where we absorb the Boltzmann constant $$k$$ into the artificial Temperature constant $$T$$.   
  

    









Restricted Boltzmann Machines RBMs

1. Restricted Boltzmann Machines RBMs
    Restricted Boltzmann Machines RBMs 









Deep Boltzmann Machines DBNs

1. Deep Boltzmann Machines DBNs
    Deep Boltzmann Machines DBNs 








^1 The unit deep within the network is doing the same thing but with different boundary conditions.  
^2 Specifically the probability of one unit being on is given by a linear model logistic regression from the values of the other units.  


 FeedForward Neural Networks and Multilayer Perceptron


FeedForward Neural Network

1. FeedForward Neural Network
    The FeedForward Neural Network FNN is an artificial neural network wherein the connections between the nodes do not form a cycle allowing the information to move only in one direction forward from the input layer to the subsequent layers.  
    

2. Architecture
    An FNN consists of one or more layers each consisting of nodes simulating biological neurons that hold a certain wight value $$w{ij}$$. Those weights are usually multiplied by the input values in the input layer in each node and then summed; finally one can apply some sort of activation function on the multiplied values to simulate a response e.g. 1 0 classification.  
    

3. Classes of FNNs
    There are many variations of FNNs. As long as they utilize FeedForward control signals and have a layered structure described above they are a type of FNN  

        A linear binary classifier the single layer perceptron is the simplest feedforward neural network. It consists of a single layer of output nodes; the inputs are multiplied by a series of weights effectively being fed directly to the outputs where they values are summed in each node and if the value is above some threshold typically 0 the neuron fires and takes the activated value typically 1; otherwise it takes the deactivated value typically 0.  
        $$f\mathbf{x}=\left\{\begin{array}{ll}{1} & {\text { if } \mathbf{w} \cdot \mathbf{x}+b>0}  {0} & {\text { otherwise }}\end{array}\right.$$  
        
     Multi Layer Perceptron#content2  
        This class of networks consists of multiple layers of computational units usually interconnected in a feed forward way. Each neuron in one layer has directed connections to the neurons of the subsequent layer. In many applications the units of these networks apply a sigmoid function as an activation function.  
        
  


Multilayer Perceptron

1. Multilayer Perceptron
    The Multilayer Perceptron MLP is a class of FeedForward Neural Networks that is used for learning from data.  
    

2. Architecture
    The MLP consists of at least three layers of nodes input layer hidden layer and an output layer.  

    The layers in a neural network are connected by certain weights and the MLP is known as a fully connected network where each neuron in one layer is connected with a weight $$w{ij}$$ to every node in the following layer.  
    
    Each node except for the input nodes uses a non linear activation function that were developed to model the frequency of action potential firing of biological neurons.  
    

3. Learning
    The MLP employs a supervised learning technique called backpropagation.  
    Learning occurs by changing the weights connecting the layers based on the amount of error in the output compared to the expected result. Those weights are changed by using gradient methods to optimize a given objective function called the loss function.  
    

4. Properties
     Due to their non linearity MLPs can distinguish and model non linearly separable data    

     Without the non linear activation functions MLPs will be identical to Perceptrons since Linear Algebra shows that the linear transformations in many hidden layers can be collapsed into one linear transformation  
    