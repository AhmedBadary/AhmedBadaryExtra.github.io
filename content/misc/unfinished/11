TITLE: Algorithms
LINK: research/algorithms.md


## Divide-and-Conquer
{: #content1}

1. **Divide-and-Conquer:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    The divide-and-conquer strategy solves a problem by:  
    1. Breaking it into subproblems that are themselves smaller instances of the same type of problem
    2. Recursively solving these subproblems
    3. Appropriately combining their answers
    <br>

    The real work is done piecemeal, in three different places:  
    {: #lst-p}
    1. in the partitioning of problems into subproblems
    2. at the very tail end of the recursion, when the subproblems are so small that they are solved outright
    3. and in the gluing together of partial answers.  

    These are held together and coordinated by the algorithm‚Äôs core recursive structure.  
    <br>



4. **Binary Search:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    img1

5. **Mergesort:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    The problem of sorting a list of numbers lends itself immediately to a divide-and-conquer strategy: split the list into two halves, recursively sort each half, and then merge the two sorted sublists.  

    img2

    __Merge Function:__{: style="color: red"}  
    Given two sorted arrays $$x[1 . . . k]$$ and $$y[1 . . . l]$$, how do we efficiently merge them into a single sorted array $$z[1 . . . k + l]$$?  
    img3

    Here ‚ó¶ denotes concatenation.  

    __Run-Time:__  
    This __merge__ procedure does a constant amount of work per recursive call (provided the required array space is allocated in advance), for a total running time of $$O(k + l)$$. Thus merge‚Äôs are __linear__, and the overall time taken by mergesort is:  
    <p>$$T(n)=2 T(n / 2)+O(n) = O(n \log n)$$</p>  
    ‚òπÔ∏è


    __Iterative MergeSort:__{: style="color: red"}  
    img4


6. **$$n \log n$$ Lower Bound for (comparison-based) Sorting (Proof):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    img5
    <br>

7. **Medians:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    
    <br>


    <br>

***

## SECOND
{: #content2}









***

## THIRD
{: #content3}









***

## FOURTH
{: #content4}









***

## FIFTH
{: #content5}









*** 

## Sixth
{: #content6}









## Seven
{: #content7}










***
***

TITLE: Reinforcement Learning
LINK: research/rl.md



[Deep RL WS1](https://sites.google.com/view/deep-rl-bootcamp/lectures)  
[Deep RL WS2](https://sites.google.com/view/deep-rl-workshop-nips-2018/home)  
[Deep RL Lec CS294 Berk](http://rail.eecs.berkeley.edu/deeprlcourse/)  
[Reinforcement Learning Course Lectures UCL](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)  
[RL CS188](https://inst.eecs.berkeley.edu/~cs188/fa18/)  
[Deep RL (CS231n Lecture)](https://www.youtube.com/watch?v=lvoHnicueoE)  
[Deep RL (CS231n Slides)](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture14.pdf)  
* [An Outsider's Tour of Reinforcement Learning (Ben Recht!!!)](http://www.argmin.net/2018/06/25/outsider-rl/)  
* [Reinforcement Learning Series (Tutorial + Code (vids))](https://www.youtube.com/playlist?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv)  



## Intro - Reinforcement Learning
{: #content1}

1. **Reinforcement Learning:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    
    ![img](https://cdn.mathpix.com/snip/images/KD106EKeWa3NIEqKiMNXELeZ6beGrRAXTkyVo1Iq2sc.original.fullsize.png){: width="80%"}  


3. **Mathematical Formulation of RL - Markov Decision Processes:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    __Markov Decision Process__  
    
    Defined by $$(\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathbb{P}, \gamma)$$:  
    {: #lst-p}
    * $$\mathcal{S}$$: set of possible states
    * $$\mathcal{A}$$: set of possible actions
    * $$\mathcal{R}$$: distribution of reward given (state, action) pair
    * $$\mathbb{P}$$: transition probability i.e. distribution over next state given (state, action) pair
    * $$\gamma$$: discount factor  

    __MDPs Algorithm/Idea:__  
    {: #lst-p}
    - At time step $$\mathrm{t}=0,$$ environment samples initial state $$\mathrm{s}_ {0} \sim \mathrm{p}\left(\mathrm{s}_ {0}\right)$$
    - Then, for $$\mathrm{t}=0$$ until done:
        - Agent selects action $$a_t$$ 
        - Environment samples reward $$\mathrm{r}_ {\mathrm{t}} \sim \mathrm{R}\left( . \vert \mathrm{s}_{\mathrm{t}}, \mathrm{a}_ {\mathrm{t}}\right)$$
        - Environment samples next state $$\mathrm{s}_ {\mathrm{t}+1} \sim \mathrm{P}\left( . \vert \mathrm{s}_ {\mathrm{t}}, \mathrm{a}_ {\mathrm{t}}\right)$$
        - Agent receives reward $$\mathrm{r}_ {\mathrm{t}}$$ and next state $$\mathrm{s}_ {\mathrm{t}+1}$$

    \- A policy $$\pi$$ is a _function_ from $$S$$ to $$A$$ that specifies what action to take in each state  
    \- Objective: find policy $$\pi^{\ast}$$ that maximizes cumulative discounted reward:  
    <p>$$\sum_{t \geq 0} \gamma^{t} r_{t}$$</p>  


    __Optimal Policy $$\pi^{\ast}$$:__{: style="color: red"}  
    We want to find optimal policy $$\mathbf{n}^{\ast}$$ that maximizes the sum of rewards.  
    We handle __randomness__  (initial state, transition probability...) by __Maximizing the *expected sum of rewards*__.  
    __Formally__,  
    <p>$$\pi^{* }=\arg \max _{\pi} \mathbb{E}\left[\sum_{t \geq 0} \gamma^{t} r_{t} | \pi\right] \quad$ \text{ with } $s_{0} \sim p\left(s_{0}\right), a_{t} \sim \pi\left(\cdot | s_{t}\right), s_{t+1} \sim p\left(\cdot | s_{t}, a_{t}\right)$$</p>  



    __The Bellman Equations:__{: style="color: red"}  
    Definition of ‚Äúoptimal utility‚Äù via expectimax recurrence gives a simple one-step lookahead relationship amongst optimal utility values.  
    The __Bellman Equations__ <span>_characterize_ optimal values</span>{: style="color: purple"}:    
    <p>$$\begin{aligned} V^{ * }(s) &= \max _{a}\left(s^{*}(s, a)\right. \\
                     Q^{ * }(s, a) &= \sum_{s^{\prime}} T\left(s, a, s^{\prime}\right)\left[R\left(s, a, s^{\prime}\right)+\gamma V^{ * }\left(s^{\prime}\right)\right] \\
                     V^{ * }(s) &= \max _{a} \sum_{s^{\prime}} T\left(s, a, s^{\prime}\right)\left[R\left(s, a, s^{\prime}\right)+\gamma V^{ * }\left(s^{\prime}\right)\right] \end{aligned}$$</p>  

    __Value Iteration Algorithm:__{: style="color: red"}  
    The __Value Iteration__ algorithm <span>_computes_ the optimal values</span>{: style="color: purple"}:  
    <p>$$V_{k+1}(s) \leftarrow \max _{a} \sum_{s^{\prime}} T\left(s, a, s^{\prime}\right)\left[R\left(s, a, s^{\prime}\right)+\gamma V_{k}\left(s^{\prime}\right)\right]$$</p>   
    \- Value iteration is just a fixed point solution method.  
    \- It is repeated bellman equations.  

    __Convergence:__  
    <button>Convergence</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/fTeplubG_bAgJuyrStw26Y1K9Tms89dphOrOcNseKzY.original.fullsize.png){: width="100%" hidden=""}  

    __Issues:__  
    {: #lst-p}
    * Problem 1: It‚Äôs slow ‚Äì $$O(S^2A)$$ per iteration
    * Problem 2: The ‚Äúmax‚Äù at each state rarely changes
    * Problem 3: The policy often converges long before the values  
    * Problem 4: Not scalable. Must compute $$Q(s, a)$$ for every state-action pair. If state is e.g. current game state pixels, computationally infeasible to compute for entire state space  


    __Policy Iteration:__{: style="color: red"}  
    It is an Alternative approach for optimal values:  
    __Policy Iteration algorithm:__  
    {: #lst-p}
    * Step \#1 __Policy evaluation:__ calculate utilities for some fixed policy (not) of to
    utilitiesl until convergence
    * Step #2: __Policy improvement:__ update policy using one-step look-ahead with resulting (but not optimall) utilities af future values  
    * Repeat steps until policy converges  

    * __Evaluation:__  
        For fixed current policy $$\pi$$, find values with policy evaluation:  
        * Iterate until values converge:  
            <p>$$V_{k+1}^{\pi_{i}}(s) \leftarrow \sum_{s^{\prime}} T\left(s, \pi_{i}(s), s^{\prime}\right)\left[R\left(s, \pi_{i}(s), s^{\prime}\right)+\gamma V_{k}^{\pi_{i}}\left(s^{\prime}\right)\right]$$</p>  
    * __Improvement:__  
        For fixed values, get a better policy using policy extraction:  
        * One-step look-ahead:  
            <p>$$\pi_{i+1}(s)=\arg \max _{a} \sum_{s^{\prime}} T\left(s, a, s^{\prime}\right)\left[R\left(s, a, s^{\prime}\right)+\gamma V^{\pi_{i}}\left(s^{\prime}\right)\right]$$</p>  

    __Properties:__  
    {: #lst-p}
    * It's still __optimal__
    * Can can converge (much) faster under some conditions  


    __Comparison - Value Iteration vs Policy Iteration:__  
    <button>Comparison</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/yoH-nwG1Hunddc_DcRunAFYup5Nf6kcspI44gQ7tujw.original.fullsize.png){: width="100%" hidden=""}  


    __Q-Learning \| Solving for Optimal Policy:__{: style="color: red"}  
    A problem with __value iteration__ was: It is Not scalable. Must compute $$Q(s, a)$$ for every state-action pair.  
    __Q-Learning__ solves this by using a function approximator to estimate the action-value function:  
    <p>$$Q(s, a ; \theta) \approx Q^{* }(s, a)$$</p>  
    __Deep Q-learning:__ the case where the function approximator is a deep neural net.  

    __Training:__  
    ![img](https://cdn.mathpix.com/snip/images/Ppeh18nh-ofk-A0puhgN4__m90utYwRBYO9KTtpYDwg.original.fullsize.png){: width="80%"}  

    <button>Example Network - Learning Atari Games</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/H9XsqlC8tNbJo2VJUdZvjZkFzpCwFjhsXcd4EUgX1I0.original.fullsize.png){: width="100%" hidden=""}  

    __Experience Replay__  
    ![img](https://cdn.mathpix.com/snip/images/v_zXml9JyIzF83-gfy-07iBEQt65M22vCc5qqVLs95s.original.fullsize.png){: width="60%"}  

    __Deep Q-learning with Experience Replay - Algorithm:__  
    <button>Algorithm</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/uco5J96x_Bnz6KA8VE3XMUF2OM6rRfOsI399USWjDms.original.fullsize.png){: width="70%" hidden=""}  


    <br>
    __Policy Gradients:__{: style="color: red"}  
    An alternative to learning a Q-function.  
    Q-functions can be very complicated.  
    > Example: a robot grasping an object has a very high-dimensional state => hard to learn exact value of every (state, action) pair.  

    \- Define a __class of parameterized policies__:  
    <p>$$\Pi=\left\{\pi_{\theta}, \theta \in \mathbb{R}^{m}\right\}$$</p>  
    \- For each policy, define its __value__:  
    <p>$$J(\theta)=\mathbb{E}\left[\sum_{t \geq 0} \gamma^{t} r_{t} | \pi_{\theta}\right]$$</p>  
    \- Find the __optimal policy__ $$\theta^{ * }=\arg \max _ {\theta} J(\theta)$$ by __gradient ascent on *policy parameters*__ (__REINFORCE Algorithm__)   

    __REINFORCE Algorithm:__{: style="color: red"}  
    __Expected Reward:__  
    <p>$$\begin{aligned} J(\theta) &=\mathbb{E}_{\tau \sim p(\tau ; \theta)}[r(\tau)] \\ &=\int_{\tau} r(\tau) p(\tau ; \theta) \mathrm{d} \tau \end{aligned}$$</p>  
    where $$r(\tau)$$ is the reward of a trajectory $$\tau=\left(s_{0}, a_{0}, r_{0}, s_{1}, \dots\right)$$.  
    __The Gradient:__  
    <p>$$\nabla_{\theta} J(\theta)=\int_{\tau} r(\tau) \nabla_{\theta} p(\tau ; \theta) \mathrm{d} \tau$$</p>  
    \- The Gradient is __Intractable__. Gradient of an expectation is problematic when $$p$$ depends on $$\theta$$.  
    \- __Solution:__  
    * __Trick:__  
        <p>$$\nabla_{\theta} p(\tau ; \theta)=p(\tau ; \theta) \frac{\nabla_{\theta} p(\tau ; \theta)}{p(\tau ; \theta)}=p(\tau ; \theta) \nabla_{\theta} \log p(\tau ; \theta)$$</p>  
    * __Injecting Back:__  
        <p>$$\begin{aligned} \nabla_{\theta} J(\theta) &=\int_{\tau}\left(r(\tau) \nabla_{\theta} \log p(\tau ; \theta)\right) p(\tau ; \theta) \mathrm{d} \tau \\ &=\mathbb{E}_{\tau \sim p(\tau ; \theta)}\left[r(\tau) \nabla_{\theta} \log p(\tau ; \theta)\right] \end{aligned}$$</p>  
    * __Estimating the Gradient:__ Can estimate with *__Monte Carlo sampling__*.  
        * The gradient does NOT depend on _transition probabilities:_  
            * $$p(\tau ; \theta)=\prod_{t \geq 0} p\left(s_{t+1} | s_{t}, a_{t}\right) \pi_{\theta}\left(a_{t} | s_{t}\right)$$  
            * $$\log p(\tau ; \theta)=\sum_{t \geq 0} \log p\left(s_{t+1} | s_{t}, a_{t}\right)+\log \pi_{\theta}\left(a_{t} | s_{t}\right)$$  
                $$\implies$$ 
            * $$\nabla_{\theta} \log p(\tau ; \theta)=\sum_{t \geq 0} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right)$$  
        * Therefore when sampling a trajectory $$\tau,$$ we can estimate $$J(\theta)$$ with:  
            <p>$$\nabla_{\theta} J(\theta) \approx \sum_{t \geq 0} r(\tau) \nabla_{\theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right)$$</p>  
    * __Gradient Estimator__:  
        <p>$$\nabla_{\theta} J(\theta) \approx \sum_{t \geq 0} r(\tau) \nabla_{\theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right)$$</p>  
        * __Intuition/Interpretation__:  
            * If $$\mathrm{r}(\tau)$$ is high, push up the probabilities of the actions seen
            * If $$\mathrm{r}(\tau)$$ is low, push down the probabilities of the actions seen
        Might seem simplistic to say that if a trajectory is good then all its actions were good. But in expectation, it averages out!  
        * __Variance__:  
            * __Issue__: This also suffers from __high variance__ because credit assignment is really hard.  
            * __Variance Reduction - Two Ideas__:  
                1. Push up probabilities of an action seen, only by the cumulative future reward from that state:  
                    <p>$$\nabla_{\theta} J(\theta) \approx \sum_{t \geq 0}\left(\sum_{t^{\prime} \geq t} r_{t^{\prime}}\right) \nabla_{\theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right)$$</p>  
                2. Use discount factor $$\gamma$$ to ignore delayed effects  
                    <p>$$\nabla_{\theta} J(\theta) \approx \sum_{t \geq 0}\left(\sum_{t^{\prime} \geq t} \gamma^{t^{\prime}-t} r_{t^{\prime}}\right) \nabla_{\theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right)$$</p>  

                \- __Problem:__ The raw value of a trajectory isn‚Äôt necessarily meaningful. For example, if rewards are all positive, you keep pushing up probabilities of actions.  
                \- __What is important then:__ Whether a reward is better or worse than what you expect to get.  
                \- __Solution:__ Introduce a __baseline function__ dependent on the state.  
                \-Concretely, estimator is now:  
                <p>$$\nabla_{\theta} J(\theta) \approx \sum_{t \geq 0}\left(\sum_{t^{\prime} \geq t} \gamma^{t^{\prime}-t} r_{t^{\prime}}-b\left(s_{t}\right)\right) \nabla_{\theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right)$$</p>  
                * __Choosing a Baseline__:  
                    * __Vanilla REINFORCE__:  
                        A simple baseline: constant moving average of rewards experienced so far from all trajectories.  
                    * __Actor-Critic__:  
                        We want to push up the probability of an action from a state, if this action was better than the __expected value of what we should get from that state__.  
                        Intuitively, we are happy with an action $$a_{t}$$ in a state $$s_{t}$$ if $$Q^{\pi}\left(s_{t}, a_{t}\right)-V^{\pi}\left(s_{t}\right)$$ is large. On the contrary, we are unhappy with an action if it's small.  
                        Now, the estimator:  
                        <p>$$\nabla_{\theta} J(\theta) \approx \sum_{t \geq 0}\left(Q^{\pi_{\theta}}\left(s_{t}, a_{t}\right)-V^{\pi_{\theta}}\left(s_{t}\right)\right) \nabla_{\theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right)$$</p>  
                        * __Learning $$Q$$ and $$V$$__:  
                            We learn $$Q, V$$ using the __Actor-Critic Algorithm__.  

    __Actor-Critic Algorithm:__{: style="color: red"}  
    An algorithm to learn $$Q$$ and $$V$$.  
    We can combine Policy Gradients and Q-learning by training both:  
    * __Actor:__ the policy, and 
    * __Critic:__ the Q-function  

    __Details:__  
    - The actor decides which action to take, and the critic tells the actor how good its action was and how it should adjust
    - Also alleviates the task of the critic as it only has to learn the values of (state, action) pairs generated by the policy
    - Can also incorporate Q-learning tricks e.g. experience replay
    - Remark: we can define by the advantage function how much an action was better than expected  

    __Algorithm:__  
    <button>Algorithm</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/oRVXry6DNumat6k3h_a3IYPz4Qjb-3_iiCg1ShkjwSk.original.fullsize.png){: width="80%" hidden=""}  


    <br>
    __Summary:__{: style="color: red"}  
    {: #lst-p}
    - __Policy gradients:__ very general but suffer from high variance so
    requires a lot of samples. Challenge: sample-efficiency
    - __Q-learning:__ does not always work but when it works, usually more
    sample-efficient. Challenge: exploration  

    - __Guarantees:__  
        - __Policy Gradients:__ Converges to a local minima of J(ùúÉ), often good enough!
        - __Q-learning:__ Zero guarantees since you are approximating Bellman equation with a complicated function approximator




***


***
***

TITLE: Probability Theory <br /> Mathematics of Deep Learning
LINK: research/stats_prob/prob.md



[Review of Probability Theory (Stanford)](http://cs229.stanford.edu/section/cs229-prob.pdf)  
[A First Course in Probability (Book: _Sheldon Ross_)](http://julio.staff.ipb.ac.id/files/2015/02/Ross_8th_ed_English.pdf)  
[Statistics 110: Harvard](https://www.youtube.com/playlist?list=PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo)  
[Lecture Series on Probability (following DL-book)](https://www.youtube.com/playlist?list=PLR6O_WZHBlOELxOrXlzB1LCXd2cUIXkkm)  
[Probability Quora FAQs](https://www.quora.com/What-is-the-probability-statistics-topic-FAQ)  
[Math review for Stat 110](https://projects.iq.harvard.edu/files/stat110/files/math_review_handout.pdf)  
[Deep Learning Probability](https://jhui.github.io/2017/01/05/Deep-learning-probability-and-distribution/)  
[Probability as Extended Logic](http://bjlkeng.github.io/posts/probability-the-logic-of-science/)  
[CS188 Probability Lecture (very intuitive)](https://www.youtube.com/watch?v=sMNbLXsvRig&list=PL7k0r4t5c108AZRwfW-FhnkZ0sCKBChLH&index=13&t=0s)  


## Motivation
{: #content1}

1. **Uncertainty in General Systems and the need for a Probabilistic Framework:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    1. __Inherent stochasticity in the system being modeled:__  
        Take Quantum Mechanics, most interpretations of quantum mechanics describe the dynamics of sub-atomic particles as being probabilistic.  
    2. __Incomplete observability__:  
        Deterministic systems can appear stochastic when we cannot observe all the variables that drive the behavior of the system.  
        > i.e. Point-of-View determinism (Monty-Hall)  
    3. __Incomplete modeling__:  
        Building a system that makes strong assumptions about the problem and discards (observed) information result in uncertainty in the predictions.    
    <br />

2. **Bayesian Probabilities and Frequentist Probabilities:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    __Frequentist Probabilities__ describe the predicted number of times that a __repeatable__ process will result in a given output in an absolute scale.  

    __Bayesian Probabilities__ describe the _degree of belief_ that a certain __non-repeatable__ event is going to result in a given output, in an absolute scale.      
    
    We assume that __Bayesian Probabilities__ behaves in exactly the same way as __Frequentist Probabilities__.  
    This assumption is derived from a set of _"common sense"_ arguments that end in the logical conclusion that both approaches to probabilities must behave the same way - [Truth and probability (Ramsey 1926)](https://socialsciences.mcmaster.ca/econ/ugcm/3ll3/ramseyfp/ramsess.pdf).

3. **Probability as an extension of Logic:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    "Probability can be seen as the extension of logic to deal with uncertainty. Logic provides a set of formal rules for determining what propositions are implied to be true or false given the assumption that some other set of propositions is true or false. Probability theory provides a set of formal rules for determining the likelihood of a proposition being true given the likelihood of other propositions." - deeplearningbook p.54


***

## Basics
{: #content2}

0. **Elements of Probability:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents20}  
    * __Sample Space $$\Omega$$__: The set of all the outcomes of a stochastic experiment; where each _outcome_ is a complete description of the state of the real world at the end of the experiment.  
    * __Event Space $${\mathcal {F}}$$__: A set of _events_; where each event $$A \in \mathcal{F}$$ is a subset of the sample space $$\Omega$$ - it is a collection of possible outcomes of an experiment.  
    * __Probability Measure $$\operatorname {P}$$__: A function $$\operatorname {P}: \mathcal{F} \rightarrow \mathbb{R}$$ that satisfies the following properties:  
        * $$\operatorname {P}(A) \geq 0, \: \forall A \in \mathcal{f}$$, 
        * $$\operatorname {P}(\Omega) = 1$$, $$\operatorname {P}(\emptyset) = 0$$[^1]  
        * $${\displaystyle \operatorname {P}(\bigcup_i A_i) = \sum_i \operatorname {P}(A_i) }$$, where $$A_1, A_2, ...$$ are [_disjoint_ events](#bodyContents102)  

    __Properties:__{: style="color: red"}  
    * $${\text { If } A \subseteq B \Longrightarrow P(A) \leq P(B)}$$,   
    * $${P(A \cap B) \leq \min (P(A), P(B))} $$,  
    * __Union Bound:__ $${P(A \cup B) \leq P(A)+P(B)}$$  
    * $${P(\Omega \backslash A)=1-P(A)}$$.  
    * __Law of Total Probability (LOTB):__ $$\text { If } A_{1}, \ldots, A_{k} \text { are a set of disjoint events such that } \cup_{i=1}^{k} A_{i}=\Omega, \text { then } \sum_{i=1}^{k} P\left(A_{k}\right)=1$$  
    * __Inclusion-Exclusion Principle__:  
        <p>$$\mathbb{P}\left(\bigcup_{i=1}^{n} A_{i}\right)=\sum_{i=1}^{n} \mathbb{P}\left(A_{i}\right)-\sum_{i< j} \mathbb{P}\left(A_{i} \cap A_{j}\right)+\sum_{i< j < k} \mathbb{P}\left(A_{i} \cap A_{j} \cap A_{k}\right)-\cdots+(-1)^{n-1} \sum_{i< \ldots< n} \mathbb{P}\left(\bigcap_{i=1}^{n} A_{i}\right)$$</p>  
        * [**Example 110**](https://www.youtube.com/embed/LZ5Wergp_PA?start=2057){: value="show" onclick="iframePopA(event)"}
        <a href="https://www.youtube.com/embed/LZ5Wergp_PA?start=2057"></a>
            <div markdown="1"> </div>    
              

    * [**Properties and Proofs 110**](https://www.youtube.com/embed/LZ5Wergp_PA?start=1359){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/LZ5Wergp_PA?start=1359"></a>
        <div markdown="1"> </div>    

[^1]: Corresponds to "wanting" the probability of events that are __certain__ to have p=1 and events that are __impossible__ to have p=0  
                

1. **Random Variables:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    A __Random Variable__ is a variable that can take on different values randomly.  
    Formally, a random variable $$X$$ is a _function_ that maps outcomes to numerical quantities (labels), typically real numbers:
    <p>$${\displaystyle X\colon \Omega \to \mathbb{R}}$$</p>  

    Think of a R.V.: as a numerical "summary" of an aspect of the experiment.  

    __Types__:
    * *__Discrete__*: is a variable that has a finite or countably infinite number of states  
    * *__Continuous__*: is a variable that is a real value  

    __Examples:__  
    * __Bernoulli:__ A r.v. $$X$$ is said to have a __Bernoulli__ distribution if $$X$$ has only $$2$$ possible values, $$0$$ and $$1$$, and $$P(X=1) = p, P(X=0) = 1-p$$; denoted $$\text{Bern}(p)$$.    
    * __Binomial__: The distr. of #successes in $$n$$ independent __$$\text{Bern}(p)$$__ trials and its distribution is $$P(X=k) = \left(\begin{array}{l}{n} \\ {k}\end{array}\right) p^k (1-p)^{n-k}$$; denoted $$\text{Bin}(n, p)$$.          
    <br>

2. **Probability Distributions:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    A __Probability Distribution__ is a function that describes the likelihood that a random variable (or a set of r.v.) will take on each of its possible states.  
    Probability Distributions are defined in terms of the __Sample Space__.  
    * __Classes__:  
        * *__Discrete Probability Distribution:__* is encoded by a discrete list of the probabilities of the outcomes, known as a __Probability Mass Function (PMF)__.  
        * *__Continuous Probability Distribution:__* is described by a __Probability Density Function (PDF)__.  
    * __Types__:  
        * *__Univariate Distributions:__* are those whose sample space is $$\mathbb{R}$$.  
        They give the probabilities of a single random variable taking on various alternative values 
        * *__Multivariate Distributions__* (also known as *__Joint Probability distributions__*):  are those whose sample space is a vector space.   
        They give the probabilities of a random vector taking on various combinations of values.  


    A __Cumulative Distribution Function (CDF)__: is a general functional form to describe a probability distribution:  
    <p>$${\displaystyle F(x)=\operatorname {P} [X\leq x]\qquad {\text{ for all }}x\in \mathbb {R} .}$$</p>  
    > Because a probability distribution P on the real line is determined by the probability of a scalar random variable X being in a half-open interval $$(‚àí\infty, x]$$, the probability distribution is completely characterized by its cumulative distribution function (i.e. one can calculate the probability of any event in the event space)  
    
 

3. **Probability Mass Function:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    A __Probability Mass Function (PMF)__ is a function (probability distribution) that gives the probability that a discrete random variable is exactly equal to some value.  
    __Mathematical Definition__:  
    Suppose that $$X: S \rightarrow A, \:\:\: (A {\displaystyle \subseteq }  \mathbb{R})$$ is a discrete random variable defined on a sample space $$S$$. Then the probability mass function $$f_X: A \rightarrow [0, 1]$$ for $$X$$ is defined as:   
    <p>$$p_{X}(x)=P(X=x)=P(\{s\in S:X(s)=x\})$$</p>  
    The __total probability for all hypothetical outcomes $$x$$ is always conserved__:  
    <p>$$\sum _{x\in A}p_{X}(x)=1$$</p>
    __Joint Probability Distribution__ is a PMF over many variables, denoted $$P(\mathrm{x} = x, \mathrm{y} = y)$$ or $$P(x, y)$$.  

    A __PMF__ must satisfy these properties:  
    * The domain of $$P$$ must be the set of all possible states of $$\mathrm{x}$$.  
    * $$\forall x \in \mathrm{x}, \: 0 \leq P(x) \leq 1$$. Impossible events has probability $$0$$. Guaranteed events have probability $$1$$.  
    * $${\displaystyle \sum_{x \in \mathrm{x}} P(x) = 1}$$, i.e. the PMF must be normalized.  
    <br>
            
4. **Probability Density Function:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    A __Probability Density Function (PDF)__ is a function (probability distribution) whose value at any given sample (or point) in the sample space can be interpreted as providing a relative likelihood that the value of the random variable would equal that sample.  
    The __PDF__ is defined as the _derivative_ of the __CDF__:  
    <p>$$f_{X}(x) = \dfrac{dF_{X}(x)}{dx}$$</p>  
    A Probability Density Function $$p(x)$$ does not give the probability of a specific state directly; instead the probability of landing inside an infinitesimal region with volume $$\delta x$$ is given by $$p(x)\delta x$$.  
    We can integrate the density function to find the actual probability mass of a set of points. Specifically, the probability that $$x$$ lies in some set $$S$$ is given by the integral of $$p(x)$$ over that set.  
    > In the __Univariate__ example, the probability that $$x$$ lies in the interval $$[a, b]$$ is given by $$\int_{[a, b]} p(x)dx$$  


    A __PDF__ must satisfy these properties:  
    * The domain of $$P$$ must be the set of all possible states of $$x$$.  
    * $$\forall x \in \mathrm{x}, \: 0 \leq P(x) \leq 1$$. Impossible events has probability $$0$$. Guaranteed events have probability $$1$$.  
    * $$\int p(x)dx = 1$$, i.e. the integral of the PDF must be normalized.  
    <br>


44. **Cumulative Distribution Function:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents244}  
    A __Cumulative Distribution Function (CDF)__ is a function (probability distribution) of a real-valued random variable $$X$$, or just distribution function of $$X$$, evaluated at $$x$$, is the probability that $$X$$ will take a value less than or equal to $$x$$.    
    <p>$$F_{X}(x)=\operatorname {P} (X\leq x)$$ </p>  
    The probability that $$X$$ lies in the semi-closed interval $$(a, b]$$, where $$a  <  b$$, is therefore  
    <p>$${\displaystyle \operatorname {P} (a<X\leq b)=F_{X}(b)-F_{X}(a).}$$</p>  
    
    __Properties__:    
    * $$0 \leq F(x) \leq 1$$, 
    * $$\lim_{x \rightarrow -\infty} F(x) = 0$$, 
    * $$\lim_{x \rightarrow \infty} F(x) = 1$$, 
    * $$x \leq y \implies F(x) \leq F(y)$$.  
    <br>

5. **Marginal Probability:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    The __Marginal Distribution__ of a subset of a collection of random variables is the probability distribution of the variables contained in the subset.  
    __Two-variable Case__:  
    Given two random variables $$X$$ and $$Y$$ whose joint distribution is known, the marginal distribution of $$X$$ is simply the probability distribution of $$X$$ averaging over information about $$Y$$.
    * __Discrete__:    
        <p>$${\displaystyle \Pr(X=x)=\sum_ {y}\Pr(X=x,Y=y)=\sum_ {y}\Pr(X=x\mid Y=y)\Pr(Y=y)}$$</p>    
    * __Continuous__:    
        <p>$${\displaystyle p_{X}(x)=\int _{y}p_{X,Y}(x,y)\,\mathrm {d} y=\int _{y}p_{X\mid Y}(x\mid y)\,p_{Y}(y)\,\mathrm {d} y}$$</p>  
    * *__Marginal Probability as Expectation__*:    
    <p>$${\displaystyle p_{X}(x)=\int _{y}p_{X\mid Y}(x\mid y)\,p_{Y}(y)\,\mathrm {d} y=\mathbb {E} _{Y}[p_{X\mid Y}(x\mid y)]}$$</p>  
    <button>Intuitive Explanation</button>{: .showText value="show"  
     onclick="showTextPopHide(event);"}
    ![img](/main_files/math/prob/1.png){: width="100%" hidden=""}  

    __Marginalization:__{: style="color: red"} the process of forming the marginal distribution with respect to one variable by summing out the other variable  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * __Marginal Distribution of a variable__: is just the prior distr of the variable  
    * __Marginal Likelihood__: also known as the evidence, or model evidence, is the denominator of the Bayes equation. Its only role is to guarantee that the posterior is a valid probability by making its area sum to 1.  
        ![Example](https://cdn.mathpix.com/snip/images/UPUhBUhhUivvvIHO3nt5S52UcqPkSMS_eZEg3mhDXhk.original.fullsize.png)  
    * __both terms above are the same__  
    * __Marginal Distr VS Prior__:  
        * [Discussion](https://stats.stackexchange.com/questions/249275/whats-the-difference-between-prior-and-marginal-probabilities?rq=1)  
        * __Summary__:  
            Basically, it's a conceptual difference.  
            The prior, denoted $$p(\theta)$$, denotes the probability of some event ùúî even before any data has been taken.  
            A marginal distribution is rather different. You hold a variable value and integrate over the unknown values.  
            But, in some contexts they are the same.  

            


            


6. **Conditional Probability:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    __Conditional Probability__ is a measure of the probability of an event given that another event has occurred.  
    Conditional Probability is only defined when $$P(x) > 0$$ - We cannot compute the conditional probability conditioned on an event that never happens.   
    __Definition__:  
    <p>$$P(A|B)={\frac {P(A\cap B)}{P(B)}} = {\frac {P(A, B)}{P(B)}}$$</p>  

    > Intuitively, it is a way of updating your beliefs/probabilities given new evidence. It's inherently a sequential process.  



7. **The Chain Rule of Conditional Probability:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  
    Any joint probability distribution over many random variables may be decomposed into conditional distributions over only one variable.  
    The chain rule permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities:    
    <p>$$\mathrm {P} \left(\bigcap _{k=1}^{n}A_{k}\right)=\prod _{k=1}^{n}\mathrm {P} \left(A_{k}\,{\Bigg |}\,\bigcap _{j=1}^{k-1}A_{j}\right)$$</p>  

8. **Independence and Conditional Independence:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}  
    Two random variables $$x$$ and $$y$$ (or events ) are __independent__ if their probability distribution can be expressed as a product of two factors, one involving only $$x$$ and one involving only $$y$$:  
    <p>$$\mathrm{P}(A \cap B) = \mathrm{P}(A)\mathrm{P}(B)$$</p>  

    Two random variables $$A$$ and $$B$$ are __conditionally independent__ _given a random variable $$Y$$_ if the conditional probability distribution over $$A$$ and $$B$$ factorizes in this way for every value of $$Y$$:  
    <p>$$\Pr(A\cap B\mid Y)=\Pr(A\mid Y)\Pr(B\mid Y)$$</p>  
    or equivalently,  
    <p>$$\Pr(A\mid B\cap Y)=\Pr(A\mid Y)$$</p>  
    > In other words, $$A$$ and $$B$$ are conditionally independent given $$Y$$ if and only if, given knowledge that $$Y$$ occurs, knowledge of whether $$A$$ occurs provides no information on the likelihood of $$B$$ occurring, and knowledge of whether $$B$$ occurs provides no information on the likelihood of $$A$$ occurring.  


    __Pairwise VS Mutual Independence:__{: style="color: red"}  
    * __Pairwise__:  
        <p>$$\mathrm{P}\left(A_{m} \cap A_{k}\right)=\mathrm{P}\left(A_{m}\right) \mathrm{P}\left(A_{k}\right)$$</p>  
    * __Mutual Independence:__
        <p>$$\mathrm{P}\left(\bigcap_{i=1}^{k} B_{i}\right)=\prod_{i=1}^{k} \mathrm{P}\left(B_{i}\right)$$</p>  
        for *__all subsets__* of size $$k \leq n$$  

    __Pairwise__ independence does __not__ imply __mutual__ independence, but the other way around is TRUE (by definition).  



    __Notation:__  
    * *__$$A$$ is Independent from $$B$$__*:  $$A{\perp}B$$
    * *__$$A$$ and $$B$$ are conditionally Independent given $$Y$$__*:  $$A{\perp}B \:\vert Y$$  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * Unconditional Independence is very rare (there is usually some hidden factor influencing the interaction between the two events/variables)  
    * _Conditional Independence_ is the most basic and robust form of knowledge about uncertain environments  
            
    <br>
                
9. **Expectation:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents29}  
    The __expectation__, or __expected value__, of some function $$f(x)$$ with respect to a probability distribution $$P(x)$$ is the _"theoretical"_ average, or mean value, that $$f$$ takes on when $$x$$ is drawn from $$P$$.  
    > The Expectation of a R.V. is a weighted average of the values $$x$$ that the R.V. can take -- $$\operatorname {E}[X] = \sum_{x \in X} x \cdot p(x)$$  
    * __Discrete case__:  
        <p>$${\displaystyle \operatorname {E}_{x \sim P} [f(X)]=f(x_{1})p(x_{1})+f(x_{2})p(x_{2})+\cdots +f(x_{k})p(x_{k})} = \sum_x P(x)f(x)$$</p>             
    * __Continuous case__:  
    <p>$${\displaystyle \operatorname {E}_ {x \sim P} [f(X)] = \int p(x)f(x)dx}$$</p>   
    __Linearity of Expectation:__  
    <p>$${\displaystyle {\begin{aligned}\operatorname {E} [X+Y]&=\operatorname {E} [X]+\operatorname {E} [Y],\\[6pt]\operatorname {E} [aX]&=a\operatorname {E} [X],\end{aligned}}}$$</p>   
    __Independence:__   
    If $$X$$ and $$Y$$ are independent $$\implies \operatorname {E} [XY] = \operatorname {E} [X] \operatorname {E} [Y]$$  
    <br>

10. **Variance:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents210}  
    __Variance__ is the expectation of the squared deviation of a random variable from its mean.  
    It gives a measure of how much the values of a function of a random variable $$x$$ vary as we sample different values of $$x$$ from its probability distribution:  
    <p>$$\operatorname {Var} (f(x))=\operatorname {E} \left[(f(x)-\mu )^{2}\right] = \sum_{x \in X} (x - \mu)^2 \cdot p(x)$$</p>  
    __Variance expanded__:  
    <p>$${\displaystyle {\begin{aligned}\operatorname {Var} (X)&=\operatorname {E} \left[(X-\operatorname {E} [X])^{2}\right]\\
        &=\operatorname {E} \left[X^{2}-2X\operatorname {E} [X]+\operatorname {E} [X]^{2}\right]\\
        &=\operatorname {E} \left[X^{2}\right]-2\operatorname {E} [X]\operatorname {E} [X]+\operatorname {E} [X]^{2}\\
        &=\operatorname {E} \left[X^{2}\right]-\operatorname {E} [X]^{2}\end{aligned}}}$$  </p>   
    __Variance as Covariance__: 
    Variance can be expressed as the covariance of a random variable with itself: 
    <p>$$\operatorname {Var} (X)=\operatorname {Cov} (X,X)$$</p>   
    
    __Properties:__  
    * $$\operatorname {Var} [a] = 0, \forall a \in \mathbb{R}$$ (constant $$a$$)  
    * $$\operatorname {Var} [af(X)] = a^2 \operatorname {Var} [f(X)]$$ (constant $$a$$)
    * $$\operatorname {Var} [X + Y] = a^2 \operatorname {Var} [X] + \operatorname {Var} [Y] + 2 \operatorname {Cov} [X, Y]$$.  
    <br>

11. **Standard Deviation:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents211}  
    The __Standard Deviation__ is a measure that is used to quantify the amount of variation or dispersion of a set of data values.  
    It is defined as the square root of the variance:  
    <p>$${\displaystyle {\begin{aligned}\sigma &={\sqrt {\operatorname {E} [(X-\mu )^{2}]}}\\&={\sqrt {\operatorname {E} [X^{2}]+\operatorname {E} [-2\mu X]+\operatorname {E} [\mu ^{2}]}}\\&={\sqrt {\operatorname {E} [X^{2}]-2\mu \operatorname {E} [X]+\mu ^{2}}}\\&={\sqrt {\operatorname {E} [X^{2}]-2\mu ^{2}+\mu ^{2}}}\\&={\sqrt {\operatorname {E} [X^{2}]-\mu ^{2}}}\\&={\sqrt {\operatorname {E} [X^{2}]-(\operatorname {E} [X])^{2}}}\end{aligned}}}$$</p>  
    
    __Properties:__  
    * 68% of the data-points lie within $$1 \cdot \sigma$$s from the mean
    * 95% of the data-points lie within $$2 \cdot \sigma$$s from the mean
    * 99% of the data-points lie within $$3 \cdot \sigma$$s from the mean
    <br>

12. **Covariance:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents212}  
    __Covariance__ is a measure of the joint variability of two random variables.  
    It gives some sense of how much two values are linearly related to each other, as well as the scale of these variables:  
    <p>$$\operatorname {cov} (X,Y)=\operatorname {E} { {\big[ }(X-\operatorname {E} [X])(Y-\operatorname {E} [Y]){ \big] } }$$ </p>  
    __Covariance expanded:__  
    <p>$${\displaystyle {\begin{aligned}\operatorname {cov} (X,Y)&=\operatorname {E} \left[\left(X-\operatorname {E} \left[X\right]\right)\left(Y-\operatorname {E} \left[Y\right]\right)\right]\\&=\operatorname {E} \left[XY-X\operatorname {E} \left[Y\right]-\operatorname {E} \left[X\right]Y+\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]\right]\\&=\operatorname {E} \left[XY\right]-\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]-\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]+\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]\\&=\operatorname {E} \left[XY\right]-\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right].\end{aligned}}}$$ </p>  
    > when $${\displaystyle \operatorname {E} [XY]\approx \operatorname {E} [X]\operatorname {E} [Y]} $$, this last equation is prone to catastrophic cancellation when computed with floating point arithmetic and thus should be avoided in computer programs when the data has not been centered before.  

    __Covariance of Random Vectors__:  
    <p>$${\begin{aligned}\operatorname {cov} (\mathbf {X} ,\mathbf {Y} )&=\operatorname {E} \left[(\mathbf {X} -\operatorname {E} [\mathbf {X} ])(\mathbf {Y} -\operatorname {E} [\mathbf {Y} ])^{\mathrm {T} }\right]\\&=\operatorname {E} \left[\mathbf {X} \mathbf {Y} ^{\mathrm {T} }\right]-\operatorname {E} [\mathbf {X} ]\operatorname {E} [\mathbf {Y} ]^{\mathrm {T} },\end{aligned}}$$ </p>  

    __The Covariance Matrix__ of a random vector $$x \in \mathbb{R}^n$$ is an $$n \times n$$ matrix, such that:    
    <p>$$ \operatorname {cov} (X)_ {i,j} = \operatorname {cov}(x_i, x_j) \\
        \operatorname {cov}(x_i, x_j) = \operatorname {Var} (x_i)$$</p>   
    __Interpretations__:  
    * High absolute values of the covariance mean that the values change very much and are both far from their respective means at the same time.
    * __The sign of the covariance__:   
        The sign of the covariance shows the tendency in the linear relationship between the variables:  
        * *__Positive__*:  
            the variables tend to show similar behavior
        * *__Negative__*:  
            the variables tend to show opposite behavior  
        * __Reason__:  
        If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, (i.e., the variables tend to show similar behavior), the covariance is positive. In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, (i.e., the variables tend to show opposite behavior), the covariance is negative.  

    __Covariance and Variance:__  
    <p>$$\operatorname{Var}[X+Y]=\operatorname{Var}[X]+\operatorname{Var}[Y]+2 \operatorname{Cov}[X, Y]$$</p>  

    __Covariance and Independence:__  
    If $$X$$ and $$Y$$ are independent $$\implies \operatorname{cov}[X, Y]=\mathrm{E}[X Y]-\mathrm{E}[X] \mathrm{E}[Y] = 0$$.  
    * Independence $$\Rightarrow$$ Zero Covariance  
    * Zero Covariance $$\nRightarrow$$ Independence

    __Covariance and Correlation:__  
    If $$\operatorname{Cov}[X, Y]=0 \implies $$ $$X$$ and $$Y$$ are __Uncorrelated__.  

    * [**Covariance/Correlation Intuition**](https://www.youtube.com/embed/KDw3hC2YNFc){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/KDw3hC2YNFc"></a>
        <div markdown="1"> </div>    
    * [**Covariance and Correlation (Harvard Lecture)**](https://www.youtube.com/embed/IujCYxtpszU){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/IujCYxtpszU"></a>
        <div markdown="1"> </div>    
    * [**Covariance as slope of the Regression Line**](https://www.youtube.com/embed/ualmyZiPs9w){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/ualmyZiPs9w"></a>
        <div markdown="1"> </div>    

    <br>  

13. **Mixtures of Distributions:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents213}  
    It is also common to define probability distributions by combining other simpler probability distributions. One common way of combining distributions is to construct a __mixture distribution__.    
    A __Mixture Distribution__ is the probability distribution of a random variable that is derived from a collection of other random variables as follows: first, a random variable is selected by chance from the collection according to given probabilities of selection, and then the value of the selected random variable is realized.    
    On each trial, the choice of which component distribution should generate the sample is determined by sampling a component identity from a multinoulli distribution:  
    <p>$$P(x) = \sum_i P(x=i)P(x \vert c=i)$$</p>    
    where $$P(c)$$ is the multinoulli distribution over component identities.    

14. **Bayes' Rule:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents214}  
    __Bayes' Rule__ describes the probability of an event, based on prior knowledge of conditions that might be related to the event.    
    <p>$${\displaystyle P(A\mid B)={\frac {P(B\mid A)\,P(A)}{P(B)}}}$$</p>  
    where,   
    <p>$$P(B) =\sum_A P(B \vert A) P(A)$$</p>  


15. **Common Random Variables:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents215}  
    __Discrete RVs:__{: style="color: red"}    
    * __Bernoulli__:  
    ![img](/main_files/math/prob/2.png){: width="90%"}  
    * __Binomial__:  
    ![img](/main_files/math/prob/3.png){: width="90%"}  
    * __Geometric__:  
    ![img](/main_files/math/prob/4.png){: width="90%"}  
    * __Poisson__:  
    ![img](/main_files/math/prob/5.png){: width="90%"}  

    __Continuous RVs:__{: style="color: red"}  
    * __Uniform__:  
    ![img](/main_files/math/prob/6.png){: width="90%"}  
    * __Exponential__:  
    ![img](/main_files/math/prob/7.png){: width="90%"}  
    * __Normal/Gaussian__:  
    ![img](/main_files/math/prob/8.png){: width="90%"}  
            
            
16. **Summary of Distributions:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents216}  
    ![img](/main_files/math/prob/9.png){: width="80%"}  


17. **Formulas:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents217}  
    * $$\overline{X} = \hat{\mu}$$,  
    * $$\operatorname {E}[\overline{X}]=\operatorname {E}\left[\frac{X_{1}+\cdots+X_{n}}{n}\right] = \mu$$,  
    * $$\operatorname{Var}[\overline{X}]=\operatorname{Var}\left[\frac{X_{1}+\cdots+X_{n}}{n}\right] = \dfrac{\sigma^2}{n}$$,    
    * $$\operatorname {E}\left[X_{i}^{2}\right]=\operatorname {Var} [X]+\operatorname {E} [X]^{2} = \sigma^{2}+\mu^{2}$$,  
    * $$\operatorname {E}\left[\overline{X}^{2}\right]=\operatorname {E}\left[\hat{\mu}^{2}\right]=\frac{\sigma^{2}}{n}+\mu^{2}\:$$, [^2]  

    <br>


18. **Correlation:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents218}  
    In the broadest sense __correlation__ is any statistical association, though it commonly refers to the degree to which a pair of variables are linearly related.  

    There are several correlation coefficients, often denoted $${\displaystyle \rho }$$ or $$r$$, measuring the degree of correlation:  

    __Pearson Correlation Coefficient [\[wiki\]](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient):__{: style="color: red"}  
    It is a measure of the __linear correlation__ between two variables $$X$$ and $$Y$$.  
    <p>$$\rho_{X, Y}=\frac{\operatorname{cov}(X, Y)}{\sigma_{X} \sigma_{Y}}$$</p>  
    where, $${\displaystyle \sigma_{X}}$$ is the standard deviation of $${\displaystyle X}$$ and $${\displaystyle \sigma_{Y}}$$  is the standard deviation of $${\displaystyle Y}$$, and $$\rho \in [-1, 1]$$.   



    __Correlation and Independence:__{: style="color: red"}  
    1. Uncorrelated $$\nRightarrow$$ Independent  
    2. Independent $$\implies$$ Uncorrelated  

    Zero correlation will indicate no linear dependency, however won't capture non-linearity. Typical example is uniform random variable $$x$$, and $$x^2$$ over $$[-1,1]$$ with zero mean. Correlation is zero but clearly not independent.  

    <br> 

19. **Probabilistic Inference:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents219}  
    __Probabilistic Inference:__ compute a desired probability from other known probabilities (e.g. conditional from joint).  

    __We generally compute Conditional Probabilities:__  
    {: #lst-p}
    * $$p(\text{sun} \vert T=\text{12 pm}) = 0.99$$  
    * These represent the agents beliefs given the evidence  

    __Probabilities change with new evidence:__  
    {: #lst-p} 
    * $$p(\text{sun} \vert T=\text{12 pm}, C=\text{Stockholm}) = 0.85$$  
    $$\longrightarrow$$  
    * $$p(\text{sun} \vert T=\text{12 pm}, C=\text{Stockholm}, M=\text{Jan}) = 0.40$$  
    * Observing new evidence causes beliefs to be updated

    __Inference by Enumeration:__{: style="color: red"}  
    {: #lst-p}
    * [**CS188 Lec. 10-2**](https://www.youtube.com/embed/sMNbLXsvRig?start=3508){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/sMNbLXsvRig?start=3508"></a>
        <div markdown="1"> </div>    

    __Problems:__  
    * Worst-case time complexity $$\mathrm{O}\left(\mathrm{d}^{n}\right)$$
    * Space complexity $$\mathrm{O}\left(\mathrm{d}^{n}\right)$$ to store the joint distribution  

    __Inference with Bayes Theorem:__{: style="color: red"}  
    * __Diagnostic Probability from Causal Probability:__  
        <p>$$P(\text { cause } | \text { effect })=\frac{P(\text { effect } | \text { cause }) P(\text { cause })}{P(\text { effect })}$$</p>  




[^2]: Comes from $$\operatorname{Var}[\overline{X}]=\operatorname {E}\left[\overline{X}^{2}\right]-\{\operatorname {E}[\overline{X}]\}^{2}$$  


***

## Discrete Distributions
{: #content9}

1. **Uniform Distribution:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents91}  
    :   

2. **Bernoulli Distribution:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents92}  
    :   A distribution over a single binary random variable.  
        It is controlled by a single parameter $$\phi \in [0, 1]$$, which fives the probability of the r.v. being equal to $$1$$.  
        > It models the probability of a single experiment with a boolean outcome (e.g. coin flip $$\rightarrow$$ {heads: 1, tails: 0})  
    :   __PMF:__  
    :   $${\displaystyle P(x)={\begin{cases}p&{\text{if }}p=1,\\q=1-p&{\text{if }}p=0.\end{cases}}}$$  
    :   __Properties:__  
        <p>$$P(X=1) = \phi$$</p>
        <p>$$P(X=0) = 1 - \phi$$</p>
        <p>$$P(X=x) = \phi^x (1 - \phi)^{1-x}$$</p>
        <p>$$\operatorname {E}[X] = \phi$$</p>
        <p>$$\operatorname {Var}(X) = \phi (1 - \phi)$$</p>

3. **Binomial Distribution:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents93}     
    > $${\binom {n}{k}}={\frac {n!}{k!(n-k)!}}$$ is the number of possible ways of getting $$x$$ successes and $$n-x$$ failures


***

## 110
{: #content99}

1. **Problems:**{: style="color: SteelBlue"}{: .bodyContents99 #bodyContents991}  
    * [**deMortmonts/Matching problem**](https://www.youtube.com/embed/LZ5Wergp_PA?start=2305){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/LZ5Wergp_PA?start=2305"></a>
        <div markdown="1"> </div>    
        Sol: Inclusion-Exclusion  
    * [**Newton-Pepys: most likely event of rolling 6's in dice**](https://www.youtube.com/embed/P7NE4WF8j-Q?start=1057){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/P7NE4WF8j-Q?start=1057"></a>
        <div markdown="1"> </div>    

    :   

    :   


    
***

## Notes, Tips and Tricks
{: #content10}

* It is more practical to use a simple but uncertain rule rather than a complex but certain one, even if the true rule is deterministic and our modeling system has the fidelity to accommodate a complex rule.  
    For example, the simple rule ‚ÄúMost birds fly‚Äù is cheap to develop and is broadly useful, while a rule of the form, ‚ÄúBirds fly, except for very young birds that have not yet learned to fly, sick or injured birds that have lost the ability to fly, flightless species of birds including the cassowary, ostrich and kiwi. . .‚Äù is expensive to develop, maintain and communicate and, after all this effort, is still brittle and prone to failure.

* __Disjoint Events (Mutually Exclusive):__{: .bodyContents10 #bodyContents102} are events that cannot occur together at the same time
    Mathematically:  
    * $$A_i \cap A_j = \varnothing$$ whenever $$i \neq j$$  
    * $$p(A_i, A_j) = 0$$,  

* __Complexity of Describing a Probability Distribution__:  
    A description of a probability distribution is _exponential_ in the number of variables it models.  
    The number of possibilities is __exponential__ in the number of variables.  

* __Probability VS Likelihood__:  
    __Probabilities__ are the areas under a fixed distribution  
    $$pr($$data$$|$$distribution$$)$$  
    i.e. probability of some _data_ (left hand side) given a distribution (described by the right hand side)  
    __Likelihoods__ are the y-axis values for fixed data points with distributions that can be moved..  
    $$L($$distribution$$|$$observation/data$$)$$  

    > Likelihood is, basically, a specific probability that can only be calculated after the fact (of observing some outcomes). It is not normalized to $$1$$ (it is __not__ a probability). It is just a way to quantify how likely a set of observation is to occur given some distribution with some parameters; then you can manipulate the parameters to make the realization of the data more _"likely"_ (it is precisely meant for that purpose of estimating the parameters); it is a _function_ of the __parameters__.  
    Probability, on the other hand, is absolute for all possible outcomes. It is a function of the __Data__.  

* __Maximum Likelihood Estimation__:  
    A method that tries to find the _optimal value_ for the _mean_ and/or _stdev_ for a distribution *__given__* some observed measurements/data-points.

* __Variance__:  
    When $$\text{Var}(X) = 0 \implies X = E[X] = \mu$$. (not interesting)  

* __Reason we sometimes prefer Biased Estimators__:  
        

***
***

TITLE: Optimization Problems
LINK: research/optimization/opt_probs.md


## Geometry and Lin-Alg [Hyper-Planes]
{: #content1}

1. **Minimum Distance from a point to a hyperplane/Affine-set?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   $$d = \dfrac{\| w \cdot x_0 + b \|}{\|w\|},$$  
    :   where we have an n-dimensional hyperplane: $$w \cdot x + b = 0$$ and a point $$x_0$$.
    > Also known as, **The Signed Distance**.  
    * **Proof.**  
        * Suppose we have an affine hyperplane defined by $$w \cdot x + b$$ and a point $$x_0$$.
        * Suppose that $$\vec{v} \in \mathbf{R}^n$$ is a point satisfying $$w \cdot \vec{v} + b = 0$$, i.e. it is a point on the plane.
        * We construct the vector $$x_0‚àí\vec{v}$$ which points from $$\vec{v}$$ to $$x_0$$, and then, project it onto the unique vector perpendicular to the plane, i.e. $$w$$,  

            $$d=\| \text{proj}_{w} (x_0-\vec{v})\| = \left\| \frac{(x_0-\vec{v})\cdot w}{w \cdot w} w \right\| = \|x_0 \cdot w - \vec{v} \cdot w\|\frac{\|w\|}{\|w\|^2} = \frac{\|x_0 \cdot w - \vec{v} \cdot w\|}{\|w\|}.$$

        * We chose $$\vec{v}$$ such that $$w\cdot \vec{v}=-b$$ so we get  

            $$d=\| \text{proj}_{w} (x_0-\vec{v})\| = \frac{\|x_0 \cdot w +b\|}{\|w\|}$$

2. **Every symmetric positive semi-definite matrix is a covariance matrix:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    * **Proof.**  
        * Suppose $$M$$ is a $$(p\times p)$$ positive-semidefinite matrix.  

        * From the finite-dimensional case of the spectral theorem, it follows that $$M$$ has a nonnegative symmetric square root, that can be denoted by $$M^{1/2}$$.  

        * Let $${\displaystyle \mathbf {X} }$$ be any $$(p\times 1)$$ column vector-valued random variable whose covariance matrix is the $$(p\times p)$$ identity matrix.   

        * Then,   

            $${\displaystyle \operatorname {var} (\mathbf {M} ^{1/2}\mathbf {X} )=\mathbf {M} ^{1/2}(\operatorname {var} (\mathbf {X} ))\mathbf {M} ^{1/2}=\mathbf {M} \,}$$


***

## Statistics
{: #content2}


2. **Every symmetric positive semi-definite matrix is a covariance matrix:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22}
    * **Proof.**  
        * Suppose $$M$$ is a $$(p\times p)$$ positive-semidefinite matrix.  

        * From the finite-dimensional case of the spectral theorem, it follows that $$M$$ has a nonnegative symmetric square root, that can be denoted by $$M^{1/2}$$.  

        * Let $${\displaystyle \mathbf {X} }$$ be any $$(p\times 1)$$ column vector-valued random variable whose covariance matrix is the $$(p\times p)$$ identity matrix.   

        * Then,   

            $${\displaystyle \operatorname {var} (\mathbf {M} ^{1/2}\mathbf {X} )=\mathbf {M} ^{1/2}(\operatorname {var} (\mathbf {X} ))\mathbf {M} ^{1/2}=\mathbf {M} \,}$$

***

## Inner Products Over Balls
{: #content3}

1. **Extrema of inner product over a ball:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}
    :   Let $$y \in \mathbf{R}^n$$ be  a  given non-null vector, and let $$\chi = \left\{x \in \mathbf{R}^n :\  \|x\|_2 \leq r\right\}$$,  
    where $$r$$ is some given positive number.
    1. **Determine the optimal value $$p_1^\ast$$ and the optimal set of the problem $$\min_{x \in \chi} \; |y^Tx|$$:**    
        The minimum value of $$\min_{x \in \chi} \; |y^Tx|$$ is $$p_1^\ast = 0$$.  
        This value is attained either by $$x = 0$$, or by any vector $$x\in\chi_r$$ orthogonal to $$y$$.  
        The optimal set: $$\chi_{opt} = \left\{x : \ x = Vz, \|z\|_2 \leq r\right\}$$        
    2. **Determine the optimal value $$p_2^\ast$$ and the optimal set of the problem $$\max_{x\in \chi} \; |y^Tx|:$$**
        The optimal value of $$\max_{x\in \chi} \; |y^Tx|$$ is attained for any $$x = \alpha y$$ with $$\|x\|_2 = r$$.  
        Thus for $$|\alpha| = \dfrac{r}{\|y\|_2}$$, for which we have $$p_2^\ast‚àó = r\|y\|_2.$$  
        The optimal set contains two points: $$\chi_{opt} = \left\{x : \  x = \alpha y, \alpha = ¬± \dfrac{r}{\|y\|_2} \right\}$$.
    3. **Determine the optimal value $$p_3^\ast$$ and the optimal set of the problem $$\min_{x\in \chi} \; y^Tx$$:**  
        We have $$p_3^\ast = ‚àír\|y\|_2,$$ which is attained at the unique optimal poin   
        $$x^\ast = ‚àí\dfrac{r}{\|y\|_2} y.$$ 

    4. **Determine the optimal value $$p_4^\ast$$ and the optimal set of the problem $$\max_{x\in\chi} \; y^Tx$$:**  
        We have $$p_4^\ast = r \|y\|_2,$$ which is attained at the unique optimal point  
         $$x^\ast = \dfrac{r}{\|y\|_2} y.$$

***

## Gradients and Derivatives
{: #content4}

1. **Gradient of log-sum-exp function:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41}
    :   Find the gradient at $$x$$ of the function $$lse : \  \mathbf{R}^n \rightarrow \mathbf{R}$$ defined as,  
    :   $$
        lse(x) = \log{(\sum_{i=1}^n e^{x_i})}.
        $$

    :   **Solution.**  
        $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \nabla_x \  lse(x)$$
    :   $$
        \begin{align}
        & \ = \nabla_x \log(\sum_{i=1}^n e^{x_i}) \\
        & \ = \dfrac{\dfrac{d}{dx_i} (\sum_{i=1}^n e^{x_i})}{\log(\sum_{i=1}^n e^{x_i})} \\
        & \ = \dfrac{e^{x_i}}{lse(x)} \\
        & \ = \dfrac{[e^{x_1} \  e^{x_2} \  \ldots \  e^{x_n}]^T}{lse(x)}
        \end{align}
        $$


***
***

TITLE: Optimization <br > Cheat Sheet
LINK: research/optimization/cheat.md


## FIRST
{: #content1}

1. **Functions:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}
    * __*Graph* of a function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$__: is the set of input-output pairs that $$f$$ can attain, that is:  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ G(f) := \left \{ (x,f(x)) \in \mathbf{R}^{n+1} : x \in \mathbf{R}^n \right \}.$$ \\
    > It is a subset of $$\mathbf{R}^{n+1}$$.
    * __*Epigraph* of a function $$f$$__: describes the set of input-output pairs that $$f$$ can achieve, as well as "anything above":  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathop{\bf epi} f := \left \{ (x,t) \in \mathbf{R}^{n+1} ~:~ x \in \mathbf{R}^n, \ \  t \ge f(x) \right \}.$$
    * __Level sets__:  is the set of points that achieve exactly some value for the function $$f$$.  
    For $$t \in \mathbf{R}$$, the $$t-$$level set of the function $$f$$ is defined as:  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{L}_t(f) := \left\{ x \in \mathbf{R}^{n} ~:~ x \in \mathbf{R}^n, \ \  t = f(x) \right \}.$$
    * __Sub-level sets__: is the set of points that achieve at most a certain value for  $$f$$, or below:  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \mathbf{S}_t(f) := \left\{ x \in \mathbf{R}^{n} ~:~ x \in \mathbf{R}^n, \ \  t \ge f(x) \right\}.$$  



2. **Optimization Problems:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}
    * __Functional Form__:  An optimization problem is a problem of the form
    $$\:\:\:\:\:\:\:$$ $$p^\ast := \displaystyle\min_x f_0(x) :  f_i(x) \le 0, \ \  i=1,\ldots, m$$  
    where: $$x \in \mathbf{R}^n$$ is the decision variable;  $$f_0 : \mathbf{R}^n \rightarrow \mathbf{R}$$ is the objective function, or cost;  $$f_i : \mathbf{R}^n \rightarrow \mathbf{R}, \ \  i=1, \ldots, m$$ represent the constraints;  $$p^\ast$$ is the optimal value.
    * __Feasibility Problems__:  Sometimes an objective function is not provided. This means that we are just _interested_ in _finding_ a _feasible point_, or determine that the problem is _infeasible_. 
    > By convention, we set $$f_0$$ to be a constant in that case, to reflect the fact that we are indifferent to the choice of a point x as long as it is feasible.
                

3. **Optimality:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}
    * __Feasible Set__:  
    $$\:\:\:\:\:\:\:$$ $$ \mathbf{X} :=  \left\{ x \in \mathbf{R}^n ~:~  f_i(x) \le 0, \ \  i=1, \ldots, m \right\}.$$  
    * __Optimal Value__:  
    $$\:\:\:\:\:\:\:$$$$\:\:\:\:\:\:\:$$  $$p^\ast := \min_x : f_0(x) ~:~ f_i(x) \le 0, \ \  i=1, \ldots, m.$$   
    * __Optimal Set__:  The set of feasible points for which the objective function achieves the optimal value:  
    $$\:\:\:\:\:\:\:$$$$\:\:\:\:\:\:\:$$  $$ \mathbf{X}^{\rm opt} :=  \left\{ x \in \mathbf{R}^n ~:~  f_0(x) = p^\ast, \ \ f_i(x) \le 0, \ \  i=1,\ldots, m \right\} = \mathrm{arg min}_{x \in \mathbf{X}}  f_0(x)$$  
    > By convention, the optimal set is empty if the problem is not feasible.  
    > A _point_ $$x$$ is said to be **optimal** if it belongs to the optimal set.  
    > If the optimal value is **ONLY** attained in the **limit**, then it is **NOT** in the optimal set.    
    * __Suboptimality__:  the $$\epsilon$$-suboptimal set is defined as:  
    $$\:\:\:\:\:\:\:$$$$\:\:\:\:\:\:\:$$  $$ \mathbf{X}_\epsilon := \left\{ x \in \mathbf{R}^n ~:~ f_i(x) \le 0, \ \  i=1, \ldots, m, \ \  f_0(x) \le p^\ast + \epsilon \right\}.$$  
    > $$\implies \  \mathbf{X}_0 = \mathbf{X}_{\rm opt}$$.  
    * __Local Optimality__:  
        A point $$z$$ is **Locally Optimal**: if there is a value $$R>0$$ such that $$z$$ is optimal for the following problem:  
    $$\:\:\:\:\:\:\:$$ $$\:\:\:\:\:\:\:$$ $$min_x : f_0(x) ~:~ f_i(x) \le 0, \ \ i=1, \ldots, m,  \ \ \|z-x\|_2 \le R$$.  
    > i.e. a _local minimizer_ $$x$$ _minimizes_ $$f_0$$, but **only** for _nearby points_ on the feasible set.  
    * __Global Optimality__:  
        A point $$z$$ is **Globally Optimal**: if it is the _optimal value_ of the original problem on all of the feasible region.   
            

4. **Problem Classes:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}
* **Least-squares:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41}
        :   $$\min_x \;\;\;\; \sum_{i=1}^m \left( \sum_{j=1}^n A_{ij} . x_j - b_i \right)^2$$
        :   where $$A_{ij}, \  b_i, \  1 \le i \le m,  \ 1 \le j \le n$$, are given numbers, and $$x \in \mathbf{R}^n$$ is the variable.

    * **Linear Programming:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} 
        :   $$ \min \sum_{j=1}^n c_jx_j ~:~ \sum_{j=1}^n A_{ij} . x_j  \le b_i , \;\; i=1, \ldots, m, $$ 
        :   where $$ c_j, b_i$$ and $$A_{ij}, \  1 \le i \le m, \  1 \le j \le n$$, are given real numbers.  

    * **Quadratic Programming:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents43}
        :   $$\min_x \;\;\;\; \displaystyle\sum_{i=1}^m \left(\sum_{j=1}^n C_{ij} . x_j+d_i\right)^2 + \sum_{i=1}^n c_ix_i \;:\;\;\;\; \sum_{j=1}^m A_{ij} . x_j \le b_i, \;\;\;\; i=1,\ldots,m.$$  
        > Includes a **sum of squared linear functions**, in addition to a **linear term**, in the _objective_.  

    * **Nonlinear optimization:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents44}
        :   A broad class that includes _Combinatorial Optimization_.

        > One of the reasons for which non-linear problems are hard to solve is the issue of _local minima_.

    * **Convex optimization:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents45}
        :    A generalization of QP, where the objective and constraints involve "bowl-shaped", or convex, functions.

        > They are _easy_ to solve because they _do not suffer_ from the "curse" of _local minima_.

    * **Combinatorial optimization:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents46}
        :   In combinatorial optimization, some (or all) the variables are boolean (or integers), reflecting discrete choices to be made.

        > Combinatorial optimization problems are in general extremely hard to solve. Often, they can be approximately solved with linear or convex programming.

    * **NON-Convex Optimization Problems [Examples]:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents47} \\
        * **Boolean/integer optimization:** some variables are constrained to be Boolean or integers.  
        > Convex optimization can be used for getting (sometimes) good approximations.
        * **Cardinality-constrained problems:** we seek to bound the number of non-zero elements in a vector variable.  
        > Convex optimization can be used for getting good approximations.
        * **Non-linear programming:** usually non-convex problems with differentiable objective and functions.  
        > Algorithms provide only local minima.  


***

## Linear Algebra
{: #content2}

1. **Basics:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}
    * **Linear Independence:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
        :   A set of vectors $$\{x_1, ... , x_m\} \in {\mathbf{R}}^n, i=1, \ldots, m$$ is said to be independent if and only if the following condition on a vector $$\lambda \in {\mathbf{R}}^m$$:  
        :   $$\sum_{i=1}^m \lambda_i x_i = 0 \ \ \ \implies  \lambda = 0.$$

            > i.e. no vector in the set can be expressed as a linear combination of the others.

    * **Subspace:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
        :   A subspace of $${\mathbf{R}}^n$$ is a subset that is closed under addition and scalar multiplication. Geometrically, subspaces are "flat" (like a line or plane in 3D) and pass through the origin.  

        * A **Subspace** $$\mathbf{S}$$ can always be represented as the span of a set of vectors $$x_i \in {\mathbf{R}}^n, i=1, \ldots, m$$, that is, as a set of the form:  
        $$\mathbf{S} = \mbox{ span}(x_1, \ldots, x_m) := \left\{ \sum_{i=1}^m \lambda_i x_i ~:~ \lambda \in {\mathbf{R}}^m \right\}.$$
        $$\\$$ 

    * **Affine Sets (Cosets | Abstract Algebra):**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13}
        :   An affine set is a translation of a subspace‚Äâ‚Äî‚Äâit is "flat" but does not necessarily pass through 0, as a subspace would. 
        :   An affine set $$\mathbf{A}$$ can always be represented as the translation of the subspace spanned by some vectors:
        :   $$\:\:\:\:\:\:\:$$ $$ \mathbf{A} = \left\{ x_0 + \sum_{i=1}^m \lambda_i x_i ~:~ \lambda \in {\mathbf{R}}^m \right\}\ \ \ $$  
        for some vectors $$x_0, x_1, \ldots, x_m.$$  
        $$\implies \mathbf{A} = x_0 + \mathbf{S}.$$

        * **(Special case)** **lines**: When $$\mathbf{S}$$ is the span of a single non-zero vector, the set $$\mathbf{A}$$ is called a line passing through the point $$x_0$$. Thus, lines have the form
        $$\left\{ x_0 + tu ~:~ t \in \mathbf{R} \right\}$$,  \\
        where $$u$$ determines the direction of the line, and $$x_0$$ is a point through which it passes.

    * **Basis:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14}
        :    A **basis** of $${\mathbf{R}}^n$$ is a set of $$n$$ independent vectors. If the vectors $$u_1, \ldots, u_n$$ form a basis, we can express any vector as a linear combination of the $$u_i$$'s:
        :   $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ x = \sum_{i=1}^n \lambda_i u_i, \ \ \ \text{for appropriate numbers } \lambda_1, \ldots, \lambda_n$$.


    * **Dimension:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} 
        :   The number of vectors in the span of the (sub-)space.

2. **Norms and Scalar Products:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}
    * **Scalar Product:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}
    :   The scalar product (or, inner product, or dot product) between two vectors $$x,y \in \mathbf{R}^n$$ is the scalar denoted $$x^Ty$$, and defined as: 
    :   $$x^Ty = \sum_{i=1}^n x_i y_i.$$ 

    * **Norms:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} 
    :   A measure of the "length" of a vector in a given space.
    :   **Theorem.** A function from $$\chi$$ to $$\mathbf{R}$$ is a norm, if:  
        1. $$\|x\| \geq 0, \: \forall x \in \chi$$, and $$\|x\| = 0 \iff x = 0$$.
        2. $$\|x+y\| \leq \|x\| + \|y\|,$$ for any $$x, y \in \chi$$ (triangle inequality).
        3. $$\|\alpha x\| = \|\alpha\| \|x\|$$, for any scalar $$\alpha$$ and any $$x\in \chi$$.

    * **$$l_p$$ Norms:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents200} 
    :   $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  $$\ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  $$\|x\|_p = \left( \sum_{k=1}^n \|x_k\|_p \right)^{1/p}, \ 1 \leq p < \infty$$.


    * **The $$l_1-norm$$:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24}
    :   $$ \|x\|_1 := \sum_{i=1}^n \| x_i \| $$   
    :   Corresponds to the distance travelled on a rectangular grid to go from one point to another.  \\
        > Induces a diamond shape

    * **The $$l_2-norm$$ (Euclidean Norm):**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} 
    :   $$  \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \| x \|_2 := \sqrt{ \sum_{i=1}^n x_i^2 } = \sqrt{x^Tx} $$.  
    :   Corresponds to the usual notion of distance in two or three dimensions.
    :   > The $$l_2-norm$$ is invariant under orthogonal transformations,     
        > i.e., $$\|x\|_2 = \|Vz\|_2 = \|z\|_2,$$ where $$V$$ is an orthogonal matrix. 
    :   > The set of points with equal l_2-norm is a circle (in 2D), a sphere (in 3D), or a hyper-sphere in higher dimensions. 

    * **The $$l_\infty-norm$$:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25}
    :   $$ \| x \|_\infty := \displaystyle\max_{1 \le i \le n} \| x_i \|$$  
    :   > useful in measuring peak values.  
        > Induces a square

    * **The Cardinality:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents20}
    :   The **Cardinality** of a vector $$\vec{x}$$ is often called the $$l_0$$ (pseudo) norm and denoted with,  
    :   $$\|\vec{x}\|_0$$.
    :   > Defined as the number of non-zero entries in the vector.


    * **Cauchy-Schwartz inequality:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26}
    :   For any two vectors $$x, y \in \mathbf{R}^n$$, we have  
    :   $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  $$x^Ty \le \|x\|_2 \cdot \|y\|_2$$.
    :   > The above inequality is an equality if and only if $$x, y$$ are collinear:  
        > : $$ {\displaystyle \max_{x : \: \|x\|_2 \le 1} \: x^Ty = \|y\|_2,}$$ 
        > with optimal $$x$$ given by  
        > $$x^\ast = \dfrac{y}{\|y\|_2}, \ $$ if $$y$$ is non-zero.

    * **Angles between vectors:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27} 
    :   When none of the vectors x,y is zero, we can define the corresponding angle as theta such that,
    :    $$\cos\  \theta = \dfrac{x^Ty}{\|x\|_2 \|y\|_2} .$$ 


3. **Notes:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}
    * __Norms and Metrics__:  
    Norms induce metrics on topological spaces but not the other way around.  
    Technically, a norm is a metric with two additional properties: (1) Translation Invariance (2) Absolute homogeneouity/scalability  
    We can always define a metric from a norm: $$d(x,y) = \|x - y \|$$  
    A metric is a function of two variables and a norm is a function of one variable.
    * __Collinearity__:  In geometry, collinearity of a set of points is the property of their lying on a single line  
            





***
***

TITLE: The Generalized Max Sub-Array Problem
LINK: research/optimization/hw/hw4_161.md


# Final Thought and Conclusions

## **Regarding my answers during the Interview**
{: style="color: SteelBlue"}

* During the interview I was thinking of the dynamic programming approach of trying out the matrices and growing them in sizes after having precomputed their sum values.  
* I, also, tried exlporing the $$\mathcal{O}(n^3)$$ after you discusses the 1D approach.  
* The branch and bound method is interesting but solves the problem from a different perspective.  

## **Further Development**
{: style="color: SteelBlue"}

* I believe that the $$\mathcal{O}(n^3)$$ utilizing Kadane algorithm could be improved by calling the algorithm only in the first loop (not the second) by smartly computing the overlapping values and going across cols then rows instead (two runs, i.e. constant).  
    This will lead the algorithm to be $$\mathcal{O}(n^2)$$ instead but the idea needs further exploration.  

* Another approach would be to rely on looking at the distribution of the numbers in the matrix (linear), then to sample smartly using an ML approach, perhaps by fitting a hough transform that detects large sum "chunks".  

## **Final Comments**
{: style="color: SteelBlue"}

* I will be updating this post whenever I have time.  
* Code has been Unit-Tested and _most but not all_ has been stress-tested with edge-cases.

<p class="message">Please note that all code and descriptions here were completely written by me. <br /> However, credit was given for the "C++" implementation of the "Box-Struct". <br /> All code, descriptions and explanations are under a public license <br /> Copyright (C) 2017 MIT</p>



***
***

TITLE: 1.1 | 1.2 <br /> Optimization Models
LINK: research/optimization/1/1.2.md


## Mathematical Background
{: #content1}

1. **Maps:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   We reserve the term map to refer to vector-valued functions. That is, maps are
    functions which return more than a single value.

2. **Graph:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    Consider a function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$.  
    The _**graph**_ of $$f$$ is the set of input-output pairs that $$f$$ can attain, that is:
    $$G(f) := \left \{ (x,f(x)) \in \mathbf{R}^{n+1} : x \in \mathbf{R}^n \right \}.$$ \\
    > It is a subset of $$\mathbf{R}^{n+1}$$.

3. **Epigraph:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    Consider a function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$.  
    The _**epigraph**_, denoted $$\mathop{\bf epi} f$$, describes the set of input-output pairs that $$f$$ can achieve, as well as "anything above":  
    $$\mathop{\bf epi} f := \left \{ (x,t) \in \mathbf{R}^{n+1} ~:~ x \in \mathbf{R}^n, \ \  t \ge f(x) \right \}.$$
    > epi in Greek means "above"  

    <button>Example [image]</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/1/1.1_2/1.png){: hidden=""}

4. **Level and Sub-level Sets:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    > _**Level**_ and _**sub-level**_ sets correspond to the notion of contour of a function. Both are indexed on some scalar value $$t$$.  

    * **Level sets**: is simply the set of points that achieve exactly some value for the function $$f$$.  
    For $$t \in \mathbf{R}$$, the $$t-$$level set of the function $$f$$ is defined as:  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{L}_t(f) := \left\{ x \in \mathbf{R}^{n} ~:~ x \in \mathbf{R}^n, \ \  t = f(x) \right \}.$$

    * **Sub-level sets**: is the set of points that achieve at most a certain value for  $$f$$, or below:  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \mathbf{S}_t(f) := \left\{ x \in \mathbf{R}^{n} ~:~ x \in \mathbf{R}^n, \ \  t \ge f(x) \right\}.$$  

    <button>Example [image]</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/1/1.1_2/2.png){: hidden=""}

***

## Mathematical Formulation [Standard Forms]
{: #content2}

1. **Functional Form:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}
    :   An optimization problem is a problem of the form
    $$p^\ast := \displaystyle\min_x f_0(x) :  f_i(x) \le 0, \ \  i=1,\ldots, m$$,  
    where:  
        * $$x \in \mathbf{R}^n$$ is the decision variable;

        * $$f_0 : \mathbf{R}^n \rightarrow \mathbf{R}$$ is the objective function, or cost; 

        * $$f_i : \mathbf{R}^n \rightarrow \mathbf{R}, \ \  i=1, \ldots, m$$ represent the constraints;

        * $$p^\ast$$ is the optimal value.  

    * [**Example.**](http://livebooklabs.com/keeppies/c5a5868ce26b8125/e5deae3e0c61b80e){: value="show" onclick="iframePopA(event)"}
        <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/e5deae3e0c61b80e">` Visit the Book`</a>
        <div markdown="1"> </div>

2. **Epigraph form:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22}
    :   TODO

3. **Other Standard-Forms:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23}
    :   TODO

***

## Nomenclature
{: #content3}

1. **Feasible set:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}
    :   $$ \mathbf{X} :=  \left\{ x \in \mathbf{R}^n ~:~  f_i(x) \le 0, \ \  i=1, \ldots, m \right\}.$$  

2. **Solution:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32}
    :   In an optimization problem, we are usually interested in computing the optimal value of the objective function, and also often a minimizer, which is a vector which achieves that value, if any.

3. **Feasibility problems:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} 
    :   Sometimes an objective function is not provided. This means that we are just _interested_ in _finding_ a _feasible point_, or determine that the problem is _infeasible_.  
    > By convention, we set $$f_0$$ to be a constant in that case, to reflect the fact that we are indifferent to the choice of a point x as long as it is feasible.

4. **Optimal value:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34}
    :    $$p^\ast := \min_x : f_0(x) ~:~ f_i(x) \le 0, \ \  i=1, \ldots, m.$$   

    > Denoted $$p^\ast$$.

5. **Optimal set:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents35} 
    :   The set of feasible points for which the objective function achieves the optimal value:  
    $$ \mathbf{X}^{\rm opt} :=  \left\{ x \in \mathbf{R}^n ~:~  f_0(x) = p^\ast, \ \ f_i(x) \le 0, \ \  i=1,\ldots, m \right\}$$.  
    Equivalently,  
    $$ \mathbf{X}^{\rm opt} = \mathrm{arg min}_{x \in \mathbf{X}}  f_0(x)$$.  

    > We take the convention that the optimal set is empty if the problem is not feasible.  

    > A _point_ $$x$$ is said to be **optimal** if it belongs to the optimal set.

    > If the optimal value is **ONLY** attained in the **limit**, then it is **NOT** in the optimal set.

6. **When is a problem "_Attained_"?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents36}
    :   If the optimal set is _not empty_, we say that the problem is **_attained_**.

7. **Suboptimality:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents37}
    :   The $$\epsilon$$-suboptimal set is defined as:  
    :   $$ \mathbf{X}_\epsilon := \left\{ x \in \mathbf{R}^n ~:~ f_i(x) \le 0, \ \  i=1, \ldots, m, \ \  f_0(x) \le p^\ast + \epsilon \right\}.$$  

    > $$\implies \  \mathbf{X}_0 = \mathbf{X}_{\rm opt}$$.

8. **(Local and Global) _Optimality_:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents38} \\
    * A point $$z$$ is **Locally Optimal**: if there is a value $$R>0$$ such that $$z$$ is optimal for the following problem:
    $$min_x : f_0(x) ~:~ f_i(x) \le 0, \ \ i=1, \ldots, m,  \ \ \|z-x\|_2 \le R$$.  
    > i.e. a _local minimizer_ $$x$$ _minimizes_ $$f_0$$, but **only** for _nearby points_ on the feasible set.

    * A point $$z$$ is **Globally Optimal**: if it is the _optimal value_ of the original problem on all of the feasible region.   

***

## Problem Classes
{: #content4}

1. **Least-squares:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41}
    :   $$\min_x \;\;\;\; \sum_{i=1}^m \left( \sum_{j=1}^n A_{ij} . x_j - b_i \right)^2,$$
    :   where $$A_{ij}, \  b_i, \  1 \le i \le m,  \ 1 \le j \le n$$, are given numbers, and $$x \in \mathbf{R}^n$$ is the variable.

2. **Linear Programming:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} 
    :   $$ \min \sum_{j=1}^n c_jx_j ~:~ \sum_{j=1}^n A_{ij} . x_j  \le b_i , \;\; i=1, \ldots, m, $$ 
    :   where $$ c_j, b_i$$ and $$A_{ij}, \  1 \le i \le m, \  1 \le j \le n$$, are given real numbers.  

    > This corresponds to the case where the functions $$f_i(i=0, \ldots, m)$$ in the standard problem are all affine (that is, linear plus a constant term).  

    > Denoted $$LP$$.

3. **Quadratic Programming:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents43}
    :   $$\min_x \;\;\;\; \displaystyle\sum_{i=1}^m \left(\sum_{j=1}^n C_{ij} . x_j+d_i\right)^2 + \sum_{i=1}^n c_ix_i \;:\;\;\;\; \sum_{j=1}^m A_{ij} . x_j \le b_i, \;\;\;\; i=1,\ldots,m.$$  

    > Includes a **sum of squared linear functions**, in addition to a **linear term**, in the _objective_.  

    > QP's are popular in finance, where the _linear term_ in the _objective_ refers to the _expected negative return_ on an investment, and the _squared terms_ corresponds to the _risk_ (or variance of the return).  

    > QP was introduced by "Markowitz"

4. **Nonlinear optimization:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents44}
    :   A broad class that includes _Combinatorial Optimization_.

    > One of the reasons for which non-linear problems are hard to solve is the issue of _local minima_.

5. **Convex optimization:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents45}
    :    A generalization of QP, where the objective and constraints involve "bowl-shaped", or convex, functions.

    > They are _easy_ to solve because they _do not suffer_ from the "curse" of _local minima_.

6. **Combinatorial optimization:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents46}
    :   In combinatorial optimization, some (or all) the variables are boolean (or integers), reflecting discrete choices to be made.

    > Combinatorial optimization problems are in general extremely hard to solve. Often, they can be approximately solved with linear or convex programming.

7. **NON-Convex Optimization Problems [Examples]:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents47} \\
    * **Boolean/integer optimization:** some variables are constrained to be Boolean or integers.  
    > Convex optimization can be used for getting (sometimes) good approximations.
    * **Cardinality-constrained problems:** we seek to bound the number of non-zero elements in a vector variable.  
    > Convex optimization can be used for getting good approximations.
    * **Non-linear programming:** usually non-convex problems with differentiable objective and functions.  
    > Algorithms provide only local minima.  

    > Most (but not all) non-convex problems are hard!

***
***

TITLE: 1.2 <br /> Introduction to Optimization
LINK: research/optimization/1/1.md



## Nomenclature
{: #content3}

1. **Feasible set:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}
    :   $$ \mathbf{X} :=  \left\{ x \in \mathbf{R}^n ~:~  f_i(x) \le 0, \ \  i=1, \ldots, m \right\}.$$  

2. **Solution:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32}
    :   In an optimization problem, we are usually interested in computing the optimal value of the objective function, and also often a minimizer, which is a vector which achieves that value, if any.

3. **Feasibility problems:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} 
    :   Sometimes an objective function is not provided. This means that we are just _interested_ in _finding_ a _feasible point_, or determine that the problem is _infeasible_.  
    > By convention, we set $$f_0$$ to be a constant in that case, to reflect the fact that we are indifferent to the choice of a point x as long as it is feasible.

4. **Optimal value:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34}
    :    $$p^\ast := \min_x : f_0(x) ~:~ f_i(x) \le 0, \ \  i=1, \ldots, m.$$   

    > Denoted $$p^\ast$$.

5. **Optimal set:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents35} 
    :   The set of feasible points for which the objective function achieves the optimal value:  
    $$ \mathbf{X}^{\rm opt} :=  \left\{ x \in \mathbf{R}^n ~:~  f_0(x) = p^\ast, \ \ f_i(x) \le 0, \ \  i=1,\ldots, m \right\}$$.  
    Equivalently,  
    $$ \mathbf{X}^{\rm opt} = \mathrm{arg min}_{x \in \mathbf{X}}  f_0(x)$$.  

    > We take the convention that the optimal set is empty if the problem is not feasible.  

    > A _point_ $$x$$ is said to be **optimal** if it belongs to the optimal set.

    > If the optimal value is **ONLY** attained in the **limit**, then it is **NOT** in the optimal set.

6. **When is a problem "_Attained_"?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents36}
    :   If the optimal set is _not empty_, we say that the problem is **_attained_**.

7. **Suboptimality:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents37}
    :   The $$\epsilon$$-suboptimal set is defined as:  
    :   $$ \mathbf{X}_\epsilon := \left\{ x \in \mathbf{R}^n ~:~ f_i(x) \le 0, \ \  i=1, \ldots, m, \ \  f_0(x) \le p^\ast + \epsilon \right\}.$$  

    > $$\implies \  \mathbf{X}_0 = \mathbf{X}_{\rm opt}$$.

8. **(Local and Global) _Optimality_:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents38} \\
    * A point $$z$$ is **Locally Optimal**: if there is a value $$R>0$$ such that $$z$$ is optimal for the following problem:
    $$min_x : f_0(x) ~:~ f_i(x) \le 0, \ \ i=1, \ldots, m,  \ \ \|z-x\|_2 \le R$$.  
    > i.e. a _local minimizer_ $$x$$ _minimizes_ $$f_0$$, but **only** for _nearby points_ on the feasible set.

    * A point $$z$$ is **Globally Optimal**: if it is the _optimal value_ of the original problem on all of the feasible region.   

***

## Problem Classes
{: #content4}

1. **Least-squares:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41}
    :   $$\min_x \;\;\;\; \sum_{i=1}^m \left( \sum_{j=1}^n A_{ij} . x_j - b_i \right)^2,$$
    :   where $$A_{ij}, \  b_i, \  1 \le i \le m,  \ 1 \le j \le n$$, are given numbers, and $$x \in \mathbf{R}^n$$ is the variable.

2. **Linear Programming:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} 
    :   $$ \min \sum_{j=1}^n c_jx_j ~:~ \sum_{j=1}^n A_{ij} . x_j  \le b_i , \;\; i=1, \ldots, m, $$ 
    :   where $$ c_j, b_i$$ and $$A_{ij}, \  1 \le i \le m, \  1 \le j \le n$$, are given real numbers.  

    > This corresponds to the case where the functions $$f_i(i=0, \ldots, m)$$ in the standard problem are all affine (that is, linear plus a constant term).  

    > Denoted $$LP$$.

3. **Quadratic Programming:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents43}
    :   $$\min_x \;\;\;\; \displaystyle\sum_{i=1}^m \left(\sum_{j=1}^n C_{ij} . x_j+d_i\right)^2 + \sum_{i=1}^n c_ix_i \;:\;\;\;\; \sum_{j=1}^m A_{ij} . x_j \le b_i, \;\;\;\; i=1,\ldots,m.$$  

    > Includes a **sum of squared linear functions**, in addition to a **linear term**, in the _objective_.  

    > QP's are popular in finance, where the _linear term_ in the _objective_ refers to the _expected negative return_ on an investment, and the _squared terms_ corresponds to the _risk_ (or variance of the return).  

    > QP was introduced by "Markowitz"

4. **Nonlinear optimization:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents44}
    :   A broad class that includes _Combinatorial Optimization_.

    > One of the reasons for which non-linear problems are hard to solve is the issue of _local minima_.

5. **Convex optimization:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents45}
    :    A generalization of QP, where the objective and constraints involve "bowl-shaped", or convex, functions.

    > They are _easy_ to solve because they _do not suffer_ from the "curse" of _local minima_.

6. **Combinatorial optimization:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents46}
    :   In combinatorial optimization, some (or all) the variables are boolean (or integers), reflecting discrete choices to be made.

    > Combinatorial optimization problems are in general extremely hard to solve. Often, they can be approximately solved with linear or convex programming.

7. **NON-Convex Optimization Problems [Examples]:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents47} \\
    * **Boolean/integer optimization:** some variables are constrained to be Boolean or integers.  
    > Convex optimization can be used for getting (sometimes) good approximations.
    * **Cardinality-constrained problems:** we seek to bound the number of non-zero elements in a vector variable.  
    > Convex optimization can be used for getting good approximations.
    * **Non-linear programming:** usually non-convex problems with differentiable objective and functions.  
    > Algorithms provide only local minima.  

    > Most (but not all) non-convex problems are hard!

***
***

TITLE: 1.1 <br /> Background
LINK: research/optimization/1/1.1.md


## Mathematical Background
{: #content1}

1. **Maps:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   We reserve the term map to refer to vector-valued functions. That is, maps are
    functions which return more than a single value.

2. **Graph:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    Consider a function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$.  
    The _**graph**_ of $$f$$ is the set of input-output pairs that $$f$$ can attain, that is:
    $$G(f) := \left \{ (x,f(x)) \in \mathbf{R}^{n+1} : x \in \mathbf{R}^n \right \}.$$ \\
    > It is a subset of $$\mathbf{R}^{n+1}$$.

3. **Epigraph:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    Consider a function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$.  
    The _**epigraph**_, denoted $$\mathop{\bf epi} f$$, describes the set of input-output pairs that $$f$$ can achieve, as well as "anything above":  
    $$\mathop{\bf epi} f := \left \{ (x,t) \in \mathbf{R}^{n+1} ~:~ x \in \mathbf{R}^n, \ \  t \ge f(x) \right \}.$$
    > epi in Greek means "above"  

    <button>Example [image]</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/1/1.1_2/1.png){: hidden=""}

4. **Level and Sub-level Sets:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    > _**Level**_ and _**sub-level**_ sets correspond to the notion of contour of a function. Both are indexed on some scalar value $$t$$.  

    * **Level sets**: is simply the set of points that achieve exactly some value for the function $$f$$.  
    For $$t \in \mathbf{R}$$, the $$t-$$level set of the function $$f$$ is defined as:  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{L}_t(f) := \left\{ x \in \mathbf{R}^{n} ~:~ x \in \mathbf{R}^n, \ \  t = f(x) \right \}.$$

    * **Sub-level sets**: is the set of points that achieve at most a certain value for  $$f$$, or below:  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \mathbf{S}_t(f) := \left\{ x \in \mathbf{R}^{n} ~:~ x \in \mathbf{R}^n, \ \  t \ge f(x) \right\}.$$  

    <button>Example [image]</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/1/1.1_2/2.png){: hidden=""}

***

## Mathematical Formulation [Standard Forms]
{: #content2}

1. **Functional Form:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}
    :   An optimization problem is a problem of the form
    $$p^\ast := \displaystyle\min_x f_0(x) :  f_i(x) \le 0, \ \  i=1,\ldots, m$$,  
    where:  
        * $$x \in \mathbf{R}^n$$ is the decision variable;

        * $$f_0 : \mathbf{R}^n \rightarrow \mathbf{R}$$ is the objective function, or cost; 

        * $$f_i : \mathbf{R}^n \rightarrow \mathbf{R}, \ \  i=1, \ldots, m$$ represent the constraints;

        * $$p^\ast$$ is the optimal value.  

    * [**Example.**](http://livebooklabs.com/keeppies/c5a5868ce26b8125/e5deae3e0c61b80e){: value="show" onclick="iframePopA(event)"}
        <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/e5deae3e0c61b80e">` Visit the Book`</a>
        <div markdown="1"> </div>

2. **Epigraph form:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22}
    :   TODO

3. **Other Standard-Forms:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23}
    :   TODO

***
***

TITLE: Convex Optimization
LINK: research/optimization/3/3.1.md


## Point-Set Topology
{: #content1}

1. **Open Set:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    :   A set $$\chi \subseteq \mathbf{R}^n$$ is said to be open if for any point $$x \in \chi$$ there exist a ball centered in $$x$$ which is contained in $$\chi$$. 
    :   Precisely, for any $$x \in \mathbf{R}^n$$ and $$\epsilon > 0$$ define the Euclidean ball of radius $$r$$ centered at $$x$$:
    :   $$B_\epsilon(x) = {z : \|z ‚àí x\|_2 < \epsilon}$$
    :   Then, $$\chi \subseteq \mathbf{R}^n$$ is open if
    :   $$\forall x \: \epsilon \: \chi, \:\: \exists \epsilon > 0 : B_\epsilon(x) \subset \chi .$$
    :   **Equivalently**,
    :   * A set $$\chi \subseteq \mathbf{R}^n$$ is open if and only if $$\chi = int\; \chi$$.
    :   * An open set does not contain any of its boundary points.
    :   * A closed set contains all of its boundary points. 
    :   * Unions and intersections of open (resp. closed) sets are open (resp. closed).

2. **Closed Set:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} 
    :   A set $$\chi \subseteq \mathbf{R}^n$$ is said to be closed if its complement $$ \mathbf{R}^n \text{ \ } \chi$$ is open.

3. **Interior of a Set:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13}
    :   The interior of a set $$\chi \subseteq \mathbf{R}^n$$ is defined as 
    :   $$int\: \chi = \{z \in \chi : B_\epsilon(z) \subseteq \chi, \:\: \text{for some } \epsilon > 0 \}$$

4. **Closure of a Set:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14}
    :   The closure of a set $$\chi \subseteq \mathbf{R}^n$$ is defined as
    :   $$\bar{\chi} = \{z ‚àà \mathbf{R}^n : \: z = \lim_{k\to\infty} x_k, \: x_k \in \chi , \: \forall k\},$$  
    :   > i.e., the closure of $$\chi$$ is the set of limits of sequences in $$\chi$$.

5. **Boundary of a Set:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15}
    :   The boundary of X is defined as
    :   $$\partial \chi = \bar{\chi} \text{ \ }  int\: \chi$$

6. **Bounded Set:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16}
    :   A set $$\chi \subseteq \mathbf{R}^n$$ is said to be bounded if it is contained in a ball of finite radius, that is if there exist $$x \in \mathbf{R}^n$$ and $$r > 0$$ such that $$\chi \subseteq B_r(x)$$.

7. **Compact Set:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17}
    :   A set $$\chi \subseteq \mathbf{R}^n$$ is compact $$\iff$$ it is **Closed** and **Bounded**.

8. **Relative Interior [$$\operatorname{relint}$$]:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents18}
    :   We define the relative interior of the set $$\chi$$, denoted $$\operatorname{relint} \chi$$, as its interior relative to $$\operatorname{aff} C$$:
    :   $$\operatorname{relint} \chi = \{x \in \chi : \: B(x, r) \cap \operatorname{aff} \chi \subseteq \chi \text{ for some } r > 0\},$$
    :   where $$B(x, r) = \{y : ky ‚àí xk \leq r\}$$, the ball of radius $$r$$ and center $$x$$ in the norm $$\| ¬∑ \|$$.

9. **Relative Boundary:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents19}
    :   We can then define the relative boundary of a set $$\chi$$ as $$\mathbf{cl}  \chi \text{ \ } \operatorname{relint} \chi,$$ where $$\mathbf{cl} \chi$$ is the closure of $$\chi$$.

***

## Sets Combinations and Hulls
{: #content2}

1. **Lines and Line Segments [Linear Sets]:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}
    :   Suppose $$x_1 \ne x_2$$ are two points in $$\mathbf{R}^n$$
    :   Points of the form, 
    :   $$y = \theta x_1 + (1 ‚àí \theta)x_2$$,
    :   where, $$\theta \in \mathbf{R}$$, form the line passing through $$x_1$$ and $$x_2$$. 
    :   The parameter value $$\theta = 0$$ corresponds to $$y = x_2$$, and the parameter value $$\theta = 1$$ corresponds to $$y = x_1$$.
    :   Values of the parameter $$\theta$$ between 0 and 1 correspond to the (closed) line segment between $$x_1$$ and $$x_2$$.

2. **Affine Sets:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22}
    :   An affine set is a translation of a subspace‚Äâ‚Äî‚Äâit is "flat" but does not necessarily pass through 0, as a subspace would. 
        > (Think for example of a line, or a plane, that does not go through the origin.)
    :   An affine set $$\mathbf{A}$$ can always be represented as the translation of the subspace spanned by some vectors:
    :   $$ \mathbf{A} = \left\{ x_0 + \sum_{i=1}^m \lambda_i x_i ~:~ \lambda \in {\mathbf{R}}^m \right\}\ \ \ $$, for some vectors $$x_0, x_1, \ldots, x_m.$$  

    $$\implies \mathbf{A} = x_0 + \mathbf{S}.$$

    * **(Special case)** **lines**: When $$\mathbf{S}$$ is the span of a single non-zero vector, the set $$\mathbf{A}$$ is called a line passing through the point $$x_0$$. Thus, lines have the form
    $$\left\{ x_0 + tu ~:~ t \in \mathbf{R} \right\}$$,  \\
    where $$u$$ determines the direction of the line, and $$x_0$$ is a point through which it passes.

    * <button>Example: Diminsion of Affine Subspaces</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/2/2.1/affine_sub.png){: hidden=""}  

3. **Cones [Cone Sets]:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23}
    :   A set $$C$$ is a cone if $$x \in C$$, then $$\alpha x \in C$$, for every $$\alpha \geq 0$$. 
    :   A set C is said to be a convex cone if it is convex and it is a cone.  
        > The conic hull of a set is a convex cone.

77. **Linear Combination:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents277}  
    :   A **Linear Combination** is an expression constructed from a set of terms by multiplying each term by a constant and adding the results.
    :   $$ \sum_{i=1}^n \lambda_i x_i $$

88. **Affine Combination:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents288}  
    :   An **Affine Combination** of the points is a special type of linear combination, in which
    the coefficients $$\lambda_i$$ are restricted to sum up to one, that is
    :   $$\sum_{i=1}^n \lambda_i x_i \: : \:\: \sum_{i=1}^m \lambda_i = 1$$
    :   > Intuitively, a convex combination is a weighted average of the points, with weights
    given by the $$\lambda_i$$ coefficients.

99. **Conical Combination:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents299}  
    :   A **Conical Combination** of the points is a special type of linear combination, in which
    the coefficients $$\lambda_i$$ are restricted to be nonnegative, that is:
    :   $$\sum_{i=1}^n \lambda_i x_i \: : \:\: \lambda_i \geq 0 \: \text{for all } i$$

10. **Convex Combination:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents210}  
    :   A **Convex Combination** of the points is a special type of linear combination, in which
    the coefficients $$\lambda_i$$ are restricted to be nonnegative and to sum up to one, that is
    :   $$\lambda_i \geq 0 \: \text{for all }  i, \: \text{ and } \sum_{i=1}^m \lambda_i = 1$$
    :   > Intuitively, a convex combination is a weighted average of the points, with weights
    given by the $$\lambda_i$$ coefficients.

8. **Linear Hull:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents28}
    :   Given a set of points (vectors) $$\in  \mathbf{R}^n:$$  
    :   $$P = \{x^{(1)} , . . . , x^{(m)} \},$$
    :   The **linear hull** (subspace) generated by these points is the set of all possible linear
    combinations of the points:
    :   $$x=\lambda_1x^{(1)} + \cdots + \lambda_mx^{(m)}, \:\: \text{for } \lambda_i \in \mathbf{R}, \: i \in \{1, \cdots, m\}$$

9. **Affine Hull:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents29}
    :   The affine hull, $$\operatorname{aff}\: P$$, of $$P$$ is the set generated by taking all possible linear
    combinations of the points in $$P$$, under the restriction that the coefficients $$\lambda_i$$ sum up to one, that is $$\sum_{i=1}^m \lambda_i = 1$$.
    :   $$\operatorname{aff}\: P$$ is the smallest affine set containing $$P$$.
    :   * **Props.**  
            * It is the smallest affine set containing $$\chi$$. 
            * or, The intersection of all affine sets containing $$\chi$$.
            * $${\displaystyle \mathrm {aff} (\mathrm {aff} (S))=\mathrm {aff} (S)}$$
            * $${\mathrm{aff}}(S)$$ is a closed set
            * $${\displaystyle \mathrm {aff} (S+F)=\mathrm {aff} (S)+\mathrm {aff} (F)}$$
            * Affine Hull is bigger than or equal to the convex hull.
            * The linear span of $$\chi$$ contains the affine hull of $$\chi$$.
    :   * **Examples:**  
            * The affine hull of a singleton (a set made of one single element) is the singleton itself.
            * The affine hull of a set of two different points is the line through them.
            * The affine hull of a set of three points not on one line is the plane going through them.
            * The affine hull of a set of four points not in a plane in $$\mathbf{R}^3$$ is the entire space $$\mathbf{R}^3$$.

11. **Convex Hull:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents211}
    :   The set of all possible convex combination is called the **convex hull** of the point set $$\chi$$ in the Euclidean plane or in a Euclidean space (or, more generally, in an affine space over the reals) is the smallest convex set that contains $$\chi$$:
    :   $$\mathbf{conv} (x^{(1)}, \cdots, x^{(m)}) = \left\{\sum_{i=1}^m \lambda_i x^{(i)} : \: \lambda_i \geq 0, \: i \in \{1, \cdots, m\}; \:\: \sum_{i=1}^m \lambda_i = 1\right\}$$
    :   * **Props.**  
            * The convex hull of the given points is identical to the set of all their convex combinations.
            * It is the intersection of all convex sets containing $$\chi$$.
            * or, The set of all convex combinations of points in $$\chi$$.
            * or, The (unique) minimal convex set containing $$\chi$$.
            * or, The union of all simplices with vertices in $$\chi$$.
            * The algorithmic problem of finding the convex hull of a finite set of points in the plane or other low-dimensional Euclidean spaces is one of the fundamental problems of computational geometry.
            * The convex hull of a finite point set $${\displaystyle S\subsetneq \mathbb {R} ^{n}}$$ forms a convex polygon when $$n = 2$$,
            * or more generally a convex polytope in $${\displaystyle \mathbb {R} ^{n}}$$.

12. **Conic Hull:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents212}
    :   The set of all possible conical combinations is called the **conic hull** of the point set:
    :   $$\mathbf{conic} (x^{(1)}, \cdots, x^{(m)}) = \left\{\sum_{i=1}^m \lambda_i x^{(i)} : \: \lambda_i \geq 0, \: i \in \{1, \cdots, m\} \right\}$$
    :   * **Props.**  
            * The conical hull of a set $$\chi$$ is a convex set.
            * In fact, it is the intersection of all convex cones containing $$\chi$$ plus the origin.
            * If $$\chi$$ is a compact set (in particular, when it is a finite non-empty set of points), then the condition "plus the origin" is unnecessary.
            * If we discard the origin, we can divide all coefficients by their sum to see that a conical combination is a convex combination scaled by a positive factor.
            * Conical combinations and hulls may be considered as convex combinations and convex hulls in the projective space.
            * The conic hull of a closed set is not, even, necessarily a closed set.
            * While the convex hull of a compact set is a compact set as well, this is not so for the conical hull: the latter is _Unboudned_.

***

## Convex Set
{: #content3}

0. **Convex Set:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents30} 
    :   A subset $$\mathbf{C}$$ of $$\mathbf{R}^n$$ is said to be convex if and only if it contains the line segment between any two points in it:  
    : $$ \forall : x_1, x_2 \in \mathbf{C}, \;\; \forall : \lambda \in [0,1] \::\: \lambda x_1 + (1-\lambda)  x_2 \in \mathbf{C}$$ 

11. **Strictly Convex Set:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents311} 
    :   A set C is said to be strictly convex if it is convex, and 
    :   $$x_1 \ne x_2 \in C, \: \lambda \in (0, 1) \implies \lambda x_1 + (1 ‚àí \lambda)x_2 \in \mathbf{relint} C$$

2. **Strongly Convex:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32}
    :   A function $$ f: \mathbf{R}^n \rightarrow \mathbf{R}$$ is strongly convex if there exist a $$m > 0$$ such that $$\tilde{f}(x) = f(x) ‚àí \dfrac{m}{2}\|x\|_2^2$$ is convex, that is if
    :   $$ f(\theta x + (1-\theta) y) \leq \theta f(x) + (1-\theta) f(y) - \dfrac{m}{2}\theta(1-\theta) \|x-y\|_2^2$$ 

1. **Diminsion:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}
    :   The dimension d of a convex set $$C \subseteq \mathbf{R}^n$$ is defined as the dimension of its affine hull. 
    :   It can happen that $$d < n$$.  
        > e.g., $$C = \left\{x = \left[\alpha 0\right]^T : \; \alpha ‚àà [0, 1]\right\}$$ is a convex subset of $$\mathbf{R}^2$$ , with affine dimension $$d = 1$$.


***

## Prominant Examples
{: #content4}

1. **Convex Examples:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41}
    :   Subspaces and affine sets, such as lines and hyperplanes are obviously convex, as
    they contain the entire line passing through any two points. 
    :   Half-spaces are also convex.
    :       * <button>Convex and Non-Convex Sets</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/1.png){: hidden=""}
    :       * <button>The probability simplex</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/prob_simplex.png){: hidden=""}




## Operators and Convexity
{: #content5}

1. **Intersection:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents51} 
    :   The intersection of a (possibly infinite) family of convex sets is convex. This property can be used to prove convexity for a wide variety of situations.
    :   Ex: An halfspace $$H = \{x \in \mathbf{R}^n : \: c^Tx \leq d\}, c \ne 0$$ is a convex set. The intersection of $$m$$ halfspaces $$H_i, i = 1, \cdots, m$$, is a convex set called a **polyhedron**.
    :       * <button>Second-order Cone</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/soc.png){: hidden=""}
    :       * <button>Semi-definite Cone</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/2.png){: hidden=""}

2. **Affine Transformation:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents52}
    :   If a map $$f \: \mathbf{R}^n \rightarrow \mathbf{R}^m$$ is affine, and $$\mathbf{C}$$ is convex, then the set
    :   $$f(\mathbf{C}) \: = \left\{ f(x) : \: x \in \mathbf{C} \right\}$$
    :   is convex.  
        > In particular, the projection of a convex set on a subspace is convex.
    :   <button>Proof.</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/pf1.png){: hidden=""}
    :    <button>Example: Projection of a convex set on a subspace.</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/3.png){: hidden=""}

3. **Composition w/ Affine Function:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents53}
    :   The composition with an affine function preserves convexity: 
    :   If $$A \in \mathbf{R}^{m \times n}, b \in \mathbf{R}^m \text{ and } f : \mathbf{R}^m \rightarrow \mathbf{R}$$
    :   is convex, then the function $$g : \mathbf{R}^n \rightarrow \mathbf{R}$$ with values $$g(x) = f(Ax+b)$$ is convex.

4. **Point-Wise Maximum:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents54}  
    :   The pointwise maximum of a family of convex functions is convex:  
    :   If $$(f_\alpha)_{\alpha \in {\cal A}}$$ is a family of convex functions index by $$\alpha$$, then the function
    :   $$f(x) := \max_{\alpha \in {\cal A}} : f_\alpha(x)$$
    :   is convex. 
    :   <button>Proof.</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/19.png){: hidden=""}
    :    <button>Ex. Convexity of the Dual Norm</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/9.png){: hidden="" width="80%"}
    :    <button>Ex. Convexity of the Largest Singular Value Function</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/10.png){: hidden="" width="80%"}


5. **Nonnegative Weighted Sum:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents55}
    :   The nonnegative weighted sum of convex functions is convex.
    :    <button>Ex. Convexity of the Negative Entropy Function</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/11.png){: hidden="" width="80%"}    
    :   <button>Proof.</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/14.png){: hidden=""}


6. **Partial Minimum:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents56}
    :   If $$f$$ is a convex function in $$x=(y,z),$$ then the function 
    :   $$g(y) := \min_z \: f(y,z) $$
    :   is convex.
    :   > Note that joint convexity in $$(y,z)$$ is essential.
    :    <button>Ex. Schurs Complement Lemma</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/12.png){: hidden="" width="80%"}  

7. **Composition W/ Monotone Convex Functions.:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents57}
    :   The composition with another function does not always preserve convexity. However, if f = h circ g, with h,g convex and h increasing, then f is convex.
    :   Indeed, the condition $$f(x) \le t$$ is equivalent to the existence of $$y$$ such that
    :   $$h(y) \le t, \;\; g(x) \le y$$ 
    :   The condition above defines a convex set in the space of $$(x,y,t)$$-variables. 
    :   The epigraph of $$f$$ is thus the projection of that convex set on the space of $$(x,t)$$-variables, hence it is convex.
    :    <button>Ex. Convexity via Monotonicity.</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/13.png){: hidden="" width="80%"}  

* _Further Analysis_:  
    :   More generally, if the functions $$g_i : \mathbf{R}^n \rightarrow \mathbf{R}, i=1, \ldots, k$$ are convex and $$h : \mathbf{R}^k \rightarrow \mathbf{R}$$ is convex and non-decreasing in each argument, with $$\mathbf{dom}g_i = \mathbf{dom} h = \mathbf{R}$$, then
    :   $$x \rightarrow (h \circ g)(x) \: = h(g_1(x), \ldots, g_k(x)) $$
    :   is convex.
    :   > For example, if $$g_i$$'s are convex, then  $$log \sum_i \exp{g_i}$$ also is.

## Seperation Theorems
{: #content6}
<p class="message">Separation theorems are one of the most important tools in convex optimization. They convey the intuitive idea that two convex sets that do not intersect can be separated by a straight line.</p>

1. **Theorem. Supporting Hyperplane:**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents61}
    :   If $$\mathbf{C} \subseteq \mathbf{R}^n$$ is convex and non-empty, then for any $$x_0$$ at the boundary of $$\mathbf{C}$$, there exist a supporting hyperplane to $$\mathbf{C}$$ at $$x_0$$,   
    meaning that there exist $$a \in \mathbf{R}^n, \: a \ne 0, $$ such that $$a^T(x-x_0) \le 0$$ for every $$x \in \mathbf{C}$$.
    :    <button>Proof.</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/pf2.png){: hidden=""}

2. **Theorem. Separating Hyperplane:**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents62}
    :   If $$\mathbf{C}, \mathbf{D}$$ are two convex subsets of $$\mathbf{R}^n$$ that do not intersect, then there is an hyperplane that separates them,  
    that is, $$\exists a \in \mathbf{R}^n, \: a \ne 0, $$ and $$b \in \mathbf{R}$$, such that   $$a^Tx \le b$$ for every $$x \in \mathbf{C}$$, and $$a^Tx \ge b$$ for every $$x \in \mathbf{D}$$.
    :   Equivalently, 
    :   Let C, D ‚äÜ Rn be nonempty convex disjoint sets i.e., $$C \cap D = \varnothing$$.
    :   Then, there exists a hyperplane separating these sets, i.e., 
    :   $$ \exists  a \in \mathbf{R}^n, \: a \ne 0$$, such that
    :   $$  \sup_{x\in C} a^Tx \: \leq \:  \sup_{z\in D}a^Tz$$  
    :   <img src="/main_files/conv_opt/3/3.1/4.png" width="30%" style="position: relative;left:235px">
    :    <button>Proof.</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/pf3.png){: hidden=""}
    :   > When two convex sets do not intersect, it is possible to find a hyperplane that separates them.

3. **Theorem. Strictly Separating Hyperplane:**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents63}
    :   Let $$C, D \subseteq \mathbf{R}^n$$ be nonempty convex disjoint sets.
    :   Assume that $$C ‚àí D$$ is closed. Then, there exists a hyperplane strictly. separating the sets, i.e., $$\exists \: a \in \mathbf{R}^n, \: a \ne 0,$$ such that
    :   $$  \sup_{x\in C} a^Tx \: < \:  \sup_{z\in D}a^Tz$$  
    :   >  When is $$(C ‚àí D)$$ closed?   
        >   > One of conditions: $$C$$ is closed and $$D$$ is compact.

4. **Farkas lemma:**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents64}
    :   Let $$A \in \mathbf{R}^{m \times n}$$ and $$y \in \mathbf{R}^m$$. Then, one and only one of the following two conditions is satisfied:  
        1. The system of linear equations $$Ax = y$$ admits a nonnegative solution $$x \geq 0$$.
        2. There exist $$z \in \mathbf{R}^m$$ such that $$z^TA \geq 0, \: z^Ty < 0$$.
    :   * **Equivalent Formulation:** statement (2) above implies the negation of statement (1), and vice versa. Thus, the following two statements are equivalent:   
        1. There exist $$x \geq 0$$ such that $$Ax = y$$.
        2. $$z^Ty \geq 0, \:\: \forall z : \: z^TA \geq 0$$.
    :   * **Interpretation in terms of systems of linear inequalities:**
            Let $$a_i \in \mathbf{R}^m, i = 1, \cdots, n$$, be the columns of $$A$$, then
    :   $$y^Tz \geq 0, \forall z : a_i^Tz \geq 0, i = 1, \cdots, n$$
    :   if and only if there exist multipliers $$x_i \geq 0, i = 1, \cdots, n$$ such that $$y$$ is a conic combination of the $$a_i$$‚Äôs:
    :   $$ \exists x_i \geq 0, i = 1, \cdots, m : \: y = a_1x_1 + \cdots + a_nx_n.$$


***


## Convex Functions
{: #content7}

1. **Domain:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents71}
    :   The domain of a function $$f: \mathbf{R}^n \rightarrow \mathbf{R}$$ is the set $$\mathbf{dom} f \subseteq \mathbf{R}^n$$ over which $$f$$ is well-defined, in other words:
    :   $$\mathbf{dom} f \: = \{ x \in \mathbf{R}^n : \: -\infty < f(x) < +\infty\}.$$

2. **Convex Function:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents72}
    :   A function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$ is convex if
    :   (1) Its domain $$\mathbf{dom} f$$ is convex.
    :   (2) And, $$\forall \:\: x, y \in \mathbf{dom} f , \;\; \forall \theta \in [0,1] : \:\: f(\theta x + (1-\theta) y) \le \theta f(x) + (1-\theta) f(y).$$
    :   > Note that the convexity of the domain is required.

3. **Concave Function:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents73}
    :   A function $$f$$ is concave if the function $$-f$$ is convex.

4. **Convexity and the Epigraph:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents74}
    :   A function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$, is convex if and only if, its epigraph,
    :   $$ \mathbf{ epi } f \:=  \: \left\{ (x,t) \in \mathbf{R}^{n+1} : \: t \ge f(x) \right\} $$
    :   is convex.
    :   **Example:** We can us this result to prove for example, that the largest eigenvalue function $$\lambda_{\rm max} : \mathcal{S}^n \rightarrow \mathbf{R}$$, which to a given $$n \times n$$ symmetric matrix $$X$$ associates its largest eigenvalue, is convex, since the condition $$\lambda_{\rm max}(X) \le t$$ is equivalent to the condition that $$t I - X \in \mathcal{S}_+^n$$.

5. **First-order condition:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents75}
    :   If f is differentiable (that is, $$\mathbf{dom} f$$ is open and the gradient exists everywhere on the domain), then $$f$$ is convex if and only if
    :   $$ \forall : x, y : \: f(y) \ge f(x) + \nabla f(x)^T(y-x) .$$
    :   > The geometric interpretation is that the graph of $$f$$ is bounded below everywhere by anyone of its tangents.
    :   <button>Convexity of the log-sum-exp function.</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/15.png){: hidden=""}
    :   ![img](/main_files/conv_opt/3/3.1/16.png){: width="100%"}

6. **Restriction to a line:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents76}
    :   The function $$f$$ is convex if and only if its restriction to any line is convex, meaning that 
    :   for every $$x_0 \in \mathbf{R}^n$$, and $$v \in \mathbf{R}^n$$, the function $$g(t) := f(x_0+tv)$$ is convex. 
    :   Note that the "if" part is a direct consequence of the "composition with an affine function" result below.
    :   <button>Example: Diminsion of Affine Subspaces</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/5.png){: hidden=""}
    :   <button>Example: sum log</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/18.png){: hidden=""}

7. **Second-order Condition:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents77}
    :   If $$f$$ is twice differentiable, then it is **convex** if and only if its Hessian $$\nabla^2 f$$ is positive semi-definite everywhere on the domain of $$f$$.
    :   > This is perhaps the most commonly known characterization of convexity.
    :   Also, If $$f$$ is twice differentiable, then it is _**Strictly**_ **convex** if and only if its Hessian $$\nabla^2 f$$ is positive definite everywhere on the domain of $$f$$.
    :   Finally, If $$f$$ is twice differentiable, then it is _**Strongly**_ **convex** if and only if its Hessian $$\nabla^2 f \succeq ml$$, for some $$m > 0$$ and for all $$x \in \mathbf{dom} f$$.
    :   <button>Convexity of a quadratic function.</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/8.png){: hidden=""}
    :   <button>Convexity of the square-to-linear function</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/6.png){: hidden=""}
    :   <button>Convexity of the log-sum-exp function.</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/7.png){: hidden=""}
    :   <button>Example.</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/3/3.1/17.png){: hidden=""}


***
***

TITLE: 3.6 <br /> Robust Linear Programming
LINK: research/optimization/3/3.6.md



## Introduction
{: #content1}

1. **Robust Linear Programming:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   **Robust Linear Programming** addresses linear programming problems where the data is uncertain, and a solution which remains feasible despite that uncertainty, is sought.
    :   The robust counterpart to an LP is not an LP in general, but is always convex. The figure on the left illustrates the feasible set of the "robust counterpart" of an LP after we take into account uncertainty in the facets' directions.

2. **Uncertainty Models:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    :   We have three models for _Tractable_ cases of uncertainty:  
        1. Scenario uncertainty
        2. Box uncertainty
        3. Ellipsoidal uncertainty

***

## Tractable Cases
{: #content2}

1. **Scenario Uncertainty:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}
    :   * **Uncertainty model:** In the scenario uncertainty model, the uncertainty on a coefficient vector a is described by a finite set of points:
    :    $$\mathbf{U} = \left\{ a^1, \ldots, a^K \right\},$$
    :   where each vector $$a^k \in \mathbf{R}^n, k=1, \ldots, K$$ corresponds to a particular ‚Äúscenario‚Äù.
    :   * **The robust counterpart to a half-space constraint:**
    :   $$\forall \: a \in \mathbf{U} , \;\; a^Tx \le b ,$$
    :   can be simply expressed as a set of $$K$$ affine inequalities:
    :   $$ (a^k)^Tx \le b, \;\; k= 1, \ldots, K. $$
    :   > Note that the scenario model actually enforces more than feasibility at the ‚Äúscenario‚Äù points $$a^k$$.   
        >   > In fact, for any $$a$$ that is in the convex hull of the set $$\mathbf{U}$$, the robust counterpart holds.
    :   Indeed, if the above holds, then for any set of nonnegative weights $$\lambda_1, \ldots, \lambda_K$$ summing to one, we have  
    :   $$ \sum_{k=1}^K \lambda_k (a^k)^Tx \le b, \;\; k= 1, \ldots, K. $$
    :   $$\implies$$
    :   * **The robust counterpart to the original LP:**
    :   $$\min_x \: c^Tx ~:~ \forall \: a_i \in \mathbf{U}_i , \;\; a_i^Tx \le b_i , \;\; i= 1, \ldots, m,$$
    :   with $$\mathbf{U}_i = \{ a_i^1, \ldots, a_i^{K_i} \}, i= 1, \ldots, m,$$ becomes
    :   $$\min_x \: c^Tx ~:~ (a_i^k)^Tx \le b_i, \;\; k=1, \ldots, K_i, \;\; i= 1, \ldots, m,$$
    :   where this is an LP, with a total of $$K_1+...+K_m$$ constraints, where $$K_i$$ is the number of elements in the finite set $$\mathbf{U}_i$$, and $$m$$ is the number of constraints in the original (nominal) LP.
    :   * The scenario model is attractive for its simplicity. However, the number of scenarios can result in too large a problem.
 
2. **Box Uncertainty:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22}
    :   * **Uncertainty model:** The box uncertainty model assumes that every coefficient vector $$a_i$$ lies in a "box", or more generally, a hyper-rectangle $$\in \mathbf{R}^n$$, but is otherwise unknown.
    :   * _In its simplest case, the uncertainty model has the following **form**_:
    :   $$ \mathbf{U} = \left\{ a ~:~ |a-\hat{a}|_\infty \le \rho \right\},$$
    :   where $$\rho \ge 0$$ is a measure of the size of the uncertainty, and $$\hat{a}$$ represents a "nominal" vector.   
        This describes a "box" of half-diameter $$\rho$$ around the center $$\hat{a}$$.
    :   










***

## THIRD
{: #content3}










***
***

TITLE: 3.2 <br /> Linear Programming
LINK: research/optimization/3/3.2.md


## Polyhedra
{: #content1}

1. **Linear Programs:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   A linear program (LP) is an optimization problem in standard form, in which all the functions involved are affine. The feasible set is thus a polyhedron, that is, an intersection of half-spaces.

2. **Polyhedral function:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    :   Polyhedral functions are functions with a polyhedral epigraph, and include maxima or sums of maxima of linear or affine functions. Such functions can be minimized via LP.

3. **Half-spaces:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    :   A half-space is a set defined by a single affine inequality. Precisely, a half-space $$\in \mathbf{R}^n$$ is a set of the form
 mathbf{H} = left{ x ~:~ a^Tx le b right}, 
where a in mathbf{R}^n, b in mathbf{R}.


***
***

TITLE: 2.1 <br /> Basics and Definitions
LINK: research/optimization/2/2.1.md


## Definitions
{: #content1}

1. **Linear Independence:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   A set of vectors $$\{x_1, ... , x_m\} \in {\mathbf{R}}^n, i=1, \ldots, m$$ is said to be independent if and only if the following condition on a vector $$\lambda \in {\mathbf{R}}^m$$:  
    :   $$\sum_{i=1}^m \lambda_i x_i = 0 \ \ \ \implies  \lambda = 0.$$

        > i.e. no vector in the set can be expressed as a linear combination of the others.


2. **Subspace:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    :   A subspace of $${\mathbf{R}}^n$$ is a subset that is closed under addition and scalar multiplication. Geometrically, subspaces are "flat" (like a line or plane in 3D) and pass through the origin.  

    * A **Subspace** $$\mathbf{S}$$ can always be represented as the span of a set of vectors $$x_i \in {\mathbf{R}}^n, i=1, \ldots, m$$, that is, as a set of the form:  
    $$\mathbf{S} = \mbox{ span}(x_1, \ldots, x_m) := \left\{ \sum_{i=1}^m \lambda_i x_i ~:~ \lambda \in {\mathbf{R}}^m \right\}.$$


3. **Affine Sets (Cosets | Abstract Algebra):**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13}
    :   An affine set is a translation of a subspace‚Äâ‚Äî‚Äâit is "flat" but does not necessarily pass through 0, as a subspace would. 
        > (Think for example of a line, or a plane, that does not go through the origin.)
    :   An affine set $$\mathbf{A}$$ can always be represented as the translation of the subspace spanned by some vectors:
    :   $$ \mathbf{A} = \left\{ x_0 + \sum_{i=1}^m \lambda_i x_i ~:~ \lambda \in {\mathbf{R}}^m \right\}\ \ \ $$, for some vectors $$x_0, x_1, \ldots, x_m.$$  

    $$\implies \mathbf{A} = x_0 + \mathbf{S}.$$

    * **(Special case)** **lines**: When $$\mathbf{S}$$ is the span of a single non-zero vector, the set $$\mathbf{A}$$ is called a line passing through the point $$x_0$$. Thus, lines have the form
    $$\left\{ x_0 + tu ~:~ t \in \mathbf{R} \right\}$$,  \\
    where $$u$$ determines the direction of the line, and $$x_0$$ is a point through which it passes.

    * <button>Example: Diminsion of Affine Subspaces</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/2/2.1/affine_sub.png){: hidden=""}


4. **Basis:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14}
    :    A **basis** of $${\mathbf{R}}^n$$ is a set of $$n$$ independent vectors. If the vectors $$u_1, \ldots, u_n$$ form a basis, we can express any vector as a linear combination of the $$u_i$$'s:
    :   $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ x = \sum_{i=1}^n \lambda_i u_i, \ \ \ \text{for appropriate numbers } \lambda_1, \ldots, \lambda_n$$.


5. **Dimension:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} 
    :   The number of vectors in the span of the (sub-)space.

***

## Norms and Scalar Products
{: #content2}

1. **Scalar Product:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}
    :   The scalar product (or, inner product, or dot product) between two vectors $$x,y \in \mathbf{R}^n$$ is the scalar denoted $$x^Ty$$, and defined as: 
    :   $$x^Ty = \sum_{i=1}^n x_i y_i.$$ 

    * [**Example.**](http://livebooklabs.com/keeppies/c5a5868ce26b8125/73a4ae787085d554){: value="show" onclick="iframePopA(event)"}
        <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/73a4ae787085d554">` Visit the Book`</a>
        <div markdown="1"> </div>

2. **Norms:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} 
    :   A measure of the "length" of a vector in a given space.
    :   **Theorem.** A function from $$\chi$$ to $$\mathbf{R}$$ is a norm, if:  
        1. $$\|x\| \geq 0, \: \forall x \in \chi$$, and $$\|x\| = 0 \iff x = 0$$.
        2. $$\|x+y\| \leq \|x\| + \|y\|,$$ for any $$x, y \in \chi$$ (triangle inequality).
        3. $$\|\alpha x\| = \|\alpha\| \|x\|$$, for any scalar $$\alpha$$ and any $$x\in \chi$$.

00. **$$l_p$$ Norms:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents200} 
    $$\|x\|_p = \left( \sum_{k=1}^n \|x_k\|^p \right)^{1/p}, \ 1 \leq p < \infty$$


4. **The $$l_1-norm$$:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24}
    :   $$ \|x\|_1 := \sum_{i=1}^n \| x_i \| $$   
    :   Corresponds to the distance travelled on a rectangular grid to go from one point to another.  \\
    ![image](/main_files/conv_opt/2/2.1/2.png){: width="32%"}

3. **The $$l_2-norm$$ (Euclidean Norm):**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} 
    :   $$  \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \| x \|_2 := \sqrt{ \sum_{i=1}^n x_i^2 } = \sqrt{x^Tx} $$.  
    :   Corresponds to the usual notion of distance in two or three dimensions.
    :   > The $$l_2-norm$$ is invariant under orthogonal transformations,     
        > i.e., $$\|x\|_2 = \|Vz\|_2 = \|z\|_2,$$ where $$V$$ is an orthogonal matrix. 
    :   > The set of points with equal l_2-norm is a circle (in 2D), a sphere (in 3D), or a hyper-sphere in higher dimensions.  \\
    ![image](/main_files/conv_opt/2/2.1/1.png){: width="32%"}

5. **The $$l_\infty-norm$$:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25}
    :   $$ \| x \|_\infty := \displaystyle\max_{1 \le i \le n} \| x_i \|$$  
    :   > useful in measuring peak values.  \\
    ![image](/main_files/conv_opt/2/2.1/3.png){: width="32%"}

0. **The Cardinality:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents20}
    :   The **Cardinality** of a vector $$\vec{x}$$ is often called the $$l_0$$ (pseudo) norm and denoted with,  
    :   $$\|\vec{x}\|_0$$.
    :   > Defined as the number of non-zero entries in the vector.


6. **Cauchy-Schwartz inequality:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26}
    :   For any two vectors $$x, y \in \mathbf{R}^n$$, we have  
    :   $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  $$x^Ty \le \|x\|_2 \cdot \|y\|_2$$.
    > The above inequality is an equality if and only if $$x, y$$ are collinear:  
    > : $$ {\displaystyle \max_{x : \: \|x\|_2 \le 1} \: x^Ty = \|y\|_2,}$$ 
    > with optimal $$x$$ given by  
    > $$x^\ast = \dfrac{y}{\|y\|_ 2}, \ $$ if $$y$$ is non-zero.

7. **Angles between vectors:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27} 
    :   When none of the vectors x,y is zero, we can define the corresponding angle as theta such that,
    :    $$\cos\  \theta = \dfrac{x^Ty}{\|x\|_ 2 \|y\|_ 2} .$$ 

    * [**Example Usage. (Document Similarity | Bag of Words)**](http://livebooklabs.com/keeppies/c5a5868ce26b8125/7137876aadf5fb5b){: value="show" onclick="iframePopA(event)"} or
        <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/7137876aadf5fb5b">` Visit the Book`</a>
        <div markdown="1"> </div>

__Notes:__  
* $$L^q$$ for $$q \in (0,1)$$ are no longer __Norms__.  
    * They have __non-convex__ contours; thus, using them makes the optimization much harder  
    * They, however, induce _more_ __sparsity__ than $$L^1$$  
    * $$L^1$$ is the best, _sparse norm_, convex approximation to the $$L^q$$ for $$q \in (0,1)$$  
    ![img](/main_files/conv_opt/2/2.1/16.png){: width="50%"}  

***

## Orthogonality
{: #content3}

1. **Orthogonal Vectors:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}
    :   We say that two vectors $$x, y \in \mathbf{R}^n$$ are orthogonal if $$x^Ty = 0.$$


***

## Projections
{: #content4}

1. **Line:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41}
    :   A line in $$\mathbf{R}^n$$ passing through $$x_0 \in \mathbf{R}^n$$ and with direction $$u \in \mathbf{R}^n$$:
    :   $$\left\{ x_0 + tu ~:~ t \in \mathbf{R} \right\}$$,  

    __Re-Written:__{: style="color: red"}  
    A line in $$\mathbf{R}^n$$ passing through the point $$x_0 \in \mathbf{R}^n$$ and with direction $$\mathbf{u} \in \mathbb{R}^n$$:  
    <p>$$\left\{ x_0 + c \mathbf{u} ~:~ c \in \mathbb{R} \right\}$$</p>  


2. **Projection on a line:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} 
    :   The projection of a given point $$\vec{x}$$ on the line is a vector $$\vec{z}$$ located on the line, that is closest to $$\vec{x}$$ (in Euclidean norm). This corresponds to a simple optimization problem:  
    :   $$\min_t \: \|x - x_0 - tu\|_ 2^2$$. 
    :   > This particular problem is part of a general class of optimization problems known as least-squares.  
    :   > It is also a special case of a Euclidean projection on a general set.  

    __Re-Written:__{: style="color: red"}  
    The projection of a given point $$\mathbf{v}$$ on the line is a vector $$\tilde{\mathbf{v}}$$ located on the line, that is closest (distance-wise) to $$\mathbf{v}$$ (in Euclidean norm). This corresponds to a simple optimization problem:  
    <p>$$\min_c \: \|\mathbf{v} - x_0 - c \mathbf{u}\|_ 2^2$$</p>  

3. **The Projection:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents43}
    :   Assuming that $$\vec{u}$$ is normalized, so that $$\|\vec{u}\|_2 = 1$$, the objecive function of the projection problem reads, after squaring:  
    :   $$\|x - x_0 - tu\|_2^2 = t^2 - 2t u^T(x-x_0) + \|x-x_0\|_2^2 = (t - u^T(x-x_0))^2 + \mbox{constant}.$$
    :   $$\implies \\$$ [the optimal solution to the projection problem is]  
    :   $$ t^\ast = u^T(x-x_0),$$
    :   and the expression for the projected vector is
    :   $$ z^\ast = x_0 + t^\ast u = x_0 + u^T(x-x_0) u.$$
    :   > The scalar product $$u^T(x-x_0)$$ is the component of $$x-x_0$$ along $$\vec{u}$$.
    :   > In the case when u is not normalized, the expression is obtained by replacing $$\vec{u}$$ with its scaled version $$\dfrac{\vec{u}}{\|\vec{u}\|_2}$$.
    :   The General Solution:  
    :   $$\vec{z}^\ast = \vec{x_0} + \dfrac{\vec{u}^T(\vec{x}-\vec{x_0})}{\vec{u}^T\vec{u}} \vec{u} .$$

4. **Interpreting the scalar product:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents44} 
    :   In general, the scalar product, $$u^Tx$$, is simply,  
     **the component of $$x$$** **along** the **normalized direction** $$\dfrac{\vec{u}}{\|\vec{u}\|_2}$$ defined by $$\vec{u}$$.  


5. **Projection:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents45}  
    A __Projection__ is a _linear transformation_ $$P$$ from a vector-space to itself such that the matrix $$P$$ is *__idempotent__*:  
    <p>$$P^2 = P$$</p>  
    It leaves its image unchanged.  

    __Mathematically:__{: style="color: red"}  
    A __Projection__ on a vector space $${\displaystyle V}$$ is a linear operator $${\displaystyle P:V\mapsto V}$$ such that $${\displaystyle P^{2}=P}$$  

    __Properties:__{: style="color: red"}  
    * The __Eigenvalues__ of a _projection matrix_ must be $$0$$ or $$1$$  
        > From the equation $$P^2 = P \iff x^2 = x = x(x-1)$$ has roots $$0, 1$$  

    * $${\displaystyle P}$$ is always a __positive semi-definite__ matrix  
        > Follows from the fact that the _eigenvalues_ are either $$0$$ or $$1$$  
    * The corresponding __eigenspaces__ are (respectively) the __kernel__ and __range__ of the projection  
    * If a projection is _nontrivial_ it has __minimal polynomial__ $${\displaystyle x^{2}-x=x(x-1)}$$, which factors into __distinct roots__, and thus $${\displaystyle P}$$ is *__diagonalizable__*  
    * The product of projections is __not__, in general, a projection, even if they are orthogonal.  
        * If projections __commute__, then their __product is a projection__.  


    __Notes:__{: style="color: red"}  
    * [__The Centering Matrix__](https://en.wikipedia.org/wiki/Centering_matrix): is an example of a projection matrix



6. **Orthogonal Projections:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents46}  
    An __Orthogonal Projection__ is a projection $$P$$ from a vector-space to itself such that the matrix $$P$$ is *__symmetric__*:  
    <p>$$P = P^T$$</p>  

    __Mathematically:__{: style="color: red"}  
    * When $${\displaystyle V}$$ has an inner product and is complete (i.e. when $${\displaystyle V}$$ is a Hilbert space) the concept of __orthogonality__ can be used.  
    Then $${\displaystyle P}$$ is called an orthogonal projection if it satisfies $${\displaystyle \langle Px,y\rangle =\langle x,Py\rangle }$$ for all $${\displaystyle x,y\in V}$$  
    * A projection on a Hilbert space that is not orthogonal is called an __oblique projection__.  
    * A __square__ matrix $${\displaystyle P}$$ is called an orthogonal projection matrix if $${\displaystyle P^{2}=P=P^{\mathrm {T} }}$$  
    * The range $${\displaystyle U}$$ and the null space $${\displaystyle V}$$ are __orthogonal subspaces__:  
        <p>$$\langle x,Py\rangle =\langle Px,Py\rangle =\langle Px,y\rangle$$</p>  
    * An orthogonal projection is a __bounded operator__.  
        By Cauchy Schwartz:  
        <p>$${\displaystyle \|Pv\|^{2}=\langle Pv,Pv\rangle =\langle Pv,v\rangle \leq \|Pv\|\cdot \|v\|} \\ \iff \\ {\displaystyle \|Pv\|\leq \|v\|}$$</p>  


    __Orthogonal Projection onto a Line:__{: style="color: red"}  
    If $$\hat{u}$$ is a __unit vector__ on the line, then the projection is given by the __outer-product__:  
    <p>$$P_{\hat{u}} = \hat{u}\hat{u}^T$$</p>  
    __Orthogonal Projection onto Subspaces:__{: style="color: red"}  
    Generalize the above definition, if $${\displaystyle \hat{u}_{1},\ldots ,\hat{u}_{k}}$$ are an __orthonormal basis__ of the subspace $$U$$, and $$A$$ is the $$n \times k$$ matrix with columns $${\displaystyle \hat{u}_{1},\ldots ,\hat{u}_{k}}$$, then the projection is given by:  
    <p>$$P_A = AA^T$$</p>  
    Equivalently:  
    <p>$$P_{A}=\sum _{i}\langle u_{i},\cdot \rangle u_{i}$$</p>  
    Dropping the __Orthonormality__ condition on the basis, we get:  
    <p>$$P_{A}=A(A^{\mathrm {T} }A)^{-1}A^{\mathrm {T} }$$</p>  
            


***

## Hyperplanes
{: #content5}

1. **Hyperplanes:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents51}
    :   A hyperplane is a set described by a single scalar product equality. Precisely, a hyperplane $$\in \mathbf{R}^n$$ is a set of the form:  
    :   $$\mathbf{H} = \left\{ x ~:~ a^Tx = b \right\},$$ 
    :   where a $$\in \mathbf{R}^n, 
    a \ne 0$$, and $$b \in \mathbf{R}$$ are given. 
    :   > When $$b=0$$, the hyperplane is simply the set of points that are orthogonal to $$a$$.
    :   > when $$b \ne 0$$, the hyperplane is a translation, along direction $$a$$, of that set.
    :   > If $$x_0 \in \mathbf{H}$$, then for any other element $$x \in \mathbf{H}$$, we have  
    :   $$ b = a^Tx_0 = a^Tx.$$ 
    :   Hence, the hyperplane can be characterized as the set of vectors $$x$$ such that $$x-x_0$$ is orthogonal to $$a$$: 
    :   $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{H} = \left\{ x ~:~ a^T(x-x_0)=0 \right\}$$. 

2. **Hyper-Planes as Affine Sets:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents52}
    :   Hyper-planes are **affine sets** of degree $$n-1$$.
    <button>Proof.</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/2/2.1/4.png){: hidden=""}

    :   > Thus, they generalize the usual notion of a plane $$\in \mathbf{R}^3$$. 
    :   > Hyperplanes are very useful because they allows to **separate** the whole **space** in **two** regions.

3. **Geometry of Hyperplanes:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents53}
    :   Geometrically, an hyperplane $$\mathbf{H} =  \left\{ x ~:~ a^Tx = b \right\}$$, with $$\|a\|_2 = 1$$, is a:
    :   * **Translation** of the set of vectors orthogonal to a.
    :   * **The Direction** of the translation is determined by a, and the amount by b.
    :   * $$abs(b)$$ is, Precisely, the *length* of the *closest point* $$x_0$$ on $$\mathbf{H}$$ from the origin.
    :   * **The sign of $$b$$** determines if $$\mathbf{H}$$ is away from the origin along the direction $$a$$ or $$-a$$.
    :   * **The magnitude of $$b$$**, determines the shifting of the hyperplane, as follows: 
            * **Increasing the magnitude:** shifts the hyperplane further away along $$\pm a$$, depending on the sign of $$b$$.
            * **Decreasing the magnitude:** shifts the hyperplane closer along $$\pm a$$, depending on the sign of $$b$$.

    :   > In the image below, the scalar b is positive, as $$x_0$$ and a point to the same direction.  
    ![image](/main_files/conv_opt/2/2.1/5.png){: width="32%"}

***

## Half-Spaces
{: #content6}

1. **Half-Space:**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents61}
    :   A half-space is a subset of $$\mathbf{R}^n$$ defined by a single inequality involving a scalar product. Precisely, a half-space $$\in \mathbf{R}^n$$ is a set of the form:  
    :   $$ \mathbf{H} = \left\{ x ~:~ a^Tx \ge b \right\},$$  
    :   where $$a \in \mathbf{R}^n, a \ne 0,$$ and $$b \in \mathbf{R}$$ are given.

2. **Geometric Interptation:**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents62}
    :   Geometrically, the half-space above is:  
    :   * **The set of points** such that $$a^T(x-x_0) \ge 0$$.
    :   > i.e. The angle between $$x-x_0$$ and $$a$$ is acute $$(\in [-90^\circ, +90^\circ])$$.  
    :   * **$$x_0$$**: is the point *closest* to the *origin* on the hyperplane defined by the equality $$a^Tx = b$$. 
    :   > When $$a$$ is normalized, as in the picture, $$x_0 = ba$$.  
    ![image](/main_files/conv_opt/2/2.1/6.png){: width="32%"}

***

## Linear Functions and Transformations, and Maps
{: #content7}

1. **Linear Functions:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents71}
    :    **Linear functions** are functions which preserve *scaling* and *addition of the input* argument.
    :   > **Formally**,
    :     A function $$f: \mathbf{R}^n \rightarrow \mathbf{R}$$ is linear if and only if $$f$$ preserves scaling and addition of its arguments:  
    :   * for every $$x \in \mathbf{R}^n$$, and $$\alpha \in \mathbf{R}, \ f(\alpha x) = \alpha f(x)$$; and
    :   * for every $$x_1, x_2 \in \mathbf{R}^n, f(x_1+x_2) = f(x_1)+f(x_2)$$.

2. **Affine Functions:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents72}
    :   **Affine functions** are linear functions plus constant functions.
    :   **Formally,**  
    :   A function f is affine if and only if the function $$\tilde{f}: \mathbf{R}^n \rightarrow \mathbf{R}$$ with values $$\tilde{f}(x) = f(x)-f(0)$$ is linear. $$\diamondsuit$$
    :   > **Equivalently**,
    :   A map $$f : \mathbf{R}^n \rightarrow \mathbf{R}^m$$ is affine if and only if the map $$g : \mathbf{R}^n \rightarrow \mathbf{R}^m$$ with values $$g(x) = f(x) - f(0)$$ is linear.


3. **Equivalent Definitions of Linear Functions [Theorem]:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents73}
    :   A map $$f : \mathbf{R}^n \rightarrow \mathbf{R}^m$$ is linear if and only if either one of the following conditions hold:
    :   * $$f$$ preserves scaling and addition of its arguments:
            *  for every $$x \in \mathbf{R}^n$$, and $$\alpha \in \mathbf{R},  f(\alpha x) = \alpha f(x)$$; and
            * for every $$x_1, x_2 \in \mathbf{R}^n, f(x_1+x_2) =  f(x_1)+f(x_2).$$
    :   * $$f$$ vanishes at the origin:
            * $$f(0) = 0$$, and
            * It transforms any line segment $$\in \mathbf{R}^n$$ into another segment $$\in \mathbf{R}^m$$:
            $$\forall \: x, y \in \mathbf{R}^n, \; \forall \: \lambda \in [0,1] ~:~ f(\lambda x + (1-\lambda) y) = \lambda f(x) + (1-\lambda) f(y)$$.  
                * $$f$$ is differentiable, vanishes at the origin, and the matrix of its derivatives is constant.
                * There exist $$A \in \mathbf{R}^{m \times n}$$ such that, $$\ \forall  x \in \mathbf{R}^n ~:~ f(x) = Ax$$. 
    <button>Example</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/2/2.1/7.png){: hidden=""}

4. **Vector Form (and the scalar product):**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents74}  \\
    **Theorem**: *Representation of affine function via the scalar product.*  
    $$\ \ \ \ \ \ \ \ $$    A function $$f: \mathbf{R}^n \rightarrow \mathbf{R}$$ is affine if and only if it can be expressed via a scalar product:  
        $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ \ \ \ \ \ \ \ \ $$  $$f(x) = a^Tx + b$$ ,  
        $$\ \ \ \ \ \ \ \ $$ for some unique pair $$(a,b)$$, with $$a \in \mathbf{R}^{n}$$ and $$b \in \mathbf{R}$$, given by $$a_i = f(e_i)-f(0)$$, with $$e_i$$ $$\ \ \ \ \ \ \ \ \ $$the $$i-th$$ unit vector $$\in \mathbf{R}^n, i=1, \ldots, n,$$ and $$\ b = f(0)$$.  \\
    > The function is linear $$\iff b = 0$$.  

    > The theorem shows that a vector can be seen as a (linear) function from the "input" space $$\mathbf{R}^n$$ to the "output" space $$\mathbf{R}$$.  

    > Both points of view (matrices as simple collections of numbers, or as linear functions) are useful.

0. **Gradient of a Linear Function:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents70} \\
    ![img](/main_files/conv_opt/2/2.1/8.png){: width="60%"}
    

5. **Gradient of an Affine Function:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents75}
    :   The **gradient** of a function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$ at a point $$x$$, denoted $$\nabla f(x)$$, is the vector of first derivatives with respect to $$x_1, \ldots, x_n$$.
    :   > When $$n=1$$ (there is only one input variable), the gradient is simply the derivative.  
    :   An affine function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$, with values $$f(x) = a^Tx+b$$ has the gradient:
    :   $$\nabla f(x) = a$$.  
    :   > i.e. For all Affine Functions, the gradient is the constant vector $$a$$.

6. **Interpreting $$a$$ and $$b$$:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents76}
    :   * The $$b=f(0)$$ is the constant term. For this reason, it is sometimes referred to as the bias, or intercept.  
            > as it is the point where $$f$$ intercepts the vertical axis if we were to plot the graph of the function.
    :   * The terms $$a_j, j=1, \ldots, n,$$ which correspond to the gradient of $$f$$, give the coefficients of influence of $$x_j$$ on $$f$$. 
            > **For example**, if $$a_1 >> a_3$$, then the first component of $$x$$ has much greater influence on the value of $$f(x)$$ than the third.

7. **First-order approximation of non-linear functions:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents77}
    :   * **One-dimensional case**:  
        Consider a function of one variable $$f : \mathbf{R} \rightarrow \mathbf{R}$$, and assume it is differentiable everywhere.  
        Then we can approximate the values function at a point $$x$$ near a point $$x_0$$ as follows:  
    :   $$ f(x) \simeq l(x) := f(x_0) + f'(x_0) (x-x_0) , $$
    :   $$\ \ \ \ \  \ \ \ $$ where $$f'(x)$$ denotes the derivative of $$f$$ at $$x$$.
    :   * **Multi-dimensional:**  
        Let us approximate a differentiable function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$ by a linear function $$l$$, so that $$f$$ and $$l$$ coincide up and including to the first derivatives.  
        The approximate function l must be of the form:  
    :   $$l(x) = a^Tx + b, $$  
    :   $$\ \ \ \ \  \ \ \ $$ where $$a \in \mathbf{R}^n$$ and $$b \in \mathbf{R}$$.  
    :   > The corresponding approximation $$l$$ is called the first-order approximation to $$f$$ at $$x_0$$.  

    :   * Our condition that $$l$$ coincides with $$f$$ up and including to the first derivatives shows that we must have:  
    :   $$  \nabla l(x) = a = \nabla f(x_0), \;\; a^Tx_0 + b = f(x_0), $$  
    :   $$\ \ \ \ \  \ \ \ $$   where $$\nabla f(x_0)$$ is the gradient, of $$f$$ at $$x_0$$. 

8. **First-order Expansion of a function [Theorem]:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents78}
    :   The first-order approximation of a differentiable function $$f$$ at a point $$x_0$$ is of the form:  
    :   $$f(x) \approx l(x) = f(x_0) + \nabla f(x_0)^T (x-x_0)$$   
    :   where $$\nabla f(x_0) \in \mathbf{R}^n$$ is the gradient of $$f$$ at $$x_0$$.
    <button>Example: a linear approximation to a non-linear function.</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/2/2.1/9.png){: hidden="" width="70%"}

***

## Matrices
{: #content8}

0. **Matrix Transpose:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents80}
    :   $$ A_{ij} =  A_{ji}^T, \; \forall i, j \in \mathbf{F}$$  
    * **Properties:**  
        * $$(AB)^T = B^TA^T.$$  

1. **Matrix-vector product:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents81}
    :    $$(Ax)_i = \sum_{j=1}^n A_{ij}x_j , \;\; i=1, \ldots, m. $$
    :    Where the Matrix is $$\in {\mathbf{R}}^{m \times n}$$ and the vector is $$ \in {\mathbf{R}}^m$$.
    :    **Interpretations:**  
    :       1. **A _linear combination_ of the _columns_ of $$A$$:**    
            $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \   Ax = \left( \begin{array}{c} a_1^Tx  \ldots  a_m^Tx \end{array} \right)^T$$ .   
            where the columns of $$A$$ are given by the vectors $$a_i, i=1, \ldots, n$$, so that $$A = (a_1 , \ldots, a_n)$$.

            2. **_Scalar Products_ of _Rows_ of $$A$$ with $$x$$:**    
            $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  Ax = \sum_{i=1}^n x_i a_i$$ .   
            where the rows of $$A$$ are given by the vectors $$a_i^T, i=1, \ldots, m$$:
            $$A = \left( \begin{array}{c} a_1^T  \ldots  a_m^T \end{array} \right)^T$$.

    <button>Example: Network Flows</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/2/2.1/10_11.png){: hidden=""}

2. **Left Product:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents82}
    :    If $$z \in \mathbf{R}^m$$, then the notation $$z^TA$$ is the row vector of size $$n$$ equal to the transpose of the column vector $$A^Tz \in \mathbf{R}^n$$:  
    :   $$ (z^TA)_j = \sum_{i=1}^m A_{ij}z_i , \;\; j=1, \ldots, n. $$
    <button>Example: Representing the constraint that the columns of a matrix sum to zero.</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/2/2.1/12.png){: hidden=""}


3. **Matrix-matrix product:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents83}
    :   $$  (AB)_{ij} = \sum_{k=1}^n A_{ik} B_{kj}$$.  
    :   where $$A \in \mathbf{R}^{m \times n}$$ and $$B \in \mathbf{R}^{n \times p}$$, and the notation $$AB$$ denotes the $$m \times p$$ matrix given above.
    :    **Interpretations:**  
    :       1. **_Transforming_ the _columns_ of $$B$$ into $$Ab_i$$:**    
            $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \    AB = A \left( \begin{array}{ccc} b_1 & \ldots & b_n \end{array} \right) =  \left( \begin{array}{ccc} Ab_1 & \ldots & Ab_n \end{array} \right)$$ .   
            where the columns of $$B$$ are given by the vectors $$b_i, i=1, \ldots, n$$, so that $$B = (b_1 , \ldots, b_n)$$.  
            2. **_Transforming_ the _Rows_ of $$A$$ into $$a_i^TB$$:**      
            $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  AB = \left(\begin{array}{c} a_1^T \\ \vdots \\ a_m^T \end{array}\right) B = \left(\begin{array}{c} a_1^TB \\ \vdots \\ a_m^TB \end{array}\right)$$.   
            where the rows of $$A$$ are given by the vectors $$a_i^T, i=1, \ldots, m$$:
            $$A = \left( \begin{array}{c} a_1^T  \ldots  a_m^T \end{array} \right)^T$$.

4. **Block Matrix Products:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents84}  \\
    ![img](/main_files/conv_opt/2/2.1/block.png){: width="100%"}

5. **Outer Products:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents85}
    ![img](/main_files/conv_opt/2/2.1/outer_products.png){: width="100%"}

6. **Trace:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents86}
    :   The trace of a square $$n \times n$$ matrix $$A$$, denoted by $$\mathbf{Tr} A$$, is the sum of its diagonal elements:  
    :   $$ \mathbf{Tr} A = \sum_{i=1}^n A_{ii}$$.  
    * **Properties:**  
        * $$\mathbf{Tr} A = \mathbf{Tr} A^T$$.  
        * $$\mathbf{Tr} (AB) = \mathbf{Tr} (BA)$$.
        * $$\mathbf{Tr}(XYZ) = \mathbf{Tr}(ZXY) = \mathbf{Tr}(YZX)$$.
        * $${\displaystyle \operatorname{tr} (A+B) = \operatorname{tr} (A)+\operatorname{tr} (B)}$$.
        * $${\displaystyle \operatorname{tr} (cA) = c\operatorname{tr} (A)}$$.
        * $${\displaystyle \operatorname{tr} \left(X^{\mathrm {T} }Y\right)=\operatorname{tr} \left(XY^{\mathrm {T} }\right)=\operatorname{tr} \left(Y^{\mathrm {T} }X\right)=\operatorname{tr} \left(YX^{\mathrm {T} }\right)=\sum _{i,j}X_{ij}Y_{ij}}$$.
        * $${\displaystyle \operatorname{tr} \left(X^{\mathrm {T} }Y\right)=\sum _{ij}(X\circ Y)_{ij}}\ \ \ \ $$ (The _Hadamard_ product).
        * Arbitrary permutations of the product of matrices is not allowed. Only, **cyclic permutations** are.
            > However, if products of three symmetric matrices are considered, any permutation is allowed.
        * The trace of an idempotent matrix $$A$$, is the dimension of A.
        * The trace of a nilpotent matrix is zero.
        * If $$f(x) = (x ‚àí \lambda_1)^{d_1} \cdots (x ‚àí \lambda_k)^{d_k}$$ is the characteristic polynomial of a matrix $$A$$, then $${\displaystyle \operatorname{tr} (A)=d_{1}\lambda_{1} + \cdots + d_{k} \lambda_{k}}$$.
        * When both $$A$$ and $$B$$ are $$n \times n$$, the trace of the (ring-theoretic) commutator of $$A$$ and $$B$$ vanishes: $$\mathbf{tr}([A, B]) = 0$$; one can state this as "the trace is a map of Lie algebras $${\displaystyle \mathbf{GL_{n}} \to k}$$ from operators to scalars", as the commutator of scalars is trivial (it is an abelian Lie algebra).
        * The trace of a projection matrix is the dimension of the target space.
            $${\displaystyle 
            P_{X} = X\left(X^{\mathrm {T} }X\right)^{-1}X^{\mathrm {T} } \\
            \Rightarrow \\
            \operatorname {tr} \left(P_{X}\right) = \operatorname {rank} \left(X\right)}$$



7. **Scalar Product:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents87}
    :   $$\langle A, B \rangle := \mathbf{Tr}(A^TB) = \displaystyle\sum_{i=1}^m\sum_{j=1}^n A_{ij}B_{ij}.$$  
    :   > The above definition is **Symmetric**:  
    :   $$\implies \langle A,B \rangle =  \mathbf{Tr} (A^TB) = \mathbf{Tr} (A^TB)^T =  \mathbf{Tr} (B^TA) = \langle B,A \rangle .$$  
    :   > We can **interpret** the matrix scalar product as the _vector scalar product between two long vectors_ of length $$mn$$ each, obtained by stacking all the columns of $$A, B$$ on top of each other.

8. **Special Matrices:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents88} \\
    * [**Diagonal matrices:**](/work_files/research/la/sym_mat) are square matrices $$A$$ with $$A_{ij} = 0$$ when $$i \ne j$$.  
    * **Symmetric matrices:** are square matrices that satisfy $$A_{ij} = A_{ji} $$for every pair $$(i,j)$$.
    * **Triangular matrices:** are square matrices that satisfy $$A_{ij} = A_{ji} $$for every pair $$(i,j)$$.    

***

## Matrix Norms
{: #content9}

1. **Norm:**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents91}
    :   A matrix norm is a functional  
    :   $${\displaystyle \|\cdot \|:K^{m\times n}\to \mathbf{R} }$$  
    :   on the vector space $${\displaystyle K^{m\times n}}, $$ that must satisfy the following properties:
    :   For all scalars $${\displaystyle \alpha }  \in {\displaystyle K} $$ and for all matrices $${\displaystyle A} $$ and $${\displaystyle B}  \in {\displaystyle K^{m\times n}}$$,  
    :   * $$\|\alpha A\|=|\alpha| \|A\|$$ 
        > i.e. being absolutely homogeneous
    :   * $${\displaystyle \|A+B\|\leq \|A\|+\|B\|}$$
        > i.e. being sub-additive or satisfying the triangle inequality 
    :   * $${\displaystyle \|A\|\geq 0} $$
        > i.e. being positive-valued 
    :   * $${\displaystyle \|A\|=0} \iff {\displaystyle A=0_{m,n}}$$
        > i.e. being definite
    :   * $${\displaystyle \|AB\|\leq \|A\|\|B\|}$$ for all _square_ matrices $${\displaystyle A}$$ and $${\displaystyle B} \in {\displaystyle K^{n\times n}}.$$
        > **Submultiplicativity**.
        >   > Not satisfied by all Norms.

2. **$$l_{p,q}$$ norms:**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents92}
    :   $${\displaystyle \Vert A\Vert _{p,q}=\left(\sum _{j=1}^{n}\left(\sum _{i=1}^{m}|a_{ij}|^{p}\right)^{q/p}\right)^{1/q}}$$

    * $$l_{2,1}$$:  
    :   $${\displaystyle \Vert A\Vert _{2,1}= \sum _{j=1}^{n}\left(\sum _{i=1}^{m}|a_{ij}|^{2}\right)^{1/2}}$$

3. **$$l_{2,2}$$ (Frobenius norm):**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents93}
    :   $${\displaystyle \|A\|_{\rm {F}}={\sqrt {\sum _{i=1}^{m}\sum _{j=1}^{n}|a_{ij}|^{2}}}={\sqrt {\operatorname {trace} (A^{\dagger }A)}}={\sqrt {\sum _{i=1}^{\min\{m,n\}}\sigma _{i}^{2}(A)}}}, $$  
    :   where $${\displaystyle A^{\dagger }}$$ denotes the conjugate transpose of $${\displaystyle A}$$, and $${\displaystyle \sigma _{i}(A)}$$ are the singular values of $${\displaystyle A}$$.

    * **Properties:**  
        1. Submultiplicative.

        2. Invariant under rotations.  
            > i.e. $${\displaystyle \|A\|_{\rm {F}}^{2}=\|AR\|_{\rm {F}}^{2}=\|RA\|_{\rm {F}}^{2}} {\displaystyle \|A\|_{\rm {F}}^{2}=\|AR\|_{\rm {F}}^{2}=\|RA\|_{\rm {F}}^{2}}$$ for any rotation matrix $$R$$.

        3. Invariant under a unitary transformation for complex matrices.

        4. $${\displaystyle \|A^{\rm {T}}A\|_{\rm {F}}=\|AA^{\rm {T}}\|_{\rm {F}}\leq \|A\|_{\rm {F}}^{2}}$$.

        5. $${\displaystyle \|A+B\|_{\rm {F}}^{2}=\|A\|_{\rm {F}}^{2}+\|B\|_{\rm {F}}^{2}+2\langle A,B\rangle _{\mathrm {F} }}$$.


4. **$$l_{\infty,\infty}$$ (Max Norm):**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents94}
    :   $$ \|A\|_{\max} = \max_{ij} |a_{ij}|.$$

    * **Properties:**  
        1. **NOT** Submultiplicative.

5. **The Spectral Norm:**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents95}
    :   $${\displaystyle \|A\|_{2}={\sqrt {\lambda _{\max }(A^{^{*}}A)}}=\sigma _{\max }(A)} = {\displaystyle \max_{\|x\|_2!=0}(\|Ax\|_2)/(\|x\|_2)}.$$  
    > The spectral norm of a matrix $${\displaystyle A} $$ is the largest singular value of $${\displaystyle A}$$. 
    > i.e. the square root of the largest eigenvalue of the positive-semidefinite matrix $${\displaystyle A^{*}A}.$$

    * **The Spectral Radius of $$A \ $$  [denoted $$\rho(A)$$]:**
    :   $$ \lim_{r\rightarrow\infty}\|A^r\|^{1/r}=\rho(A).$$

    * **Properties:**  
        1. Submultiplicative.

        2. Satisfies, $${\displaystyle \|A^{r}\|^{1/r}\geq \rho (A),}$$, where $$\rho(A)$$ is **the spectral radius** of $$A$$.

        3. It is an "_induced vector-norm_".



8. **Equivalence of Norms:**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents98} \\
    <button>CLICK TO VIEW</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/2/2.1/13.png){: hidden=""}

8. **Applications:**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents98} \\
    1. **RMS Gain:** Frobenius Norm.

    2. **Peak Gain:** Spectral Norm.

    3. **Distance between Matrices:** Frobenius Norm.
        <button>Click to View</button>{: .showText value="show"
         onclick="showTextPopHide(event);"}
        ![img](/main_files/conv_opt/2/2.1/14.png){: hidden=""}

    4. **Direction of Maximal Variance:** Spectral Norm.
        <button>Click to View</button>{: .showText value="show"
         onclick="showTextPopHide(event);"}
        ![img](/main_files/conv_opt/2/2.1/15.png){: hidden=""}






## NOTES

* __Distance between 2 vectors (from $$y$$ to $$x$$)__:  
    $$d = \|x-y\|_2^2$$ 

***
***

TITLE: 2.1 <br /> Basics and Definitions
LINK: research/optimization/2/2.1.study.md


## Definitions
{: #content1}

6. **Span:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16}  
    <button>Show</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    _**Span:** of a set of vectors is the set of all linear combinations of the vectors; $$\text{span}(v_1,\ldots,v_d) = \sum_{i=0}^d \lambda_iv_i : \lambda \in \mathbb{R}^d$$_{: hidden=""}

1. **Linear Independence:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}



2. **Subspace:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    * __Definition:__  
    * __Geometrical Interpretation:__  
    * __Mathematical Representation__:  
            

3. **Affine Sets and Subspaces (Cosets - Abstract Algebra):**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13}  
    * __Definition:__  
    * __Geometrical Interpretation:__  
    * __Mathematical Representation__:  
    * __Special Case of a single basis vector__:  
    * __Find the Affine Subspace Corresponding to the following set__:  
        The set $$\boldsymbol{L}$$ in $$\mathbb{R}^3$$ defined by:  
        $$x_{1}-13 x_{2}+4 x_{3}=2, \quad 3 x_{2}-x_{3}=9$$  
        $$5x_{1}-8x_{2}+17 x_{3}=2, \quad 6 x_{2}-2x_{3}=13$$  

    * __Mathematical Representation of a line__:  


4. **Basis:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14}


5. **Dimension:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} 

***

## Norms and Scalar Products
{: #content2}

1. **Scalar/Inner/Dot Product:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}  


2. **Norms:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} 
    * __Definition + Theorem (properties)__:  
            

00. **$$l_p$$ Norms:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents200} 


4. **The $$l_1-norm$$:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24}  
    * __(Geometrically) Corresponds to__:  



3. **The $$l_2-norm$$ (Euclidean Norm):**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23}  
    * __(Geometrically) Corresponds to__:  
    * __Properties__:  

5. **The $$l_\infty-norm$$:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25}  
    * __(Geometrically) Corresponds to__:  
    * __Application__:  

            

0. **The Cardinality:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents20}


6. **Cauchy-Schwartz inequality:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26}  

7. **Angles between vectors:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27} 


***

## Orthogonality
{: #content3}

1. **Orthogonal Vectors:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}

2. **Orthogonal Matrix:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32}  


***

## Projections
{: #content4}

1. **Line:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41}
    * __Definition__:  
    * __Mathematical Representation__:  
            

2. **Projection on a line:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42}  
    * __Set up Equation__:  
            

3. **The Projection:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents43}  
    * __Solve Equation__:  


4. **Interpreting the scalar product:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents44}  
            

***

## Hyperplanes
{: #content5}

1. **Hyperplanes:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents51}  
    * __Two definitions__:  

    

2. **Hyper-Planes as Affine Sets:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents52}  
    * __How are they useful?__:  


3. **Geometry of Hyperplanes:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents53}
    

***

## Half-Spaces
{: #content6}

1. **Half-Space:**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents61}

2. **Geometric Interptation:**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents62}

## Linear Functions and Transformations, and Maps
{: #content7}

1. **Linear Functions:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents71}

2. **Affine Functions:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents72}


3. **Equivalent Definitions of Linear Functions [Theorem]:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents73}
    


4. **Vector Form (and the scalar product):**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents74}  
    

0. **Gradient of a Linear Function:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents70} 
    

5. **Gradient of an Affine Function:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents75}

6. **Interpreting $$a$$ and $$b$$:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents76}

7. **First-order approximation of non-linear functions:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents77}
        


8. **First-order Expansion of a function [Theorem]:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents78}

***

## Matrices
{: #content8}

0. **Matrix Transpose:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents80}


1. **Matrix-vector product:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents81}
   
            

2. **Left Product:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents82}


3. **Matrix-matrix product:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents83}
   
           

4. **Block Matrix Products:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents84}  

5. **Outer Products:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents85}

6. **Trace:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents86}


7. **Scalar Product:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents87}

8. **Special Matrices:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents88} 

***

## Matrix Norms
{: #content9}

1. **Norm:**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents91}

2. **$$l_{p,q}$$ norms:**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents92}


3. **$$l_{2,2}$$ (Frobenius norm):**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents93}


4. **$$l_{\infty,\infty}$$ (Max Norm):**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents94}


5. **The Spectral Norm:**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents95




8. **Equivalence of Norms:**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents98} 

8. **Applications:**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents98} 


***
***

TITLE: PCA <br /> Principle Compnent Analysis
LINK: research/optimization/2/pca.md


## PCA
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   It is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.

2. **Goal?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    :   Given points $$ \in \mathbf{R}^d$$, find k-directions that capture most of the variation.

3. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13}
    :   1. Find a small basis for representing variations in complex things.
            > e.g. faces, genes.
        2. Reducing the number of dimensions makes some computations cheaper.
        3. Remove irrelevant dimensions to reduce overfitting in learnging algorithms.
            > Like "_subset selection_" but the features are __not__ _axis aligned_.  
            > They are linear combinations of input features.

4. **Finding Principle Components:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    * Let '$$X$$' be an $$(n \times d)$$ design matrix, centered, with mean $$\hat{x} = 0$$.
    * Let '$$w$$' be a unit vector.
    * The _Orthogonal Projection_ of the point '$$x$$' onto '$$w$$' is $$\tilde{x} = (x.w)w$$.
        > Or $$\tilde{x} = \dfrac{x.w}{\|w\|_2^2}w$$, if $$w$$ is not a unit vector.
    * Let '$$X^TX$$' be the _sample covariance matrix_,  
        $$0 \leq \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_d$$ be its eigenvalues  and let $$v_1, v_2, \cdots, v_d$$ be the corresponding _Orthogonal Unit Eigen-vectors_.
    * Given _Orthonormal directions (vectors)_ $$v_1, v_2, \ldots, v_k$$, we can write:   

        $$\tilde{x} = \sum_{i=1}^k (x.v_i)v_i.$$  

    > **The Principle Components:** are precisely the eigenvectors of the data's covariance matrix. 

5. **Total Variance and Error Measurement:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} 
    :    * **The Total Variance** of the data can be expressed as the sum of all the eigenvalues:
    :   $$
        \mathbf{Tr} \Sigma = \mathbf{Tr} (U \Lambda U^T) = \mathbf{Tr} (U^T U \Lambda) = \mathbf{Tr} \Lambda = \lambda_1 + \ldots + \lambda_n. 
        $$
    :    * **The Total Variance** of the **_Projected_** data is:
    :   $$
         \mathbf{Tr} (P \Sigma P^T ) = \lambda_1 + \lambda_2 + \cdots + \lambda_k. 
        $$
    :    * **The Error in the Projection** could be measured with respect to variance.
            * We define the **ratio of variance** "explained" by the projected data as:
    :   $$
        \dfrac{\lambda_1 + \ldots + \lambda_k}{\lambda_1 + \ldots + \lambda_n}. 
        $$
    :   > If the ratio is _high_, we can say that much of the variation in the data can be observed on the projected plane.

***

## Derivation 1. Fitting Gaussians to Data with MLE
{: #content2}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}
    1. Fit a Gaussian to data with MLE
    2. Choose k Gaussian axes of greatest variance.
    > Notice: MLE estimates a _covariance matrix_; $$\hat{\Sigma} = \dfrac{1}{n}X^TX$$.

2. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    1. Center $$X$$
    2. Normalize $$X$$.
        > Optional. Should only be done if the units of measurement of the features differ.
    3. Compute the unit Eigen-values and Eigen-vectors of $$X^TX$$
    4. Choose '$$k$$' based on the Eigenvalue sizes
        > Optional. Top to bottom.
    5. For the best k-dim subspace, pick Eigenvectors $$v_{d-k+1}, \cdots, v_d$$.
    6. Compute the coordinates '$$x.v_i$$' of the trainning/test data in PC-Space.

***

## Derivation 2. Maximizing Variance
{: #content3}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    1. Find a direction '$$w$$' that maximizes the variance of the projected data.
    2. Maximize the variance

2. **Derivation:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32}
    :   $$\max_{w : \|w\|_2=1} \: Var(\left\{\tilde{x_1}, \tilde{x_2}, \cdots, \tilde{x_n} \right\})$$
    :   $$
        \begin{align}
        & \ = \max_{w : \|w\|_2=1}  \dfrac{1}{n} \sum_{i=1}{n}(x_i.\dfrac{w}{\|w\|})^2 \\
        & \ = \max_{w : \|w\|_2=1}  \dfrac{1}{n} \dfrac{\|xw\|^2}{\|w\|^2} \\
        & \ = \max_{w : \|w\|_2=1}  \dfrac{1}{n} \dfrac{w^TX^TXw}{w^Tw}
        \end{align}
        $$
    :   Where $$\dfrac{1}{n}\dfrac{w^TX^TXw}{w^Tw}$$ is the **_Rayleigh Quotient_**.
    :   For any Eigen-vector $$v_i$$, the _Rayleigh Quotient_ is $$ = \lambda_i$$.
    :   $$\implies$$ the vector $$v_d$$ with the largest $$\lambda_d$$, achieves the maximum variance: $$\dfrac{\lambda_d}{n}.$$
    :   Thus, the maximum of the _Rayleigh Quotient_ is achieved at the Eigen-vector that has the highest correpsonding Eigen-value.
    :   We find subsequent vectors by finding the next biggest $$\lambda_i$$ and choosing its corresponding Eigen-vector.

3. **Another Derivation from Statistics:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33}
    :   The data matrix has points $$x_i$$; its component along a proposed axis $$u$$ is $$(x ¬∑ u)$$.
    :   The variance of this is $$E(x ¬∑ u ‚àí E(x ¬∑ u))^2$$
    :   and the optimization problem is
    :   $$
        \begin{align}
        \max_{x : \|x\|_2=1} \: E(x ¬∑ u ‚àí E(x ¬∑ u))^2 & \\
        & \ = \max_{u : \|u\|_2=1} \:  E[(u \cdot (x ‚àí Ex))^2] \\
        & \ = \max_{u : \|u\|_2=1} \:  uE[(x ‚àí Ex) \cdot (x ‚àí Ex)^T]u \\
        & \ = \max_{u : \|u\|_2=1} \:  u^T \Sigma u
        \end{align}
        $$
    :   where the matrix $${\displaystyle \Sigma \:= \dfrac{1}{n} \sum_{j=1}^n (x_j-\hat{x})(x_j-\hat{x})^T}.$$
    :   Since $$\Sigma$$ is symmetric, the $$u$$ that gives the maximum value to $$u^T\Sigma u$$ is the eigenvector of $$\Sigma$$ with the largest eigenvalue.
    :   The second and subsequent principal component axes are the other eigenvectors sorted by eigenvalue.

***

## Derivation 3. Minimize Projection Error
{: #content4}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} \\
    1. Find direction '$$w$$' that minimizes the _Projection Error_.

2. **Derivation:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42}
    :   $$
        \begin{align}
        \min_{\tilde{x} : \|\tilde{x}\|_2 = 1} \; \sum_{i=1}^n \|x_i - \tilde{x_i}\|^2 & \\
        & \ = \min_{w : \|w\|_2 = 1} \; \sum_{i=1}^n \|x_i -\dfrac{x_i \cdot w}{\|w\|_2^2}w\|^2 \\
        & \ = \min_{w : \|w\|_2 = 1} \; \sum_{i=1}^n \left[\|x_i\|^2 - (x_i \cdot \dfrac{w}{\|w\|_2})^2\right] \\
        & \ = \min_{w : \|w\|_2 = 1} \; c - n*\sum_{i=1}^n(x_i \cdot \dfrac{w}{\|w\|_2})^2 \\
        & \ = \min_{w : \|w\|_2 = 1} \; c - n*Var(\left\{\tilde{x_1}, \tilde{x_2}, \cdots, \tilde{x_n} \right\}) \\
        & \ = \max_{w : \|w\|_2 = 1} \; Var(\left\{\tilde{x_1}, \tilde{x_2}, \cdots, \tilde{x_n} \right\})
        \end{align}
        $$
    :   Thus, minimizing projection error is equivalent to maximizing variance.


***
***

TITLE: 2.4 <br /> Singular-Stuff(Values)
LINK: research/optimization/2/2.4.md


[SVD and PCA Blog](https://medium.com/@jonathan_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491)   


## The Singular Value Decomposition
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   Recall from here that any matrix $$A \in \mathbf{R}^{m \times n}$$ with rank one can be written as   $$A = \sigma u v^T$$,  where $$u \in \mathbf{R}^m, v \in \mathbf{R}^n,$$ and $$\sigma >0.$$
    :   It turns out that a similar result holds for matrices of arbitrary rank $$r$$.  :   That is, we can express any matrix $$A \in \mathbf{R}^{m \times n}$$ as sum of rank-one matrices:
    :   $$
        A = \sum_{i=1}^r \sigma_i u_i v_i^T,  
        $$
    :   where $$u_1, \ldots, u_r$$ are mutually orthogonal, $$v_1, \ldots, v_r$$ are also mutually orthogonal, and the $$\sigma_i$$‚Äôs are positive numbers called the singular values of $$A$$.

2. **The SVD Theorem:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    :   An arbitrary matrix $$A \in \mathbf{R}^{m \times n}$$ admits a decomposition of the form:
    :   $$
        A = \sum_{i=1}^r \sigma_i u_i v_i^T = U \tilde{ {S}} V^T, \;\; \tilde{ {S}} := \left( \begin{array}{cc}  {S} & 0 \\ 0 & 0 \end{array} \right) ,  
        $$
    :   where $$U \in \mathbf{R}^{m \times m}, V \in \mathbf{R}^{n \times n}$$ are both orthogonal matrices, and the matrix $$S$$ is diagonal: 
    :   $$S = \mathbf{diag}(\sigma_1 , \ldots, \sigma_r),   $$
    :   where,  
        * The positive numbers $$\sigma_1 \ge \ldots \ge \sigma_r > 0$$ are unique, and are called the **_singular values_** of A.  
        * The number $$r \le min(m,n)$$ is equal to the rank of $$A$$.  
        * The triplet $$(U, \tilde{ {S}}, V)$$ is called a **_singular value decomposition_** (SVD) of $$A$$.  
        * The first $$r$$ columns of $$U: u_i, i=1, \ldots, r$$ (resp. $$V: v_i,  i=1, \ldots, r)$$ are called left (resp. right) singular vectors of $$A$$, and satisfy:  
    :   $$
        Av_i = \sigma_i u_i, \;\;\;\; A^Tu_i = \sigma_i v_i, \;\;\;\; i=1,...,r.
        $$

    :   <button>Proof.</button>{: .showText value="show"
        onclick="showTextPopHide(event);"}
        ![img](/main_files/conv_opt/2/2.4/pf0.png){: hidden="" width="100%"}  

    __Notes:__{: style="color: red"}  
    * $$\begin{array}{l}{U^{T} U=I} \\ {V^{T} V=I}\end{array}$$ are orthogonal matrices with orthonormal eigenvector basis  
    * If $$\boldsymbol{v}$$ is eigenvector of $$X^TX$$ then $$X\boldsymbol{v}$$ is eigenvector of $$XX^T$$  

    <br>

3. **Computing the SVD:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13}
    :   To find the SVD of a matrix $$A$$, we solve the following equation:
    :   $$
        \begin{align}
        & (1)\  A^TA = V\Lambda^T\Lambda V^T \\
        & (2)\  AV\  = U \Lambda
        \end{align}
        $$

4. **Complexity of the SVD:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    1. **Normal Matrices:** the complexity grows as $$\mathcal{O}(nm \: min(n,m))$$. 
    2. **Sparse Matrices:** good approximations can be calculated very efficiently.

5. **Geometric Interpretation:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} 
    :   The theorem allows to decompose the action of A on a given input vector as a sequence of **three** _elementary transformations_.
        1. First, we form $$\tilde{x} := V^Tx \in \mathbf{R}^n$$.
            > $$V$$ orthogonal $$\implies \tilde{x}$$ is a rotated version of $$x$$, which still lies in the input space.
        2. Then we act on the rotated vector $$\tilde{x}$$ by scaling its elements
            > The first $$k$$ elements of $$\tilde{x}$$ are scaled by the singular values $$\sigma_1, \ldots, \sigma_r$$; the remaining $$n-r$$ elements are set to zero.  
            > This step results in a new vector $$\tilde{y}$$ which now belongs to the output space $$\mathbf{R}^m$$.
        3. Finally, we rotate the vector $$\tilde{y}$$ by the orthogonal matrix $$U$$, which results in $$y = U\tilde{y} = Ax$$.   
        > Notice also: $$\tilde{x} : = V^Tx, \ x = V\tilde{x}.$$
    :   **Summary:**  
        1. A rotation in the input space
        2. A scaling that goes from the input space to the output space
        3. A rotation in the output space. 
        > In contrast with symmetric matrices, input and output directions are different.
    :   <button>Example: A $$4 \times 4$$ Matrix</button>{: .showText value="show"
        onclick="showTextPopHide(event);"}
        ![img](/main_files/conv_opt/2/2.4/pf1.png){: hidden="" width="100%"}

6. **Link with the Spectral Theorem:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16}
    :   If $$A$$ admits an SVD, then the matrices $$AA^T$$ and $$A^TA$$ has the following SEDs:
    :   $$
        AA^T = U \Lambda_m U^T, \;\; A^TA = V \Lambda_n V^T,  
        $$
    :   where $$\Lambda_m := \tilde{ {S}}\tilde{ {S}}^T = \mathbf{ diag}(\sigma_1^2, \ldots, \sigma_r^2, 0, \ldots, 0)$$ is $$m \times m$$ (so it has $$m-r$$ trailing zeros),   
        and $$\Lambda_n := \tilde{ {S}}^T\tilde{ {S}} = \mathbf{ diag}(\sigma_1^2, \ldots, \sigma_r^2, 0, \ldots, 0)$$ is $$n \times n$$ (so it has $$n-r$$ trailing zeros). 
    :   > The eigenvalues of $$AA^T$$ and $$A^TA$$ are the same, and equal to the squared singular values of $$A$$.  
        > The corresponding eigenvectors are the left and right singular vectors of $$A$$.

***

## Matrix Properties via SVD
{: #content2}

1. **Nullspace:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}
    :   The SVD allows to **compute an orthonormal basis** for the nullspace of a matrix.

2. **Theorem: Nullspace via SVD:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22}
    :   The nullspace of a matrix A with SVD
    :   $$
        A = U \tilde{S} V^T, \;\; \tilde{S} := \left( \begin{array}{cc}S & 0 \\ 0 & 0 \end{array} \right) , \;\; S = \mathbf{diag}(\sigma_1 , \ldots, \sigma_r),  
        $$
    :   where $$U \in \mathbf{R}^{m \times m}, V \in \mathbf{R}^{n \times n}$$ are both orthogonal matrices, admits the last $$n-r$$ columns of $$V$$ as an orthonormal basis.


3. **Full Column-Rank Matrices:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23}
    :   ne-to-one (or, full column rank) matrices are the matrices with nullspace reduced to {0}. If the dimension of the nullspace is zero, then we must have n=r. Thus, full column rank matrices are ones with SVD of the form
    :   $$
        A = U \left( \begin{array}{c}  {S} \\ 0 \end{array} \right) V^T. 
        $$

4. **Theorem: Range and Rank via SVD:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24}
    :   The range of a matrix $$A$$ with SVD   
    :   $$
        A = U \tilde{ {S}} V^T, \;\; \tilde{ {S}} = \mathbf{diag}(\sigma_1, \ldots, \sigma_r, 0, \ldots, 0)
        $$  
    :   where $$U \in \mathbf{R}^{m \times m}, V \in \mathbf{R}^{n \times n}$$ are both orthogonal matrices, admits the first $$r$$ columns of $$U$$ as an orthonormal basis.

5. **Full Row Rank Matrices.:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25}
    :   An onto (or, full row rank) matrix has a range $$r=m$$.  
        These matrices are characterized by an SVD of the form  
    :   $$
        A = U \left( \begin{array}{cc}  {S} & 0 \end{array} \right) V^T.
        $$ 

6. **Fundamental theorem of linear algebra:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26}
    :   Let $$A \in \mathbf{R}^{m \times n}$$. The sets $$\mathbf{N} (A)$$ and $$\mathbf{R} (A^T)$$ form an orthogonal decomposition of $$\mathbf{R}^n$$, in the sense that any vector  $$x \in \mathbf{R}^n$$ can be written as   
    :   $$
        x = y + z, \;\; y \in \mathbf{N} (A), \;\; z \in \mathbf{R} (A^T), \;\; y^Tz = 0.
        $$
    :   In particular, we obtain that the condition on a vector $$x$$ to be orthogonal to any vector in the nullspace implies that it must be in the range:
    :   $$
        x^Ty = 0 \mbox{ whenever } Ay = 0 \Longleftrightarrow \exists \: \lambda \in \mathbf{R}^m ~:~ x = A^T\lambda.
        $$ 
    :   <button>Proof.</button>{: .showText value="show"
        onclick="showTextPopHide(event);"}
        ![img](/main_files/conv_opt/2/2.4/pf0.png){: hidden="" width="100%"}

7. **Matrix Norms:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27}
    :   Matrix norms, which are useful to measure the size of a matrix, can be interpreted in terms of input-output properties of the corresponding linear map; for example, _the Frobenius norm measure the average response to unit vectors_, while _the largest singular (LSV) norm measures the peak gain_.  
        These two norms can be easily read from the SVD.

    :   * **Frobenius Norm:** 
    :   $$
        \|A\|_F  = \sqrt{\mathbf{Tr} A^TA}
        $$
    :   $$\:\:\:\:\:\:\:\:\:$$    Using the SVD $$(U, \tilde{ {S}}, V)$$ of $$A$$, we obtain
    :   $$
        \|A\|_F^2 = \mathbf{Tr} (V \tilde{ {S}}^T \tilde{ {S}} V^T) = \mathbf{Tr} (V^TV \tilde{ {S}}^T \tilde{ {S}}) = \mathbf{Tr} (\tilde{ {S}}^T \tilde{ {S}}) = \sum_{i=1}^r \sigma_i^2.  
        $$  

        > Hence the squared Frobenius norm is nothing else than the sum of the squares of the singular values.
    :   * **Largest Singular Value Norm.:** measures a matrix size based on asking the maximum ratio of the norm of the output to the norm of the input. When the norm used is the Euclidean norm, the corresponding quantity
    :   $$
        \|A\|_{\rm LSV} := \max_x : \|Ax\|_2 ~:~ \|x\|_2 \le 1 = \max_{x \:\:\: \|x\|_2 \le 1} : \|Ax\|_2 = \sigma_1(A),
        $$
    :   $$\:\:\:\:\:\:\:\:\:$$ where $$\sigma_1(A)$$ is the largest singular value of $$A$$, is called the largest singular value (LSV) norm.  
    > Any left singular vector associated with the largest singular value achieves the maximum in the above.

8. **Condition Number:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents28}
    :   The condition number of an invertible n times n matrix A is the ratio between the largest and the smallest singular values: 
    :   $$
        \kappa(A) = \frac{\sigma_1}{\sigma_n}  = \|A\| \cdot \|A^{-1}\|.  
        $$  
    >  Provides a measure of the sensitivity of the solution of a linear equation to changes in $$A$$.



***
***

TITLE: 2.3 <br /> Eigen-Stuff
LINK: research/optimization/2/2.3.md


## Quadratic Functions
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    :   A function $$q : \mathbf{R}^n \rightarrow \mathbf{R}$$ is said to be a quadratic function if it can be expressed as
    :   $$
        q(x) = \sum_{i=1}^n \sum_{j=1}^n A_{ij} x_i x_j + 2 \sum_{i=1}^n b_i x_i + c, 
        $$  
    :   for numbers $$A_{ij}, b_i,$$ and $$c, i, j \in {1, \ldots, n}$$.
        > A quadratic function is thus an affine combination of the $$\ x_i$$'s and all the "cross-products" $$x_ix_j$$.  
    :   > We observe that the coefficient of $$x_ix_j$$ is $$(A_{ij} + A_{ji})$$.  
    :   > The function is said to be a quadratic form if there are no linear or constant terms in it: $$b_i = 0, c=0.$$
    :   > The _Hessian_ of a quadratic function is always constant.

2. **Link between Quadratic Func's & Symmetric Matrices:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} 
    :   Indeed, any quadratic function $$q : \mathbf{R}^n \rightarrow \mathbf{R}$$ can be written as
    :   $$
        q(x) = \left( \begin{array}{c} x \\ 1 \end{array} \right)^T \left( \begin{array}{cc} A & b \\ b^T & c \end{array} \right) \left( \begin{array}{c} x \\ 1 \end{array} \right) = x^TAx + 2 b^Tx + c, 
        $$
    :   for an appropriate symmetric matrix $$A \in \mathbf{S}^{n}$$, vector $$b \in \mathbf{R}^n$$ and scalar $$c \in \mathbf{R}$$. 
    :   > $$A_{ii}$$ is the coefficient of $$x_i^2$$ in q;   
        > $$2A_{ij}$$ (for $$i \ne j$$) is the coefficient of the term $$x_ix_j$$ in q;  
        > $$2b_i$$ is that of $$x_i$$;  
        > $$c$$ is the constant term, $$q(0)$$.  
    :   >  If q is a quadratic form, then $$b=0, c=0,$$ and we can write $$q(x) = x^TAx$$ where now $$A \in \mathbf{S}^n$$.

3. **Second-order approximations [1-D]:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} 
    :    If $$f : \mathbf{R} \rightarrow \mathbf{R}$$ is a twice-differentiable function of a single variable, then the second order approximation (or, second-order Taylor expansion) of $$f$$ at a point $$x_0$$ is of the form: 
    :   $$
        f(x) \approx q(x) = f(x_0) + f(x_0)' (x-x_0) + \dfrac{1}{2} f''(x_0)(x-x_0)^2, 
        $$
    :   where $$f'(x_0)$$ is the first derivative, and f''(x_0) the second derivative, of $$f$$ at $$x_0$$.  
        > We observe that the quadratic approximation $$q$$ has the same value, derivative, and second-derivative as $$f$$, at $$x_0$$.

4. **Second-order approximations [n-D]:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} 
    :   Let us approximate a twice-differentiable function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$ by a quadratic function $$q$$, so that $$f$$ and $$q$$ coincide up and including to the second derivatives.
    :   The function $$q$$ must be of the form
    :   $$q(x) = x^TAx + 2b^Tx + c, $$  
    :   where $$A \in \mathbf{S}^n, b \in \mathbf{R}^n\text{, and } c \in \mathbf{R}$$. Our condition that q coincides with f up and including to the second derivatives shows that we must have:
    :   $$
        (1)\ \ \ \ \ \ \  \nabla^2 q(x)\ \  =\ \  2 A =\ \  \nabla^2 f(x_0), \\
        (2)\ \nabla q(x) = 2(Ax_0+b) = \nabla f(x_0), \\
        (3)\ \ \ \ \ \ x_0^TAx_0 + 2b^Tx_0 + c = f(x_0). \\
        $$  
    :   Solving for A,b,c we obtain the following result:
    :   $$
        f(x) \approx q(x) = f(x_0) + \nabla f(x_0)^T (x-x_0) + \dfrac{1}{2} (x-x_0)^T \nabla^2 f(x_0) (x-x_0), 
        $$
    :   where $$\nabla f(x_0) \in \mathbf{R}^n$$ is the gradient of $$f$$ at $$x_0$$, and the symmetric matrix $$\nabla^2 f(x_0)$$ is the Hessian of $$f$$ at $$x_0$$. 

***

## Basics and Definitions
{: #content2}

1. **Eigenvalue:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} 
    :   A real scalar $$\lambda$$ is said to be an eigenvalue of a matrix $$A$$ if there exist a non-zero vector $$v \in \mathbf{R}^n$$ such that:
    :   $$ A v = \lambda u. $$
    :   > The interpretation of $$v$$ is that it defines a direction along $$A$$ behaves just like scalar multiplication. The amount of scaling is given by $$\lambda$$. 

2. **Eigenvector:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22}
    :   



***

## THIRD
{: #content3}


***

## Eigen-Stuff of Symmetric Matrices
{: #content4}

1. **The Spectral Theorem (for Symmetric Matrices):**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} 
    :   We can decompose any symmetric matrix $$A \in \mathbf{S}^n$$ with the symmetric eigenvalue decomposition (SED)
    :   $$
        A = \sum_{i=1}^n \lambda_i u_iu_i^T  = U \Lambda U^T, \;\; \Lambda = \mathbf{diag}(\lambda_1, \ldots, \lambda_n) . 
        $$
    :   where the matrix of $$U := [u_1 , \ldots, u_n]$$ is orthogonal (that is, $$U^TU=UU^T = I_n$$), and contains the eigenvectors of $$A$$, while the diagonal matrix Lambda contains the eigenvalues of $$A$$.  
    > The SED provides a decomposition of the matrix in simple terms, namely dyads.

        <button>Proof.</button>{: .showText value="show"
        onclick="showTextPopHide(event);"}
        ![img](/main_files/conv_opt/2/2.3/ex1.png){: hidden="" width="100%"}

2. **Spectral Decomposition:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} 
    :   $$ Au_j = \sum_{i=1}^n \lambda_i u_iu_i^Tu_j = \lambda_j u_j, \;\; j=1, \ldots, n. $$

3. **Rayleigh Quotients:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents43} 
    :   Given a symmetric matrix $$A$$, we can express the smallest and largest eigenvalues of $$A$$, denoted $$\lambda_{\rm min}$$ and $$\lambda_{\rm max}$$ respectively, in the so-called variational form:
    :   $$
        \lambda_{\rm min}(A)  = \min_{x} : \left\{ x^TAx ~:~ x^Tx = 1 \right\} , \\ \lambda_{\rm max}(A)  = \max_{x} : \left\{ x^TAx ~:~ x^Tx = 1 \right\} . 
        $$
    :   > The term _"variational"_ refers to the fact that the eigenvalues are given as optimal values of optimization problems, which were referred to in the past as variational problems.  
        > Variational representations exist for all the eigenvalues, but are more complicated to state.  
    :   <button>Proof.</button>{: .showText value="show"
        onclick="showTextPopHide(event);"}
        ![img](/main_files/conv_opt/2/2.3/pf1.png){: hidden="" width="100%"}

    :   * **Interptation:**   
            The interpretation of the above identities is that the largest and smallest eigenvalues is a measure of the range of the quadratic function $$x \rightarrow x^TAx$$ over the unit Euclidean ball.  
            The quantities above can be written as the minimum and maximum of the so-called Rayleigh quotient $$\dfrac{x^TAx}{x^Tx}$$.

    :   [**Example:** Largest singular value norm of a matrix](http://livebooklabs.com/keeppies/c5a5868ce26b8125/7befdacf56b34f21){: value="show" onclick="iframePopA(event)"}
        <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/7befdacf56b34f21">` Visit the Book`</a>
        <div markdown="1"> </div>



***

## Positive Definitness
{: #content5}

0. **Associated Quadratic Form:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents51}    
    :   For a given symmetric matrix $$A \in \mathbf{R}^{n \times n}$$, the associated quadratic form is the function $$q : \mathbf{R}^n \rightarrow \mathbf{R}$$ with values: $$q(x) = x^TAx.$$

1. **Positive Definite Matrices:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents51} 
    :   A symmetric matrix $$A$$ is said to be positive definite (PD, notation: $$A \succ 0$$) if and only if the associated quadratic form $$q$$ is positive everywhere:
    :   $$
        q(x) > 0 \mbox{ for every } x \in \mathbf{R}^n. 
        $$

2. **Positive Semi-Definite Matrices:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents52} 
    :   A symmetric matrix $$A$$ is said to be positive semi-definite (PSD, notation: $$A \succeq 0$$) if and only if the associated quadratic form $$q$$ is non-negative everywhere:
    :   $$
        q(x) \ge 0 \mbox{ for every } x \in \mathbf{R}^n. 
        $$

3. **Definite Matrices:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents53} 
    :   When $$q = 0$$.

0. **Diagonal Matrices and Positive Definitness:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents50} 

    :   **Diagonal matrices.** A diagonal matrix is PSD (resp. PD) if and only if all of its (diagonal) elements are non-negative (resp. positive).

4. **Theorem. Spectral Decomposition of PSD Matrices:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents54} 
    :   A quadratic form $$q(x) = x^TAx$$, with $$A \in \mathbf{S}^n$$ is non-negative (resp. positive-definite) if and only if every eigenvalue of the symmetric matrix A is non-negative (resp. positive).

    <button>Proof.</button>{: .showText value="show"
        onclick="showTextPopHide(event);"}
        ![img](/main_files/conv_opt/2/2.3/pf2.png){: hidden="" width="100%"}

5. **Square Roots of PSD Matrices:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents55}
    :    If A is PSD, there exist a unique PSD matrix, denoted $$A^{1/2}$$, such that $$A = (A^{1/2})^2$$. 
    :   We can express this matrix square root in terms of the SED of $$A = U\Lambda U^T,$$ as $$A^{1/2} = U \Lambda^{1/2} U^T$$, where $$\Lambda^{1/2}$$ is obtained from $$\Lambda$$ by taking the square root of its diagonal elements. 
    :   If $$A$$ is PD, then so is its square root.



6. **The Cholesky Decomposition:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents56} \\
    :   Any PSD matrix can be written as a product $$A = LL^T$$ for an appropriate matrix $$L$$. 
    :   The decomposition is not unique, and $$L = A^{1/2}$$ is only a possible choice (the only PSD one). 
    :   Another choice, in terms of the SED of $$A = U^T \Lambda U$$, is $$L = U^T \Lambda^{1/2}$$.
    :   If $$A$$ is positive-definite, then we can choose $$L$$ to be lower triangular, and invertible. The decomposition is then known as **the Cholesky decomposition** of $$A$$.

7. **Ellipsoids and PSDs:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents57} 
    :   **Definition.** We define an ellipsoid to be affine transformation of the unit ball for the Euclidean norm:
    :   $$
        \mathbf{E} = \left\{ \hat{x} + L z ~:~ \|z\|_2 \le 1 \right\} , 
        $$
    :   where $$L \in \mathbf{R}^{n \times n}$$ is an arbitrary non-singular (invertible) matrix. 
    :   We can **express the ellipsoid as:**
    :   $$
        \mathbf{E} = \left\{ x ~:~ \|L^{-1}(x-\hat{x})\|_2 \le 1 \right\}  =  \left\{ x ~:~ (x-\hat{x})^T A^{-1} (x-\hat{x}) \le 1 \right\} , 
        $$
    :   where  $$A=LL^T$$ is PD.
    
8. **Geometric Interpretation via SED:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents58} 
    :   We interpret the eigenvectors and associated eigenvalues of A in terms of geometrical properties of the ellipsoid, as follows.
    :   Consider the SED of $$A: A = U \Lambda U^T$$, with $$U^TU = I$$ and $$\Lambda$$ diagonal, with diagonal elements positive.
    :   The SED of its inverse is $$A^{-1} = L L^T = U \Lambda^{-1} U^T$$.
    :   Let $$\tilde{x} = U^T(x-\hat{x})$$.
    :   We can express the condition $$x \in \mathbf{E}$$ as:
    :   $$
        \tilde{x}^T\Lambda^{-1}\tilde{x} = \displaystyle\sum_{i=1}^n \frac{\tilde{x}_i^2}{\lambda_i} \le 1.
        $$
    :   * Now set $$\bar{x}_i := \tilde{x}_i/\sqrt{\lambda_i} , i=1, \ldots, n$$.
        * The above writes $$\bar{x}^T\bar{x} \le 1: \in \bar{x}-$$space, the ellipsoid is simply an unit ball. 
        * In $$\tilde{x}-$$space, the ellipsoid corresponds to scaling each $$\bar{x}-$$axis by the square roots of the eigenvalues.
        * The ellipsoid has principal axes parallel to the coordinate axes in $$\tilde{x}-$$space. 
        * We then apply a rotation and a translation, to get the ellipsoid in the original x-space. 
        * The rotation is determined by the eigenvectors of $$A^{-1}$$, which are contained in the orthogonal matrix $$U$$.
        * Thus, the geometry of the ellipsoid can be read from the SED of the PD matrix $$A^{-1} = LL^T \\ \implies$$
        > (1) The eigenvectors give the principal directions, and  
        > (2) The semi-axis lengths are the square root of the eigenvalues.
    :   <button>Example.</button>{: .showText value="show"
        onclick="showTextPopHide(event);"}
        ![img](/main_files/conv_opt/2/2.3/ex1.png){: hidden="" width="100%"}
    :   > It is possible to define degenerate ellipsoids, which correspond to cases when the matrix B in the above, or its inverse A, is degenerate. For example, cylinders or slabs (intersection of two parallel half-spaces) are degenerate ellipsoids.


***
***

TITLE: Topology and Smooth Manifolds
LINK: research/math/manifolds.md


## Introduction and Definitions
{: #content1}

1. **Topology:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   is a mathematical field concerned with the properties of space that are preserved under continuous deformations, such as stretching, crumpling and bending, but not tearing or gluing

2. **Topological Space:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   is defined as a set of points $$\mathbf{X}$$, along with a set of neighbourhoods (sub-sets) $$\mathbf{T}$$ for each point, satisfying the following set of axioms relating points and neighbourhoods:  
        * __$$\mathbf{T}$$ is the Open Sets__:     
            1. The __Empty Set__ $$\emptyset$$ is in $$\mathbf{T}$$
            2. $$\mathbf{X}$$ is in $$\mathbf{T}$$
            3. The __Intersection of a finite number of Sets__ in $$\mathbf{T}$$ is, also, in $$\mathbf{T}$$
            4. The __Union of an arbitrary number of Sets__ in $$\mathbf{T}$$ is, also, in $$\mathbf{T}$$  
        * __$$\mathbf{T}$$ is the Closed Sets__:     
            1. The __Empty Set__ $$\emptyset$$ is in $$\mathbf{T}$$
            2. $$\mathbf{X}$$ is in $$\mathbf{T}$$
            3. The __Intersection of an arbitrary number of Sets__ in $$\mathbf{T}$$ is, also, in $$\mathbf{T}$$
            4. The __Union of a finite number of Sets__ in $$\mathbf{T}$$ is, also, in $$\mathbf{T}$$

3. **Homeomorphism:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   Intuitively, a __Homeomorphism__ or __Topological Isomorphism__ or __bi-continuous Function__ is a continuous function between topological spaces that has a continuous inverse function.  
    :   Mathematically, a function $${\displaystyle f:X\to Y}$$ between two topological spaces $${\displaystyle (X,{\mathcal {T}}_{X})}$$ and $${\displaystyle (Y,{\mathcal {T}}_{Y})}$$ is called a __Homeomorphism__ if it has the following properties:  
        * $$f$$ is a bijection (one-to-one and onto)  
        * $$f$$ is continuous
        * the inverse function $${\displaystyle f^{-1}}$$ is continuous ($${\displaystyle f}$$ is an open mapping).  
    :   > i.e. There exists a __continuous map__ with a __continuous inverse__

4. **Maps and Spaces:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   | __Map__ | __Space__ | __Preserved Property__ |  
        | Linear Map | Vector Space | Linear Structure: $$f(aw+v) = af(w)+f(v)$$ |  
        | Group Homomorphism | Group | Group Structure: $$f(x \ast y) = f(x) \ast f(y)$$ |  
        | Continuous Map | Topological Space | Openness/Closeness: $$f^{-1}(\{\text{open}\}) \text{ is open}$$ |  
        | _Smooth Map_ | _Topological Space_ | 

5. **Smooth Maps:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    :   
        * __Continuous__: 
        * __Unique Limits__:       

6. **Hausdorff:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  

 
***

## Point-Set Topology
{: #content2}

1. **Open Set:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} 
    :   A set $$\chi \subseteq \mathbf{R}^n$$ is said to be open if for any point $$x \in \chi$$ there exist a ball centered in $$x$$ which is contained in $$\chi$$. 
    :   Precisely, for any $$x \in \mathbf{R}^n$$ and $$\epsilon > 0$$ define the Euclidean ball of radius $$r$$ centered at $$x$$:
    :   $$B_\epsilon(x) = {z : \|z ‚àí x\|_2 < \epsilon}$$
    :   Then, $$\chi \subseteq \mathbf{R}^n$$ is open if
    :   $$\forall x \: \epsilon \: \chi, \:\: \exists \epsilon > 0 : B_\epsilon(x) \subset \chi .$$
    :   **Equivalently**,
    :   * A set $$\chi \subseteq \mathbf{R}^n$$ is open if and only if $$\chi = int\; \chi$$.
    :   * An open set does not contain any of its boundary points.
    :   * A closed set contains all of its boundary points. 
    :   * Unions and intersections of open (resp. closed) sets are open (resp. closed).

2. **Closed Set:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} 
    :   A set $$\chi \subseteq \mathbf{R}^n$$ is said to be closed if its complement $$ \mathbf{R}^n \text{ \ } \chi$$ is open.

3. **Interior of a Set:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23}
    :   The interior of a set $$\chi \subseteq \mathbf{R}^n$$ is defined as 
    :   $$int\: \chi = \{z \in \chi : B_\epsilon(z) \subseteq \chi, \:\: \text{for some } \epsilon > 0 \}$$

4. **Closure of a Set:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24}
    :   The closure of a set $$\chi \subseteq \mathbf{R}^n$$ is defined as
    :   $$\bar{\chi} = \{z ‚àà \mathbf{R}^n : \: z = \lim_{k\to\infty} x_k, \: x_k \in \chi , \: \forall k\},$$  
    :   > i.e., the closure of $$\chi$$ is the set of limits of sequences in $$\chi$$.

5. **Boundary of a Set:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25}
    :   The boundary of X is defined as
    :   $$\partial \chi = \bar{\chi} \text{ \ }  int\: \chi$$

6. **Bounded Set:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26}
    :   A set $$\chi \subseteq \mathbf{R}^n$$ is said to be bounded if it is contained in a ball of finite radius, that is if there exist $$x \in \mathbf{R}^n$$ and $$r > 0$$ such that $$\chi \subseteq B_r(x)$$.

7. **Compact Set:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27}
    :   A set $$\chi \subseteq \mathbf{R}^n$$ is compact $$\iff$$ it is **Closed** and **Bounded**.

8. **Relative Interior [$$\operatorname{relint}$$]:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents28}
    :   We define the relative interior of the set $$\chi$$, denoted $$\operatorname{relint} \chi$$, as its interior relative to $$\operatorname{aff} C$$:
    :   $$\operatorname{relint} \chi = \{x \in \chi : \: B(x, r) \cap \operatorname{aff} \chi \subseteq \chi \text{ for some } r > 0\},$$
    :   where $$B(x, r) = \{y : ky ‚àí xk \leq r\}$$, the ball of radius $$r$$ and center $$x$$ in the norm $$\| ¬∑ \|$$.

9. **Relative Boundary:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents29}
    :   We can then define the relative boundary of a set $$\chi$$ as $$\mathbf{cl}  \chi \text{ \ } \operatorname{relint} \chi,$$ where $$\mathbf{cl} \chi$$ is the closure of $$\chi$$.

***

## Manifolds
{: #content3}

1. **Manifold:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   is a topological space that locally resembles Euclidean space near each point  
        > i.e. around every point, there is a neighborhood that is topologically the same as the open unit ball in $$\mathbb{R}^n$$  
    :   

2. **Smooth Manifold:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   A topological space $$M$$ is called a __$$n$$-dimensional smooth manifold__ if:  
        * Is is __Hausdorff__
        * It is __Second-Countable__
        * It comes with a family $$\{(U_\alpha, \phi_\alpha)\}$$ with:  
            * __Open sets__ $$U_\alpha \subset_\text{open} M$$ 
            * __Homeomorphisms__ $$\phi_\alpha : U_\alpha \rightarrow \mathbb{R}^n$$   
    such that $${\displaystyle M = \bigcup_\alpha U_\alpha}$$  
    and given $${\displaystyle U_\alpha \cap U_\beta \neq \emptyset}$$ the map $$\phi_\beta \circ \phi_\alpha^{-1}$$ is smooth



***
***

TITLE: Topology and Smooth Manifolds
LINK: research/math/proofs.md


## Introduction and Definitions
{: #content1}

1. **Power Sets:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   __Lemma:__ the size of the power set of a set of size $$n$$ is $$2^n$$
    :   __Proof:__  
        Assume that we n

2. **Topological Space:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  


***
***

TITLE: Topology and Smooth Manifolds
LINK: research/math/topology/manifolds.md


## Introduction and Definitions
{: #content1}

1. **Topology:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   is a mathematical field concerned with the properties of space that are preserved under continuous deformations, such as stretching, crumpling and bending, but not tearing or gluing

2. **Topological Space:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   is defined as a set of points $$\mathbf{X}$$, along with a set of neighbourhoods (sub-sets) $$\mathbf{T}$$ for each point, satisfying the following set of axioms relating points and neighbourhoods:  
        * __$$\mathbf{T}$$ is the Open Sets__:     
            1. The __Empty Set__ $$\emptyset$$ is in $$\mathbf{T}$$
            2. $$\mathbf{X}$$ is in $$\mathbf{T}$$
            3. The __Intersection of a finite number of Sets__ in $$\mathbf{T}$$ is, also, in $$\mathbf{T}$$
            4. The __Union of an arbitrary number of Sets__ in $$\mathbf{T}$$ is, also, in $$\mathbf{T}$$  
        * __$$\mathbf{T}$$ is the Closed Sets__:     
            1. The __Empty Set__ $$\emptyset$$ is in $$\mathbf{T}$$
            2. $$\mathbf{X}$$ is in $$\mathbf{T}$$
            3. The __Intersection of an arbitrary number of Sets__ in $$\mathbf{T}$$ is, also, in $$\mathbf{T}$$
            4. The __Union of a finite number of Sets__ in $$\mathbf{T}$$ is, also, in $$\mathbf{T}$$

3. **Homeomorphism:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   Intuitively, a __Homeomorphism__ or __Topological Isomorphism__ or __bi-continuous Function__ is a continuous function between topological spaces that has a continuous inverse function.  
    :   Mathematically, a function $${\displaystyle f:X\to Y}$$ between two topological spaces $${\displaystyle (X,{\mathcal {T}}_{X})}$$ and $${\displaystyle (Y,{\mathcal {T}}_{Y})}$$ is called a __Homeomorphism__ if it has the following properties:  
        * $$f$$ is a bijection (one-to-one and onto)  
        * $$f$$ is continuous
        * the inverse function $${\displaystyle f^{-1}}$$ is continuous ($${\displaystyle f}$$ is an open mapping).  
    :   > i.e. There exists a __continuous map__ with a __continuous inverse__

4. **Maps and Spaces:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   | __Map__ | __Space__ | __Preserved Property__ |  
        | Linear Map | Vector Space | Linear Structure: $$f(aw+v) = af(w)+f(v)$$ |  
        | Group Homomorphism | Group | Group Structure: $$f(x \ast y) = f(x) \ast f(y)$$ |  
        | Continuous Map | Topological Space | Openness/Closeness: $$f^{-1}(\{\text{open}\}) \text{ is open}$$ |  
        | _Smooth Map_ | _Topological Space_ | 

5. **Smooth Maps:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    :   
        * __Continuous__: 
        * __Unique Limits__:       

6. **Hausdorff:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    :   

    :   

    :   

***

## Point-Set Topology
{: #content2}

1. **Open Set:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} 
    :   A set $$\chi \subseteq \mathbf{R}^n$$ is said to be open if for any point $$x \in \chi$$ there exist a ball centered in $$x$$ which is contained in $$\chi$$. 
    :   Precisely, for any $$x \in \mathbf{R}^n$$ and $$\epsilon > 0$$ define the Euclidean ball of radius $$r$$ centered at $$x$$:
    :   $$B_\epsilon(x) = {z : \|z ‚àí x\|_2 < \epsilon}$$
    :   Then, $$\chi \subseteq \mathbf{R}^n$$ is open if
    :   $$\forall x \: \epsilon \: \chi, \:\: \exists \epsilon > 0 : B_\epsilon(x) \subset \chi .$$
    :   **Equivalently**,
    :   * A set $$\chi \subseteq \mathbf{R}^n$$ is open if and only if $$\chi = int\; \chi$$.
    :   * An open set does not contain any of its boundary points.
    :   * A closed set contains all of its boundary points. 
    :   * Unions and intersections of open (resp. closed) sets are open (resp. closed).

2. **Closed Set:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} 
    :   A set $$\chi \subseteq \mathbf{R}^n$$ is said to be closed if its complement $$ \mathbf{R}^n \text{ \ } \chi$$ is open.

3. **Interior of a Set:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23}
    :   The interior of a set $$\chi \subseteq \mathbf{R}^n$$ is defined as 
    :   $$int\: \chi = \{z \in \chi : B_\epsilon(z) \subseteq \chi, \:\: \text{for some } \epsilon > 0 \}$$

4. **Closure of a Set:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24}
    :   The closure of a set $$\chi \subseteq \mathbf{R}^n$$ is defined as
    :   $$\bar{\chi} = \{z ‚àà \mathbf{R}^n : \: z = \lim_{k\to\infty} x_k, \: x_k \in \chi , \: \forall k\},$$  
    :   > i.e., the closure of $$\chi$$ is the set of limits of sequences in $$\chi$$.

5. **Boundary of a Set:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25}
    :   The boundary of X is defined as
    :   $$\partial \chi = \bar{\chi} \text{ \ }  int\: \chi$$

6. **Bounded Set:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26}
    :   A set $$\chi \subseteq \mathbf{R}^n$$ is said to be bounded if it is contained in a ball of finite radius, that is if there exist $$x \in \mathbf{R}^n$$ and $$r > 0$$ such that $$\chi \subseteq B_r(x)$$.

7. **Compact Set:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27}
    :   A set $$\chi \subseteq \mathbf{R}^n$$ is compact $$\iff$$ it is **Closed** and **Bounded**.

8. **Relative Interior [$$\operatorname{relint}$$]:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents28}
    :   We define the relative interior of the set $$\chi$$, denoted $$\operatorname{relint} \chi$$, as its interior relative to $$\operatorname{aff} C$$:
    :   $$\operatorname{relint} \chi = \{x \in \chi : \: B(x, r) \cap \operatorname{aff} \chi \subseteq \chi \text{ for some } r > 0\},$$
    :   where $$B(x, r) = \{y : ky ‚àí xk \leq r\}$$, the ball of radius $$r$$ and center $$x$$ in the norm $$\| ¬∑ \|$$.

9. **Relative Boundary:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents29}
    :   We can then define the relative boundary of a set $$\chi$$ as $$\mathbf{cl}  \chi \text{ \ } \operatorname{relint} \chi,$$ where $$\mathbf{cl} \chi$$ is the closure of $$\chi$$.

***

## Manifolds
{: #content3}

1. **Manifold:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   is a topological space that locally resembles Euclidean space near each point  
        > i.e. around every point, there is a neighborhood that is topologically the same as the open unit ball in $$\mathbb{R}^n$$  
    :   

2. **Smooth Manifold:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   A topological space $$M$$ is called a __$$n$$-dimensional smooth manifold__ if:  
        * Is is __Hausdorff__
        * It is __Second-Countable__
        * It comes with a family $$\{(U_\alpha, \phi_\alpha)\}$$ with:  
            * __Open sets__ $$U_\alpha \subset_\text{open} M$$ 
            * __Homeomorphisms__ $$\phi_\alpha : U_\alpha \rightarrow \mathbb{R}^n$$   
    such that $${\displaystyle M = \bigcup_\alpha U_\alpha}$$  
    and given $${\displaystyle U_\alpha \cap U_\beta \neq \emptyset}$$ the map $$\phi_\beta \circ \phi_\alpha^{-1}$$ is smooth



***
***

TITLE: TensorFlow 
LINK: research/math/calculus/mv_calc.md


* Graph slices (holding one var constant) is important for understanding __partial derivatives__   
* [Contour plots](https://www.youtube.com/watch?v=WsZj5Rb6do8&list=PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7&index=5)  
* __Vector Field__: basically a way of visualizing functions that have the same number of inputs as outputs (outputs are vectors).  

* __Partial Derivative__: 
  * Interpret as: how does a _tiny change in the input in the $$x$$OR$$y$$ direction_ __influence__ _the output $$f$$_; keeping the other variable __constant__. (technically, you then take the ratio of the nudges)   
  * On a graph, interpret as _slicing the graph at the constant variable_ and then looking at the slope on the projected/sliced graph.  
  * __Properties__:  
    * $$f_{x y} = f_{y x}$$  

* __Tangent Hyperplanes__:  
  The tangent hyperplane to a curve at a given point $$\mathbf{x}$$ is the __best linear approximation__ of the curve at that point.  
  * __Tangent Line__:  
    <p>$$y-y_{0}=f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)$$</p>   
  * __Tangent Plane__:  
    <p>$$y-y_{0}=f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)$$</p>  
    of the surface $$z=f(x, y)$$  at the point $$P\left(x_{0}, y_{0}, z_{0}\right)$$  

        
* __Gradients__:  
  * __Properties__:  
    * Always __normal__ to _contour lines_  


## FIRST
{: #content1}









***

## SECOND
{: #content2}









***

## THIRD
{: #content3}











***
***

TITLE: Single Variable Calculus
LINK: research/math/calculus/sv_calc.md


[Essence of Calculus - 3b1b](https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&index=1)  



__Notes:__  
* The derivative is the __best constant approximation__ of the __rate of change__.  
Not the _instantaneous rate of change_.    


## FIRST
{: #content1}









***

## SECOND
{: #content2}









***

## THIRD
{: #content3}











***
***

TITLE: The Essence of Linear Algebra
LINK: research/math/la/linalg.md


[Essence of LA - 3b1b](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=1)  
[LA (Stanford Review)](http://web.stanford.edu/class/cs224n/readings/cs229-linalg.pdf)  



## Definitions and Intuitions
{: #content1}

1. **Linear Algebra:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __Linear Algebra__ is about two operations on _a list of numbers_:  
    1. Scalar Multiplication
    2. Vector Addition

2. **Vectors:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    Think of each element in the vector as a __scalar__ that scales the corresponding basis vectors.  
    > Meaning, think about how each one stretches or squishes vectors (in this case, the basis vectors $$\hat{i}, \hat{j}$$)  

    <p>$$\begin{bmatrix}
            x \\
            y 
        \end{bmatrix} = \begin{bmatrix}
            1 & 0  \\
            0 & 1 
        \end{bmatrix}   \begin{bmatrix}
                            x \\
                            y 
                        \end{bmatrix} = \color{red} x \color{red} {\underbrace{\begin{bmatrix}
            1 \\
            0 
        \end{bmatrix}}_ {\hat{i}}} + \color{red} y \color{red} {\underbrace{\begin{bmatrix}
            0 \\
            1 
        \end{bmatrix}}_ {\hat{j}}} = \begin{bmatrix}
            1\times x + 0 \times y \\
            0\times x + 1 \times y 
        \end{bmatrix}
    $$</p>  

3. **Span:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    The __Span__ of two vectors $$\mathbf{v}, \mathbf{w}$$ is the set of all linear combinations of the two vectors: 
    <p>$$a \mathbf{v} + b \mathbf{w} \: ; \: a,b \in \mathbb{R}$$</p>  


4. **Linearly Dependent Vectors:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    If one vector is in the span of the other vectors.  
    Mathematically:  
    <p>$$a \vec{v}+b \vec{w}+c \vec{u}=\overrightarrow{0} \implies a=b=c=0$$</p>  

44. **The Basis:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents144}  
    The __Basis__ of a vector space is a set of _linearly independent_ vectors that __span__ the full space.  

5. **Matrix as a Linear Transformation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    __Summary__:
    1. Each *__column__*  is a _transformed_ version of the *__basis vectors__* (e.g. $$\hat{i}, \hat{j}$$)  
    2. The result of a __Matrix-Vector Product__ is the __linear combination__ of the *__vectors__* with the appropriate *__transformed coordinate/basis vectors__*  
        > i.e. __Matrix-Vector Product__ is a way to compute what the corresponding linear transformation does to the given vector.  


    __Matrix as a Linear Transformation:__{: style="color: red"}  
    Always think of Matrices as __Transformations of Space__:    
    * A Matrix represents a _specific linear transformation_
        * Where the *__columns__* represent the *__coordinates__* of the _transformed_ *__basis vectors__*   
    * & __Multiplying a matrix by a vector__ is EQUIVALENT to __Applying the transformation to that vector__  
    > The word _"transformation"_ suggests that you think using *__movement__*  
    > If a transformation takes some input vector to some output vector, we imagine that input vector moving over to the output vector.  
    > Then to understand the transformation as a whole, we might imagine watching every possible input vector move over to its corresponding output vector.  
    > This transformation/_"movement"_ is __Linear__, if it keeps all the vectors *__parallel__* and *__evenly spaced__*, and *__fixes the origin__*.  


    __Matrices and Vectors | The Matrix-Vector Product:__{: style="color: red"}  
    Again, We think of _each element_ in a vector as a *__scalar__* that scales the corresponding _basis vectors_.  
    * Thus, if we know how the basis vectors get transformed, we can then just scale them (by multiplying with our vector elements).  
        __Mathematically__, we think of the vector:   
        <p>$$\mathbf{v} = \begin{bmatrix}x \\y \end{bmatrix} = x\hat{i} + y\hat{j}$$</p>
        and its _transformed_ version:  
        <p>$$\text{Transformed } \mathbf{v} = x (\text{Transformed } \hat{i}) + y (\text{Transformed }  \hat{j})$$</p>  
        $$\implies$$ we can describe where any vector $$\mathbf{v}$$ go, by describing where the *__basis vectors__* will land.  
        > If you're given a _two-by-two matrix_ describing a _linear transformation_ and some specific _vector_ and you want to know where that linear transformation takes that vector, you can (1) take the coordinates of the vector (2) multiply them by the corresponding *__columns__* of the matrix, (3) then add together what you get.  
        This corresponds with the idea of adding the scaled versions of our new basis vectors.  
            ![img](/main_files/math/la/linalg/1.png){: width="60%"}  


    <p>$$ \mathbf{v} = 
        \begin{bmatrix}
            x \\
            y 
        \end{bmatrix} = \begin{bmatrix}
            1 & 0  \\
            0 & 1 
        \end{bmatrix}   \begin{bmatrix}
                            x \\
                            y 
                        \end{bmatrix} = \color{red} x \color{red} {\underbrace{\begin{bmatrix}
            1 \\
            0 
        \end{bmatrix}}_ {\hat{i}}} + \color{red} y \color{red} {\underbrace{\begin{bmatrix}
            0 \\
            1 
        \end{bmatrix}}_ {\hat{j}}} = \begin{bmatrix}
            1\times x + 0 \times y \\
            0\times x + 1 \times y 
        \end{bmatrix} = x\hat{i} + y\hat{j}
    $$</p>  

    __The Matrix-Vector Product:__  
    ![img](/main_files/math/la/linalg/2.png){: width="100%"}  


    __Non-Square Matrices $$(N \times M)$$:__{: style="color: red"}  
    Map vectors from $$\mathbb{R}^M \rightarrow \mathbb{R}^N$$.  
    They are _transformations between **dimensions**_.   


6. **The Product of Two Matrices:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    The Product of Two Matrices Corresponds to the *__composition of the transformations__*, being applied from right to left.  
    This is __very important__ intuition:  
    > e.g. _do matrices commute?_  
    if you think of the matrices as _transformations of space_ then answer quickly is no.  
    Equivalently, _Are matrices associative?_  Yes, function composition is associative ($$f \circ(g \circ h) = (f \circ g) \circ h$$)  


7. **Linear Transformations:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    __Linear Transformations__ are transformations that preserve the following properties:  
    1. All vectors that are _parallel_ remain parallel 
    2. All vectors are _evenly spaced_ 
    3. The __origin__ remains _fixed_  

    * [**An Equivalent Definition of Linearity**](https://www.youtube.com/embed/LyGKycYT2v0?start=295){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/LyGKycYT2v0?start=295"></a>
        <div markdown="1"> </div>    


8. **The Determinant:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    __The Determinant__ of a transformation is the _"scaling factor"_ by which the transformation changed any __area__ in the vector space.  

    __The Negative Determinant__ determines the *__orientation__*.  

    __Linearity of the Determinant__:  
    <p>$$\text{det}(AB) = \text{det}(A) \text{det}(B)$$</p>  


10. **Solving Systems of Equations:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents110}  
    The Equation $$A\mathbf{x} = \mathbf{b}$$, finds the vector $$\mathbf{x}$$ that _lands on the vector $$\mathbf{b}$$_ when the transformation $$A$$ is applied to it.    

    Again, the __intuition__: is to think of a linear system of equations, __geometrically__, as trying to find a particular vector that once transformed/moved, lands on the output vector $$\mathbf{b}$$.  
    * This becomes more important when you think of the different __properties__, of that transformation/function, encoded (now) in the *__matrix__* $$A$$:  
        * When the $$det(A) \neq 0$$ we know that space is preserved, and from the _properties of linearity_, we know there will always be one (*__unique__*) vector that would land on $$\mathbf{b}$$ once transformed (and you can find it by _"playing the transformation in reverse"_ i.e. the __inverse matrix__).  
        * When $$det(A) = 0$$ then the space is __squished__ down to a lower representation, resulting in __information loss__.  


9. **The Inverse of a matrix:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents19}  
    The __Inverse__ of a matrix is the matrix such that if we "algebraically" multiply the two matrices, we get back to the original coordinates (the identity).  
    It is, basically, the transformation applied in _reverse_.  

    __Why inverse transformation/matrix DNE when det is Zero (i.e. space is squished:__{: style="color: red"}  
    To do so, is equivalent to _transforming_ a __line__ into a __plane__, which would require _mapping_ each, individual, __vector__ into a __"whole line full of vectors" (multiple vectors)__; which is _not something a **Function**_ can do.  
    Functions map *__single input__* to *__single output__*.  


11. **The Determinants, Inverses, & Solutions to Equations:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents111}  
    When the $$\text{det} = 0$$ the area gets _squashed to $$0$$_, and _information is lost_. Thus:   
    1. The Inverse DNE
    2. A unique solution DNE  
    > i.e. there is no function that can take a line onto a plane; info loss


12. **The Rank:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents112}  
    __The Rank__ is the _dimensionality_ of the output of a transformation.  
    __Viewed as a Matrix__, it is the _number of independent vectors (as columns)_ that make up the matrix.  


13. **The Column Space:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents113}  
    __The Column Space__ is the set of all possible outputs of a transformation/matrix.  
    * View each column as a _basis vector_; there span is then, all the possible outputs
    * __The Zero Vector (origin)__: is always in the column space (corresponds to preserving the origin)
        
    The __Column Space__ allows us to understand *__when a solution exists__*.  
    For example, even when the matrix is not full-rank (det=0) a solution might still exist; if, when $$A$$ squishes space onto a line, the vector $$\mathbf{b}$$ lies on that line (in the __span__ of that line).  
    * Formally, solution exists if $$\mathbf{b}$$ is in the __column space__ of $$A$$.   


14. **The Null Space:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents114}  
    __The Null Space__ is the set of vectors that get _mapped to the origin_; also known as __The Kernel__.   

    The __Null Space__ allows us to understand _what the set of all possible solutions look like_.   

     __Rank and the Zero Vector:__{: style="color: red"}  
    A __Full-Rank__ matrix maps __only the origin__ to itself.  
    A __non-full rank (det=0)__ (rank=n-1) matrix maps a whole __line__ to the origin, (rank=n-2) a __plane__ to the origin, etc.  


15. **The Dot Product/Scalar Product:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents115}  
    (for vectors $$\mathbf{u}, \mathbf{v}$$)  
    <p>$$\mathbf{u} \cdot \mathbf{v} = \mathbf{u}^T \mathbf{v}$$</p>   
    * __Geometrically__:  
        1. project 
            

16. **Cramer's Rule:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents116}  
    * [**Cramer's Rule Geometrically**](https://www.youtube.com/embed/jBsC34PxzoM?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/jBsC34PxzoM?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab"></a>
        <div markdown="1"> </div>    



17. **Coordinate Systems:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents117}  

    A __Coordinate System__ is just a way to formalize the basis vectors and their lengths. All coordinate systems agree on where the __origin__ is.  
    __Coordinate Systems__ are a way to _translate_ between *__vectors__* (defined w.r.t. basis vectors being scaled and added in space) and *__sets of numbers__* (the elements of the vector/list/array of numbers which we define).  

    * [**Translating between Coordinate Systems**](https://www.youtube.com/embed/P2LTAUO1TdA?start=257){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/P2LTAUO1TdA?start=257"></a>
        <div markdown="1"> </div>    
        > Imp: _6:47_  

    * [**Translating a Matrix/Transformation between Coordinate Systems**](https://www.youtube.com/embed/P2LTAUO1TdA?start=552){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/P2LTAUO1TdA?start=552"></a>
        <div markdown="1"> </div>    

    __Implicit Assumptions in a Coordinate System:__  
    * Direction of each _basis vector_ our vector is scaling
    * The unit of _distance_  


18. **Eigenstuff:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents118}  

    * [**Computing the Eigenvalues/vectors Intuition**](https://www.youtube.com/embed/PFDu9oVAE-g?start=322){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/PFDu9oVAE-g?start=322"></a>
        <div markdown="1"> </div>    

    * [**Diagonalization**](https://www.youtube.com/embed/PFDu9oVAE-g?start=881){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/PFDu9oVAE-g?start=881"></a>
        <div markdown="1"> </div>    


    __Notes:__  
    * Complex Eigenvalues, generally, correspond to some kind of rotation in the transformation/matrix (think, multiplication by $$i$$ in $$\mathbb{C}$$ is a $$90^{\deg}$$ rotation).  
    * For a __diagonal matrix__, all the *__basis vectors__* are *__eigenvectors__* and the *__diagonal entries__* are their *__eigenvalues__*.  





***
***

TITLE: Classes of Matrices
LINK: research/math/la/cls_mat.md


[Special Matrices in ML](https://medium.com/@jonathan_hui/machine-learning-linear-algebra-special-matrices-c750cd742dfe)   
[Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)  



## Diagonal Matrices
{: #content1}

1. **Definition.**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    **Diagonal matrices** are square matrices $$A$$ with $$A_{ij} = 0 \text{, when } i \ne j.$$  
    <br>

2. **Properties:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    1. Diagonal matrices correspond to quadratic functions that are simple sums of squares, of the form:  
        <p>$$q(x) = \sum_{i=1}^n \lambda_i x_i^2 = x^T \mathbf{diag}(\lambda) x.$$</p>  
    2. They are easy to invert $$A^{-1}_ {ii} = 1/A_ {ii} \: \forall i \in [1, n]$$  
    3. The __pseudo-inverse__ is easy to compute: keep zero diagonal elements as zero  
    4. Allow easier matrix multiplication and powers  
    <br>


***

## Symmetric Matrices
{: #content2}

1. **Definition:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}    
    **Symmetric matrices** are square matrices that satisfy $$A_{ij} = A_{ji}$$ for every pair $$(i,j).$$  

    **The set of symmetric** $$(n \times n)$$ matrices is denoted $$\mathbf{S}^n$$. This set is a subspace of $$\mathbf{R}^{n \times n}$$.  
    <br>

2. **Properties:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}    
    * Has __orthonormal__ eigenvectors (even with repeated eigenvalues) 
    * Its inverse is also symmetric
    * All eigenvalues of a symmetric matrix are __real__ 
    * $$A^TA$$ is __invertible__ iff columns of $$A$$ are linearly independent  
    * Every symmetric matrix $$S$$ can be diagonalized (factorized) with $$Q$$ formed by the orthonormal eigenvectors $$v_i$$ of $$S$$ and $$\Lambda$$ is a diagonal matrix holding all the eigenvalues:  
        <p>$$\begin{aligned} S&=Q \Lambda Q^{T} \\
            &= \lambda_{1} v_{1} v_{1}^{T}+\ldots+\lambda_{n} v_{n} v_{h}^{T} = \sum_{i=1}^n \lambda_i v_i v_i^T
            \end{aligned}$$</p>  
    * One can create a symmetric matrix from any matrix by:  
        <p>$$M = {\displaystyle {\tfrac {1}{2}}\left(A+A^{\textsf {T}}\right)}$$</p>  
    <br>

3. **Examples:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}   
    1. [**Representation of a weighted, undirected graph.**](http://livebooklabs.com/keeppies/c5a5868ce26b8125/3c5245ebb8a556da){: value="show" onclick="iframePopA(event)"}
        <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/73a4ae787085d554">` Visit the Book`</a>
        <div markdown="1"> </div>

    2. [**Laplacian matrix of a graph.**](http://livebooklabs.com/keeppies/c5a5868ce26b8125/0e696ef8a78e090c){: value="show" onclick="iframePopA(event)"}
        <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/73a4ae787085d554">` Visit the Book`</a>
        <div markdown="1"> </div>
    3. [**Hessian of a function.**](http://livebooklabs.com/keeppies/c5a5868ce26b8125/6c0afdfbf11892c6){: value="show" onclick="iframePopA(event)"}
        <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/73a4ae787085d554">` Visit the Book`</a>
        <div markdown="1"> </div>
    4. [**Gram matrix of data points.**](http://livebooklabs.com/keeppies/c5a5868ce26b8125/e236418f4e2d6b3b){: value="show" onclick="iframePopA(event)"}
        <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/73a4ae787085d554">` Visit the Book`</a>
        <div markdown="1"> </div>
    <br>

*** 

## Skew-Symmetric Matrices
{: #content3}

1. **Definition.**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  

2. **Properties:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}   


***

## Covariance Matrices
{: #content4}

1. **Definition.**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}   
    **Standard Form:**  
    <p>$$\Sigma :=\mathrm {E} \left[\left(\mathbf {X} -\mathrm {E} [\mathbf {X} ]\right)\left(\mathbf {X} -\mathrm {E} [\mathbf {X} ]\right)^{\rm {T}}\right]$$</p>  
    <p>$$ \Sigma := \dfrac{1}{m} \sum_{k=1}^m (x_k - \hat{x})(x_k - \hat{x})^T. $$</p>  


    **Matrix Form:**  
    <p>$$ \Sigma := \dfrac{X^TX}{n} $$</p>  

    > valid only for (1) $$X$$ w/ samples in rows and variables in columns  (2) $$X$$ is centered (mean=0)  



2. **Properties:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}   
    1. The sample covariance matrix allows to find the variance along any direction in data space.

    2. The diagonal elements of $$\Sigma$$ give the variances of each vector in the data.

    3. The trace of $$\Sigma$$ gives the sum of all the variances.

    4. The matrix $$\Sigma$$ is positive semi-definite, since the associated quadratic form $$u \rightarrow u^T \Sigma u$$ is non-negative everywhere.

    4. It is Symmetric.

    4. Every symmetric positive semi-definite matrix is a covariance matrix.  
        [**Proof.**](http://ahmedbadary.ml/work_files/research/opt_probs#bodyContents12){: value="show" onclick="iframePopA(event)"}  
        <a href="http://ahmedbadary.ml/work_files/research/opt_probs#bodyContents12">` OR, Visit the website`</a>  
        <div markdown="1"> </div>

    5. The sample variance along direction $$u$$ can be expressed as a quadratic form in $$u$$:  
        $$ \sigma^2(u) = \dfrac{1}{n} \sum_{k=1}^n [u^T(x_k-\hat{x})]^2 = u^T \Sigma u,$$  
    6. The diminsion of the matrix is $$(n \times n)$$, where $$n$$ is the number of variables/features/columns.

    7. The inverse of this matrix, $${\displaystyle \Sigma ^{-1},}$$ if it exists, is the inverse covariance matrix, also known as the _concentration matrix_ or _precision matrix_.

    7. If a vector of $$n$$ possibly correlated random variables is jointly normally distributed, or more generally elliptically distributed, then its probability density function can be expressed in terms of the covariance matrix.

    8. $$\Sigma =\mathrm {E} (\mathbf {XX^{\rm {T}}} )-{\boldsymbol {\mu }}{\boldsymbol {\mu }}^{\rm {T}}$$.

    9. $${\displaystyle \operatorname {var} (\mathbf {AX} +\mathbf {a} )=\mathbf {A} \,\operatorname {var} (\mathbf {X} )\,\mathbf {A^{\rm {T}}} }$$.

    10. $$\operatorname {cov} (\mathbf {X} ,\mathbf {Y} )=\operatorname {cov} (\mathbf {Y} ,\mathbf {X} )^{\rm {T}}$$.

    11. $$\operatorname {cov} (\mathbf {X}_ {1}+\mathbf {X}_ {2},\mathbf {Y} )=\operatorname {cov} (\mathbf {X}_ {1},\mathbf {Y} )+\operatorname {cov} (\mathbf {X}_ {2},\mathbf {Y} )$$.  

    12. If $$(p = q)$$, then $$\operatorname {var} (\mathbf {X} +\mathbf {Y} )=\operatorname {var} (\mathbf {X} )+\operatorname {cov} (\mathbf {X} ,\mathbf {Y} )+\operatorname {cov} (\mathbf {Y} ,\mathbf {X} )+\operatorname {var} (\mathbf {Y} )$$.

    13. $$\operatorname {cov} (\mathbf {AX} +\mathbf {a} ,\mathbf {B} ^{\rm {T}}\mathbf {Y} +\mathbf {b} )=\mathbf {A} \,\operatorname {cov} (\mathbf {X} ,\mathbf {Y} )\,\mathbf {B}$$.

    14. If $${\displaystyle \mathbf {X} }$$  and $${\displaystyle \mathbf {Y} }$$  are independent (or somewhat less restrictedly, if every random variable in $${\displaystyle \mathbf {X} }$$ is uncorrelated with every random variable in $${\displaystyle \mathbf {Y} }$$), then $${\displaystyle \operatorname {cov} (\mathbf {X} ,\mathbf {Y} )=\mathbf {0} }$$.

    15. $$\operatorname {var} (\mathbf {b} ^{\rm {T}}\mathbf {X} )= \mathbf {b} ^{\rm {T}}\operatorname {cov} (\mathbf {X} )\mathbf {b} = \operatorname {cov} (\mathbf{b}^T\mathbf {X}, \mathbf{b}^T\mathbf {X} ) \geq 0,\,$$.
        > This quantity is NON-Negative because it's variance.  
        > Maybe $$= \mathbf {b} ^{\rm {T}}\operatorname {var} (\mathbf {X} )\mathbf {b}$$??  
    

    16. An identity covariance matrix, $$\Sigma = I$$ has variance $$= 1$$ for all variables.
 
    17. A covariance matrix of the form, $$\Sigma=\sigma^2I$$ has variance $$= \sigma^2$$ for all variables.

    18. A diagonal covariance matrix has variance $$\sigma_i^2$$ for the $$i-th$$  variable.

    19. When the mean $$\hat{x}$$ is not known the denominator of the "SAMPLE COVARIANCE MATRIX" should be $$(n-1)$$ and not $$n$$.

    > where,
     $${\displaystyle \mathbf {X} ,\mathbf {X} _{1}},$$ and $${\displaystyle \mathbf {X} _{2}}$$ are random $$(p\times 1)$$ vectors, $${\displaystyle \mathbf {Y} }$$  is a random $$(q\times 1)$$ vector, $${\displaystyle \mathbf {a} }$$  is a $$(q\times 1)$$ vector, $${\displaystyle \mathbf {b} }$$ is a $$(p\times 1)$$ vector, and $${\displaystyle \mathbf {A} }$$ and $${\displaystyle \mathbf {B} }$$  are $$(q\times p)$$ matrices of constants.

3. **$$\Sigma$$ as a Linear Operator:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}   
    * **Applied to one vector**, the covariance matrix _maps a linear combination_, $$c$$, of the random variables, $$X$$, onto a vector of covariances with those variables:   

    $${\displaystyle \mathbf {c} ^{\rm {T}}\Sigma =\operatorname {cov} (\mathbf {c} ^{\rm {T}}\mathbf {X} ,\mathbf {X} )}$$

    * **Treated as a bilinear form**, it yields the covariance between the two linear combinations:  

    $${\displaystyle \mathbf {d} ^{\rm {T}}\Sigma \mathbf {c} =\operatorname {cov} (\mathbf {d} ^{\rm {T}}\mathbf {X} ,\mathbf {c} ^{\rm {T}}\mathbf {X} )}$$

    * **The variance of a linear combination** is then (its covariance with itself)

    $${\displaystyle \mathbf {c} ^{\rm {T}}\Sigma \mathbf {c} }$$  

    * **The (pseudo-)inverse covariance matrix** provides an _inner product_,  $${\displaystyle \langle c-\mu \|\Sigma ^{+}\| c-\mu \rangle }$$  which induces the _Mahalanobis distance_, a measure of the "unlikelihood" of $$c$$.


4. **Applications [Examples]:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents44}   
    1. [**The Whitening Transformation:**](https://en.wikipedia.org/wiki/Whitening_transformation) allows one to completely decorrelate the data,  Equivalently,  
    allows one to find an optimal basis for representing the data in a compact way.

    2. [**Rayleigh Quotient:**](https://en.wikipedia.org/wiki/Rayleigh_quotient)

    2. [**Principle Component Analysis [PCA]**](https://en.wikipedia.org/wiki/Principal_components_analysis)

    2. [**The Karhunen-Lo√®ve transform (KL-transform)**](https://en.wikipedia.org/wiki/Karhunen-Lo%C3%A8ve_transform)

    2. [**Mutual fund separation theorem**](https://en.wikipedia.org/wiki/Mutual_fund_separation_theorem)

    5. [**Capital asset pricing model**](https://en.wikipedia.org/wiki/Capital_asset_pricing_model)

    7. [**Portfolio Theory:**](https://en.wikipedia.org/wiki/Modern_portfolio_theory) The matrix of covariances among various assets' returns is used to determine, under certain assumptions, the relative amounts of different assets that investors should (in a normative analysis) or are predicted to (in a positive analysis) choose to hold in a context of diversification.
    <br>

***

## Positive Semi-Definite Matrices
{: #content5}

1. **Definition:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents51}  
    A *__symmetric__* $${\displaystyle n\times n}$$ *__real__* matrix $${\displaystyle M}$$ is said to be __positive semi-definite__ if the scalar $${\displaystyle z^{\textsf {T}}Mz}$$ is *__non-negative__* for *__every non-zero__* column vector $${\displaystyle z}$$  of $$n$$ real numbers.  

    __Mathematically:__  
    <p>$$M \text { positive semi-definite } \Longleftrightarrow \quad x^{\top} M x \geq 0 \text { for all } x \in \mathbb{R}^{n}$$</p>  
    <br>

2. **Properties:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents52}   
    * $$AA^T$$ and $$A^TA$$ are PSD  
    * $$M$$ is positive definite if All pivots > 0      
    * $$M$$ is positive definite if and only if __all of its eigenvalues are non-negative__.  
    * __Covariance Matrices $$\Sigma$$__ are PSD  
    <br>



***

## Positive Definite Matrices
{: #content6}

1. **Definition:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents61}    
    A *__symmetric__* $${\displaystyle n\times n}$$ *__real__* matrix $${\displaystyle M}$$ is said to be __positive definite__ if the scalar $${\displaystyle z^{\textsf {T}}Mz}$$ is strictly *__positive__* for *__every non-zero__* column vector $${\displaystyle z}$$  of $$n$$ real numbers.  


    __Mathematically:__  
    <p>$$M \text { positive definite } \Longleftrightarrow x^{\top} M x>0 \text { for all } x \in \mathbb{R}^{n} \backslash \mathbf{0}$$</p>  


2. **Properties:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents62}  
    * $$M$$ is positive definite if and only if __all of its eigenvalues are positive__  
    * The matrix $${\displaystyle M}$$ is positive definite if and only if the bilinear form $${\displaystyle \langle z,w\rangle =z^{\textsf {T}}Mw}$$ is positive definite  
    * A symmetric matrix $${\displaystyle M}$$ is positive definite if and only if its __quadratic form is a strictly convex function__  
    * Every positive definite matrix is __invertible__ and its inverse is also positive definite.  
    * $$M$$ is positive definite if All pivots > 0  
    * __Covariance Matrices $$\Sigma$$__ are positive definite unless one variable is an exact linear function of the others. Conversely, every positive semi-definite matrix is the covariance matrix of some multivariate distribution  
    * Any quadratic function from $${\displaystyle \mathbb {R} ^{n}}$$ to $${\displaystyle \mathbb {R} }$$ can be written as $${\displaystyle x^{\textsf {T}}Mx+x^{\textsf {T}}b+c}$$ where $${\displaystyle M}$$ is a symmetric $${\displaystyle n\times n}$$ matrix, $$b$$ is a real $$n$$-vector, and $$c$$ a real constant. This quadratic function is strictly convex, and hence has a unique finite global minimum, if and only if $${\displaystyle M}$$ is positive definite.  
    <br>



***

## Orthogonal Matrices
{: #content7}

1. **Definition.**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents71}  
    Orthogonal (or, unitary (complex)) matrices are square matrices, such that the columns form an orthonormal basis.  
    <br>

2. **Properties:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents72}  
    1. If $$U = [u_1, \ldots, u_n]$$ is an orthogonal matrix, then  
        <p>$$u_i^Tu_j = \left\{ \begin{array}{ll} 1 & \mbox{if } i=j, \\  0 & \mbox{otherwise.} \end{array} \right. $$</p>  

    2. $$U^TU = UU^T = I_n$$.  

    3. Easy inverse:  
        <p>$$U^{-1} = U^T$$</p>  

    4. Geometrically, orthogonal matrices correspond to rotations (around a point) or reflections (around a line passing through the origin).  
        > i.e. they preserve length and angles!  
        > Proof. Part 5 and 6.  

    5. For all vectors $$\vec{x}$$,  
        $$ \|Ux\|_2^2 = (Ux)^T(Ux) = x^TU^TUx = x^Tx = \|x\|_2^2 .$$  
        > Known as, _the rotational invariance_ of the Euclidean norm.  
        > Thus, If we multiply x with an orthogonal matrix, the errors present in x will not be magnified. This behavior is very desirable for maintaining numerical stability.  

    6. If $$x, y$$ are two vectors with unit norm, then the angle $$\theta$$ between them satisfies $$\cos \theta = x^Ty$$  
    while the angle $$\theta'$$ between the rotated vectors $$x' = Ux, y' = Uy$$ satisfies $$\cos \theta' = (x')^Ty'$$.  
    Since, $$(Ux)^T(Uy) = x^T U^TU y = x^Ty,$$ we obtain that the angles are the same.  
    <br>


3. **Examples:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents73}  
    * [**Permutation Matrices**](http://livebooklabs.com/keeppies/c5a5868ce26b8125/72793237f1b79da9){: value="show" onclick="iframePopA(event)"}
        <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/72793237f1b79da9">` Visit the Book`</a>
        <div markdown="1"> </div>
    <br>

***

## Dyads
{: #content8}

1. **Definition.**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents81}  
    A matrix $$A \in \mathbf{R}^{m \times n}$$ is a dyad if it is of the form $$A = uv^T$$ for some vectors $$u \in \mathbf{R}^m, v \in \mathbf{R}^n$$.  
    
    The dyad acts on an input vector $$x \in \mathbf{R}^n$$ as follows:  
    <p>$$ Ax = (uv^T) x = (v^Tx) u.$$</p>  
    <br>

2. **Properties:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents82}  
    1. The output always points in the same direction $$u$$ in output space ($$\mathbf{R}^m$$), no matter what the input $$x$$ is.

    2. The output is always a simple scaled version of $$u$$.

    3. The amount of scaling depends on the vector $$v$$, via the linear function $$x \rightarrow v^Tx$$.  
    <br>

3. **Examples:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents83}  
    * [**Single factor model of financial price data**](http://livebooklabs.com/keeppies/c5a5868ce26b8125/438c7a0d0fc50d09){: value="show" onclick="iframePopA(event)"}
        <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/438c7a0d0fc50d09">` Visit the Book`</a>
        <div markdown="1"> </div>

    <br>

4. **Normalized dyads:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents84}  
    :   We can always normalize the dyad, by assuming that both u,v are of unit (Euclidean) norm, and using a factor to capture their scale.  
    :   That is, any dyad can be written in normalized form:  
    :   $$ A = uv^T = (\|u\|_2 \cdot |v|_2 ) \cdot (\dfrac{u}{\|u\|_2}) ( \dfrac{v}{\|v\|_2}) ^T = \sigma \tilde{u}\tilde{v}^T,$$  
    :   where $$\sigma > 0$$, and $$\|\tilde{u}\|_2 = \|\tilde{v}\|_2 = 1.$$

5. **Symmetric dyads:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents85}  
    :    Another important class of symmetric matrices is that of the form $$uu^T$$, where $$u \in \mathbf{R}^n$$.
    The matrix has elements $$u_iu_j$$, and is symmetric.
    > If $$\|u\|_2 = 1$$, then the dyad is said to be normalized.

    :   $$
        uu^T = \left(\begin{array}{ccc} u_1^2  & u_1u_2  & u_1u_3  \\
        u_1u_2 & u_2^2   & u_2u_3  \\
        u_1u_3 & u_2u_3  & u_3^2  \end{array} \right)
        $$  
    :   * **Properties:**
            1. Symmetric dyads corresponds to quadratic functions that are simply squared linear forms:  
            $$q(x) = (u^Tx)^2$$
            2. When the vector $$u$$ is normalized (unit), then:  
            $$\mathbf{Tr}(uu^T) = \|u\|_2^2 = 1^2 = 1$$  
            > This follows from the fact that the diagonal entries of a symmetric dyad are just $$u_i^2, \forall i \in [1, n]$$
            3. 

***

## Correlation matrix
{: #content9}

1. **Definition.**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents91}  
    :   $${\text{corr}}(\mathbf {X} )=\left({\text{diag}}(\Sigma )\right)^{-{\frac {1}{2}}}\,\Sigma \,\left({\text{diag}}(\Sigma )\right)^{-{\frac {1}{2}}}$$

2. **Properties:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents92}  
    0. It is the matrix of "Pearson product-moment correlation coefficients" between each of the random variables in the random vector $${\displaystyle \mathbf {X} }$$.

    1. The correlation matrix can be seen as the covariance matrix of the standardized random variables $${\displaystyle X_{i}/\sigma (X_{i})}$$ for $${\displaystyle i=1,\dots ,n}$$.

    2. Each element on the principal diagonal of a correlation matrix is the correlation of a random variable with itself, which always equals 1.

    3. Each off-diagonal element is between 1 and ‚Äì1 inclusive.



***
***

TITLE: NO-TITLELINK: research/numerical_optimization_128a/mt/printme4.md


**4.2/**{: style="font-size: 250%; color: red; font: italic bold 50px/70px Georgia, serif"}  

____________________
***


## Extrapolation
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} \\
    * Extrapolation is used to 
        <xmp>
        </xmp>
    * Extrapolation can be applied whenever 
        <xmp>
        </xmp>
    * Suppose that for each number $$h \neq 0$$ we have a formula $$N_1(h)$$ that approximates an
    unknown constant $$ \ \ \ \ \ \ \ \ $$, and that the truncation error involved with the approximation has the
    form,  
    $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ 
    * The **truncation error** is $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  
        where,  
        (1)  
        (2)  

    and, in general,  
    $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ 
    * The object of extrapolation is 
        <xmp>

        </xmp>

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    1.  
        <xmp>

        </xmp>

    2.  
        <xmp>

        </xmp>



3. **The $$\mathcal{O}(h)$$ formula for approximating $$M$$:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    > **The First Formula:**  
    <xmp>

    </xmp>


    > **The Second Formula:**  
    <xmp>

    </xmp>
 

4. **The $$\mathcal{O}(h^2)$$ approximation formula for M:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    <xmp>


    </xmp>

    * **Derivation:**  
        <xmp>



        </xmp>


5. **When to apply Extrapolation?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    (1)  
    <xmp>
    </xmp>
    (2)  
    <xmp>
    </xmp>

6. **The $$\mathcal{O}(h^4)$$ formula for approximating $$M$$:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16} \\
    <xmp>

    </xmp>


7. **The $$\mathcal{O}(h^6)$$ formula for approximating $$M$$:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17} \\
    <xmp>

    </xmp>

8. **The $$\mathcal{O}(h^{2j})$$ formula for approximating $$M$$:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents18} \\
    <xmp>


    </xmp>

    * **Derivation:**  
        <xmp>



        </xmp>


9. **The Order the Approximations Generated:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents19} \\
    <xmp>


    </xmp>

9. **How to actually calculate a derivative using the Extrapolation formula:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents1000} \\
    <xmp>

    </xmp>

***

## Deriving n-point Formulas with Extrapolation
{: #content2}

1. **Deriving Five-point Formula:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    <xmp>

    </xmp>

***
***

**4.3/**{: style="font-size: 250%; color: red; font: italic bold 50px/70px Georgia, serif"}  

____________________
***

## Numerical Quadrature
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    <xmp> </xmp>

2. **How?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} 
    <xmp> </xmp>

3. **Based on:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} 
    <xmp> </xmp>

4. **Method:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} 
    <xmp>


    </xmp>

    * **Derivation:**  
        <xmp>





        </xmp>


5. **The Quadrature Formula:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    <xmp>
    </xmp>


6. **The Error:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16} \\
    <xmp>
    </xmp>

***

## The Trapezoidal Rule
{: #content2}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}
    <xmp>

    </xmp>


1. **Precision**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents200}
    <xmp>
    </xmp>

2. **The Trapezoidal Rule:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    <xmp>


    </xmp>

    * **Derivation:**  
        <xmp>


        </xmp>

3. **Error:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    <xmp>

    </xmp>

***

## Simpson‚Äôs Rule
{: #content3}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    <xmp>

    </xmp>

2. **Simpson's Rule:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    <xmp>

    </xmp>

    * **Derivation:**  
        <xmp>




        </xmp>


1. **Precision**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents200}
    <xmp>
    </xmp>

3. **Error:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    <xmp>

    </xmp>

***

## Measuring Precision
{: #content4}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} \\
    <xmp>

    </xmp>

2. **Precision [degree of accuracy]:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} \\
    <xmp>

    </xmp>

3. **Precision of Quadrature Formulas:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents43} \\
    * The degree of precision of a quadrature formula is $$ \mathcal{O}$$
        <xmp>
        </xmp>
    * The Trapezoidal and Simpson‚Äôs rules are examples of $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  
    * **Types of Newton-Cotes formulas:** 
        <xmp> </xmp>
***

## Closed Newton-Cotes Formulas
{: #content5}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents51} \\
    <xmp>
    </xmp>

    * **It is called closed because:**  
        <xmp>
        </xmp>

2. **Form of the Formula:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents52} \\
    <xmp>

    </xmp>

    > where,  
        <xmp>
        </xmp>

3. **The Error Analysis:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents53} \\
    <xmp>

    </xmp>


4. **Degree of Preceision:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents54} \\
    * **Even-n:** the degree of precision is $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  
    * **Odd-n:** the degree of precision is 

5. **Closed Form Formulas:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents55} \\
    * **$$n = 1$$: $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  rule**  
        <xmp>
        
        </xmp>
    * **$$n = 2$$: $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  rule**   
        <xmp>
        
        </xmp>
    * **$$n = 3$$: $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  rule**   
        <xmp>
        
        </xmp>
    * **n = 4:**  
        <xmp>

        </xmp>

***

## Open Newton-Cotes Formulas
{: #content6}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents61} \\
    * They $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  
    * They use 
        <xmp>
        </xmp>
    * This implies that 
        <xmp>
        </xmp>
    * Open formulas contain $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ 

2. **Form of the Formula:**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents62} \\
    <xmp>

    </xmp>
    > where,  
        <xmp>
        </xmp>

3. **The Error Analysis:**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents63} \\
    <xmp>

    </xmp>

4. **Degree of Preceision:**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents64} \\
    * **Even-n:** 
    * **Odd-n:** 
5. **Open Form Formulas:**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents65} \\
    * **$$n = 0$$:** [PUT NAME HERE]  
        <xmp>
        </xmp>
    * **$$n = 1$$:**   
        <xmp>
        </xmp>
    * **$$n = 2$$:**   
        <xmp>
        </xmp>
    * **n = 3:**  
        <xmp>
        </xmp>


***
***

**4.4/**{: style="font-size: 250%; color: red; font: italic bold 50px/70px Georgia, serif"}  

____________________
***

## Composite Rules
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} \\
    <xmp>
    </xmp>

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    1.  
        <xmp>
        </xmp>
    2.  
        <xmp>
        </xmp>
    3.  
        <xmp>
        </xmp>

3. **Notice:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13}
    <xmp>
    </xmp>

***

## Composite Simpson‚Äôs rule
{: #content2}

1. **Composite Simpson‚Äôs rule:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    <xmp>

    </xmp>

2. **Error in Comoposite Simpson's Rule:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    <xmp>
    </xmp>
    > **Error:**  

3. **Theorem [Rule and Error]:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    <xmp>

    </xmp>

4. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    <xmp>

    </xmp>

***

## Composite Newton-Cotes Rules
{: #content3}

1. **Composite Trapezoidal rule:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}  
    <xmp>

    </xmp>

2. **Composite Midpoint rule:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32}  
    <xmp>

    </xmp>

***

## Round-Off Error Stability
{: #content4}

1. **Stability Property:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} \\
    <xmp>

    </xmp>

2. **Proof:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} \\
    <xmp>


    </xmp>

***
***

**4.5/**{: style="font-size: 250%; color: red; font: italic bold 50px/70px Georgia, serif"}  

____________________
***

## Main Idea
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    <xmp>

    </xmp>

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    <xmp>
    </xmp>
3. **Error in Composite Trapezoidal rule:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    <xmp>

    </xmp>
    > This implies that $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ 

4. **Extrapolation Formula:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    > Extrapolation then is used to produce $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ $$  approximations by  
        <xmp>
        </xmp>

    > and according to this table,  
        <xmp>
        </xmp>

    > Calculate the Romberg table this way:


5. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    <xmp>




    </xmp> 


***
***

**4.6/**{: style="font-size: 250%; color: red; font: italic bold 50px/70px Georgia, serif"}  

____________________
***

## Main Idea
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   1.  
            <xmp>
            </xmp>
        2.  
            <xmp>
            </xmp>
2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    <xmp>
    </xmp>

3. **Approximation Formula:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13}  
    $$\int_{a}^{b} f(x) dx = $$  
        <xmp>
        </xmp>

    * **Derivation:**  
        <xmp>


        </xmp>

4. **Error Bound:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    * **Error relative to *Composite Approximations*:**  
        <xmp>
        </xmp>
    * **Error relative to *True Value*:**  
        <xmp>
        </xmp>

    * **ERROR DERIVATION:**  
        <xmp>


        </xmp> 

    > This implies 
        <xmp>
        </xmp>

5. **Procedure:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    > When the approximations in (4.38) 
        <xmp>
        </xmp>
    
    > Then we use the error estimation procedure to 
        <xmp>
        </xmp>  

    > If the approximation on one of the subintervals fails to be within the tolerance $$\ \ \ \ \ \ \ \ $$, then
        <xmp>
        </xmp>

7. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17} \\
    <xmp>


    </xmp>

8. **Derivation:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents18} \\
    <xmp>


    </xmp>

***
***

**4.7/**{: style="font-size: 250%; color: red; font: italic bold 50px/70px Georgia, serif"}  

____________________
***

## Main Idea
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    :   1. 
            <xmp>
            </xmp>
        2.  
            <xmp>

            </xmp>

        3.  
            <xmp>

            </xmp>  

    :   * 
        > **To Measure Accuracy:** 
            <xmp>

            </xmp>

    :   *  
        > The **Coefficients** $$c_1, c_2, ... , c_n$$ in the approximation formula are $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ ,   
        and,   
        The **Nodes** $$x_1, x_2, ... , x_n$$ are restricted by/to  $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  
        This gives,  
        **The number of Parameters** to choose is $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ 

    :   *  
        > If the coefficients of a polynomial are considered parameters, 
            <xmp>
            </xmp>  
        > This, then, is 
            <xmp>
            </xmp>

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} 
    <xmp>
    </xmp>

***

## Legendre Polynomials
{: #content2}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} 
    <xmp>
    </xmp>

9. **Why?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents29} 
    <xmp>

    </xmp>

2. **Properties:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    1.  
    2.  
    3. The roots of these polynomials are: 
        *   
        *   
        *   
        *   
        *   

3. **The first Legendre Polynomials:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    $$
    P_0(x) = \ \ \ \ \ , \ \ \ \ \ \ \ \ \ \ \ \ \ \  P_1(x) =  \ \ \, \ \ \ \ \ \ \ \ \ \ \ \ \ \  P_2(x) = \ \ \ \ \ \ \ \ \ \ ,
    $$  
    $$
    P_3(x) = \ \ \ \ \ \ \ \  ,\ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \   P_4(x) = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
    $$
4. **Determining the nodes:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    <xmp>

    </xmp>

    * **PROOF:**  
        <xmp>

        </xmp>

    > The nodes $$x_1, x_2, ... , x_n$$ needed to
        <xmp>
        </xmp>

***

## Gaussian Quadrature on Arbitrary Intervals
{: #content3}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    <xmp>
    </xmp>

2. **The Change of Variables:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    <xmp>
    </xmp>

3. **Gaussian quadrature [arbitrary interval]:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    <xmp>
    
    </xmp>


***
***

**4.8/**{: style="font-size: 250%; color: red; font: italic bold 50px/70px Georgia, serif"}  

____________________
***

## Approximating Double Integral
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    <xmp>
    </xmp>

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    <xmp>
    </xmp>

3. **Comoposite Trapezoidal Rule for Double Integral:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    $$ \ \  \iint_R f(x,y) \,dA \  = $$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  
    <xmp>

    </xmp>

    * **DERIVATION:**  
        <xmp>


        </xmp>

4. **Comoposite Simpsons' Rule for Double Integral:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    * **Rule:**  
        <xmp>

        </xmp>

    * **Error:**  
        <xmp>

        </xmp>

    * **Derivation:**  
        <xmp>


        </xmp>

***

## Gaussian Quadrature for Double Integral Approximation
{: #content2}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}
    <xmp>

    </xmp>

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22}
    <xmp>
    </xmp>
3. **Example:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    <xmp>

    </xmp>

***

## Non-Rectangular Regions
{: #content3}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}
    <xmp>
    </xmp>

    **Form:**  
        <xmp>
        </xmp>  
        or,  
        <xmp>
        </xmp>

2. **How?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    * We use  
    * Step Size:
        * **x:**  
        * **y:**   

3. **Simpsons' Rule for Non-Rect Regions:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    <xmp>


    </xmp>

4. **Simpsons' Double Integral [Algorithm]:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34} \\
    <xmp>


    </xmp>

5. **Gaussian Double Integral [Algorithm]:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents35} \\
    <xmp>



    </xmp>

***

## Triple Integral Approximation
{: #content4}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41}
    :   * **On what?**  
    :   * **Form:**  
            <xmp>

            </xmp>

2. **Gaussian Triple Integral [Algorithm]:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} \\
    <xmp>


    </xmp>


***
***

TITLE: NO-TITLELINK: research/numerical_optimization_128a/mt/printme1.md


**1.2/**{: style="font-size: 250%; color: red; font: italic bold 50px/70px Georgia, serif"}  

____________________
***

## Binary Machine Numbers
{: #content1}

1. **Representing Real Numbers:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    :   A $$\ \ \ \ \ \ \ \ \ \ \ \ \$$ (binary digit) representation is used for a real number. 
    > * The **first bit** is  
    > * Followed by:  
    > * and a $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \$$, called the  
    > * The base for the exponent is   
2. **Floating-Point Number Form:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    <xmp>

    </xmp>

3. **Smallest Normalized positive Number:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} 
    * **When:**  
    * **Equivalent to:**  

4. **Largest Normalized positive Number:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    * **When:**  
    * **Equivalent to:**  

5. **UnderFlow:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15}
    * **When numbers occurring in calculations have**  

6. **OverFlow:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16}
    * **When numbers occurring in calculations have**

7. **Representing the Zero:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17} \\
    * There are $$ \ \ \ \ \ $$ Representations of the number zero:
        <xmp>

        </xmp>

***

## Decimal Machine Numbers
{: #content2}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} 
    <xmp>

    </xmp>

2. **(k-digit) Decimal Machine Numbers:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22}
    <xmp>

    </xmp>

3. **Normalized Form:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} 
    <xmp>

    </xmp>

4. **Floating-Point Form of a Decimal Machine Number:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    * The floating-point form of y, denoted $$f_l(y)$$, is obtained by:
        <xmp>

        </xmp>

5. **Termination:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\
    There are two common ways of performing this termination:  
    1. **$$ \ \ \ \ \ \ \ \ \ \ $$:** 
        <br>
        > This produces the floating-point form:  

    2. **$$ \ \ \ \ \ \ \ \ \ \ $$:** $$ \ \ $$ which 
        <br>

        > This produces the floating-point form:   
        >   > For rounding, when $$d_{k+1} \geq 5$$, we  
        >   > When $$d_{k+1} < 5$$, we  
        >   > If we round down, then $$\delta_i =$$   
        >   > However, if we round up,  

6. **Approximation Errors:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26} \\
    <xmp>

    </xmp>
    * **The Absolute Error:** $$ \ \ \ \ \ \ \ $$.  

    * **The Relative Error:** $$ \ \ \ \ \ \ \ $$.

7. **Significant Digits:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27} \\
    <xmp>

    </xmp>

8. **Error in using Floating-Point Repr.:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents28} \\
    * **Chopping:**{: style="color: green"}  
        **The Relative Error =**   
        **The Machine Repr**. [for k decimial digits] =  
        :   $$ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$.  

        $$ \implies $$  
        <xmp>

        </xmp>
        **Bound** $$ \ \ \ \implies \ \  \ \ \ \ \ \ \ \ \ \$$.

    * **Rounding:**{: style="color: green"}  
        > In a similar manner, a bound for the relative error when using k-digit rounding arithmetic is   

        **Bound** $$ \ \ \ \implies \ \  \ \ \ \ \ \ \ \ \ \ \ \ $$.

9. **Distribution of Numbers:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents29} \\
    The number of decimal machine numbers in $$\ \ \ \ \ \ \ \ \ \ \ $$ is  $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ for   

***

## Finite-Digit Arithmetic
{: #content3}

1. **Values:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}
    :   $$ x = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$
    :   $$ y = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$

2. **Operations:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    <xmp>

    </xmp>

3. **Error-producing Calculations:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    * First:  
        <xmp>

        </xmp>
    * Second:  
        <xmp>

        </xmp>

4. **Avoiding Round-Off Error:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34} \\
    * First:  
        <xmp>

        </xmp>
    * Second:  
        <xmp>

        </xmp>   
        $$ 
        \implies \ \ \ \ \ \ \ \ \ \ \ \ \ \  x_1 = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ , \ \ \ \ \ \ \ \ \ \ \ \ \ \  
        x_2 = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
        $$

***

## Nested Arithmetic
{: #content4}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} 
    <xmp>

    </xmp>   
    > Remember that chopping (or rounding) is performed:   

    <br>
    *  $$  \ \ \ \ \ \ \$$
    
    <br>

    > Polynomials should always be expressed $$  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ , becasue, $$  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} 
    <xmp>

    </xmp>  

***

**1.3/**{: style="font-size: 250%; color: red; font: italic bold 50px/70px Georgia, serif"} 

____________________
***

## Main Idea
{: #content1}

1. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    <xmp>
    </xmp>

***

## Characterizing Algorithms
{: #content2}

1. **Stability:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    * **Stable Algorithm:** 
    <br>
    <br>
    * **Conditionally Stable Algorithm:**
    <br>
    <br>
2. **Error Growth:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    <xmp>

    </xmp>
3. **Stability and Error-Growth:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    * **Stable Algorithm:**  
    * **UnStable Algorithm:** 

***

## Rates of Convergence
{: #content3}

1. **Rate of Convergence:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    <xmp>

    </xmp>
    > $$\beta_n \  = \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ , \ \ \ \ $$ for 

2. **Big-Oh Notation:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    <xmp>

    </xmp>


***
***

TITLE: NO-TITLELINK: research/numerical_optimization_128a/mt/printme2.md



**2.2/**{: style="font-size: 250%; color: red; font: italic bold 50px/70px Georgia, serif"} 

____________________
***

## Fixed-Point Problems
{: #content1}

1. **Fixed Point:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    <xmp>

    </xmp>

2. **Root-finding problems and Fixed-point problems:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    > Root Finding and Fixed-point problems are  
    <xmp>
    </xmp>
    <xmp>
    </xmp>

3. **Why?:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    <xmp>

    </xmp>

4. **Existence and Uniqueness of a Fixed Point.:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    <xmp>

    </xmp>

***

## Fixed-Point Iteration
{: #content2}

1. **Approximating Fixed-Points:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    <xmp>
    </xmp>
2. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    <xmp>
    </xmp>       
    <xmp>
    </xmp>

3. **Convergence:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23}
    * **Fixed-Point Theorem:** \\
        <xmp>
        </xmp>
    * **Error bound in using $$p_n$$ for $$p$$:** \\
        <xmp>
        </xmp>

        > Notice: \\
            <xmp>
            </xmp>
4. **Using Fixed-Points:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    > **Question**: $$ \ \ \ \ \ $$ 
        <xmp>
        </xmp>
    > **Answer**:     
        <xmp>
        </xmp>

5. **Newton's Method as a Fixed-Point Problem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\
    <xmp>
    </xmp>
    <xmp>
    </xmp>

***
***


**2.3/**{: style="font-size: 250%; color: red; font: italic bold 50px/70px Georgia, serif"} 

____________________
***

## Newton‚Äôs Method
{: #content1}

1. **What?:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    * **Newton‚Äôs (or the Newton-Raphson) method is**:
        <xmp>
        </xmp>
    *   <br /> 
        <xmp>
        </xmp>
    *   <br /> 
        <xmp>
        </xmp>

2. **Derivation:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    <xmp>
    </xmp>
    <xmp>
    </xmp>
    <xmp>
    </xmp>
3. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    <xmp>
    </xmp>
    <xmp>
    </xmp>
    <xmp>
    </xmp>

4. **Stopping Criterions:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    <xmp>
    </xmp>

***

## Convergence using Newton‚Äôs Method
{: #content2}

1. **Convergence Theorem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    **Theorem:**  \\
    <xmp>
    </xmp>
    > The crucial assumption is
        <xmp>
        </xmp>

    <br>

    > Theorem 2.6 states that, \\
    > (1)  \\
        <xmp>
        </xmp>
    > (2) 
        <xmp>
        </xmp>

## The Secant Method
{: #content3}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    In Newton's Method \\
    We approximate $$f'( p_n‚àí1)$$ as:\\
        <xmp>
        </xmp>
    To produce: \\
        <xmp>
        </xmp>

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    > **$$\ \ \ \ \ \ \ \ \ $$**: 
        <xmp>
        </xmp>
    >   > Frequently, 
            <xmp>
            </xmp>

    > Note: 
        <xmp>
        </xmp>

3. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    <xmp>
    </xmp>
    <xmp>
    </xmp>


4. **Convergence Speed:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34} \\
    <xmp>
    </xmp>

***

## The Method of False Position
{: #content4}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} \\
    <xmp>
    </xmp>

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} \\
    <xmp>
    </xmp>
3. **Method:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents43} \\
    <xmp>
    </xmp>
    <xmp>
    </xmp>
4. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents44} \\
    <xmp>
    </xmp>
    <xmp>
    </xmp>

***
***


**2.4/**{: style="font-size: 250%; color: red; font: italic bold 50px/70px Georgia, serif"} 

____________________
***

## Order of Convergence 
{: #content1}

1. **Order of Convergence:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} \\
    <xmp>
    </xmp>
2. **Important, Two cases of order:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    <xmp>
    </xmp>
3. **An arbitrary technique that generates a convergent sequences does so only linearly:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    <xmp>
    </xmp>
    > Theorem 2.8 implies 
        <xmp> </xmp>

4. **Conditions to ensure Quadratic Convergence:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    <xmp>
    </xmp>

5. **Theorems 2.8 and 2.9 imply:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    (i)\\
        <xmp>
        </xmp>
    (ii)\\
        <xmp>
        </xmp>

5. **Newtons' Method Convergence Rate:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    <xmp>
    </xmp>
    <xmp>
    </xmp>

## Multiple Roots 
{: #content2}

1. **Problem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    <xmp>
    </xmp>

2. **Zeros and their Multiplicity:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    <xmp>

    </xmp>

3. **Identifying Simple Zeros:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    * **Theorem:**  
    <xmp>
    </xmp>
    * **Generalization of Theorem 2.11:**
        <xmp>
        </xmp>

        > The result in Theorem 2.12 implies 
            <xmp>

            </xmp>

4. **Why Simple Zeros:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    <xmp>
    </xmp>
    > Example:
        <xmp>
        </xmp>


5. **Handling the problem of multiple roots:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\
    * We $$  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \$$  
    * We define $$\ \ \ \ \ \ \ \ \ \ $$ as: \\
        <xmp>
        </xmp>

    * **Derivation:**  
        <xmp>
        </xmp>

    * **Properties:**
        * 
            <xmp>
            </xmp>
        * 
            <xmp>
            </xmp>
        * 
            <xmp>
            </xmp>
        *   
            <xmp>
            </xmp>

***
***


**2.5/**{: style="font-size: 250%; color: red; font: italic bold 50px/70px Georgia, serif"} 

____________________
***

## Aitken‚Äôs $$ \Delta^2 $$ Method 
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} \\
    <xmp>
    </xmp>

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    <xmp>
    </xmp>

0. **Derivation:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents100} \\
    <xmp>


    </xmp>

3. **Del [Forward Difference]:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    <xmp>
    </xmp>

4. **$$\hat{p}_n$$ [Formula]:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    <xmp>
    </xmp>

5. **Generating the Sequence [Formula]:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    <xmp>
    </xmp>


## Steffensen‚Äôs Method
{: #content2}

1. **What?:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} 
    <xmp>

    </xmp>

2. **Zeros and their Multiplicity:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    <xmp>
    </xmp>

3. **Difference from Aitken's method:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    * **Aitken's method:**  
        <xmp>
        </xmp>
    * **Steffensen‚Äôs method:**  
        <xmp>

        </xmp>

    > Notice \\
        <xmp>
        </xmp>

4. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    <xmp>
    </xmp>

5. **Convergance of Steffensen‚Äôs Method:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\
    <xmp>
    </xmp>

***
***


**2.6/**{: style="font-size: 250%; color: red; font: italic bold 50px/70px Georgia, serif"} 

____________________
***

## Algebraic Polynomials
{: #content1}

1. **Fundamental Theorem of Algebra:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} \\
    <xmp>
    </xmp>
2. **Existance of Roots:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    <xmp>
    </xmp>
3. **Polynomial Equivalence:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    <xmp>
    </xmp>
    > This result implies 
        <xmp>
        </xmp>


## Horner‚Äôs Method
{: #content2}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    <xmp>

    </xmp>

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    <xmp>
    </xmp>

3. **Horner's Method:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    <xmp>
    </xmp>    
    <xmp>
    </xmp>

4. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    <xmp>
    </xmp>
    <xmp>

    </xmp>

5. **Horner's Derivatives:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\
    <xmp>
    </xmp>

6. **Deflation:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26} \\
    <xmp>
    </xmp>

5. **MatLab Implementation:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\
    <xmp>
    </xmp>

## Complex Zeros: M√ºller‚Äôs Method
{: #content3}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    * **It is a:** 
        <xmp>

        </xmp>
    * M√ºller‚Äôs method uses
        <xmp>

        </xmp>


2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    1. **First:**  
        <br>
    2. **Second:**  
        <br>
        > If the initial approximation is a real number, 

3. **Complex Roots:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    <xmp>
    </xmp>

4. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34} \\
    <xmp>


    </xmp>
    
5. **Calculations and Evaluations:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents35} \\
    **M√ºller‚Äôs method can:**  
        <xmp>
        </xmp>


***
***

## Binary Machine Numbers
{: #content1}

1. **Representing Real Numbers:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    :   A $$\ \ \ \ \ \ \ \ \ \ \ \ \$$ (binary digit) representation is used for a real number. 
    > * The **first bit** is  
    > * Followed by:  
    > * and a $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \$$, called the  
    > * The base for the exponent is   
2. **Floating-Point Number Form:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    <xmp>

    </xmp>

3. **Smallest Normalized positive Number:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} 
    * **When:**  
    * **Equivalent to:**  

4. **Largest Normalized positive Number:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    * **When:**  
    * **Equivalent to:**  

5. **UnderFlow:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15}
    * **When numbers occurring in calculations have**  

6. **OverFlow:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16}
    * **When numbers occurring in calculations have**

7. **Representing the Zero:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17} \\
    * There are $$ \ \ \ \ \ $$ Representations of the number zero:
        <xmp>

        </xmp>

***

## Decimal Machine Numbers
{: #content2}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} 
    <xmp>

    </xmp>

2. **(k-digit) Decimal Machine Numbers:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22}
    <xmp>

    </xmp>

3. **Normalized Form:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} 
    <xmp>

    </xmp>

4. **Floating-Point Form of a Decimal Machine Number:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    * The floating-point form of y, denoted $$f_l(y)$$, is obtained by:
        <xmp>

        </xmp>

5. **Termination:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\
    There are two common ways of performing this termination:  
    1. **$$ \ \ \ \ \ \ \ \ \ \ $$:** 
        <br>
        > This produces the floating-point form:  

    2. **$$ \ \ \ \ \ \ \ \ \ \ $$:** $$ \ \ $$ which 
        <br>

        > This produces the floating-point form:   
        >   > For rounding, when $$d_{k+1} \geq 5$$, we  
        >   > When $$d_{k+1} < 5$$, we  
        >   > If we round down, then $$\delta_i =$$   
        >   > However, if we round up,  

6. **Approximation Errors:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26} \\
    <xmp>

    </xmp>
    * **The Absolute Error:** $$ \ \ \ \ \ \ \ $$.  

    * **The Relative Error:** $$ \ \ \ \ \ \ \ $$.

7. **Significant Digits:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27} \\
    <xmp>

    </xmp>

8. **Error in using Floating-Point Repr.:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents28} \\
    * **Chopping:**{: style="color: green"}  
        **The Relative Error =**   
        **The Machine Repr**. [for k decimial digits] =  
        :   $$ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$.  

        $$ \implies $$  
        <xmp>

        </xmp>
        **Bound** $$ \ \ \ \implies \ \  \ \ \ \ \ \ \ \ \ \$$.

    * **Rounding:**{: style="color: green"}  
        > In a similar manner, a bound for the relative error when using k-digit rounding arithmetic is   

        **Bound** $$ \ \ \ \implies \ \  \ \ \ \ \ \ \ \ \ \ \ \ $$.

9. **Distribution of Numbers:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents29} \\
    The number of decimal machine numbers in $$\ \ \ \ \ \ \ \ \ \ \ $$ is  $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ for   

***

## Finite-Digit Arithmetic
{: #content3}

1. **Values:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}
    :   $$ x = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$
    :   $$ y = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$

2. **Operations:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    <xmp>

    </xmp>

3. **Error-producing Calculations:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    * First:  
        <xmp>

        </xmp>
    * Second:  
        <xmp>

        </xmp>

4. **Avoiding Round-Off Error:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34} \\
    * First:  
        <xmp>

        </xmp>
    * Second:  
        <xmp>

        </xmp>   
        $$ 
        \implies \ \ \ \ \ \ \ \ \ \ \ \ \ \  x_1 = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ , \ \ \ \ \ \ \ \ \ \ \ \ \ \  
        x_2 = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
        $$

***

## Nested Arithmetic
{: #content4}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} 
    <xmp>

    </xmp>   
    > Remember that chopping (or rounding) is performed:   

    <br>
    *  $$  \ \ \ \ \ \ \$$
    
    <br>

    > Polynomials should always be expressed $$  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ , becasue, $$  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} 
    <xmp>

    </xmp>  

***

**1.3/**{: style="font-size: 250%; color: red; font: italic bold 50px/70px Georgia, serif"} 

____________________
***

## Main Idea
{: #content1}

1. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    <xmp>
    </xmp>

***

## Characterizing Algorithms
{: #content2}

1. **Stability:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    * **Stable Algorithm:** 
    <br>
    <br>
    * **Conditionally Stable Algorithm:**
    <br>
    <br>
2. **Error Growth:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    <xmp>

    </xmp>
3. **Stability and Error-Growth:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    * **Stable Algorithm:**  
    * **UnStable Algorithm:** 

***

## Rates of Convergence
{: #content3}

1. **Rate of Convergence:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    <xmp>

    </xmp>
    > $$\beta_n \  = \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ , \ \ \ \ $$ for 

2. **Big-Oh Notation:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    <xmp>

    </xmp>


***
***

TITLE: NO-TITLELINK: research/numerical_optimization_128a/mt/printme3.md


**3.3/**{: style="font-size: 250%; color: red; font: italic bold 50px/70px Georgia, serif"} 

____________________
***

## Divided Differences
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    <xmp>
    </xmp>

2. **Form of the Polynomial:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    * $$P_n(x) = $$
    <br>
    <br>
    * **Evaluated at $$x_0$$:** 
    <br>

    * **Evaluated at $$x_1$$:** 
    <br>

    * > $$\implies   a_1 = \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$.
3. **The divided differences:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    * **The *zeroth* divided difference** of the function f with respect to $$x_i$$:
        * **Denoted:** 
        * **Defined:** 
        * > $$f[x_i] =  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$.
    * > The remaining divided differences are defined: 

    * **The *first* divided difference** of $$f$$ with respect to $$x_i$$ and $$x_{i+1}$$:
        * **Denoted:** 
        * **Defined:** 
            <xmp>

            </xmp>
    * **The *second* divided difference** of $$f$$ with respect to $$x_i$$, $$x_{i+1}$$ and $$x_{i+2}$$:
        * **Denoted:** 
        * **Defined:** 
            <xmp>

            </xmp>
    * **The *Kth* divided difference** of $$f$$ with respect to $$x_i$$, $$x_{i+1},...,x_{i+k-1},x_{i+k}$$:
        * **Denoted:** 
        * **Defined:** 
            <xmp>

            </xmp>
    * > The process ends with 
    * **The *nth* divided difference** of $$f$$ with respect to $$x_i$$, $$x_{i+1},...,x_{i+k-1},x_{i+k}$$:
        * **Denoted:** 
        * **Defined:** 
            <xmp>
            </xmp>

4. **The Interpolating Polynomial:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    $$P_n(x) = $$

5. **Newton‚Äôs Divided Difference:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    $$P_n(x) = $$
    <br>
    <br>
    > The value of $$f[x_0,x_1,...,x_k]$$ is 
        <xmp> </xmp>

6. **Generation of Divided Differences:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16} \\
    <xmp>

    </xmp>

7. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17} \\
    <xmp>
    </xmp>
    <xmp>

    </xmp>

***

## Forward Differences
{: #content2}

1. **Forward Difference:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    <xmp>

    </xmp>

2. **The divided differences (with del notation):**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    <xmp>
    </xmp>  
    $$
    f[x_0,x_{1}] = \\
    f[x_0,x_{1},x_2] = \\
    $$
    and in general,  
    $$
    \\
    f[x_0,x_{1},...,x_{k-1},x_{k}] \\
    $$
    <xmp>
    </xmp>

3. **Newton Forward-Difference Formula:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    <xmp>

    </xmp>  

***

## Backward Differences
{: #content3}

1. **Backward Difference:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    <xmp>

    </xmp>

2. **The divided differences:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    <xmp>
    </xmp>

    > and in general, 

    <xmp>
    </xmp>

    > Consequently, the Interpolating Polynomial \\
    <xmp>

    </xmp>

    > If we extend the binomial coefficient notation to 
    >     <xmp> </xmp>
    <xmp>

    </xmp>
    > then \\
    <xmp>

    </xmp>

3. **Newton Backward‚ÄìDifference Formula:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    <xmp>

    </xmp>

## Centered Differences
{: #content4}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} 
    <xmp> </xmp>
2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} 
    <xmp>
    </xmp>
3. **Stirling's Formula:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents43} \\
    * **If $$n = 2m + 1$$ is odd:** 
        <xmp>
        </xmp>
    * **If $$n = 2m$$ is even:** [we use the same formula but delete the last line]
        <xmp>
        </xmp>
4. **Table of Entries:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents44} \\
    <xmp>
    
    </xmp>


***
***

TITLE: 6.3 <br /> TITLE
LINK: research/numerical_optimization_128a/6/6.3.md


## FIRST
{: #content1}









***

## SECOND
{: #content2}









***

## THIRD
{: #content3}









***

## FOURTH
{: #content4}










***
***

TITLE: 6.2 <br /> TITLE
LINK: research/numerical_optimization_128a/6/6.2.md


## FIRST
{: #content1}

$$
y' = -y + ty^{1/2}, 2 \leq t \leq 3, y(2) = 2, $$ with $$ h = 0.25 $$ \\
$$
\dfrac{d}{dy}f(t,y) = \dfrac{d}{dy} (-y + ty^{1/2}) =  \dfrac{t}{2 \cdot \sqrt{y}} - 1 \\
\implies \vert \dfrac{d}{dy}f(t,y) \vert = \vert \dfrac{t}{2\cdot \sqrt{y}} - 1 \vert, \\ 
$$  
Now, since $$ t \in [2,3] $$, we know that this is maximized at $$ t = 3 \\$$
$$
\implies Max_{f'} = \vert \dfrac{t}{2\cdot \sqrt{y}} - 1 \vert \\ $$
However, since $$y \in [-\infty, \infty]$$, at $$y=0$$ we get, \\  
$$ \vert \dfrac{t}{2\cdot \sqrt{y}} - 1 \vert = \vert \dfrac{t}{2\cdot \sqrt{0}} - 1 \vert 
= \vert \dfrac{t}{0} - 1 \vert = \infty \\$$
Thus, this problem is ill posed and doesn't satisfy lipschitz condition.  

***













***

## SECOND
{: #content2}









***

## THIRD
{: #content3}









***

## FOURTH
{: #content4}










***
***

TITLE: 6.5 <br /> TITLE
LINK: research/numerical_optimization_128a/6/6.5.md


## FIRST
{: #content1}









***

## SECOND
{: #content2}









***

## THIRD
{: #content3}









***

## FOURTH
{: #content4}










***
***

TITLE: 6.1 <br /> Linear Systems of Equations
LINK: research/numerical_optimization_128a/6/6.1.md


## Linear System of Equations
{: #content1}

1. **:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} \\

2. **Linear System of Equations:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    ![formula](/main_files/128a/6/6.1/1.png){: width="80%"}

3. **Linear Operations:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    1. Equation $$E_i$$ can be multiplied by any nonzero constant $$\lambda$$ with the resulting equation used in place of $$E_i$$. This operation is denoted $$(\lambda E_i) \rightarrow (E_i)$$.

    2. Equation $$E_j$$ can be multiplied by any constant $$\lambda$$ and added to equation $$E_i$$ with the resulting equation used in place of $$E_i$$. This operation is denoted ($$E_i + \lambda E_j) \rightarrow (E_i)$$.

    3. Equations $$E_i$$ and $$E_j$$ can be transposed in order. This operation is denoted $$(E_i) \leftrightarrow (E_j)$$.


***

## Matrices and Vectors
{: #content2}

1. **Matrix:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    ![formula](/main_files/128a/6/6.1/2.png){: width="80%"}

2. **Gaussian Elemenation:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\

3. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\

    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/6/6.1/algorithm.png){: width="80%" hidden=""}

***

## Operation Counts
{: #content3}


2. **Multiplications/divisions [for each i]:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32}
    :   $$
        (n ‚àí i) + (n ‚àí i)(n ‚àí i + 1) = (n ‚àí i)(n ‚àí i + 2)
        $$

3. **Additions/subtractions [for each i]:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    :   $$
        (n ‚àí i)(n ‚àí i + 1)
        $$

4. **Summing the operations in Steps 5 and 6:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34} \\
    ![formula](/main_files/128a/6/6.1/3.png){: width="80%"}


5. **Multiplications/divisions [Gauss-Elem]:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents35} \\
    :   $$
        (n ‚àí i) + (n ‚àí i)(n ‚àí i + 1) = (n ‚àí i)(n ‚àí i + 2)
        $$

6. **Additions/subtractions [Gauss-Elem]:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents36} \\

    $$y' = -y + ty^{1/2},\  2 \leq t \leq 3,\  y(2) = 2, $$ with $$ h = 0.25 $$
    $$\dfrac{d}{dy}f(t,y) = $$$$\dfrac{d}{dy} (-y + $$$$ty^{1/2})$$
    $$ =  \dfrac{t}{2 * \sqrt{y}} - 1$$
    $$\implies \vert \dfrac{d}{dy} f(t,y) \vert = \vert \dfrac{t}{2 \cdot \sqrt(y)} - 1 \vert,l$$  
    Now, since $$ t \in [2,3] $$, we know that this is maximized at $$ t = 3 \\$$
    $$
    \implies Max_{f'} = \vert \dfrac{t}{2\dot \sqrt(y)} - 1 \vert \\ $$
    However, since $$y \in [-\infty, \infty]$$, at $$y=0$$ we get, \\  
    $$ \vert \dfrac{t}{2\dot \sqrt(y)} - 1 \vert = \vert \dfrac{t}{2\dot \sqrt(0)} - 1 \vert 
    = \vert \dfrac{t}{0} - 1 \vert = \infty \\$$
    Thus, this problem is ill posed and doesn't satisfy lipschitz condition.  




9. ...
    For a total of $$\approx \dfrac{n^3}{3}$$ operations, $$ \implies \in \mathcal{O}(n^3)$$.

***
***

TITLE: 6.4 <br /> TITLE
LINK: research/numerical_optimization_128a/6/6.4.md


## FIRST
{: #content1}









***

## SECOND
{: #content2}









***

## THIRD
{: #content3}









***

## FOURTH
{: #content4}










***
***

TITLE: 1.2 <br /> Round-off Errors and Computer Arithmetic
LINK: research/numerical_optimization_128a/1/1.2.md


## Binary Machine Numbers
{: #content1}

1. **Representing Real Numbers:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    :   A 64-bit (binary digit) representation is used for a real number. 
    > * The **first bit** is a sign indicator, denoted **$$s$$**.  
    > * Followed by an 11-bit exponent, **$$c$$**, called the **characteristic**,  
    > * and a 52-bit binary fraction, **$$f$$** , called the **mantissa**.  
    > * The base for the exponent is 2.
2. **Floating-Point Number Form:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    :   $$ (‚àí1)^s\ \  2^{c‚àí1023} \ (1 + f)$$
3. **Smallest Normalized positive Number:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} 
    > **When:**  $$s = 0, \ \  c = 1,\ \ $$ and $$\ \ f = 0$$.  
    > **Equivalent to:**  $$2^{‚àí1022}\  \ \dot \ \ (1 + 0) \ \approx \ 0.22251 \ \dot \ 10^{‚àí307}$$.
4. **Largest Normalized positive Number:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    > **When:**  $$s = 0,\ \  c = 2046,\ \ $$ and $$\ \ f = 1 - 2^{-52}$$.  
    > **Equivalent to:**  $$2^{1023}\  \dot \  (2 - 2^{-52}) \  \approx 0.17977  √ó 10^{309}$$.

5. **UnderFlow:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15}
    :   When numbers occurring in calculations have a magnitude less than,  
        $$2^{-1022}$$.

6. **OverFlow:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16}
    :   When numbers occurring in calculations have a magnitude greater than,  
        $$2^{1023} \dot (2 - 2^{-52})$$.

7. **Representing the Zero:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17} \\
    * There are **Two** Representations of the number zero:
        1. *A positive 0:* when $$s = 0, \ \ c = 0, \ \ $$ and $$ \ f = 0$$. 
        2. *A negative 0:* when $$s = 1, \ \ c = 0, \ \ $$ and $$ \ f = 0$$. 

***

## Decimal Machine Numbers
{: #content2}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} 
    :   We assume that machine numbers are represented in the normalized decimal
        floating-point form.
2. **(k-digit) Decimal Machine Numbers:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22}
    :   $$¬±0.d_1d_2 ... d_k √ó 10^n , 1 \leq d_1 \leq 9, \text{and } 0 \leq d_i \leq 9, $$  
    :   $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \text{for each } i = 2, ... , k$$.
3. **Normalized Form:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} 
    :   $$ y = 0.d_1d_2 ... d_k √ó 10^n $$
4. **Floating-Point Form of a Decimal Machine Number:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    The floating-point form of y, denoted $$f_l(y)$$, is obtained by [**terminating**](#bodyContents25) the mantissa of $$y$$ at k-decimal digits.
5. **Termination:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\
    There are two common ways of performing this termination:  
    * **Chopping:** is to simply chop off the digits $$d_{k+1}d_{k+2}$$.
    > This produces the floating-point form: $$f_l(y) = 0.d_1d_2 ... d_k √ó 10^n$$  
    * **Rounding:** adds $$5 √ó 10^{n‚àí(k+1)}$$ to $$y$$ and then chops the result
    > This produces the floating-point form: $$f_l(y) = 0.\delta_1\delta_2 ... \delta_k √ó 10^n$$.  
    >   > For rounding, when $$d_{k+1} \geq 5$$, we add $$1$$ to $$d_k$$ to obtain $$f_l(y)$$; that is, we round up.
    >   > When $$d_{k+1} < 5$$, we simply chop off all but the first k digits; so we round down.
    >   > If we round down, then $$\delta_i = d_i$$, for each $$i = 1, 2, ... , k$$.  
    >   > However, if we round up, the digits (and even the exponent) might change.
6. **Approximation Errors:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26} \\
    ![formula](/main_files/128a/1/1.2/1.png){: width="80%"}  
    * **The Absolute Error:** $$ \ \ \ \ \ \ \ \|p ‚àí p^‚àó\|$$.  

    * **The Relative Error:** $$ \ \ \ \ \ \ \ \dfrac{\|p ‚àí p^‚àó\|}{\|p\|}$$.

7. **Significant Digits:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27} \\
    ![formula](/main_files/128a/1/1.2/2.png){: width="80%"}  

8. **Error in using Floating-Point Repr.:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents28} \\
    * **Chopping:**{: style="color: green"}  
        **The Relative Error =** $$|\dfrac{y - f_l(y)}{y}|$$  
        **The Machine Repr**. [for k decimial digits] =  
        :   $$y = 0.d_1d_2 ... d_kd_{k+1} ... √ó 10^n$$.  

        $$ \implies $$  
        ![formula](/main_files/128a/1/1.2/3.png){: width="70%"}  
        **Bound** $$ \ \ \ \implies \ \ |\dfrac{y - f_l(y)}{y}| \leq \dfrac{1}{0.1} \times 10^{-k} = 10^{-k+1}$$.

    * **Rounding:**{: style="color: green"}  
        > In a similar manner, a bound for the relative error when using k-digit rounding arithmetic is   

        **Bound** $$ \ \ \ \implies \ \ \|\dfrac{y - f_l(y)}{y}\| \leq 0.5 √ó 10^{‚àík+1}$$.

9. **Distribution of Numbers:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents29} \\
    The number of decimal machine numbers in $$[10^n, 10^{n+1}]$$ is constant for all integers $$n$$.

***

## Finite-Digit Arithmetic
{: #content3}

1. **Values:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}
    :   $$ x = f_l(x) \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1)$$  
    :   $$ y = f_l(y) \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (2)$$

2. **Operations:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    ![formula](/main_files/128a/1/1.2/4.png){: width="72%"}  
3. **Error-producing Calculations:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    * Cancelation of significant digits due to the subtraction of nearly equal numbers.  
    <button>Show Error</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/1/1.2/5.png){: width="80%" hidden=""}
    * Dividing by a number with small magnitude / Multiplying by a number with large magnitude.  
    <button>Show Error</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/1/1.2/6.png){: width="80%" hidden=""}

4. **Avoiding Round-Off Error:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34} \\
    > The loss of accuracy due to round-off error can often be avoided by a reformulation of
    the calculations.

    > We change the form of the quadratic formula by *rationalizing the numerator*:
    ![formula](/main_files/128a/1/1.2/7.png){: width="80%"}    
    $$ 
    \implies \ \ \ \ \ \ \ \ \ \ \ \ \ \  x_1 = \dfrac{‚àí2c}{b + \sqrt{b^2 ‚àí 4ac}}, \ \ \ \ \ \ \ \ \ \ \ \ \ \  
    x_2 = \dfrac{‚àí2c}{b - \sqrt{b^2 ‚àí 4ac}},
    $$

***

## Nested Arithmetic
{: #content4}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} 
    :   Rearranging calculations to reduce the number of computations.  
    > Remember that chopping (or rounding) is performed after each calculation.  

    :   
    > Polynomials should always be expressed in nested form before performing an evaluation,
    because this form minimizes the number of arithmetic calculations.
2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} 
    :   Accuracy loss due to round-off error can also be reduced by rearranging calculations to reduce the number of computations.

***
***

TITLE: 1.3 <br /> Algorithms and Convergence
LINK: research/numerical_optimization_128a/1/1.3.md


## Main Idea
{: #content1}

1. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   An **Algorithm** is a procedure that describes, in an
        unambiguous manner, a finite sequence of steps to be performed in a specified order

***

## Characterizing Algorithms
{: #content2}

1. **Stability:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    * **Stable Algorithm:** an algorithm where small changes in the initial data produce correspondingly small changes in the final results.
    * **Conditionally Stable Algorithm:** an algorithm that is stable only for certain choices of initial data.
2. **Error Growth:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    ![formula](/main_files/128a/1/1.3/1.png){: width="87%"}
3. **Stability and Error-Growth:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    * **Stable Algorithm:** an algorithm that exhibits linear growth of error.
    * **UnStable Algorithm:** an algorithm that exhibits exponential error growth.

***

## Rates of Convergence
{: #content3}

1. **Rate of Convergence:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    ![formula](/main_files/128a/1/1.3/2.png){: width="87%"}  
    > $$\beta_n \  = \  \dfrac{1}{n^p}, \ \ \ \ $$ for the largest number $$p > 0$$.

2. **Big-Oh Notation:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    ![formula](/main_files/128a/1/1.3/3.png){: width="87%"}  

3. **Example [determining rate of convergence]:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/1/1.3/4.png){: width="77%" hidden=""}  

***
***

TITLE: 1.1 <br /> Calculus
LINK: research/numerical_optimization_128a/1/1.1.md


## Limits and Continuity
{: #content1}

1. **Limit [of Function]:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} \\
    ![def](/main_files/128a/1/1.1/1.png){: width="80%"}

2. **Continuity:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    ![formula](/main_files/128a/1/1.1/2.png){: width="80%"}

3. **Limit [of Sequence]:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    ![formula](/main_files/128a/1/1.1/3.png){: width="80%"}

4. **Convergence and Continuity, Correspondance:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    ![formula](/main_files/128a/1/1.1/4.png){: width="80%"}

***

## Differentiability
{: #content2}

1. **Differentiablity:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    ![formula](/main_files/128a/1/1.1/5.png){: width="80%"}

2. **Differentiablity and Continuity, Correspondance:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    ![formula](/main_files/128a/1/1.1/6.png){: width="80%"}

3. **Rolle‚Äôs Theorem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    ![formula](/main_files/128a/1/1.1/7.png){: width="80%"}

6. **Generalized Rolle‚Äôs Theorem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26} \\
    ![formula](/main_files/128a/1/1.1/10.png){: width="80%"}

4. **Mean Value Theorem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    ![formula](/main_files/128a/1/1.1/8.png){: width="80%"}  
    > **Proof**.  
    $$
    \begin{align}
    &\ f(a) = g(a)\  \ \ \  \  \\
    & \ f(b) = g(b) \\
    & \ h(x) = f(x) - g(x)\  \text{,   [define }h(x)] \\
    & \iff h(a) = h(b) = 0 \\
    & \implies  h'(x) = f'(x) - g'(x)\\
    & \implies  h'(x) = f'(x) - g'(x) = 0, \ \ \text{[for some } x = c]\\
    & \implies  f'(c) = g'(c) = \dfrac{g(b) - g(a)}{b-a}\\
    & \implies  f'(x) = \dfrac{f(b) - f(a)}{b-a}
    \end{align}
    $$

5. **Extreme Value Theorem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\
    ![formula](/main_files/128a/1/1.1/9.png){: width="80%"}

7. **Intermediate Value Theorem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27} \\
    ![formula](/main_files/128a/1/1.1/11.png){: width="80%"}

***

## Integration
{: #content3}

1. **The Riemann Integral:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    ![formula](/main_files/128a/1/1.1/12.png){: width="80%"}  
    > Or, for **equally spaced intervals**,  
    ![formula](/main_files/128a/1/1.1/13.png){: width="38%"}

2. **Integrability and Continuity, Correspondance:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32}
    :   A function f that is continuous on an interval $$[a, b]$$ is also Riemann   integrable on
        $$[a, b]$$

3. **Weighted Mean Value Theorem for Integrals:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    ![formula](/main_files/128a/1/1.1/14.png){: width="80%"}  
    > When $$g(x) ‚â° 1$$, Theorem 1.13 is the usual Mean Value Theorem for Integrals.  
    > It gives
    the average value of the function $$f$$ over the interval $$[a, b]$$  
    > $$f(c)\  = \   \dfrac{1}{b ‚àí a} \int_a^b f(x) \  dx.$$

***

## Taylor Polynomials and Series
{: #content4}

1. **Taylor‚Äôs Theorem:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} \\
    ![formula](/main_files/128a/1/1.1/taylors.png){: width="80%"}  
    > **$$P_n(x)$$:** is called the [**nth Taylor polynomial**](#bodyContents42) for $$f$$ about $$x_0$$.  

    > **$$R_n(x)$$:** is called the [**truncation error**](#bodyContents44) (or remainder term) associated with $$P_n(x)$$. 

    > Since the number $$Œæ(x)$$ in the truncation error $$R_n(x)$$ depends on the value of x at which the polynomial $$P_n(x)$$ is being evaluated, it is a function of the variable $$x$$.  

    > Taylor‚Äôs Theorem, __only__, **ensures** that such a function $$(Œæ(x))$$ exists, and that its value lies between $$x$$ and $$x_0$$, and __not__ **how to determine the function $$(Œæ(x))$$**.

2. **Polynomials:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} \\
    * **Taylor's Polynomial:** The polynomial definied by  
    ![formula](/main_files/128a/1/1.1/16.png){: width="80%"}  
    * **Maclaurin Polynomial:** The special case Taylors Polynomial with $$x_0 = 0$$.  

3. **Series:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents43} \\
    * **Taylor's Series:** The infinite series obtained by taking the limit of $$P_n(x),\text{as }\ n \rightarrow \inf$$.
    * **Maclaurin Series:** The special case Taylors series with $$x_0 = 0$$.  

4. **Truncation Error:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents44}
    :   Refers to the error involved in using a truncated, or finite, summation to approximate  
        the sum of an infinite series.

***
***

TITLE: 4.5 <br /> Romberg Integration
LINK: research/numerical_optimization_128a/4/4.5.md


## Main Idea
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   Richardson extrapolation applied to results from the
        Composite Trapezoidal rule can be used for accurate results.
2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    :   The technique can be used to obtain high accuracy approximations with little
        computational cost.
3. **Error in Composite Trapezoidal rule:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    ![formula](/main_files/128a/4/4.5/1.png){: width="90%"}  
    > This implies that ***Richardsons' Extrapolation*** is applicable here.
4. **Extrapolation Formula:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    > Extrapolation then is used to produce $$\mathcal{O}(h_k^{2j})$$ approximations by  
    ![formula](/main_files/128a/4/4.5/2.png){: width="75%"}  

    > and according to this table,  
    ![formula](/main_files/128a/4/4.5/3.png){: width="75%"}  

    > Calculate the Romberg table one complete row at a time.
5. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    ![formula](/main_files/128a/4/4.5/4.png){: width="75%"}  
    ![formula](/main_files/128a/4/4.5/5.png){: width="75%"}  


***
***

TITLE: 4.1 <br /> Numerical Differentiation
LINK: research/numerical_optimization_128a/4/4.1.md


## The derivative
{: #content1}

1. **Derivative:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} \\
    $$f'(x_0) = \lim_{h\to\infty} \ \ \dfrac{f(x_0 + h) ‚àí f(x_0)}{h}$$

2. **The forward/backward difference formula:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    > Derivative formulat [at $$x = x_0$$]
    ![formula](/main_files/128a/4/4.1/1.png){: width="70%"}   
    > This formula is known as the forward-difference formula if $$h > 0$$
    and the backward-difference formula if $$h < 0$$.  

    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.1/derivation.png){: hidden=""}


    > **Error Bound:**
        For small values of h, the difference quotient $$\dfrac{f(x_0 + h) ‚àí f(x_0)}{h}$$ can be used to approximate $$f(x_0)$$ with an error bounded by $$M\dfrac{|h|}{2}$$, where $$M$$ is a bound on $$|f''(x)|$$ for $$x$$ between $$x_0$$ and $$x_0 + h$$.

3. **The $$(n + 1)$$-point formula to approximate $$f'(x_j)$$:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13} \\
    ![formula](/main_files/128a/4/4.1/2.png){: width="70%"}
    * Derivation:  
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.1/derivation.jpg){: hidden=""}

4. **Three-point Formula:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    ![formula](/main_files/128a/4/4.1/4.png){: width="70%"}  
    for each $$j = 0, 1, 2$$, where the notation $$\zeta_j$$ indicates that this point depends on $$x_j$$.
    * Derivation:  
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.1/3.png){: hidden=""}

***

## Three-Point Formulas
{: #content2}

1. **Equally Spaced nodes:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    > The formulas from Eq. (4.3) become especially useful if the nodes are equally spaced, that
    is, when $$x_1 = x_0 + h$$ and $$x_2 = x_0 + 2h$$, for some $$h \neq 0$$.  
    > We will assume equally-spaced nodes throughout the remainder of this section.

2. **Three-Point Endpoint Formula:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    ![formula](/main_files/128a/4/4.1/5.png){: width="70%"}  
    > The approximation in Eq. (4.4) is useful near the ends of an interval, because information about f outside the interval may not be available.  

    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.1/derivation1.png){: width="80%" hidden=""}

    > **Errors:** the errors in both Eq. (4.4) and Eq. (4.5) are $$O(h^2)$$
3. **Three-Point Midpoint Formula:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    ![formula](/main_files/128a/4/4.1/6.png){: width="70%"}  
    > **Errors:** Although the errors in both Eq. (4.4) and Eq. (4.5) are $$O(h^2)$$, the error in Eq. (4.5) is approximately half the error in Eq. (4.4).  
    >   > This is because Eq. (4.5) uses data on both sides of $$x_0$$ and Eq. (4.4) uses data  
        on only one side. Note also that f needs to be evaluated at only two points in Eq. (4.5), whereas in Eq. (4.4) three evaluations are needed.

***

## Five-Point Formulas
{: #content3}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    They are five-point formulas that involve evaluating the function at two additional points to the three-point formulas.
2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    One common five-point formula is used to determine approximations for the derivative at the midpoint.
3. **Error:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    The error term for these formulas is $$O(h^4)$$.  

4. **Five-Point Midpoint Formula:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34} \\
    ![formula](/main_files/128a/4/4.1/7.png){: width="70%"}  

    > **Used** for approximation at **Mid-Points**  

5. **Five-Point Endpoint Formula:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents35} \\
    ![formula](/main_files/128a/4/4.1/8.png){: width="70%"}  

    > **Used** for approximation at **End-Points**  

    > ***Left-endpoint** approximations* are found using this formula with $$h > 0$$ and ***right-endpoint** approximations* with $$h < 0$$.  

    > The five-point endpoint formula is particularly useful for the
    clamped cubic spline interpolation of Section 3.5.  

***

## Approximating Higher Derivatives
{: #content4}

1. **Approximations to Second Derivatives:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} \\
    ![formula](/main_files/128a/4/4.1/9.png){: width="70%"}  
    > [Derivation below](#bodyContents42)
    

2. **Second Derivative Midpoint Formula:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} \\
    ![formula](/main_files/128a/4/4.1/10.png){: width="70%"}  
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.1/derivation2.jpg){: width="80%" hidden=""}

    > **Error Bound:** If $$f^{(4)}$$ is continuous on $$[x_0 ‚àí h, x_0 + h]$$ it is also bounded, and the approximation is $$O(h^2)$$.  

***

## Round-Off Error Instability
{: #content5}

1. **Form of Error:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents51} \\
    * We assume that our computations actually use the values $$\tilde{f}(x_0 + h)$$ and $$\tilde{f}(x_0 ‚àí h)$$   
    * which are related to the true values $$f(x_0 + h)$$ and $$f(x_0 ‚àí h)$$ by:  
    > $$ f(x_0 + h) = \tilde{f}(x_0 + h) + e(x_0 + h) \ \ $$  &  
    > $$f(x_0 ‚àí h) = \tilde{f}(x_0 ‚àí h) + e(x_0 ‚àí h) $$
2. **The Total Error:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents52} \\
    ![formula](/main_files/128a/4/4.1/11.png){: width="70%"}  
    > It is due both to round-off error, the first part, and to truncation error.  

3. **Error Bound:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents53} \\
    > If we assume that the round-off errors $$e(x_0 ¬± h)$$ are bounded by some number $$Œµ > 0$$and that the third derivative of $$f$$ is bounded by a number $$M > 0$$, then  
    ![formula](/main_files/128a/4/4.1/12.png){: width="64%"}  
4. **Reducing Truncation Error:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents54} \\
    * **How?** To reduce the truncation error, $$\dfrac{h**2}{6}M$$, we need to reduce $$h$$. 
    * **Effect of reducing $$h$$:** But as $$h$$ is reduced, the roundoff error $$\dfrac{Œµ}{h}$$ grows.

5. **Conclusion:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents55} \\
    * It is seldom advantageous to let $$h$$ be too small, because in that case the round-off error will dominate the calculations.  
    * But we must remain aware that reducing the step size will not always improve the approximation.
    * As approximation methods, numerical differentiation is unstable.

***
***

TITLE: 4.4 <br /> Composite Numerical Integration
LINK: research/numerical_optimization_128a/4/4.4.md


## Composite Rules
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} \\
    A piecewise approach to numerical integration that uses the
    low-order Newton-Cotes formulas.
2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    * The Newton-Cotes formulas are generally unsuitable for use over large integration intervals.  
    * High-degree formulas would be required, and the values of the coefficients in these
    formulas are difficult to obtain.  
    * Newton-Cotes formulas are based on interpolatory
    polynomials that use equally-spaced nodes, a procedure that is inaccurate over large
    intervals because of the oscillatory nature of high-degree polynomials.
3. **Notice:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13}
    :    $$h = \dfrac{b ‚àí a}{n}$$ and $$x_j = a + jh$$

***

## Composite Simpson‚Äôs rule
{: #content2}

1. **Composite Simpson‚Äôs rule:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    ![formula](/main_files/128a/4/4.4/2.png){: width="70%"}
2. **Error in Comoposite Simpson's Rule:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    ![formula](/main_files/128a/4/4.4/3.png){: width="20%"}  
    > **Error** $$\ \ \in \ \ \  \mathcal{O}(h^4)$$
3. **Theorem [Rule and Error]:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    ![formula](/main_files/128a/4/4.4/1.png){: width="90%"}
4. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    ![formula](/main_files/128a/4/4.4/4.png){: width="60%"}

***

## Composite Newton-Cotes Rules
{: #content3}

1. **Composite Trapezoidal rule:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    ![formula](/main_files/128a/4/4.4/5.png){: width="77%"}
2. **Composite Midpoint rule:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    ![formula](/main_files/128a/4/4.4/6.png){: width="77%"}

***

## Round-Off Error Stability
{: #content4}

1. **Stability Property:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} \\
    An important property shared by all the composite integration techniques is a stability with respect to round-off error.  
    The round-off error does not depend on the number of calculations performed.
2. **Proof:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} \\
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.4/derivation.jpg){: width="80%" hidden=""}

***
***

TITLE: 4.3 <br /> Elements of Numerical Integration
LINK: research/numerical_optimization_128a/4/4.3.md


## Numerical Quadrature
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    :   The basic method involved in approximating $$\int_{a}^{b} f(x) dx$$.


2. **How?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} 
    :       It uses a sum $$\sum_{i=0}^{n} a_i f(x_i)$$ to approximate $$\int_{a}^{b} f(x) dx$$.
3. **Based on:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} 
    :   The methods of quadrature in this section are based on the interpolation polynomials
    given in Chapter 3.
4. **Method:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} 
    * Select a set of distinct nodes $${x_0, ... , x_n}$$ from the
    interval $$[a, b]$$.  
    * Then integrate the Lagrange interpolating polynomial  
    $$P_n(x) = \sum_{i=0}^{n} f(x_i)L_i(x)$$  
    and its truncation error term over $$[a, b]$$ to obtain  
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.3/1.png){: width="80%" hidden=""}
5. **The Quadrature Formula:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    ![formula](/main_files/128a/4/4.3/2.png){: width="27%"}

6. **The Error:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16} \\
    ![formula](/main_files/128a/4/4.3/3.png){: width="45%"}

***

## The Trapezoidal Rule
{: #content2}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}
    :   Approximation method for integrals produced by using first (linear) Lagrange polynomials with equally-spaced nodes.
2. **The Trapezoidal Rule:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    ![formula](/main_files/128a/4/4.3/5.png){: width="45%"}
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.3/4.png){: width="80%" hidden=""}

3. **Error:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    The error term for the Trapezoidal rule involves $$f$$ , so the rule gives the exact
    result when applied to any function whose second derivative is identically zero, that is, any
    polynomial of degree one or less.

***

## Simpson‚Äôs Rule
{: #content3}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    A method to approximate an integral that results from integrating over $$[a, b]$$ the second Lagrange polynomial with equally-spaced nodes $$x_0 = a, x_2 = b,$$ and $$x_1 = a + h$$, where $$h = \dfrac{(b ‚àí a)}{2}$$.
2. **Simpson's Rule:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    ![formula](/main_files/128a/4/4.3/6.png){: width="50%"}
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.3/derivation.jpg){: width="80%" hidden=""}
3. **Error:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    The error term in Simpson‚Äôs rule involves the fourth derivative of $$f$$ , so it gives exact
    results when applied to any polynomial of degree three or less.

***

## Measuring Precision
{: #content4}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} \\
    The standard derivation of quadrature error formulas is based on determining the class of
    polynomials for which these formulas produce exact results.
2. **Precision [degree of accuracy]:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} \\
    ![formula](/main_files/128a/4/4.3/7.png){: width="90%"}  
    > Definition 4.1 implies that the Trapezoidal and Simpson‚Äôs rules have degrees of precision
    one and three, respectively.
3. **Precision of Quadrature Formulas:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents43} \\
    * The degree of precision of a quadrature formula is n if and only if the error is zero for
    all polynomials of degree $$k = 0, 1, ... , n$$, but is not zero for some polynomial of degree $$n + 1$$.
    * The Trapezoidal and Simpson‚Äôs rules are examples of a class of methods known as **Newton-Cotes formulas**.
    * **Types of Newton-Cotes formulas:** There are 2 types, *Open*, and *Closed*.

***

## Closed Newton-Cotes Formulas
{: #content5}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents51} \\
    * The $$(n+1)$$-point closed Newton-Cotes formula uses nodes $$x_i = x_0 +ih$$, for $$i = 0, 1, ... , n,$$ where $$x_0 = a, x_n = b$$ and $$h = \dfrac{(b ‚àí a)}{n}$$ .
    * It is called closed because the endpoints of the closed interval [a, b] are included as nodes.
2. **Form of the Formula:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents52} \\
    ![formula](/main_files/128a/4/4.3/8.png){: width="27%"}  
    > where,  
    ![formula](/main_files/128a/4/4.3/9.png){: width="38%"}  
3. **The Error Analysis:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents53} \\
    ![formula](/main_files/128a/4/4.3/10.png){: width="84%"}  

4. **Degree of Preceision:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents54} \\
    * **Even-n:** the degree of precision is $$n + 1$$ (although the interpolation polynomial is of degree at most n)
    * **Odd-n:** the degree of precision is only $$n$$.

5. **Closed Form Formulas:**{: style="color: SteelBlue  "}{: .bodyContents5 #bodyContents55} \\
    * **$$n = 1$$: Trapezoidal rule**  
    ![formula](/main_files/128a/4/4.3/11.png){: width="70%"}  
    * **$$n = 2$$: Simpson‚Äôs rule**   
    ![formula](/main_files/128a/4/4.3/12.png){: width="70%"}  
    * **$$n = 3$$: Simpson‚Äôs Three-Eighths rule**   
    ![formula](/main_files/128a/4/4.3/13.png){: width="70%"}  
    * **n = 4:**  
    ![formula](/main_files/128a/4/4.3/14.png){: width="70%"}  

***

## Open Newton-Cotes Formulas
{: #content6}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents61} \\
    * They do not include the endpoints of $$[a, b]$$ as nodes.
    * They use the nodes $$x_i = x_0 + ih$$, for each $$i = 0, 1, ... , n$$, where $$h = \dfrac{b ‚àí a}{n + 2}$$ and $$x_0 = a + h$$.
    * This implies that $$x_n = b ‚àí h$$, so we label the endpoints by setting $$x_{‚àí1} = a$$ and $$x_{n+1} = b$$.
    * Open formulas contain all the nodes used for the approximation within the open interval $$(a, b)$$
2. **Form of the Formula:**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents62} \\
    ![formula](/main_files/128a/4/4.3/15.png){: width="45%"}  
    > where,  
    ![formula](/main_files/128a/4/4.3/16.png){: width="20%"}  

3. **The Error Analysis:**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents63} \\
    ![Thm](/main_files/128a/4/4.3/17.png){: width="74%"}  

4. **Degree of Preceision:**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents64} \\
    * **Even-n:** Higher.
    * **Odd-n:** Lower.
5. **Open Form Formulas:**{: style="color: SteelBlue  "}{: .bodyContents6 #bodyContents65} \\
    * **$$n = 0$$: Midpoint rule**  
    ![formula](/main_files/128a/4/4.3/18.png){: width="70%"}  
    * **$$n = 1$$:**   
    ![formula](/main_files/128a/4/4.3/19.png){: width="70%"}  
    * **$$n = 2$$:**   
    ![formula](/main_files/128a/4/4.3/20.png){: width="70%"}  
    * **n = 3:**  
    ![formula](/main_files/128a/4/4.3/21.png){: width="70%"}  


***
***

TITLE: 4.7 <br /> Gaussian Quadrature
LINK: research/numerical_optimization_128a/4/4.7.md


## Main Idea
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    :   * A technique that is used to determine the nodes and coefficients  
        for formulas that give exact results for higher-degree polynomials.  
        * Gaussian quadrature chooses the points for evaluation in an optimal, rather than    
        equally-spaced, way.
        * The nodes $$x_1, x_2, ... , x_n$$ in the interval $$[a, b]$$ and coefficients $$c_1, c_2, ... , c_n$$, are chosen to minimize the expected error obtained in the approximation,  $$\int_{a}^{b} f(x) dx  = \sum_{i=1}^{n} c_i f(x_i).$$  

    :   * 
        > **To Measure Accuracy:** we assume that the best choice of these values produces the exact result for the largest class of polynomials, that is, the choice that gives the greatest degree of precision.  

    :   *  
        > The **Coefficients** $$c_1, c_2, ... , c_n$$ in the approximation formula are arbitrary,  
        and,   
        The **Nodes** $$x_1, x_2, ... , x_n$$ are restricted only by the fact that they must lie in $$[a, b]$$, the interval of integration.  
        This gives,  
        **The number of Parameters** to choose is $$2n$$.  

    :   *  
        > If the coefficients of a polynomial are considered parameters, the class of polynomials of degree at most $$2n ‚àí 1$$ also contains $$2n$$ parameters.  
        > This, then, is **The Largest Class of Polynomials** for which it is reasonable to
        expect a **formula to be exact**.

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} 
    :   The fact that, *Newton-Cotes Formulas*  use values at *equally-spaced nodes*, can
        significantly *decrease* the *accuracy* of the approximation.

***

## Legendre Polynomials
{: #content2}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} 
    :   A series of solutions to *"Legendre's differential equation"* that form a polynomial    
        sequence of orthogonal polynomials.

9. **Why?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents29} 
    :   The roots of the nth-deg Legnedre Polynomial are the nodes needed for the approximation formula that gives exact results for any polynomial of degree less than $$2n$$.
2. **Properties:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    1. For each $$n$$, $$P_n(x)$$ is a monic polynomial of degree $$n$$.
    2. $$\int_{-1}^{1} P(x) P_n(x) dx = 0, $$ whenever $$P(x)$$ is a polynomial of degree less than $$n$$.
    3. The roots of these polynomials are: 
        * *Distinct*, 
        * *lie in the interval $$(‚àí1, 1)$$*,
        * *have a symmetry* with respect to the origin,
        * *the correct choice* for *determining* the *parameters* that *give* us the *nodes* and *coefficients* for our *quadrature method*.
3. **The first Legendre Polynomials:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    $$
    P_0(x) = 1, \ \ \ \ \ \ \ \ \ \ \ \ \ \  P_1(x) = x, \ \ \ \ \ \ \ \ \ \ \ \ \ \  P_2(x) = x^2 ‚àí \dfrac{1}{3},
    $$  
    $$
    P_3(x) = x^3 ‚àí \dfrac{3}{5}x,\ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \   P_4(x) = x^4 ‚àí \dfrac{6}{7}x^2 +\dfrac{3}{35}.
    $$
4. **Determining the nodes:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    ![formula](/main_files/128a/4/4.7/1.png){: width="87%"}  
    <button>Show Proof</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.7/proof.jpg){: width="80%" hidden=""}  

    > The nodes $$x_1, x_2, ... , x_n$$ needed to produce an integral approximation formula that
    gives exact results for any polynomial of degree less than $$2n$$ are the roots of the nth-degree Legendre polynomial.

***

## Gaussian Quadrature on Arbitrary Intervals
{: #content3}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    * **Change of Intervals:**
        An integral $$\int_{a}^{b} f(x) dx$$ over an arbitrary $$[a, b]$$ can be transformed into an integral over $$[‚àí1, 1]$$ by using a *change of variables*.
2. **The Change of Variables:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    $$ \ \ \ t = \dfrac{2x ‚àí a ‚àí b}{b ‚àí a} \ \ \ \iff \ \ \ x = \dfrac{1}{2}[(b ‚àí a)t + a + b].$$
3. **Gaussian quadrature [arbitrary interval]:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    ![formula](/main_files/128a/4/4.7/2.png){: width="83%"}  


***
***

TITLE: 4.6 <br /> Adaptive Quadrature Methods
LINK: research/numerical_optimization_128a/4/4.6.md


## Main Idea
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   * Efficient techniques for calculating integrals in intervals with high functional variations.  
        * They predict the amount of functional variation and adapt the step size as necessary.
2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    :   The composite formulas suffer because they require the use of equally-spaced nodes.  
        This is inappropriate when integrating a function on an interval that contains both regions with large functional variation and regions with small functional variation.
3. **Approximation Formula:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13}
    $$\int_{a}^{b} f(x) dx = $$  

    :   ![formula](/main_files/128a/4/4.6/2.png){: width="27%"}

    <button>Show Formula Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.6/derivation.png){: width="80%" hidden=""}

4. **Error Bound:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    * **Error relative to *Composite Approximations*:**  
        ![formula](/main_files/128a/4/4.6/1.png){: width="65%"}  
    * **Error relative to *True Value*:**  
        ![formula](/main_files/128a/4/4.6/3.png){: width="65%"} 
    <button>Show Error Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.6/derivation2.png){: width="80%" hidden=""} 

    > This implies that this procedure approximates the integral about **15** times better than it agrees with the computed value $$S(a, b)$$.

5. **Procedure:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    > When the approximations in (4.38) differ by more than $$15\epsilon$$, we can apply the Simpson‚Äôs rule technique individually to the subintervals $$[a,\dfrac{a + b}{2}]$$ and $$[\dfrac{a + b}{2}, b]$$.  
    
    > Then we use the error estimation procedure to determine if the approximation to the integral on each subinterval is within a tolerance of $$\epsilon/2$$. If so, we sum the approximations to produce an approximation to $$\int_{a}^{b} f(x) dx$$, within the tolerance $$\epsilon$$.  

    > If the approximation on one of the subintervals fails to be within the tolerance $$\epsilon/2$$, then
    that subinterval is itself subdivided, and the procedure is reapplied to the two subintervals to determine if the approximation on each subinterval is accurate to within $$\epsilon/4$$. This halving procedure is continued until each portion is within the required tolerance.

7. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17} \\
    <button>Show Algorithm</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.6/Algorithm.jpg){: width="75%" hidden=""}

8. **Derivation:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents18} \\
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.6/derivation.jpg){: width="75%" hidden=""}


***
***

TITLE: 4.2 <br /> Richardson's Extrapolation
LINK: research/numerical_optimization_128a/4/4.2.md


## Extrapolation
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} \\
    * Extrapolation that is used to generate high-accuracy results while using low-order
    formulas.
    * Extrapolation can be applied whenever it is known that an approximation technique
    has an error term with a predictable form, one that depends on a parameter, usually the step
    size $$h$$.
    * Suppose that for each number $$h \neq 0$$ we have a formula $$N_1(h)$$ that approximates an
    unknown constant $$M$$, and that the truncation error involved with the approximation has the
    form,  
    $$ M ‚àí N_1(h) = K_1h + K_2h^2 + K_3h^3 +¬∑¬∑¬∑ ,$$  
    for some collection of (unknown) constants $$K_1, K_2, K_3, ...$$ .  
    * The **truncation error** is *$$O(h)$$*, so unless there was a large variation in magnitude among the constants $$K_1, K_2, K_3, ... ,$$  
    $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ M ‚àí N_1(0.1) \approx 0.1K_1,\ \ \ \ \ \ \ \ M ‚àí N_1(0.01) \approx 0.01K_1, $$  
    and, in general,  
    $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ M ‚àí N_1(h) \approx K_1h$$ .
    * The object of extrapolation is to find an easy way to combine these rather inaccurate
    $$O(h)$$ approximations in an appropriate way to produce formulas with a higher-order
    truncation error.
2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    * **We can** combine the $$N_1(h)$$ formulas to **produce an $$\mathcal{O}(h^2)$$
    approximation formula, $$N_2(h)$$, for $$M$$** with  
    $$M ‚àí N_2(h) = \hat{K}_2h^2 + \hat{K}_3h^3 +¬∑¬∑¬∑$$ ,  
    for some, again unknown, collection of constants $$\hat{K}_2, \hat{K}_3, ... $$.  
    Then we would have  
    $$M ‚àí N_2(0.1) \approx 0.01\hat{K}_2, M ‚àí N_2(0.01) \approx 0.0001\hat{K}_2,$$  
    * If the constants $$K_1$$ and $$\hat{K}_2$$ are roughly of the same magnitude, then the $$N_2(h)$$ approximations would be much better than the corresponding $$N_1(h)$$ approximations.  
  


3. **The $$\mathcal{O}(h)$$ formula for approximating $$M$$:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    > **The First Formula:**  
    ![formula](/main_files/128a/4/4.2/1.png){: width="70%"}  

    > **The Second Formula:**  
    ![formula](/main_files/128a/4/4.2/2.png){: width="70%"}  

4. **The $$\mathcal{O}(h^2)$$ approximation formula for M:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    ![formula](/main_files/128a/4/4.2/3.png){: width="70%"}  
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.2/derivation.png){: width="80%" hidden=""}

5. **When to apply Extrapolation?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    > Extrapolation can be applied whenever the truncation error for a formula has the form:  
    ![formula](/main_files/128a/4/4.2/4.png){: width="26%"}  

    > for a collection of constants $$K_j$$ and when $$\alpha_1 < \alpha_2 < \alpha_3 < ¬∑¬∑¬∑ < \alpha_m$$.  

    > The extrapolation is much more effective than when all powers of $$h$$ are present because the averaging process produces results with errors $$\mathcal{O}(h^2), \mathcal{O}(h^4), \mathcal{O}(h^6), ... $$, with essentially no increase in computation, over the results with errors, $$\mathcal{O}(h), \mathcal{O}(h^2), \mathcal{O}(h^3), ...$$ .
6. **The $$\mathcal{O}(h^4)$$ formula for approximating $$M$$:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16} \\
    ![formula](/main_files/128a/4/4.2/5.png){: width="60%"}  
    > [Derivation below](#bodyContents18)  
7. **The $$\mathcal{O}(h^6)$$ formula for approximating $$M$$:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17} \\
    ![formula](/main_files/128a/4/4.2/6.png){: width="58%"}  
    > [Derivation below](#bodyContents18)
8. **The $$\mathcal{O}(h^{2j})$$ formula for approximating $$M$$:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents18} \\
    ![formula](/main_files/128a/4/4.2/7.png){: width="50%"}  
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.2/derivation2.jpg){: width="80%" hidden=""}

9. **The Order the Approximations Generated:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents19} \\
    ![formula](/main_files/128a/4/4.2/8.png){: width="58%"}  
    > It is conservatively assumed that the true result is accurate at least to within the agreement of the bottom two results in the diagonal, in this case, to within  
    $$|N_3(h) ‚àí N_4(h)|$$.  

    ![formula](/main_files/128a/4/4.2/9.png){: width="50%"}  

***

## Deriving n-point Formulas with Extrapolation
{: #content2}

1. **Deriving Five-point Formula:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.2/derivation3.jpg){: width="80%" hidden=""}

***
***

TITLE: 4.9 <br /> TITLE
LINK: research/numerical_optimization_128a/4/4.9.md


## FIRST
{: #content1}









***

## SECOND
{: #content2}









***

## THIRD
{: #content3}









***

## FOURTH
{: #content4}










***
***

TITLE: 4.8 <br /> Multiple Integrals
LINK: research/numerical_optimization_128a/4/4.8.md


## Approximating Double Integral
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    :   The techniques discussed in the previous sections can be modified for use in the approximation of multiple integrals.
2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}

3. **Comoposite Trapezoidal Rule for Double Integral:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    $$ \ \  \iint_R f(x,y) \,dA \  = \  \int_{a}^{b} \ \big( \ \int_{c}^{d} \ \  f(x,y) \ \ dy \ \ \big) \  dx \ \ \ $$  

    $$ \approx \dfrac{(b ‚àí a)(d ‚àí c)}{16} \bigg[f(a,c)+f(a,d) + f(b,c) + f(b,d)+ $$
    $$\ \ \ \ \ \ \ \ 2\Big[f\big(\dfrac{a + b}{2} , c\big) + 
     f\big(\dfrac{a + b}{2} , d\big) + f\big(a, \dfrac{c + d}{2}\big) + f\big(b, \dfrac{c + d}{2}\big)\Big] + 4f\big(\dfrac{a + b}{2}, \dfrac{c + d}{2}\big)\bigg]$$

    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.8/derivation.jpg){: width="80%" hidden=""}
4. **Comoposite Simpsons' Rule for Double Integral:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    * **Rule:**  
    <button>Show Rule</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.8/1.png){: width="80%" hidden=""}  
    * **Error:**  
    <button>Show Error</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.8/2.png){: width="80%" hidden=""}  
    * **Derivation:**  
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.8/derivation2.jpg){: width="80%" hidden=""}

***

## Gaussian Quadrature for Double Integral Approximation
{: #content2}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}
    :   More efficient methods such as Gaussian
        quadrature, Romberg integration, or Adaptive quadrature can be incorporated in place of the Newton-Cotes formulas.
2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22}
    :   To reduce the number of functional evaluations.
3. **Example:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    <button>Show Example</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.8/4.png){: width="80%" hidden=""}

***

## Non-Rectangular Regions
{: #content3}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}
    :   Regions that don't have a rectangular shape.  
    :   **Form:**  
    :   $$ \ \int_{a}^{b} \bigg( \int_{c(x)}^{d(x)} f(x,y) dy \bigg) dx \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (4.42)$$
    :   or,  
    :   $$ \int_{c}^{d} \bigg( \int_{a(y)}^{b(y)} f(x,y) dx \bigg) dy \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (4.43)$$
2. **How?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    * We use Simpson's Rule for Approximation.
    * Step Size:
        * **x:** $$\ \  h = \dfrac{b ‚àí a}{2} $$
        * **y:** $$\ \ k(x) = \dfrac{d(x) ‚àí c(x)}{2}$$
3. **Simpsons' Rule for Non-Rect Regions:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    <button>Show Rule</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.8/3.png){: width="80%" hidden=""}  
4. **Simpsons' Double Integral [Algorithm]:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34} \\
    <button>Show Algorithm</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.8/Algorithm1.jpg){: width="80%" hidden=""}  
5. **Gaussian Double Integral [Algorithm]:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents35} \\
    <button>Show Algorithm</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.8/Algorithm2.jpg){: width="80%" hidden=""} 

***

## Triple Integral Approximation
{: #content4}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41}
    :   * **Triple** integrals.  
    :   * **Form:**  
    :   $$\ \int_{a}^{b} \  \int_{c(x)}^{d(x)} \   \int_{\alpha(x)}^{\beta(x)} f(x,y) dz \  dy \   dx $$
2. **Gaussian Triple Integral [Algorithm]:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} \\
    <button>Show Algorithm</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/4/4.8/Algorithm3.jpg){: width="80%" hidden=""} 


***
***

TITLE: 3.4 <br /> Hermite Interpolation
LINK: research/numerical_optimization_128a/3/3.4.md


## FIRST
{: #content1}









***

## SECOND
{: #content2}









***

## THIRD
{: #content3}









***

## FOURTH
{: #content4}










***
***

TITLE: 3.5 <br /> TITLE
LINK: research/numerical_optimization_128a/3/3.5.md


## FIRST
{: #content1}









***

## SECOND
{: #content2}









***

## THIRD
{: #content3}









***

## FOURTH
{: #content4}










***
***

TITLE: 3.1 <br /> Interpolation and the Lagrange Polynomial
LINK: research/numerical_optimization_128a/3/3.1.md


## Algebraic Polynomials
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   Set of Functions of the form:
    :   $$P_n(x) = a_nx^n + a_{n‚àí1}x^{n‚àí1} +¬∑¬∑¬∑+ a_1x + a_0$$
2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    :   Polynomials uniformly approximate continuous functions. By this we mean that
        given any function, defined and continuous on a closed and bounded interval, there exists a polynomial that is as ‚Äúclose‚Äù to the given function as desired.
3. **Weierstrass Approximation Theorem:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    ![(Weierstrass Approximation Theorem)](/main_files/128a/3/3.1/1.png){:width="90%"})
    > i.e. Polynomials uniformly approximate continuous functions.
4. **Taylor Polynomials:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    > Taylor Polynomials are generally bad at approximating functions anywhere but at a certain point $$x_0$$.
    > To approximate an interval, we do not use Taylors Polynomials.

***

## Lagrange Interpolating Polynomials
{: #content2}

1. **The linear Lagrange interpolating polynomial:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    ![Lag Poly](/main_files/128a/3/3.1/2.png){:width="80%"}

2. **The nth Lagrange interpolating polynomial:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    ![Lag Poly Thm](/main_files/128a/3/3.1/3.png){:width="90%"}
3. **The error term (bound):**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    ![Error Thm](/main_files/128a/3/3.1/4.png){:width="90%"}

    <button>PROOF</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/3/3.1/derivation2.png){: hidden=""}


***
***

TITLE: 3.2 <br />  Data Approximation and Neville‚Äôs Method
LINK: research/numerical_optimization_128a/3/3.2.md


## Neville‚Äôs Method
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    :   A recursive method definition used to generate successively higher-degree   
        polynomial approximations at a specific point.
2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} 
    :   A practical difficulty with Lagrange interpolation is that the error term is    
        difficult to apply, so the degree of the polynomial needed for the desired accuracy is generally not known
    until computations have been performed.
3. **The lagrange Polynomial of the point $$x_{m_i}$$:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    ![definition](/main_files/128a/3/3.2/1.png){:width="80%"}
4. **Method to recursively generate Lagrange polynomial:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    * **Method:**  
    ![definition](/main_files/128a/3/3.2/2.png){:width="80%"}  
    * **Examples:**
        $$ 
        P_{0,1} = \dfrac{1}{x_1 ‚àí x_0}[(x ‚àí x_0)P_1 ‚àí (x ‚àí x_1)P_0], \\
        P_{1,2} = \dfrac{1}{x_2 ‚àí x_1}[(x ‚àí x_1)P_2 ‚àí (x ‚àí x_2)P_1], \\
        P_{0,1,2} = \dfrac{1}{x_2 ‚àí x_0}[(x ‚àí x_0)P_{1,2} ‚àí (x ‚àí x_2)P_{0,1}]
        $$
    * **Generated according to the following Table:**
    ![Table](/main_files/128a/3/3.2/4.png){:width="80%"}

5. **Notation and subscripts:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    * Proceeding down the table corresponds to using consecutive points $$x_i$$ with larger i, and proceeding to the right corresponds to increasing the degree of the interpolating polynomial.

    * **To avoid the multiple subscripts**, we let $$Q_{i,j}(x),$$ for $$0 ‚â§ j ‚â§ i,$$ denote the interpolating polynomial of degree j on the (j + 1) numbers $$x_{i‚àíj}, x_{i‚àíj+1}, ... , x_{i‚àí1}, x_i$$; that is,
    :   $$Q_{i,j} = P_{i‚àíj},_{i‚àíj+1},...,_{i‚àí1},_i$$
6. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16} \\
    ![Nevilles Method](/main_files/128a/3/3.2/5.png){:width="80%"}
7. **Stopping Criterion:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17} \\
    * Criterion:  
    $$|Q_{i,i} ‚àí Q_{i‚àí1,i‚àí1}| < \epsilon$$
    * If the inequality is true, $$Q_{i,i}$$ is a reasonable approximation to $$f(x)$$.
    * If the inequality is false, a new interpolation point, $$x_{i+1}$$, is added.

0. **OMG**:  
    $$ 
    P_{j,..,i} = \dfrac{1}{x_i ‚àí x_j}[(x ‚àí x_j)P_{j+1,..,i} ‚àí (x ‚àí x_i)P_{j,..,i-1}], \\        
    Q_{i,j} = \dfrac{1}{x_{i} ‚àí x_{i-j}}[(x ‚àí x_{i-j})Q_{i,j-1} ‚àí (x ‚àí x_i)Q_{i-1,j-1}], \\
    $$

***
***

TITLE: 3.3 <br /> Divided Differences
LINK: research/numerical_optimization_128a/3/3.3.md


## Divided Differences
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   A recursive method definition used to successively generate the approximating   
        polynomials.
2. **Form of the Polynomial:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    * $$P_n(x) = a_0 + a_1(x ‚àí x_0) + a_2(x ‚àí x_0)(x ‚àí x_1) +¬∑¬∑¬∑+ a_n(x ‚àí x_0)¬∑¬∑¬∑(x ‚àí x_{n‚àí1}),\ \ \  (3.5)$$
    * **Evaluated at $$x_0$$:** $$\ P_n(x_0) = a_0 = f(x_0)$$

    * **Evaluated at $$x_1$$:** $$\ P_n(x_1) = f(x_0) + a_1(x_1 ‚àí x_0) = f(x_1)$$

    * > $$\implies \ \ \ \ \ \ \ \  a_1 = \dfrac{f(x_1) ‚àí f(x_0)}{x_1 ‚àí x_0}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.6)$$.
3. **The divided differences:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    * **The *zeroth* divided difference** of the function f with respect to $$x_i$$:
        * **Denoted:** $$f[x_i]$$
        * **Defined:** as the value of $$f$$ at $$x_i$$
        * > $$f[x_i] = f(x_i) \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.7)$$.
    * > The remaining divided differences are defined **recursively**.

    <button>Recursive Divdided Differences</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    * **The *first* divided difference** of $$f$$ with respect to $$x_i$$ and $$x_{i+1}$$:
        * **Denoted:** $$f[x_i,x_{i+1}]$$
        * **Defined:** as
        * > $$f[x_i,x_{i+1}] = \dfrac{f[x_{i+1}] ‚àí f[x_i]}{x_{i+1} ‚àí x_i} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3.8)$$.
    * **The *second* divided difference** of $$f$$ with respect to $$x_i$$, $$x_{i+1}$$ and $$x_{i+2}$$:
        * **Denoted:** $$f[x_i,x_{i+1},x_{i+2}]$$
        * **Defined:** as
        * > $$f[x_i,x_{i+1},x_{i+2}] = \dfrac{f[x_{i+1},x_{i+2}] ‚àí f[x_i,x_{i+1}]}{x_{i+2} ‚àí x_i} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ (3.85)$$.
    * **The *Kth* divided difference** of $$f$$ with respect to $$x_i$$, $$x_{i+1},...,x_{i+k-1},x_{i+k}$$:
        * **Denoted:** $$f[x_i,x_{i+1},...,x_{i+k-1},x_{i+k}]$$
        * **Defined:** as
        * > $$f[x_i,x_{i+1},...,x_{i+k-1},x_{i+k}] = \dfrac{f[x_{i+1},x_{i+2},...,x_{i+k}] ‚àí f[x_i,x_{i+1},...,x_{i+k-1}]}{x_{i+k} ‚àí x_i} \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ (3.9)$$
    * > The process ends with the **nth divided difference**
    * **The *nth* divided difference** of $$f$$ with respect to $$x_i$$, $$x_{i+1},...,x_{i+k-1},x_{i+k}$$:
        * **Denoted:** $$f[x_0,x_1,...,x_n]$$
        * **Defined:** as
        * > $$f[x_0,x_1,...,x_n] = \dfrac{f[x_1,x_2,...,x_n] ‚àí f[x_0,x_1,...,x_{n-1}]}{x_n ‚àí x_0} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  (3.95)$$.
    {: hidden=''}

4. **The Interpolating Polynomial:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    $$P_n(x) = f[x_0] + f[x_0, x_1](x ‚àí x_0) + a_2(x ‚àí x_0)(x ‚àí x_1)+¬∑¬∑¬∑+ a_n(x ‚àí x_0)(x ‚àí x_1)¬∑¬∑¬∑(x ‚àí x_{n‚àí1})$$

5. **Newton‚Äôs Divided Difference:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15}  
    $$P_n(x) = f[x_0]+ \sum^n_{k=1}f[x_0, x_1, ... , x_k](x-x_0)¬∑¬∑¬∑(x ‚àí x_{k‚àí1}) \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  (3.10)$$
    > The value of $$f[x_0,x_1,...,x_k]$$ is independent of the order of the numbers $$x_0, x_1, ... ,x_k$$

6. **Generation of Divided Differences:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16} \\
    <button>Table</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![table](/main_files/128a/3/3.3/table.png){: hidden=''}
7. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17} \\
    ![formula](/main_files/128a/3/3.3/1.png){:width="85%"}

***

## Forward Differences
{: #content2}

1. **Forward Difference:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    * [Check here](/work_files/school/128a/2_5#bodyContents13) \\
    ![definition](/main_files/128a/2/2.5/2.png){:width="85%"}

2. **The divided differences (with del notation):**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    ![formula](/main_files/128a/3/3.3/3.png){:width="55%"} \\
    and in general, \\
    ![formula](/main_files/128a/3/3.3/4.png){:width="35%"}
3. **Newton Forward-Difference Formula:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    ![formula](/main_files/128a/3/3.3/5.png){:width="65%"}

    <button>derivation</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![definition](/main_files/128a/3/3.3/deriv.png){: hidden=""}
   
***

## Backward Differences
{: #content3}

1. **Backward Difference:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    ![definition](/main_files/128a/3/3.3/6.png){:width="85%"}

2. **The divided differences:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    > ![definition](/main_files/128a/3/3.3/7.png){:width="60%"} \\
    > and in general, \\
    ![definition](/main_files/128a/3/3.3/8.png){:width="40%"}

    > Consequently, the Interpolating Polynomial \\
    ![definition](/main_files/128a/3/3.3/9.png){:width="75%"} 

    > If we extend the binomial coefficient notation to include all real values of s by letting \\
    ![definition](/main_files/128a/3/3.3/10.png){:width="60%"} \\
    > then \\
    ![definition](/main_files/128a/3/3.3/11.png){:width="75%"}

3. **Newton Backward‚ÄìDifference Formula:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    ![definition](/main_files/128a/3/3.3/12.png){:width="68%"}

## Centered Differences
{: #content4}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} 

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} 
    :   The Newton forward- and backward-difference formulas are not appropriate for    
        approximating $$f(x)$$ when x lies near the center of the table because neither will permit the highest-order difference to have $$x_0$$ close to x.
3. **Stirling's Formula:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents43} \\
    * **If $$n = 2m + 1$$ is odd:** 
    ![definition](/main_files/128a/3/3.3/13.png){:width="85%"}
    * **If $$n = 2m$$ is even:** [we use the same formula but delete the last line]
    <button>Formula</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![definition](/main_files/128a/3/3.3/14.png){: hidden=""}

4. **Table of Entries:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents44} \\
    <button>Table</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![definition](/main_files/128a/3/3.3/15.png){: hidden=""}



0. **OMG**:  
    $$ 
    F_{i,j} = \dfrac{1}{x_{i} ‚àí x_{i-j}}[(F_{i,j-1} ‚àí F_{i-1,j-1})] \\
    Q_{i,j} = \dfrac{1}{x_{i} ‚àí x_{i-j}}[Q_{i,j-1} ‚àí Q_{i-1,j-1}], \\
    $$

***
***

TITLE: 2.1 <br /> The Bisection Method
LINK: research/numerical_optimization_128a/2/2.1.md


## Bisection Technique
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   The first technique, based on the Intermediate Value Theorem, is called the Bisection, or
        Binary-search, method.

0. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents10}
    :   Can be used to accelerate the convergence of a sequence that is linearly convergent,    
        regardless of its origin or application.

2. **Method:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    ![definition](/main_files/128a/2/2.1/2.png){:width="70%"}

3. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    <button>Show Algorithm</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![definition](/main_files/128a/2/2.1/3.png){:width="75%" hidden=""}

4. **Drawbacks:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    > **It is,** relatively, **slow to converge**: \\
    (that is, N may become quite large before $$| p ‚àí p_N|$$ is sufficiently
    small) \\
    and a good intermediate approximation might be inadvertently discarded


5. **Stopping Criterions:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    ![definition](/main_files/128a/2/2.1/6.png){:width="50%"}
    > The best criterion is (2.2)

6. **Convergence:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16} \\
    ![Img](/main_files/128a/2/2.1/7.png){:width="80%"}
    > It Always converges to a solution!

7. **Rate of Convergence \ Error Bound:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17} \\
    ![Img](/main_files/128a/2/2.1/8.png){:width="60%"} \\
    ![Img](/main_files/128a/2/2.1/9.png){:width="42%"}

8. **The problem of Percision:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents18} \\
    We use, \\
    ![Img](/main_files/128a/2/2.1/10.png){:width="42%"}

9. **The Signum Function:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents19} \\
    We use, \\
    ![Img](/main_files/128a/2/2.1/11.png){:width="24%"}

10. **MatLab Implementation:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents110} \\
    ![Img](/main_files/128a/2/2.1/Matlab_Bisection.png){:width="50%"}

***
***

TITLE: 2.5 <br /> Accelerating Convergence 
LINK: research/numerical_optimization_128a/2/2.5.md



## Aitken‚Äôs $$ \Delta^2 $$ Method 
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} \\
    ![definition](/main_files/128a/2/2.5/1.png){:width="80%"}

    [Derivation can be found here!](/main_files/128a/2/2.5/derivation.png)

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    Can be used to accelerate the convergence of a sequence that is linearly convergent, regardless of its origin or application.

3. **Del [Forward Difference]:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    ![definition](/main_files/128a/2/2.5/2.png){:width="85%"}

4. **$$\hat{p}_n$$ [Formula]:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    ![definition](/main_files/128a/2/2.5/3.png){:width="65%"}

5. **Generating the Sequence [Formula]:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    ![definition](/main_files/128a/2/2.5/seq.png){:width="55%"}


## Steffensen‚Äôs Method
{: #content2}

1. **What?:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} 
    :    By applying a modification of Aitken‚Äôs $$ \Delta^2 $$ method to a linearly convergent  
        sequence obtained from fixed-point iteration, we can accelerate the convergence to quadratic.

2. **Zeros and their Multiplicity:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
![definition](/main_files/128a/2/2.4/7.png)

3. **Difference from Aitken's method:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    * **Aitken's method:** Constructs the terms in order
    ![seq](/main_files/128a/2/2.5/seq.png){:height="63px"}
    * **Steffensen‚Äôs method:** constructs the same
    first four terms, $$p_0, p_1, p_2,$$ and $$\hat{p}_0$$. However, at this step we assume that $$\hat{p}_0$$ is a better
    approximation to $$p$$ than is $$p_2$$ and apply fixed-point iteration to $$\hat{p}_0$$ instead of $$p_2$$
    ![seq_2](/main_files/128a/2/2.5/5.png){:width="85%"}

    > Notice \\
    > Every third term of the Steffensen sequence is generated by Eq. (2.15);\\
    > the others use fixed-point iteration on the previous term.

4. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    <button>Show Algorithm</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![Algorithm](/main_files/128a/2/2.5/6.png){:width="70%" hidden=""}

5. **Convergance of Steffensen‚Äôs Method:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\
    ![Thm 2.15](/main_files/128a/2/2.5/7.png){:width="85%"}

    > Steffensen‚Äôs Method gives quadratic convergence without evaluating a derivative.

6. **MatLab Implementation:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents6} \\
    ![Implementation](/main_files/128a/2/2.5/Matlab_Steffensen.png){:width="60%"}



***
***

TITLE: 2.4 <br /> Error Analysis For Iterative Methods 
LINK: research/numerical_optimization_128a/2/2.4.md



## Order of Convergence 
{: #content1}

1. **Order of Convergence:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} \\
![definition](/main_files/128a/2/2.4/1.png)

2. **Important, Two cases of order:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
<img src="/main_files/128a/2/2.4/2.png" alt="Ahmad Badary" style="width: 70%;"/>

3. **An arbitrary technique that generates a convergent sequences does so only linearly:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
![definition](/main_files/128a/2/2.4/3.png){: width="85%"}
    > Theorem 2.8 implies that higher-order convergence for fixed-point methods of the form
    > $$ g(p) = p $$ can occur only when $$ g'(p) = 0 $$.

4. **Conditions to ensure Quadratic Convergence:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
![definition](/main_files/128a/2/2.4/4.png){: width="85%"}


5. **Theorems 2.8 and 2.9 imply:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    (i)\\
    ![definition](/main_files/128a/2/2.4/5.png){:width="90%"} \\
    (ii)\\
    ![definition](/main_files/128a/2/2.4/6.png){:width="90%"}

5. **Newtons' Method Convergence Rate:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    ![definition](/main_files/128a/2/2.4/6.png){:width="90%"}
    <button>Show Proof</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/2/2.4/derivation1.png){: width="80%" hidden=""}

## Multiple Roots 
{: #content2}

1. **Problem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    Newton‚Äôs method and the Secant method will generally give
    problems if $$ f'( p) = 0$$ when $$f ( p) = 0 $$.

2. **Zeros and their Multiplicity:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
![definition](/main_files/128a/2/2.4/7.png)

3. **Identifying Simple Zeros:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    ![Thm](/main_files/128a/2/2.4/8.png)
    * **Generalization of Theorem 2.11:**
    ![Thm](/main_files/128a/2/2.4/9.png)

        > The result in Theorem 2.12 implies that an interval about p exists where Newton‚Äôs
        > method converges quadratically to p for any initial approximation $$ p_0 = p$$, provided that p
        > is a simple zero.

4. **Why Simple Zeros:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    Quadratic convergence might not occur if the zero is not simple
    > Example:
    > Let $$f (x) = e^x ‚àí x ‚àí 1$$ 
    > Notice that Newton‚Äôs method with $$p_0 = 1$$ converges to the zero $$x=0$$ but not quadratically


5. **Handling the problem of multiple roots:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\
    We Modify Newton's Method \\
    We define $$g(x)$$ as: \\
        ![definition](/main_files/128a/2/2.4/12.png){:height="80px"} 

    [Derivation can be found here!](/main_files/128a/2/2.4/derivation.png)

    * **Properties:**
        * If g has the required continuity conditions, functional iteration applied to $$g$$ will be
        quadratically convergent regardless of the multiplicity of the zero of $$f$$ .
        * Theoretically, the only drawback to this method is the additional calculation of $$f
        (x)$$ and the more laborious procedure of calculating the iterates.
        * In practice, multiple roots can cause serious round-off problems because the denominator of (2.13) consists of the difference of two numbers that are both close to 0.
        * In the case of a simple zero the original Newton‚Äôs method requires substantially less computation.


***
***

TITLE: 2.3 <br /> Newton‚Äôs Method and Its Extensions
LINK: research/numerical_optimization_128a/2/2.3.md


## Newton‚Äôs Method
{: #content1}

1. **What?:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    * Newton‚Äôs (or the Newton-Raphson) method is one of the most powerful and well-known
    numerical methods for solving a root-finding problem
    *   <br /> 
    ![definition](/main_files/128a/2/2.3/10.png){:width="90%"}
    *   <br /> 
    ![definition](/main_files/128a/2/2.3/recursive_def.png){:width="90%"}


2. **Derivation:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    [Derivation can be found here](/main_files/128a/2/2.3/derivation.png)

3. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    <button>Click to show Algorithm</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![definition](/main_files/128a/2/2.3/1.png){:width="80%" hidden="" .hides}

4. **Stopping Criterions:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    ![definition](/main_files/128a/2/2.3/stopping.png){:height="150px"}

5. **MatLab Implementation:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    ![definition](/main_files/128a/2/2.3/Matlab_Newton.png){:width="60%"}

***

## Convergence using Newton‚Äôs Method
{: #content2}

1. **Convergence Theorem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    ![definition](/main_files/128a/2/2.3/4.png){:width="90%"}
    > The crucial assumption is that the
    > term involving $$( p ‚àí p_0)^2$$ is, by comparison with $$| p ‚àí p_0|$$, so small that it can be deleted

    > Theorem 2.6 states that, \\
    > (1) Under reasonable assumptions, Newton‚Äôs method converges
    >   provided a sufficiently accurate initial approximation is chosen. \\
    > (2) It also implies that the constant k that bounds the derivative of g, and, consequently, indicates, the speed of convergence
    >   of the method, decreases to 0 as the procedure continues.

## The Secant Method
{: #content3}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    In Newton's Method \\
    We approximate $$f'( p_n‚àí1)$$ as:\\
    ![definition](/main_files/128a/2/2.3/12.png){:width="65%"} \\
    To produce: \\
    ![definition](/main_files/128a/2/2.3/11.png){:width="75%"}

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    > **Newton's Method Weakness**: 
    > the need to know the value of the derivative of f at each approximation.
    >   > Frequently, $$f'(x)$$ is harder and needs more arithmetic operations to calculate than $$f(x)$$.

    > Note: only one function evaluation is needed per step for the Secant method after $$p_2$$ has been 
    determined. In contrast, each step of Newton‚Äôs method requires an evaluation of both the function and its derivative.
3. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    <button>Click to show Algorithm</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![definition](/main_files/128a/2/2.3/6.png){:width="75%" hidden="" .hides}

4. **Convergence Speed:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34} \\
    Generally, \\
    The convergence of the *Secant method* is much faster than *functional iteration* but slightly slower than *Newton‚Äôs method*.

## The Method of False Position
{: #content4}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} \\
    The method of False Position (also called Regula Falsi) generates approximations
    in the same manner as the Secant method, but it includes a test to ensure that the root is
    always bracketed between successive iterations.

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} \\
    Root bracketing is not guaranteed for either Newton‚Äôs method or the Secant method.

3. **Method:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents43} \\
    ![definition](/main_files/128a/2/2.3/13.png){:height="320px"}

4. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents44} \\
    <button>Click to show Algorithm</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![definition](/main_files/128a/2/2.3/8.png){:width="70%" .hides hidden=""}



***
***

TITLE: 2.2 <br /> Fixed-Point Iteration
LINK: research/numerical_optimization_128a/2/2.2.md


## Fixed-Point Problems
{: #content1}

1. **Fixed Point:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
:   A fixed point for a function is a number at which the value of the function does not change
    when the function is applied.
    ![definition](/main_files/128a/2/2.2/1.png){:width="80%"}

2. **Root-finding problems and Fixed-point problems:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    > Root Finding and Fixed-point problems are equivalent in the following sense 

    ![definition](/main_files/128a/2/2.2/11.png){:width="80%"}

3. **Why?:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    Although the problems we wish to solve are in the root-finding form, the fixed-point
    form is easier to analyze, and certain fixed-point choices lead to very powerful root-finding
    techniques.

4. **Existence and Uniqueness of a Fixed Point.:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    ![definition](/main_files/128a/2/2.2/4.png){:height="170px"}

***

## Fixed-Point Iteration
{: #content2}

1. **Approximating Fixed-Points:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
        ![definition](/main_files/128a/2/2.2/5.png){:width="90%"}

2. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
        ![definition](/main_files/128a/2/2.2/6.png){:width="75%"}
        ![definition](/main_files/128a/2/2.2/7.png){:width="75%"}

3. **Convergence:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23}
    * **Fixed-Point Theorem:** \\
        ![definition](/main_files/128a/2/2.2/9.png){:width="90%"}
    * **Error bound in using $$p_n$$ for $$p$$:** \\
        ![definition](/main_files/128a/2/2.2/10.png){:width="90%"}

        > Notice: \\
        > The rate of convergence depends on the factor $$k^n$$. The smaller the
        > value of $$k$$, the faster the convergence, which may be very slow if $$k$$ is close to 1.
4. **Using Fixed-Points:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    > **Question**. How can we find a fixed-point problem that produces a sequence that reliably
    > and rapidly converges to a solution to a given root-finding problem?
    
    > **Answer**. Manipulate the root-finding problem into a fixed point problem that satisfies the
    > conditions of Fixed-Point Theorem 2.4 and has a derivative that is as small as possible
    > near the fixed point.

5. **Newton's Method as a Fixed-Point Problem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\
    ![definition](/main_files/128a/2/2.2/Newtons metod as fixed-point problem.png){:width="57%"}

6. **Convergence Example:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26} \\
    <button>Show Example</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![definition](/main_files/128a/2/2.2/example.png){:width="75%" hidden=""}

7. **MatLab Implementation:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27} \\
    ![definition](/main_files/128a/2/2.2/Matlab_Fixed_point.png){:width="60%"}


***
***

TITLE: 2.6 <br /> Zeros of Polynomials and M√ºller‚Äôs Method
LINK: research/numerical_optimization_128a/2/2.6.md



## Algebraic Polynomials
{: #content1}

1. **Fundamental Theorem of Algebra:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} \\
    ![definition](/main_files/128a/2/2.6/1.png)

2. **Existance of Roots:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    ![definition](/main_files/128a/2/2.6/2.png)

3. **Polynomial Equivalence:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    ![definition](/main_files/128a/2/2.6/3.png)
    > This result implies that to show that two polynomials of degree less than or equal to $$n$$ are the same, we only need to show that they agree at $$n + 1$$ values.


## Horner‚Äôs Method
{: #content2}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    Horner‚Äôs method incorporates the [Section 1.2](/work_files/school/128a/2/1_2) nesting technique, and,
    as a consequence, requires only n multiplications and n additions to evaluate an arbitrary
    nth-degree polynomial.

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    To use Newton‚Äôs method to locate approximate zeros of a polynomial P(x), we need to
    evaluate $$P(x)$$ and $$P'(x)$$ at specified values, Which could be really tedious.

3. **Horner's Method:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    ![Thm](/main_files/128a/2/2.6/4.png){:height="130px"} \\
    ![Thm_2](/main_files/128a/2/2.6/5.png){:height="130px"}

4. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    ![Thm_2](/main_files/128a/2/2.6/6.png){:width="70%"}

5. **Horner's Derivatives:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\
    ![Thm_2](/main_files/128a/2/2.6/Horners Derivatives.png){:width="50%"}

6. **Deflation:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26} \\
    ![deflation](/main_files/128a/2/2.6/Deflation.png){:width="50%"}

5. **MatLab Implementation:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\
    ![Implementation](/main_files/128a/2/2.6/Matlab_Horner.png){:width="50%"}


## Complex Zeros: M√ºller‚Äôs Method
{: #content3}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    * A synthetic division involving quadratic polynomials can be devised to approximately
    factor the polynomial so that one term will be a quadratic polynomial whose complex roots
    are approximations to the roots of the original polynomial
    * M√ºller‚Äôs method uses three initial approximations,
    $$p_0, p_1,$$ and $$p_2$$, and determines the next approximation $$p_3$$ by considering the intersection
    of the x-axis with the parabola through $$( p_0,\ f ( p_0)), \ \ ( p_1,\ f ( p_1))$$, and $$\ \ ( p_2,\ f ( p_2))$$

    [Derivation can be found here!](/main_files/128a/2/2.6/derivation.jpg)

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    > **Newton's Method/Secant/False Postion Weakness**:
    > The possibility that the polynomial having complex roots even when all the coefficients are real numbers.
    >   > If the initial approximation is a real number, all subsequent approximations
        will also be real numbers.

3. **Complex Roots:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    ![Thm_1](/main_files/128a/2/2.6/7.png)


4. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34} \\
    <button>Show Algorithm</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![Thm_2](/main_files/128a/2/2.6/derivation.png){:width="77%" hidden=""}
    
5. **Calculations and Evaluations:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents35} \\
    [HERE!](/main_files/128a/2/2.6/Evaluations.png)
    > M√ºller‚Äôs method can approximate the roots of polynomials
    with a variety of starting values.

***
***

TITLE: 5.4 <br /> Runge-Kutta Methods
LINK: research/numerical_optimization_128a/5/5.4.md


## Runge-Kutta methods
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    :   The Taylor methods outlined in the previous section have the desirable property of highorder local truncation error, but the disadvantage of requiring the computation and evaluation of the derivatives of $$f(t, y)$$. This is a complicated and time-consuming procedure for most problems, so the Taylor methods are seldom used in practice.

    :   > **The Runge-Kutta methods** have the high-order local truncation error of the Taylor methods but eliminate the need to compute and evaluate the derivatives of $$f(t, y)$$

3. **Taylor‚Äôs Theorem [2-variables]:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    ![formula](/main_files/128a/5/5.4/1.png){: width="80%"}

***

## Runge-Kutta Methods of Order Two
{: #content2}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\

2. **Midpoint Method:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    * **2nd order:**  
        ![formula](/main_files/128a/5/5.4/2.png){: width="80%"}
    * **Higher order:**  
        ![formula](/main_files/128a/5/5.4/3.png){: width="80%"}
        > The fact that (5.21) has four parameters, however, gives a flexibility in their choice, so a number of $$O(h^2)$$ methods can be derived.  
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.4/derivation.png){: hidden=""}

3. **Modified Euler Method:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    > One of the most important is the *Modified
    Euler method*, which corresponds to choosing $$a_1 = a_2 = \dfrac{1}{2}$$
    and $$\alpha_2 = \delta_2 = h$$. It has the following difference-equation form:  
    ![formula](/main_files/128a/5/5.4/4.png){: width="80%"}

4. **Error Order:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    > The order of error for this new method is the same as that of the Taylor method of order two.

***

## Higher-Order Runge-Kutta Methods
{: #content3}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}
    The term $$T^{(3)}(t, y)$$ can be approximated with error $$O(h^3)$$ by an expression of the form
    $$f(t + \alpha_1, y + \delta_1 f(t + \alpha_2, y + \delta_2 f(t, y)))$$,
    involving four parameters, the algebra involved in the determination of $$\alpha_1, \delta_1, \alpha_2,$$ and $$\delta_2$$ is quite involved.
2. **Heun‚Äôs method:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    ![formula](/main_files/128a/5/5.4/5.png){: width="80%"}

    * **Local Truncation Error:** $$ \mathcal{O}(h^3)$$.

3. **Runge-Kutta Order Four:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    ![formula](/main_files/128a/5/5.4/6.png){: width="35%"}    
    * **Local Truncation Error:** $$ \mathcal{O}(h^4)$$.
        > Provided the solution $$y(t)$$ has **five continuous derivatives**.

    * **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34} \\
        <button>Show Algorithm</button>{: .showText value="show"
         onclick="showTextPopHide(event);"}
        ![formula](/main_files/128a/5/5.4/alg.png){: hidden="" width="75%"}

***

## Computational Comparisons
{: #content4}

1. **What is Compuatation Heavy?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} 
    :   The main computational effort in applying the Runge-Kutta methods is the evaluation of $$f$$.
2. **Function Evaluations:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} \\
    * **Second Order:** | 2 function evaluations per step | Error of order $$\mathcal{O}(h^2)$$.
    * **Fourth Order:** | 4 function evaluations per step | Error of order $$\mathcal{O}(h^4)$$.
3. **Relationship between number of evaluations and order of truncation error:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents43} \\
    ![formula](/main_files/128a/5/5.4/7.png){: width="80%"} 
    > This indicates why the methods of order less than five with
smaller step size are used in preference to the higher-order methods using a larger step size.   

    > This is because..

4. **Comparing lower-order Runge-Kutta Methods:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents44} \\
    The Runge-Kutta method of order four requires four evaluations per step, whereas Euler‚Äôs
    method requires only one evaluation. Hence if the Runge-Kutta method of order four is
    to be superior it should give more accurate answers than Euler‚Äôs method with one-fourth
    the step size. Similarly, if the Runge-Kutta method of order four is to be superior to the second-order Runge-Kutta methods, which require two evaluations per step, it should
    give more accuracy with step size h than a second-order method with step size h/2.

    > This indeed holds true.

***
***

TITLE: 5.11 <br /> TITLE
LINK: research/numerical_optimization_128a/5/5.11.md


## FIRST
{: #content1}









***

## SECOND
{: #content2}









***

## THIRD
{: #content3}









***

## FOURTH
{: #content4}










***
***

TITLE: 5.1 <br /> The Elementary Theory of Initial-Value Problems
LINK: research/numerical_optimization_128a/5/5.1.md


## Theory of IVP
{: #content1}

1. **Lipschitz Condition:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} \\
    ![formula](/main_files/128a/5/5.1/1.png){: width="80%"}

2. **Convex Set:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    ![formula](/main_files/128a/5/5.1/2.png){: width="80%"}

3. **Lipschitz Condition on Convex Sets:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    ![formula](/main_files/128a/5/5.1/3.png){: width="80%"}

4. **Existence and Uniqueness Theorem for First-Order ODEs:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    ![formula](/main_files/128a/5/5.1/4.png){: width="80%"}

***

## Well-Posed Problems
{: #content2}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}
    :   It is the property that small changes, or perturbations, in the statement of the problem introduce correspondingly small changes in the solution.

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22}
    :   Due to round-off errors and measurment error we need to make sure the result of such small errors is minuscule.

3. **A Well-Posed Problem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    ![formula](/main_files/128a/5/5.1/5.png){: width="80%"}
    > The problem specified by (5.3) is called a perturbed problem associated with the
    original problem (5.2).  

    > It assumes the possibility of an error being introduced in the statement of the differential equation, as well as an error Œ¥0 being present in the initial condition.

4. **Conditions for being Well-Posed:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    ![formula](/main_files/128a/5/5.1/6.png){: width="80%"}


***
***

TITLE: 5.5 <br /> Error Control and the Runge-Kutta-Fehlberg Method
LINK: research/numerical_optimization_128a/5/5.5.md


## Adaptive Methods
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}  
    :   > Techniques used to control the error of a difference equation method in an efficient manner by the appropriate choice of mesh points.  
    :   > By using methods of differing order we can predict the local truncation error
    and, using this prediction, choose a step size that will keep it and the global error in check.

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}  
    :   **Adaptive Methods** incorporate in the step-size procedure an estimate of
    the truncation error that does not require the approximation of the higher derivatives of the function.  
    :   They **adapt** the number and position of the nodes used in the approximation to ensure that the truncation error is kept within a specified bound.

3. **Derivation:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.5/derivation.png){: width="80%" hidden=""}  
    
***

## Runge-Kutta-Fehlberg Method
{: #content2}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    ![formula](/main_files/128a/5/5.5/1.png){: width="60%"}

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    > An advantage to this method is that only six evaluations of f are required per step.  

    > As opposed to requiring at least four evaluations of $$f$$ for the fourth-order method and an additional six for the fifth-order method, for a total of at least ten function evaluations.  

    > $$\implies$$ This Method has at least a $$40\%$$ decrease in the number of function evaluations over the use of a pair of arbitrary fourth- and fifth-order methods.

3. **Error Bound Order:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    $$\mathcal{O}(h^5)$$

4. **The choice of "q":**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    * The value of q determined at the ith step is used for two purposes:
        1. **When $$q < 1$$:** to reject the initial choice of $$h$$ at the ith step and repeat the calculations using $$qh$$, and
        2. **When $$q \geq 1$$: to accept the computed value at the ith step using the step size $$h$$, but change the step size to $$qh$$ for the (i + 1)st step.

    > Because of the penalty in terms of function evaluations that must be paid if the steps are repeated, q tends to be chosen conservatively.

    * The choice of q for the "Runge-Kutta-Fehlberg":
        ![formula](/main_files/128a/5/5.5/2.png){: width="60%"}

5. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.5/alg.png){: width="80%" hidden=""}
    * Notice:
        1. Step 9 is added to eliminate large modifications in step size.
        2. This is done to avoid spending too much time with small step sizes in regions with irregularities in the derivatives of y, and to avoid large step sizes, which can result in skipping sensitive regions between the steps.
        3. The step-size increase procedure could be omitted completely from the algorithm.
        4. The step-size decrease procedure used only when needed to bring the error under control.


***
***

TITLE: 5.10 <br /> TITLE
LINK: research/numerical_optimization_128a/5/5.10.md


## FIRST
{: #content1}









***

## SECOND
{: #content2}









***

## THIRD
{: #content3}









***

## FOURTH
{: #content4}










***
***

TITLE: 5.2 <br /> Euler‚Äôs Method
LINK: research/numerical_optimization_128a/5/5.2.md


## Eulers Method
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   * The object of Euler‚Äôs method is to obtain approximations to the well-posed initial-value problem  
    $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \dfrac{dy}{dt} = f(t,y), \ \ \ \ \ \  a \leq b, \ \ \ \ \ \  y(a) = \alpha \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \  (5.6)$$
    :   * A continuous approximation to the solution $$y(t)$$ will not be obtained; instead, approximations to $$y$$ will be generated at various values, called **mesh points**, in the interval $$[a, b]$$.  
    :   * Once the approximate solution is obtained at the points, the approximate solution at other points in the interval can be found by interpolation.


2. **Mesh-Points:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    :   $$t_i = a + ih, \ \ \  \text{for each } i = 0, 1, 2, ... , N $$  

    :   > The mesh points are equally distributed throughout the interval $$[a, b]$$.

3. **Step-Size:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13}
    :   $$
    h = \dfrac{b ‚àí a}{N} = t_{i+1} ‚àí t
    $$

4. **Euler's Method:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    ![formula](/main_files/128a/5/5.2/2.png){: width="80%"}

    > Equation $$(5.8)$$ is called the _**difference equation**_ associated with Euler‚Äôs method.

5. **Derivation:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.2/1.png){: width="80%" hidden=""}

6. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16} \\
    <button>Show Algorithm</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.2/3.png){: width="50%" hidden=""}

7. **Geometric Interpetation:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17} \\
    To interpret Euler‚Äôs method geometrically, note that when $$w_i$$ is a close approximation to $$y(t_i)$$, the assumption that the problem is well-posed implies that
    $$
    f(t_i, w_i) \approx y(t_i) = f(t_i, y(t_i))
    $$.  
    > i.e. each step corresponds to correcting the path by the approximation to the derivative (slope).

***

## Error Bounds for Euler‚Äôs Method
{: #content2}

1. **Comparison Lemmas:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    1. **Lemma 1:**  
    ![formula](/main_files/128a/5/5.2/4.png){: width="80%"}  
    <button>Show proof</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.2/5.png){: width="80%" hidden=""}  

    2. **Lemma 2:**  
    ![formula](/main_files/128a/5/5.2/6.png){: width="80%"}  
    <button>Show proof</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.2/7.png){: width="80%" hidden=""}

2. **Error Bound:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    ![formula](/main_files/128a/5/5.2/8.png){: width="80%"}  
    <button>Show proof</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.2/9.png){: width="80%" hidden=""}

3. **Properties of the Error Bound Theorem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    1. The **Weakness** of Theorem 5.9 lies in the requirement that a bound be known for the second derivative of the solution.  
    2. **However**, if $$\dfrac{\partial f}{\partial t}$$ and $$\dfrac{\partial f}{\partial y}$$ both exist, the chain rule for partial differentiation implies that  
    ![formula](/main_files/128a/5/5.2/10.png){: width="80%"}  
    So it is at times possible to obtain an error bound for $$y''(t)$$ without explicitly knowing $$y(t)$$.  
    3. The **Principal Importance** of the error-bound formula given in Theorem 5.9 is that the bound depends linearly on the step size h.  
    4. *Consequently*, **diminishing the step size** should give correspondingly **greater accuracy** to the approximations.

***

## Finite Digit Approximations
{: #content3}

1. **Euler Method [Finite-Digit Approximations]:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\
    ![formula](/main_files/128a/5/5.2/11.png){: width="80%"}  
    > Where $$\delta_i$$ denotes the round-off error associated with $$u_i$$.

2. **Error Bound for fin-dig approx. to $$y_i$$ given by Euler‚Äôs method:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    ![formula](/main_files/128a/5/5.2/12.png){: width="80%"}  

3. **Properties of the Error Bound on Finit-digit Approximations:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    1. The error bound (5.13) is no longer linear in h.
    2. In fact, since  
    $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \lim_{h\to 0} \ (\dfrac{hM}{2} + \dfrac{\delta}{h}) = \infty,$$  
    the error would be expected to become large for sufficiently small values of h.
    3. Calculus can be used to determine a lower bound for the step size h:  
        ![formula](/main_files/128a/5/5.2/13.png){: width="80%"}  
    4. The **Minimal value** of $$E(h)$$ occurs when,  
    :   $$
    h = \sqrt{\dfrac{2\delta}{M}}  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ (5.14)
    $$ 
    5. Decreasing h beyond this value tends to increase the total error in the approximation; however, normally, $$\delta$$ is so small that the lower bound for h doesn't affect Euler's Method.


***
***

TITLE: 5.6 <br /> Multistep Methods
LINK: research/numerical_optimization_128a/5/5.6.md


## Multi-Step Methods
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   Methods that use the information produced at the steps $$t_0, t_1, .. , t_{i-1}, t_i$$, to approximate $$t_{i+1}$$.

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    :   Since the error $$[\mid w_j ‚àí y(t_j) \mid]$$ increases with each step, we know that the previously computed values of $$t$$ are actually **more accurate** than those coming up next.  
    :   Thus, it makes sense to use these more accurate values to produce the next result.

3. **m-step Multistep Method:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    ![formula](/main_files/128a/5/5.6/1.png){: width="80%"}

4. **Types of Methods:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    1. **Open / Explicit Methods**: If $$b_m = 0$$  because Eq. $$(5.24)$$ then gives $$w_{i+1}$$ explicitly in terms of previously determined values. 

    2. **Closed / Implicit Methods**: If $$b_m \neq 0$$ because $$w_{i+1}$$ occurs on both sides of Eq. $$(5.24)$$, so $$w_{i+1}$$ is specified, only, implicitly. 

5. **Open vs Closed / Explicit vs Implicit:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    In general, the coefficients of the terms involving $$f$$ in the local truncation error are smaller for the implicit methods than for the explicit methods.

6. **Starting Values:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16}
    :   The starting values must be specified, generally by assuming $$w_0 = \alpha$$ and generating the remaining values by either a Runge-Kutta or Taylor method.

7. **Deriving Multi-Step Methods:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17} \\
    ![formula](/main_files/128a/5/5.6/14.png){: width="80%"}

    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.6/derivation.png){: width="87%" hidden=""}

8. **Example [Deriving three-step Adams-Bashforth]:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents18} \\
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.6/15.png){: width="80%" hidden=""}

***

## Adams-Bashforth Explicit Methods
{: #content2}

1. **Adams-Bashforth Two-Step Explicit Method:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    ![formula](/main_files/128a/5/5.6/2.png){: width="80%"}

2. **Adams-Bashforth Three-Step Explicit Method:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    ![formula](/main_files/128a/5/5.6/3.png){: width="80%"}

3. **Adams-Bashforth Four-Step Explicit Method:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    ![formula](/main_files/128a/5/5.6/4.png){: width="80%"}

4. **Adams-Bashforth Five-Step Explicit Method:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    ![formula](/main_files/128a/5/5.6/5.png){: width="80%"}

***

## Adams-Moulton Implicit Methods
{: #content3}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}
    :   Implicit methods are derived by using $$(t_{i+1}, f(t_{i+1}, y(t_{i+1})))$$ as an additional interpolation node in the approximation of the integral,  
    :  $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ \ \ \ \ \ \ $$  $$  \int_{t_{i+1}}^{t_i} f(t, y(t)) dt$$.


2. **Adams-Moulton Two-Step Implicit Method:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    ![formula](/main_files/128a/5/5.6/6.png){: width="80%"}

3. **Adams-Moulton Three-Step Implicit Method:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    ![formula](/main_files/128a/5/5.6/7.png){: width="80%"}

4. **Adams-Moulton Four-Step Implicit Method:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34} \\
    ![formula](/main_files/128a/5/5.6/8.png){: width="80%"}

5. **DrawBacks:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents35} \\
    To apply an implicit method, we must solve the implicit equation for $$w_{i+1}$$.  
    This is not always possible, and even when it can be done the solution for $$w_{i+1}$$
    may not be unique.

***

## Predictor-Corrector Methods
{: #content4}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41}
    :   The combination of an explicit method to predict and an implicit to improve the
    prediction.

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42}
    :   Even though the implicit methods are better than the explicit methods, they have the inherent weakness of first having to convert the method algebraically to an explicit representation for $$w_{i+1}$$.  
    :   This procedure is not always possible,  
    <button>WHY?</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.6/9.png){: width="80%" hidden=""}


3. **Derivation:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents43} \\
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.6/10.png){: width="80%" hidden=""}

4. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents44} \\
    <button>Show Algorithm</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.6/11.png){: width="80%" hidden=""}

5. **Milne‚Äôs method:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents45} \\
    ![formula](/main_files/128a/5/5.6/12.png){: width="80%"}  
    > Derived by integrating an interpolating polynomial over $$[t_{i‚àí3}, t_{i+1}]$$.  

6. **Implicit Simpson's Method:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents46} \\
    ![formula](/main_files/128a/5/5.6/13.png){: width="80%"}  
    > Derived by integrating an interpolating polynomial over $$[t_{i‚àí1}, t_{i+1}]$$.  

7. **Another Predictor-Corrector Method:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents47} \\
    * **Milne‚Äôs method** is occasionally used as a predictor for the **implicit Simpson‚Äôs method**.

    * **Properties:**
        1. The **local truncation error** involved with a predictor-corrector method of the Milne-Simpson type is generally smaller than that of the Adams-Bashforth-Moulton method.
        2. However, the technique has **limited use** because of round-off error problems, which do not occur with the Adams procedure.


***
***

TITLE: 5.7 <br /> Variable Step-Size Multistep Methods
LINK: research/numerical_optimization_128a/5/5.7.md


## Variable Step Multistep Methods
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   A Predictor-Corrector Method that uses variable step sizes for error control.

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    :   Predictor-corrector techniques always generate two approximations at each step, so they are natural candidates for error-control adaptation.

3. **Derivation:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.7/derivation.png){: width="80%" hidden=""}

4. **Choosing '$$q$$':**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    * $$q$$ is, generally, chosen *conservatively*:  
    ![formula](/main_files/128a/5/5.7/1.png){: width="37%"}

5. **Properties:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    1. A **change in step size** for a *multistep method* is **more costly** in terms of function evaluations than for a *one-step method*, because **new equally-spaced** starting values must be computed.
    2. Consequently, we ignore the step-size change whenever the local truncation error is between $$\dfrac{\epsilon}{10}$$ and $$\epsilon$$, that is, when
    ![formula](/main_files/128a/5/5.7/2.png){: width="80%"}
    3. $$q$$ is given an upper bound to ensure that a single unusually accurate approximation does not result in too large a step size.

6. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16} \\
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.7/algorithm.png){: width="80%" hidden=""}

***
***

TITLE: 5.3 <br /> Higher-Order Taylor Methods
LINK: research/numerical_optimization_128a/5/5.3.md


## Local Truncation Error
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    :   The local truncation error at a specified step measures the amount by which the exact
    solution to the differential equation fails to satisfy the difference equation being used for the approximation at that step.

2. **Why?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    > We need a means for comparing the efficiency of various approximation methods.  

    > The **local truncation** will serve quite well to determine not only the local error of a method but the actual approximation error.

3. **Definition:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    ![formula](/main_files/128a/5/5.3/1.png){: width="80%"}


4. **Why local?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    > This error is a local error because it measures the accuracy of the method at a specific step, assuming that the method was exact at the previous step.

5. **What does it depend on?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    > As such, it depends on the **differential equation**, the **step size**, and the **particular step** in the approximation.

6. **Euler Method Truncation Error:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16} \\
    ![formula](/main_files/128a/5/5.3/2.png){: width="60%"}
    <button>(5.7)</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.3/3.png){: hidden="" width="40%"}
    ![formula](/main_files/128a/5/5.3/4.png){: width="60%"}

7. **How to select difference equations methods?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17} \\
    One way to select difference-equation methods for solving ordinary differential equations
    is in such a manner that their local truncation errors are O(hp) for as large a value
    of p as possible, while keeping the number and complexity of calculations of the methods
    within a reasonable bound.

***

## Talors Method
{: #content2}

1. **Taylors Method of order n:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\
    ![formula](/main_files/128a/5/5.3/5.png){: width="80%"}
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.3/6.png){: hidden="" width="80%"}

2. **Approximation Theorem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    ![formula](/main_files/128a/5/5.3/7.png){: width="80%"}
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.3/8.png){: hidden="" width="80%"}

3. **Using Hermite Polynomials to evaluate a differential equations at a midpoint:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    <button>Show Example</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.3/hermite.png){: hidden="" width="80%"}


***
***

TITLE: 5.9 <br /> Higher-Order Equations and Systems of Differential Equations
LINK: research/numerical_optimization_128a/5/5.9.md


## Systems of First Order Differential Equations
{: #content1}

1. **An mth-order System:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} \\
    ![formula](/main_files/128a/5/5.9/1.png){: width="80%"}

2. **Lipschitz condition [several variables]:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} \\
    ![formula](/main_files/128a/5/5.9/2.png){: width="85%"}  

    ![formula](/main_files/128a/5/5.9/3.png){: width="85%"}  

3. **Existence and Uniqueness:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} \\
    ![formula](/main_files/128a/5/5.9/4.png){: width="85%"}

***

## Methods to Solve Systems of Equations
{: #content2}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}
    :   Methods to solve systems of first-order differential equations are generalizations of the methods for a single first-order equation presented earlier in this chapter.


2. **Generalizing Runge-Kutta Order-Four:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\
    <button>Show Derivation</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.9/derivation.png){: width="70%" hidden=""}

3. **Algorithm:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\
    <button>Algorithm</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.9/5.png){: width="70%" hidden=""}

4. **Example [Kirchoffs Law]:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\
    <button>Illustration</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.9/example.png){: width="70%" hidden=""}

***

## Higher-Order Differential Equations
{: #content3}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} 
    :   > New techniques are not required for solving initial-value problems whose equations  have orders higher than one.  
    :   > By relabeling the variables, we can reduce a higher-order differential equation into a system of first-order differential equations and then apply one of the methods we have already discussed.

2. **Converting an mth-order IVP to 1st-order System of Equations:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\
    ![formula](/main_files/128a/5/5.9/6.png){: width="80%"}

3. **Example:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\
    <button>Example</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/128a/5/5.9/example2.png){: width="100%" hidden=""}


***
***

TITLE: PCA <br /> Principal Component Analysis
LINK: research/ml/pca.md


[Visual of PCA, SVD](https://www.youtube.com/watch?v=5HNr_j6LmPc)  
[Derivation - Direction of Maximum Variance](https://www.youtube.com/watch?v=Axs-fuFJVvE)  
[Low-Rank Approximation w/ SVD (code, my github)](https://github.com/AhmedBadary/Statistical-Analysis/blob/master/Image%20Compression%20using%20Low-Rank%20Approximation%20(SVD).ipynb)  
[PPCA - Probabilistic PCA Slides](https://people.cs.pitt.edu/~milos/courses/cs3750-Fall2007/lectures/class17.pdf)  



## PCA
{: #content1}

1. **What?**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}    
    It is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.  
    <br>

2. **Goal?**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}    
    Given points $$\mathbf{x}_ i \in \mathbf{R}^d$$, find k-directions that capture most of the variation.  
    <br>

3. **Why?**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}    
    1. Find a small basis for representing variations in complex things.
        > e.g. faces, genes.  

    2. Reducing the number of dimensions makes some computations cheaper.  
    3. Remove irrelevant dimensions to reduce over-fitting in learning algorithms.
        > Like "_subset selection_" but the features are __not__ _axis aligned_.  
        > They are linear combinations of input features.  

    4. Represent the data with fewer parameters (dimensions)  
    <br>

4. **Finding Principal Components:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}   
    * Let '$$X$$' be an $$(n \times d)$$ design matrix, centered, with mean $$\hat{x} = 0$$.
    * Let '$$w$$' be a unit vector.
    * The _Orthogonal Projection_ of the point '$$x$$' onto '$$w$$' is $$\tilde{x} = (x.w)w$$.
        > Or $$\tilde{x} = \dfrac{x.w}{\|w\|_2^2}w$$, if $$w$$ is not a unit vector.
    * Let '$$X^TX$$' be the _sample covariance matrix_,  
        $$0 \leq \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_d$$ be its eigenvalues  and let $$v_1, v_2, \cdots, v_d$$ be the corresponding _Orthogonal Unit Eigen-vectors_.
    * Given _Orthonormal directions (vectors)_ $$v_1, v_2, \ldots, v_k$$, we can write:   

        $$\tilde{x} = \sum_{i=1}^k (x.v_i)v_i.$$  

    > **The Principal Components:** are precisely the eigenvectors of the data's covariance matrix. [Read More](#pcvspd)  

    <br>

5. **Total Variance and Error Measurement:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}   
    * **The Total Variance** of the data can be expressed as the sum of all the eigenvalues:
    <p>$$
        \mathbf{Tr} \Sigma = \mathbf{Tr} (U \Lambda U^T) = \mathbf{Tr} (U^T U \Lambda) = \mathbf{Tr} \Lambda = \lambda_1 + \ldots + \lambda_n. 
        $$</p>
    * **The Total Variance** of the **_Projected_** data is:
    <p>$$
         \mathbf{Tr} (P \Sigma P^T ) = \lambda_1 + \lambda_2 + \cdots + \lambda_k. 
        $$</p>
    * **The Error in the Projection** could be measured with respect to variance.
        * We define the **ratio of variance** "explained" by the projected data (equivalently, the ratio of information _"retained"_) as:  
    <p>$$
        \dfrac{\lambda_1 + \ldots + \lambda_k}{\lambda_1 + \ldots + \lambda_n}. 
        $$</p>  
    > If the ratio is _high_, we can say that much of the variation in the data can be observed on the projected plane.  

    <br>

8. **Mathematical Formulation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    PCA is mathematically defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.  

    Consider a data matrix, $$X$$, with column-wise zero empirical mean (the sample mean of each column has been shifted to zero), where each of the $$n$$ rows represents a different repetition of the experiment, and each of the $$p$$ columns gives a particular kind of feature (say, the results from a particular sensor).  

    Mathematically, the transformation is defined by a set of $$p$$-dimensional vectors of weights or coefficients $${\displaystyle \mathbf {v}_ {(k)}=(v_{1},\dots ,v_{p})_ {(k)}}$$ that map each row vector $${\displaystyle \mathbf {x}_ {(i)}}$$ of $$X$$ to a new vector of principal component scores $${\displaystyle \mathbf {t} _{(i)}=(t_{1},\dots ,t_{l})_ {(i)}}$$, given by:  
    <p>$${\displaystyle {t_{k}}_{(i)}=\mathbf {x}_ {(i)}\cdot \mathbf {v}_ {(k)}\qquad \mathrm {for} \qquad i=1,\dots ,n\qquad k=1,\dots ,l}$$</p>  
    in such a way that the individual variables $${\displaystyle t_{1},\dots ,t_{l}}$$  of $$t$$ considered over the data set successively inherit the maximum possible variance from $$X$$, with each coefficient vector $$v$$ constrained to be a unit vector (where $$l$$ is usually selected to be less than $${\displaystyle p}$$ to reduce dimensionality).  
    
    __The Procedure and what it does:__{: style="color: red"}  
    {: #lst-p}
    * Finds a lower dimensional subspace (PCs) that Minimizes the RSS of projection errors  
    * Produces a vector (1st PC) with the highest possible variance, each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.  
    * Results in an __uncorrelated orthogonal basis set__.  
    * PCA constructs new axes that point to the directions of maximal variance (in the original variable space)  
    <br>


9. **Intuition:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents19}   
    PCA can be thought of as fitting a p-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only a commensurately small amount of information.  

    * Its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data.  
    <br>

11. **PCA Algorithm:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents111}   
    * __Data Preprocessing__:  
        * Training set: $$x^{(1)}, x^{(2)}, \ldots, x^{(m)}$$ 
        * Preprocessing (__feature scaling__ + __mean normalization__):  
            * __mean normalization__:  
                $$\mu_{j}=\frac{1}{m} \sum_{i=1}^{m} x_{j}^{(i)}$$  
                Replace each $$x_{j}^{(i)}$$ with $$x_j^{(i)} - \mu_j$$  
            * __feature scaling__:  
                If different features on different, scale features to have comparable range  
                $$s_j = S.D(X_j)$$ (the standard deviation of feature $$j$$)  
                Replace each $$x_{j}^{(i)}$$ with $$\dfrac{x_j^{(i)} - \mu_j}{s_j}$$    
    * __Computing the Principal Components__:  
        * Compute the __SVD__ of the matrix $$X = U S V^T$$  
        * Compute the Principal Components:  
            <p>$$T = US = XV$$</p>  
            > Note: The $$j$$-th principal component is: $$Xv_j$$  
        * Choose the top $$k$$ components singular values in $$S = S_k$$  
        * Compute the Truncated Principal Components:  
            <p>$$T_k = US_k$$</p>  
    * __Computing the Low-rank Approximation Matrix $$X_k$$__:  
        * Compute the reconstruction matrix:  
            <p>$$X_k = T_kV^T = US_kV^T$$</p>  
    <br>        
    
    __Results and Definitions:__{: style="color: red"}  
    {: #lst-p}
    * Columns of $$V$$ are principal directions/axes  
    * Columns of $$US$$ are principal components ("scores")
    * [Principal Components ("scores") VS Principal Directions/Axes](https://stats.stackexchange.com/questions/174601/difference-between-principal-directions-and-principal-component-scores-in-the-co){: #pcvspd}  

    > __NOTE:__ the analysis above is valid only for (1) $$X$$ w/ samples in rows and variables in columns  (2) $$X$$ is centered (mean=0)  
    <br>

10. **Properties and Limitations:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents110}   
    __Limitations:__{: style="color: red"}  
    * PCA is highly sensitive to the (relative) scaling of the data; no consensus on best scaling.  
    <br>
            

12. **Optimality:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents112}       
    Optimal for Finding a lower dimensional subspace (PCs) that Minimizes the RSS of projection errors  
    <br>

6. **How does PCA relate to CCA:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}   
    __CCA__ defines coordinate systems that optimally describe the cross-covariance between two datasets while __PCA__ defines a new orthogonal coordinate system that optimally describes variance in a single dataset.  
    <br>

7. **How does PCA relate to ICA:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}   
    __Independent component analysis (ICA)__ is directed to similar problems as principal component analysis, but finds additively separable components rather than successive approximations.  
    <br>


13. **What's the difference between PCA estimate and OLS estimate:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents113}   
    

__Notes:__{: style="color: red"}  
{: #lst-p}
* __Variance__ is the _measure of spread_ along only *__one axis__*   
* __SVD(X) vs Spectral-Decomposition($$\Sigma = X^TX$$)__:  
    SVD is better $$\iff$$ more numerically stable $$iff$$ faster  
* __When are the PCs *independent*?__  
    Assuming that the dataset is Gaussian distributed would guarantee that the PCs are independent. [Discussion](https://datascience.stackexchange.com/questions/25789/why-does-pca-assume-gaussian-distribution)   


***

## Derivation 1. Fitting Gaussians to Data with MLE
{: #content2}

[Three Derivations of Principal Components (concise)](http://scribblethink.org/Work/PCAderivations/PCAderivations.pdf)  
[Better Derivations (longer)](https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf)  


1. **What?**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    1. Fit a Gaussian to data with MLE
    2. Choose k Gaussian axes of greatest variance.
    > Notice: MLE estimates a _covariance matrix_; $$\hat{\Sigma} = \dfrac{1}{n}X^TX$$.

2. **Algorithm:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}   
    1. Center $$X$$
    2. Normalize $$X$$.
        > Optional. Should only be done if the units of measurement of the features differ.
    3. Compute the unit Eigen-values and Eigen-vectors of $$X^TX$$
    4. Choose '$$k$$' based on the Eigenvalue sizes
        > Optional. Top to bottom.
    5. For the best k-dim subspace, pick Eigenvectors $$v_{d-k+1}, \cdots, v_d$$.
    6. Compute the coordinates '$$x.v_i$$' of the trainning/test data in PC-Space.

***

## Derivation 2. Maximizing Variance
{: #content3}

1. **What?**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}   
    1. Find a direction '$$w$$' that maximizes the variance of the projected data.
    2. Maximize the variance

2. **Derivation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   $$\max_{w : \|w\|_2=1} \: Var(\left\{\tilde{x_1}, \tilde{x_2}, \cdots, \tilde{x_n} \right\})$$
    :   $$
        \begin{align}
        & \ = \max_{w : \|w\|_2=1}  \dfrac{1}{n} \sum_{i=1}^{n}(x_i.\dfrac{w}{\|w\|})^2 \\
        & \ = \max_{w : \|w\|_2=1}  \dfrac{1}{n} \dfrac{\|xw\|^2}{\|w\|^2}  \\
        & \ = \max_{w : \|w\|_2=1}  \dfrac{1}{n} \dfrac{w^TX^TXw}{w^Tw} \\
        \end{align}
        $$
    :   where $$\dfrac{1}{n}\dfrac{w^TX^TXw}{w^Tw}$$ is the **_Rayleigh Quotient_**.
    :   For any Eigen-vector $$v_i$$, the _Rayleigh Quotient_ is $$ = \lambda_i$$.
    :   $$\implies$$ the vector $$v_d$$ with the largest $$\lambda_d$$, achieves the maximum variance: $$\dfrac{\lambda_d}{n}.$$
    :   Thus, the maximum of the _Rayleigh Quotient_ is achieved at the Eigenvector that has the highest corresponding Eigenvalue.
    :   We find subsequent vectors by finding the next biggest $$\lambda_i$$ and choosing its corresponding Eigenvector.

    * [**Full Derivation**](https://www.youtube.com/embed/Axs-fuFJVvE){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/Axs-fuFJVvE"></a>
        <div markdown="1"> </div>    
    <br>

3. **Another Derivation from Statistics:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    First, we note that, The sample variance along direction $$u$$ can be expressed as a quadratic form in $$u$$:  
    <p>$$ \sigma^2(u) = \dfrac{1}{n} \sum_{k=1}^n [u^T(x_k-\hat{x})]^2 = u^T \Sigma u,$$</p>  

    The data matrix has points $$x_i$$; its component along a proposed axis $$u$$ is $$(x ¬∑ u)$$.  
    The variance of this is $$E(x ¬∑ u ‚àí E(x ¬∑ u))^2$$  
    and the optimization problem is
    <p>$$
        \begin{align}
        \max_{x : \|x\|_2=1} \: E(x ¬∑ u ‚àí E(x ¬∑ u))^2 & \\
        & \ = \max_{u : \|u\|_2=1} \:  E[(u \cdot (x ‚àí Ex))^2] \\
        & \ = \max_{u : \|u\|_2=1} \:  uE[(x ‚àí Ex) \cdot (x ‚àí Ex)^T]u \\
        & \ = \max_{u : \|u\|_2=1} \:  u^T \Sigma u
        \end{align}
        $$</p>
    where the matrix $${\displaystyle \Sigma \:= \dfrac{1}{n} \sum_{j=1}^n (x_j-\hat{x})(x_j-\hat{x})^T}.$$  
    Since $$\Sigma$$ is symmetric, the $$u$$ that gives the maximum value to $$u^T\Sigma u$$ is the eigenvector of $$\Sigma$$ with the largest eigenvalue.  
    The second and subsequent principal component axes are the other eigenvectors sorted by eigenvalue.  

    __Proof of variance along a direction:__{: style="color: red"}  
    <p>$$\boldsymbol{u}^{\top} \operatorname{cov}(\boldsymbol{X}) \boldsymbol{u}=\boldsymbol{u}^{\top} \mathbb{E}\left[(\boldsymbol{X}-\mathbb{E}(\boldsymbol{X}))(\boldsymbol{X}-\mathbb{E}(\boldsymbol{X}))^{\top}\right] \boldsymbol{u}=\mathbb{E}\left[\langle\boldsymbol{u}, \boldsymbol{X}-\mathbb{E}(\boldsymbol{X})\rangle^{2}\right] \geq 0 \\ \implies \\ 
    \operatorname{var}(\langle\boldsymbol{u}, \boldsymbol{X}\rangle)=\mathbb{E}\left[\langle\boldsymbol{u}, \boldsymbol{X}-\mathbb{E} \boldsymbol{X}\rangle^{2}\right]=\boldsymbol{u}^{\top} \operatorname{cov}(\boldsymbol{X}) \boldsymbol{u}$$</p>  

    * [PCA and Covariance Matrices (paper)](http://www.cs.columbia.edu/~djhsu/AML/lectures/notes-pca.pdf)  

***

## Derivation 3. Minimize Projection Error
{: #content4}

1. **What?**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}   
    1. Find direction '$$w$$' that minimizes the _Projection Error_.

2. **Derivation:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}  
    <p>$$
        \begin{align}
        \min_{\tilde{x} : \|\tilde{x}\|_2 = 1} \; \sum_{i=1}^n \|x_i - \tilde{x_i}\|^2 & \\
        & \ = \min_{w : \|w\|_2 = 1} \; \sum_{i=1}^n \|x_i -\dfrac{x_i \cdot w}{\|w\|_2^2}w\|^2 \\
        & \ = \min_{w : \|w\|_2 = 1} \; \sum_{i=1}^n \left[\|x_i\|^2 - (x_i \cdot \dfrac{w}{\|w\|_2})^2\right] \\
        & \ = \min_{w : \|w\|_2 = 1} \; c - n*\sum_{i=1}^n(x_i \cdot \dfrac{w}{\|w\|_2})^2 \\
        & \ = \min_{w : \|w\|_2 = 1} \; c - n*Var(\left\{\tilde{x_1}, \tilde{x_2}, \cdots, \tilde{x_n} \right\}) \\
        & \ = \max_{w : \|w\|_2 = 1} \; Var(\left\{\tilde{x_1}, \tilde{x_2}, \cdots, \tilde{x_n} \right\})
        \end{align}
        $$</p>  
    Thus, minimizing projection error is equivalent to maximizing variance.  


***
***

TITLE: The Naive Bayes Classifier
LINK: research/ml/naive_bayes.md


[Full Treatment of Naive Bayes Classification](http://www.cs.columbia.edu/~mcollins/em.pdf)  
[Bayes Classifier and Bayes Error (paper)](https://www.cs.helsinki.fi/u/jkivinen/opetus/iml/2013/Bayes.pdf)  
[Bayes Classifier and Bayes Error (question)](https://stats.stackexchange.com/questions/296014/why-is-the-naive-bayes-classifier-optimal-for-0-1-loss?noredirect=1&lq=1)  
[Naive Bayes CS188 (+Probabilistic Calibration)](https://www.youtube.com/watch?v=1nOb0vwWkAE)  


## Introduction and the Naive Bayes Classifier
{: #content1}

0. **Naive Bayes:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents10}  
    __Naive Bayes__ is a simple technique for *__constructing classifiers__*.  

1. **Naive Bayes Classifiers:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __Naive Bayes Classifiers__ are a family of simple probabilistic classifiers based on applying [_Bayes' Theorem_](https://en.wikipedia.org/wiki/Bayes%27_theorem) with strong (naive) independence assumptions between the features.  

    __The Assumptions:__{: style="color: red"}  
    {: #lst-p}
    1. __Naive Independence__: the feature probabilities are indpendenet given a class $$c$$.   
    2. __Bag-of-Words__: we assume that the position of the words does _not_ matter.  
    <br>


    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * Not a __Bayesian Method__: the name only references the use of Bayes' theorem in the classifier's decision rule  
    * The __Naive Bayes Classifier__ is a *__Bayes Classifier__* (i.e. minimizes the prob of misclassification)  
    <br>

2. **The Probabilistic Model (Naive Bayes Probability/Statistical Model):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    Abstractly, naive Bayes is a __conditional probability model__:  
    given a problem instance to be classified, represented by a vector $${\displaystyle \: \mathbf{x} =(x_{1},\dots ,x_{n})}$$ representing some $$n$$ features (independent variables), it assigns to this instance probabilities  
    <p>$${\displaystyle p(C_{k}\mid x_{1},\dots ,x_{n}) = p(C_{k}\mid \mathbf {x})}$$</p>  
    for each of the $$k$$ possible classes $$C_k$$.  

    Using __Bayes' Theorem__ we _decompose the conditional probability_ as:  
    <p>$${\displaystyle p(C_{k}\mid \mathbf {x} )={\frac {p(C_{k})\ p(\mathbf {x} \mid C_{k})}{p(\mathbf {x} )}}\,}$$</p>  

    Notice that the *__numerator__* is equivalent to the *__joint probability distribution__*:  
    <p>$$p\left(C_{k}\right) p\left(\mathbf{x} | C_{k}\right) = p\left(C_{k}, x_{1}, \ldots, x_{n}\right)$$</p>  

    Using the __Chain-Rule__ for repeated application of the conditional probability, the _joint probability_ model can be rewritten as:  
    <p>$$p(C_{k},x_{1},\dots ,x_{n})\, = p(x_{1}\mid x_{2},\dots ,x_{n},C_{k})p(x_{2}\mid x_{3},\dots ,x_{n},C_{k})\dots p(x_{n-1}\mid x_{n},C_{k})p(x_{n}\mid C_{k})p(C_{k})$$</p>  

    Using the __Naive Conditional Independence__ assumptions:  
    <p>$$p\left(x_{i} | x_{i+1}, \ldots, x_{n}, C_{k}\right)=p\left(x_{i} | C_{k}\right)$$</p>  
    Thus, we can write the __joint model__ as:  
    <p>$${\displaystyle {\begin{aligned}p(C_{k}\mid x_{1},\dots ,x_{n})&\varpropto p(C_{k},x_{1},\dots ,x_{n})\\&=p(C_{k})\ p(x_{1}\mid C_{k})\ p(x_{2}\mid C_{k})\ p(x_{3}\mid C_{k})\ \cdots \\&=p(C_{k})\prod _{i=1}^{n}p(x_{i}\mid C_{k})\,,\end{aligned}}}$$</p>  

    Finally, the *__conditional distribution over the class variable $$C$$__* is:  
    <p>$${\displaystyle p(C_{k}\mid x_{1},\dots ,x_{n})={\frac {1}{Z}}p(C_{k})\prod _{i=1}^{n}p(x_{i}\mid C_{k})}$$</p>   
    where, $${\displaystyle Z=p(\mathbf {x} )=\sum _{k}p(C_{k})\ p(\mathbf {x} \mid C_{k})}$$ is a __constant__ scaling factor, a __dependent only__ on the, _known_, feature variables $$x_i$$s.  
    <br>

3. **The Classifier:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    We can construct the __classifier__ from the __probabilistic model__ above.  
    The __Naive Bayes Classifier__ combines this model with a __decision rule__.  

    __The Decision Rule:__{: style="color: red"}  
    we commonly use the [__Maximum A Posteriori (MAP)__](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) hypothesis, as the decision rule; i.e. pick the hypothesis that is most probable.  

    The Classifier is the _function that assigns a class label $$\hat{y} = C_k$$_ for some $$k$$ as follows:  
    <p>$${\displaystyle {\hat {y}}={\underset {k\in \{1,\dots ,K\}}{\operatorname {argmax} }}\ p(C_{k})\displaystyle \prod _{i=1}^{n}p(x_{i}\mid C_{k}).}$$</p>  

    It, basically, maximizes the probability of the class given an input $$\boldsymbol{x}$$.  
    <br>

4. **Naive Bayes Estimate VS MAP Estimate:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    __MAP Estimate:__  
    <p>$${\displaystyle {\hat {y}_{\text{MAP}}}={\underset {k\in \{1,\dots ,K\}}{\operatorname {argmax} }}\ p(C_{k})\ p(\mathbf {x} \mid C_{k})}$$</p>  
    __Naive Bayes Estimate:__  
    <p>$${\displaystyle {\hat {y}_{\text{NB}}}={\underset {k\in \{1,\dots ,K\}}{\operatorname {argmax} }}\ p(C_{k})\displaystyle \prod _{i=1}^{n}p(x_{i}\mid C_{k})}$$</p>  

5. **Estimating the Parameters of the Classifier:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    __Parameters to be Estimated:__{: style="color: red"}  
    * The __prior probability of each class__: 
        <p>$$p(C_{k})$$</p>  
    * The __conditional probability of each feature (word) given a class__:  
        <p>$$p(x_{i}\mid C_{k}) \:\: \forall i \in {1, .., n}$$</p>  

    We, generally, use __Maximum Likelihood Estimates__ for the parameters.  

    __The MLE Estimates for the Parameters:__{: style="color: red"}  
    * $$\hat{P}(C_k) = \dfrac{\text{doc-count}(C=C_k)}{N_\text{doc}}$$,  
    <br>
    * $$\hat{P}(x_i | C_i) = \dfrac{\text{count}(x_i,C_j)}{\sum_{x \in V} \text{count}(x, C_j)}$$  
    <br>

6. **MLE Derivation of the Parameter Estimates:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    [Derivation](http://www.cs.cornell.edu/courses/cs5740/2017sp/res/nb-prior.pdf)  
    
    The __Likelihood__ of the observed data (TBC)  

7. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    To estimate the parameters of the "true" MAP estimate, we need a prohibitive number of examples ~ $$\mathcal{O}(\vert x\vert^n \cdot \vert C\vert$$.  

8. **Notes:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}

***



***
***

TITLE: KNN <br> K-Nearest Neighbor
LINK: research/ml/knn.md


## K-Nearest Neighbors (k-NN)
{: #content1}

1. **KNN:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}    
    __KNN__ is a _non-parametric_ method used for classification and regression.  
    It is based on the [__Local Constancy (Smoothness) Prior__](/work_files/research/dl/theory/dl_book_pt1#bodyContents32), which states that "the function we learn should not change very much within a small region.", for generalization.  
    * <button>K-Means & Local Constancy</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](/main_files/dl_book/7.png){: hidden=""}  

    <br>

2. **Structure:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    In both classification and regression, the input consists of the $$k$$ closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:  
    {: #lst-p}
    * In __k-NN classification__, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its $$k$$ nearest neighbors ($$k$$ is a positive integer, typically small). If $$k = 1$$, then the object is simply assigned to the class of that single nearest neighbor.  
    * In __k-NN regression__, the output is the property value for the object. This value is the average of the values of $$k$$ nearest neighbors.  
    <br>

3. **Formal Description - Statistical Setting:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    Suppose we have pairs $${\displaystyle (X_{1},Y_{1}),(X_{2},Y_{2}),\dots ,(X_{n},Y_{n})}$$ taking values in $${\displaystyle \mathbb {R} ^{d}\times \{1,2\}}$$, where $$Y$$ is the class label of $$X$$, so that $${\displaystyle X|Y=r\sim P_{r}}$$ for $${\displaystyle r=1,2}$$ (and probability distributions $${\displaystyle P_{r}}$$. Given some norm $${\displaystyle \|\cdot \|}$$ on $${\displaystyle \mathbb {R} ^{d}}$$ and a point $${\displaystyle x\in \mathbb {R} ^{d}}$$, let $${\displaystyle (X_{(1)},Y_{(1)}),\dots ,(X_{(n)},Y_{(n)})}$$ be a reordering of the training data such that $${\displaystyle \|X_{(1)}-x\|\leq \dots \leq \|X_{(n)}-x\|}$$.  
    <br>

33. **Choosing $$k$$:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents133}  
    Nearest neighbors can produce very complex decision functions, and its behavior is highly dependent on the choice of $$k$$:  
    ![img](https://cdn.mathpix.com/snip/images/vPHUZyVTNj4bOgDpPeCA-_KUR6QvlArX_yCy0FVeeNY.original.fullsize.png){: width="50%" .center-image}  

    Choosing $$k = 1$$, we achieve an _optimal training error_ of $$0$$ because each training point will classify as itself, thus achieving $$100\%$$ accuracy on itself.  
    However, $$k = 1$$ __overfits__ to the training data, and is a terrible choice in the context of the bias-variance tradeoff.  

    Increasing $$k$$ leads to _increase in training error_, but a _decrease in testing error_ and achieves __better generalization__.  

    At one point, if $$k$$ becomes _too large_, the algorithm will __underfit__ the training data, and suffer from __huge bias__.  

    In general, <span>we select $$k$$ using __cross-validation__</span>{: style="color: goldenrod"}.  

    ![img](https://cdn.mathpix.com/snip/images/XaIAJQLphKue6B56LLdXoXM1UMsgVyKlXKkVeW1kjB0.original.fullsize.png){: width="50%" .center-image}  
    :   $$\text{Training and Test Errors as a function of } k \:\:\:\:\:\:\:\:\:\:\:\:$$
    {: style="margin-top: 0; font-size: 72%"}

    <br>

44. **Bias-Variance Decomposition of k-NN:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents144}  
    <button>PDF (189)</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    <iframe hidden="" src="/main_files/ml/knn/knn_bias_var.pdf" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>

    <br>

4. **Properties:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    * __Computational Complexity__:  
        * We require $$\mathcal{O}(n)$$ space to store a training set of size $$n$$. There is no runtime cost during training if we do not use specialized data structures to store the data.  
            However, predictions take $$\mathcal{O}(n)$$ time, which is costly.  
        * There has been research into __Approximate Nearest Neighbors (ANN)__ procedures that quickly find an approximation for the nearest neighbor - some common ANN methods are *__Locality-Sensitive Hashing__* and algorithms that perform dimensionality reduction via *__randomized (Johnson-Lindenstrauss) distance-preserving projections__*.  
        * k-NN is a type of _instance-based learning_, or _"lazy learning"_, where the function is only approximated locally and all computation is deferred until classification.  
    * __Flexibility__:  
        * When $$k>1,$$ k-NN can be modified to output predicted probabilities $$P(Y \vert X)$$ by defining $$P(Y \vert X)$$ as the proportion of nearest neighbors to $$X$$ in the training set that have class $$Y$$.  
        * k-NN can also be adapted for regression ‚Äî instead of taking the majority vote, take the average of the $$y$$ values for the nearest neighbors.  
        * k-NN can learn very complicated, __non-linear__ decision boundaries (highly influenced by choice of $$k$$).  
    * __Non-Parametric(ity)__:  
        k-NN is a __non-parametric method__, which means that the number of parameters in the model grows with $$n$$, the number of training points. This is as opposed to parametric methods, for which the number of parameters is independent of $$n$$.  
    * __High-dimensional Behavior__:  
        * k-NN does NOT behave well in high dimensions.  
            As the _dimension increases_, _data points drift farther apart_, so even the nearest neighbor to a point will tend to be very far away.  
        * It is sensitive to the local structure of the data (in any/all dimension/s).  
    * __Theoretical Guarantees/Properties__:  
        __$$1$$-NN__ has impressive theoretical guarantees for such a simple method:  
        * _Cover and Hart, 1967_ prove that <span>as the number of training samples $$n$$ approaches infinity, the expected prediction error for $$1-\mathrm{NN}$$ is upper bounded by $$2 \epsilon^{*}$$, where $$\epsilon^{*}$$ is the __Bayes (optimal) error__</span>{: style="color: goldenrod"}.  
        * _Fix and Hodges, 1951_ prove that <span>as $$n$$ and $$k$$ approach infinity and if $$\frac{k}{n} \rightarrow 0$$, then the $$k$$ nearest neighbor error approaches the *__Bayes error__*</span>{: style="color: goldenrod"}.  
    <br>

5. **Algorithm and Computational Complexity:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    __Training:__{: style="color: red"}  
    * __Algorithm:__ To train this classifier, we simply store our training data for future reference.  
        Sometimes we store the data in a specialized structure called a *__k-d tree__*. This data structure usually allows for faster (average-case $$\mathcal{O}(\log n)$$) nearest neighbors queries.  
        > For this reason, k-NN is sometimes referred to as *__‚Äúlazy learning‚Äù__*.  
    * __Complexity__: $$\:\:\:\:\mathcal{O}(1)$$   

    __Prediction:__{: style="color: red"}  
    {: #lst-p}
    * __Algorithm__:  
        1. Compute the $$k$$ closest training data points (_"nearest neighbors"_) to input point $$\boldsymbol{z}$$.  
            "Closeness" is quantified using some metric; e.g. __Euclidean distance__.  
        2. __Assignment Stage:__  
            * __Classification__: Find the most common class $$y$$ among these $$k$$ neighbors and classify $$\boldsymbol{z}$$ as $$y$$ (__majority vote__)   
            * __Regression__: Take the __average__ label of the $$k$$ nearest points.  
    * __Complexity__: $$\:\:\:\:\mathcal{O}(N)$$   
    <br>


    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * We choose odd $$k$$ for _binary classification_ to break symmetry of majority vote  
    <br>


6. **Behavior in High-Dimensional Space - Curse of Dimensionality:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    As mentioned, k-NN does NOT perform well in high-dimensional space. This is due to the "Curse of Dimensionality".  

    __Curse of Dimensionality (CoD):__{: style="color: red" #bodyContents16_cod}  
    To understand CoD, we first need to understand the properties of metric spaces. In high-dimensional spaces, much of our low-dimensional intuition breaks down:  

    __Geometry of High-Dimensional Space:__  
    Consider a ball in $$\mathbb{R}^d$$ centered at the origin with radius $$r$$, and suppose we have another ball of radius $$r - \epsilon$$ centered at the origin. In low dimensions, we can visually see that _much of the volume of the outer ball is also in the inner ball_.  
    In general, <span>the volume of the outer ball is proportional to $$r^{d}$$</span>{: style="color: goldenrod"}, while <span>the volume of the inner ball is proportional to $$(r-\epsilon)^{d}$$</span>{: style="color: goldenrod"}.  
    Thus the __ratio of the volume of the inner ball to that of the outer ball__ is:  
    <p>$$\frac{(r-\epsilon)^{d}}{r^{d}}=\left(1-\frac{\epsilon}{r}\right)^{d} \approx e^{-\epsilon d / r} \underset{d \rightarrow \infty}{\longrightarrow} 0$$</p>  
    Hence as $$d$$ gets large, most of the volume of the outer ball is concentrated in the annular region $$\{x : r-\epsilon < x < r\}$$ instead of the inner ball.  
    ![img](https://cdn.mathpix.com/snip/images/7N0Mv-hf0RhjKeHDEzLJuQuEtI156Jq8JjqeaD96PB0.original.fullsize.png){: width="70%" .center-image}  


    __Concentration of Measure:__  
    High dimensions also make Gaussian distributions behave counter-intuitively. Suppose $$X \sim$$ $$\mathcal{N}\left(0, \sigma^{2} I\right)$$. If $$X_{i}$$ are the components of $$X$$ and $$R$$ is the distance from $$X$$ to the origin, then $$R^{2}=\sum_{i=1}^{d} X_{i}^{2}$$. We have $$\mathbb{E}\left[R^{2}\right]=d \sigma^{2},$$ so in expectation a random Gaussian will actually be reasonably far from the origin. If $$\sigma=1,$$ then $$R^{2}$$ is distributed *__chi-squared__* with _$$d$$ degrees of freedom_.  
    One can show that in high dimensions, with high probability $$1-\mathcal{O}\left(e^{-d^{\epsilon}}\right)$$, this multivariate Gaussian will lie within the annular region $$\left\{X :\left|R^{2}-\mathbb{E}\left[R^{2}\right]\right| \leq d^{1 / 2+\epsilon}\right\}$$ where $$\mathbb{E}\left[R^{2}\right]=d \sigma^{2}$$ (one possible approach is to note that as $$d \rightarrow \infty,$$ the chi-squared approaches a Gaussian by the __CLT__, and use a __Chernoff bound__ to show exponential decay). This phenomenon is known as __Concentration of Measure__.  

    Without resorting to more complicated inequalities, we can show a simple, weaker result:  
    $$\bf{\text{Theorem:}}$$ $$\text{If } X_{i} \sim \mathcal{N}\left(0, \sigma^{2}\right), i=1, \ldots, d \text{  are independent and } R^{2}=\sum_{i=1}^{d} X_{i}^{2}, \text{ then for every } \epsilon>0, \\ 
     \text{the following holds: } $$  
    <p>$$\lim_{d \rightarrow \infty} P\left(\left|R^{2}-\mathbb{E}\left[R^{2}\right]\right| \geq d^{\frac{1}{2}+\epsilon}\right)=0$$</p>  
    Thus in the limit, the __squared radius is concentrated about its mean__{: style="color: goldenrod"}.  

    <button>Proof.</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/2sljiV63a9o4RE6sUbYN4yAl8yV0rgz0DCx15z1yTBM.original.fullsize.png){: width="100%" hidden=""}  

    Thus a <span>random Gaussian will lie within a thin annular region away from the origin in high dimensions with high probability</span>{: style="color: goldenrod"}, even though _the mode of the Gaussian bell curve is at the origin_. This illustrates the phenomenon in _high dimensions_ where *__random data is spread very far apart__*.  

    The k-NN classifier was conceived on the principle that _nearby points should be of the same class_ - however, in high dimensions, _even the nearest neighbors_ that we have to a random test point _will_ tend to _be **far** away_, so this principle is _no longer useful_.  
    <br>

7. **Improving k-NN:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    __(1) Obtain More Training Data:__{: style="color: red"}  
    More training data allows us to counter-act the sparsity in high-dimensional space.  

    __(2) Dimensionality Reduction - Feature Selection and Feature Projection:__{: style="color: red"}  
    Reduce the dimensionality of the features and/or pick better features. The best way to counteract the curse of dimensionality.  

    __(3) Different Choices of Metrics/Distance Functions:__{: style="color: red"}  
    We can modify the distance function. E.g.  
    {: #lst-p}
    * The family of __Minkowski Distances__ that are induced by the $$L^p$$ norms:  
        <p>$$D_{p}(\mathbf{x}, \mathbf{z})=\left(\sum_{i=1}^{d}\left|x_{i}-z_{i}\right|^{p}\right)^{\frac{1}{p}}$$</p>  
        > Without preprocessing the data, $$1-\mathrm{NN}$$ with the $$L^{3}$$ distance outperforms $$1-\mathrm{NN}$$ with $$L^{2}$$ on MNIST.  
    * We can, also, use __kernels__ to compute distances in a <span>_different_ feature space</span>{: style="color: goldenrod"}.  
        For example, if $$k$$ is a kernel with associated feature map $$\Phi$$ and we want to compute the Euclidean distance from $$\Phi(x)$$ to $$\Phi(z)$$, then we have:  
        <p>$$\begin{aligned}\|\Phi(\mathbf{x})-\Phi(\mathbf{z})\|_ {2}^{2} &=\Phi(\mathbf{x})^{\top} \Phi(\mathbf{x})-2 \Phi(\mathbf{x})^{\top} \Phi(\mathbf{z})+\Phi(\mathbf{z})^{\top} \Phi(\mathbf{z}) \\ &=k(\mathbf{x}, \mathbf{x})-2 k(\mathbf{x}, \mathbf{z})+k(\mathbf{z}, \mathbf{z}) \end{aligned}$$</p>  
        Thus if we define $$D(\mathrm{x}, \mathrm{z})=\sqrt{k(\mathrm{x}, \mathrm{x})-2 k(\mathrm{x}, \mathrm{z})+k(\mathrm{z}, \mathrm{z})}$$ , then we can perform Euclidean nearest neighbors in $$\mathrm{\Phi}$$-space without explicitly representing $$\Phi$$ by using the kernelized distance function $$D$$.  

    <br>

***
***

TITLE: Clustering
LINK: research/ml/clustering.md



* [Deep Clustering](https://deepnotes.io/deep-clustering)  


## Clustering - Introduction
{: #content1}

1. **Clustering:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  



8. **Notes:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    
    [EM Algorithm Video](https://www.youtube.com/watch?v=REypj2sy_5U)  


    __Clustering Types:__{: style="color: red"}  
    * __Hard Clustering__: clusters do not overlap
        * Elements either belong to a cluster or it doesn't
    * __Soft Clustering__: cluster may overlap  
        * Computes a strength of association between clusters and instances  
        
    __Mixture Models:__{: style="color: red"}  
    {: #lst-p}
    * probabilistically-grounded way of doing soft clustering 
    * each cluster: a generative model (Gaussian or multinomial) 
    * parameters (e.g. mean/covariance are unknown) 

    __Mixture Models as Latent Variable Models:__  
    A mixture model can be described more simply by assuming that each observed data point has a corresponding unobserved data point, or latent variable, specifying the mixture component to which each data point belongs.  
    <br>


    __Expectation Maximization (EM):__{: style="color: red"}  
    {: #lst-p}
    * Chicken and egg problem:  
        * need $$\left(\mu_{a}, \sigma_{a}^{2}\right),\left(\mu_{b}, \sigma_{b}^{2}\right)$$ to guess source of points 
        * need to know source to estimate $$\left(\mu_{a}, \sigma_{a}^{2}\right),\left(\mu_{b}, \sigma_{b}^{2}\right)$$  
    * EM algorithm 
        * start with two randomly placed Gaussians $$\left(\mu_{a}, \sigma_{a}^{2}\right),\left(\mu_{b}, \sigma_{b}^{2}\right)$$
        * for each point: $$P(b \vert x_i)$$ does it look like it came from $$b$$?  
        * adjust $$\left(\mu_{a}, \sigma_{a}^{2}\right),\left(\mu_{b}, \sigma_{b}^{2}\right)$$ to fit points assigned to them  




***
***

TITLE: Regression
LINK: research/ml/regression.md


[Generalized Linear Models and Exponential Family Distributions (Blog!)](http://willwolf.io/2017/05/18/minimizing_the_negative_log_likelihood_in_english/)  


* __Least-Squares Linear Regression__:  
    MLE + Noise Normally Distributed + Conditional Probability Normally Distributed  
* __Logistic Regression__:  
    MLE + Noise $$\sim$$ Logistic Distribution (latent) + Conditional Probability $$\sim$$ Bernoulli Distributed  
* __Ridge Regression__: 
    MAP + Noise Normally Distributed + Conditional Probability Normally Distributed + Weight Prior Normally Distributed  



## Regression
{: #content1}
***

## Linear Regression
{: #content2}


Assume that the target distribution is a sum of a deterministic function $$f(x; \theta)$$ and a normally distributed error $$\epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)$$:  
<p>$$y = f(x; \theta) + \epsilon$$</p>  
Thus, $$y \sim \mathcal{N}\left(f(x; \theta), \sigma^{2}\right)$$, and (we assume) there is a distribution $$p(y\vert x)$$ where $$y \sim \mathcal{N}\left(f(x; \theta), \sigma^{2}\right)$$.  
\- Notice that, $$\epsilon = y - \hat{y} \implies $$  
<p>$$\begin{align} 
    \epsilon &\sim \mathcal{N}\left(0, \sigma^{2}\right) \\
            &\sim \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{\left(\epsilon\right)^{2}}{2 \sigma^{2}}} \\
            &\sim \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{\left(y-\hat{y}\right)^{2}}{2 \sigma^{2}}}
    \end{align}$$</p>  

In LR, the equivalent is:  
We assume that we are given data $$x_{1}, \ldots, x_{n}$$ and outputs $$y_{1}, \ldots, y_{n}$$ where $$x_{i} \in \mathbb{R}^{d}$$ and $$y_{i} \in \mathbb{R}$$ and that there is a distribution $$p(y \vert x)$$ where $$y \sim \mathcal{N}\left(w^{\top} x, \sigma^{2}\right)$$.  
- In other words, we assume that the conditional distribution of $$Y_i \vert \theta$$ is a Gaussian (Each individual term $$p\left(y_{i} \vert \mathbf{x}_ {i}, \boldsymbol{\theta}\right)$$ comes from a Gaussian):  
<p>$$Y_{i} \vert \boldsymbol{\theta} \sim \mathcal{N}\left(h_{\boldsymbol{\theta}}\left(\mathbf{x}_ {i}\right), \sigma^{2}\right)$$</p>  
In other words, we assume that there is a true linear model weighted by some true $$w$$ and the values generated are scattered around it with some error $$\epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)$$.  
Then we just want to obtain the max likelihood estimation:  
<p>$$\begin{aligned} p(Y \vert X, w) &=\prod_{i=1}^{n} p\left(y_{i} \vert x_{i}, w\right) \\ \log p(\cdot) &=\sum_{i}-\log \left(2 \pi \sigma^{2}\right)-\frac{1}{2 \sigma^{2}}\left(y_{i}-w^{\top} x_{i}\right)^{2} \end{aligned}$$</p>  





***

## Logistic Regression
{: #content3}

The errors are not directly observable, since we never observe the actual probabilities directly.  


__Latent Variable Interpretation:__{: style="color: red"}  
The logistic regression can be understood simply as finding the $$\beta$$ parameters that best fit:  
<p>$$y=\left\{\begin{array}{ll}{1} & {\beta_{0}+\beta_{1} x+\varepsilon>0} \\ {0} & {\text { else }}\end{array}\right.$$</p>  
where $\varepsilon$ is an error distributed by the standard logistic distribution.  
The associated latent variable is $${\displaystyle y'=\beta _{0}+\beta _{1}x+\varepsilon }$$. The error term $$ \varepsilon $$ is __not observed__, and so the $$y'$$ is also an unobservable, hence termed "latent" (the observed data are values of $$y$$ and $$ x$$). Unlike ordinary regression, however, the $$ \beta  $$ parameters cannot be expressed by any direct formula of the $$y$$ and $$ x$$ values in the observed data. Instead they are to be found by an iterative search process.  



__Notes:__{: style="color: red"}  
{: #lst-p}
* Can be used with a polynomial kernel.
* Convex Cost Function
* No closed form solution


***
***

TITLE: Ensemble Learning - Aggregating
LINK: research/ml/ens_lern.md



* [Ensemble methods: bagging, boosting and stacking (in detail!)](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)  
* [Bagging Boosting (Lec Slides)](http://www.cs.cornell.edu/courses/cs578/2005fa/CS578.bagging.boosting.lecture.pdf)  
* [Aggregation Methods (Lec! - Caltech)](https://www.youtube.com/watch?v=ihLwJPHkMRY&t=2692)  
* [Aggregation in the context of Deep Learning and Representation Learning (Lec - Hinton)](https://www.youtube.com/watch?v=7kAlBa7yhDM)  



## Ensemble Learning
{: #content1}

1. **Ensemble Learning:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    In machine learning, __Ensemble Learning__ is a set of __ensemble methods__ that use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.  
    <br>

3. **Ensemble Theory:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    An _ensemble_ is itself a __supervised learning algorithm__, because it can be trained and then used to make predictions. The trained ensemble, therefore, represents a single hypothesis. This hypothesis, however, is not necessarily contained within the hypothesis space of the models from which it is built. Thus, ensembles can be shown to have more flexibility in the functions they can represent. This flexibility can, in theory, enable them to over-fit the training data more than a single model would, but in practice, some ensemble techniques (especially __bagging__) tend to reduce problems related to over-fitting of the training data.  

    Empirically, ensembles tend to yield better results when there is a significant diversity among the models. Many ensemble methods, therefore, seek to promote diversity among the models they combine.  
    Although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees).  
    Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to dumb-down the models in order to promote diversity.  
    <br>

    * In any network, the bias can be reduced at the cost of increased variance
    * In a group of networks, the variance can be reduced at no cost to bias

4. **Types of Ensembles:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    * Bayes optimal classifier (Theoretical)
    * Bootstrap aggregating (bagging)
    * Boosting
    * Bayesian parameter averaging
    * Bayesian model combination
    * Bucket of models
    * Stacking
    <br>

5. **Applications:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    * Remote sensing
        * Land cover mapping
        * Change detection
    * Computer security
        * Distributed denial of service
        * Malware Detection
        * Intrusion detection
    * Face recognition
    * Emotion recognition
    * Fraud detection
    * Financial decision-making
    * Medicine
    <br>

7. **Ensemble Size:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    It is an important problem that hasn't been well studied/addressed.  

    More recently, a [theoretical framework](https://static.aminer.org/pdf/fa/cikm2016/shp1026-r.-bonabA.pdf) suggested that there is an ideal number of component classifiers for an ensemble such that having more or less than this number of classifiers would deteriorate the accuracy. It is called __"the law of diminishing returns in ensemble construction"__. Their theoretical framework shows that [using the same number of independent component classifiers as class labels gives the highest accuracy](https://arxiv.org/pdf/1709.02925.pdf).  
    <br>

8. **Notes:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    * Averaging models __increases capacity__.  


    __Ensemble Averaging:__{: style="color: red"}  
    Relies 

***

## Bayes optimal classifier (Theoretical)
{: #content2}

1. **Bayes Optimal Classifier:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    [Bayes Optimal Classifier](https://en.wikipedia.org/wiki/Ensemble_learning#Bayes_optimal_classifier)  
    <br>

***

## Bootstrap Aggregating (Bagging)
{: #content3}

1. **Bootstrap Aggregating (Bagging):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    __Bootstrap Aggregating (Bagging)__ is an ensemble meta-algorithm designed to improve the stability and accuracy of ml algorithms. It is designed to __reduce variance__ and help to __avoid overfitting__.  

    It is applicable to both __classification__ and __regression__ problems.  
    
    Although it is usually applied to __decision tree methods__, it can be used with any type of method. Bagging is a special case of the model averaging approach.  
    <br>

2. **Bootstrapping:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    __Bootstrapping__ is a sampling technique. From a set $$D$$ of $$n$$ sample points, it constructs $$m$$ subsets $$D_i$$, each of size $$n'$$, by sampling from $$D$$ __uniformly__{: style="color: goldenrod"} and __with replacement__{: style="color: goldenrod"}.  
    * By sampling with replacement, __some observations may be repeated__ in each $${\displaystyle D_{i}}$$.  
    * If $$n'=n$$, then for large $$n$$ the set $$D_{i}$$ is expected to have the fraction ($$1 - 1/e$$) ($$\approx 63.2\%$$) of the unique examples of $$D$$, the rest being duplicates.  

    The point of sampling with replacement is to make the re-sampling truly random. If done without replacement, the samples drawn will be dependent on the previous ones and thus not be random.  
    <br>

3. **Aggregating:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    The predictions from the above models are aggregated to make a final combined prediction. This aggregation can be done on the basis of predictions made or the probability of the predictions made by the bootstrapped individual models.  
    <br>


4. **The Algorithm:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    Bagging uses multiple weak models and aggregates the predictions from each of them to get the final prediction. The weak models should be such that each specialize in a particular part of the feature space thus enabling us to leverage predictions from each model to maximum use. As suggested by the name, it consists of two parts, bootstrapping and aggregation.  

    * Given a set $$D$$ of $$n$$ sample points, 
    * __Bootstrapping:__ Construct $$m$$ __bootstap samples__ (subsets) $$D_i$$.  
    * Fit $$m$$ models using the $$m$$ bootstrap samples
    * __Aggregating:__ Combine the models by:  
        * __Regression__: <span>Averaging</span>{: style="color: goldenrod"}  
        * __Classification__: <span>Voting</span>{: style="color: goldenrod"}  
    <br>

    [Bagging and Random Forests (Shewchuk)](https://people.eecs.berkeley.edu/~jrs/189/lec/16.pdf)  

5. **Advantages:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}  
    * Improves __"unstable" procedures__  
    * Reduces variance $$\rightarrow$$ helps avoid overfitting  
    * Ensemble models can be used to capture the linear as well as the non-linear relationships in the data.This can be accomplished by using 2 different models and forming an ensemble of the two.  
    <br>

6. **Disadvantages:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents36}  
    * On the other hand, it can mildly degrade the performance of "stable" methods such as K-NN  
    * It causes a Reduction in the interpretability of the model  
    * Prone to high bias if not modeled properly  
    * Though improves accuracy, it is computationally expensive and hard to design:  
        It is not good for real time applications.    
    <br>

7. **Examples (bagging algorithms):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents37}  
    * __Random Forests:__ is a bagging algorithm that further reduces variance by selecting a __subset of features__   
        1. Suppose there are N observations and M features. A sample from observation is selected randomly with replacement(Bootstrapping).
        1. A subset of features are selected to create a model with sample of observations and subset of features.
        1. Feature from the subset is selected which gives the best split on the training data.(Visit my blog on Decision Tree to know more of best split)
        1. This is repeated to create many models and every model is trained in parallel
        Prediction is given based on the aggregation of predictions from all the models.
    <br>

***

## Boosting
{: #content4}

1. **Boosting:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    __Boosting__ is an ensemble meta-algorithm for primarily __reducing bias__, but _also variance_ in supervised learning. It belongs to a family of machine learning algorithms that __convert weak learners to strong ones__.  

    It is an __iterative__ technique which adjusts the weight of an observation based on the last classification. If an observation was classified incorrectly, it tries to increase the weight of this observation and vice versa. Thus, <span>future weak learners focus more on the examples that previous weak learners misclassified</span>{: style="color: goldenrod"}.  

    Boosting in general decreases the bias error and builds strong predictive models. However, they may sometimes over fit on the training data.  

    Boosting *__increases the capacity__*.  

    __Summary__{: style="color: red"}  
    __Boosting__: create different hypothesis $$h_i$$s sequentially + make each new hypothesis __decorrelated__ with previous hypothesis.  
    * Assumes that this will be combined/ensembled  
    * Ensures that each new model/hypothesis will give a different/independent output  
    <br>

2. **Motivation - "The Hypothesis Boosting Problem":**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}  
    Boosting is based on a question posed by _Kearns_ and _Valiant_ (1988):  
    > <span>"Can a set of __weak learners__ create a _single_ __strong learner__?":</span>{: style="color: blue"}  

    This question was formalized as a hypothesis called "The Hypothesis Boosting Problem".  

    __The Hypothesis Boosting Problem:__{: style="color: red"}  
    Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm [‚Ä¶] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner].  


    * A __weak learner__ is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing).  
    * A __strong learner__ is a classifier that is arbitrarily well-correlated with the true classification.  
    <br>

    __Countering BAgging Limitations:__{: style="color: red"}  
    Bagging suffered from some limitations; namely, that the models can be dependent/correlated which cause the voting to be trapped in the wrong hypothesis of the weak learners. This  motivated the intuition behind Boosting:  
    * Instead of training parallel models, one needs to train models sequentially &
    * Each model should focus on where the previous classifier performed poorly  
    <br>

3. **Boosting Theory and Convexity:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}  
    Only algorithms that are provable boosting algorithms in the __probably approximately correct (PAC) learning__ formulation can accurately be called boosting algorithms. Other algorithms that are similar in spirit to boosting algorithms are sometimes called __"leveraging algorithms"__, although they are also sometimes incorrectly called boosting algorithms.  
    <br>

    __Convexity:__{: style="color: red"}  
    Boosting algorithms can be based on convex or non-convex optimization algorithms:  
    * __Convex Algorithms__:  
        such as __AdaBoost__ and __LogitBoost__, can be <span>"defeated" by random noise</span>{: style="color: goldenrod"} such that they can't learn basic and learnable combinations of weak hypotheses.  
        This limitation was pointed out by _Long & Servedio_ in _2008_.   
    * __Non-Convex Algorithms__:  
        such as __BrownBoost__, was shown to be able to learn from noisy datasets and can specifically learn the underlying classifier of the _"Long‚ÄìServedio dataset"_.  
    <br>

33. **The Boosting MetaAlgorithm:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents433}  
    * __Finding (defining) Weak Learners__:  
        The algorithm defines weak learners as those that have __weak rules__ (rules that are not powerful enough for accurate classification)    
    * __Identifying Weak Rules:__  
        * To find weak rule, we apply base learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.
    * __Choosing different distribution for each round:__  
        1. The base learner takes all the distributions and assign equal weight or attention to each observation.
        2. If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.
        3. Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.
    * __Aggregating Outputs:__  
        Finally, it combines the outputs from weak learner and creates a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classiÔ¨Åed or have higher errors by preceding weak rules.  
    <br>


44. **Boosting Algorithms:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents444}  
    * AdaBoost (Adaptive Boosting)
    * Gradient Tree Boosting
    * XGBoost
    <br>

4. **The AdaBoost Algorithm - Adaptive Boosting:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents44}  
    __AdaBoost:__ It works on similar method as discussed above. It fits a sequence of weak learners on different weighted training data. It starts by predicting original data set and gives equal weight to each observation. If prediction is incorrect using the first learner, then it gives higher weight to observation which have been predicted incorrectly. Being an iterative process, it continues to add learner(s) until a limit is reached in the number of models or accuracy.

    Mostly, we use decision stamps with AdaBoost. But, we can use any machine learning algorithms as base learner if it accepts weight on training data set. We can use AdaBoost algorithms for both classification and regression problem.  


    * [The AdaBoost Boosting Algorithm in detail](https://maelfabien.github.io/machinelearning/adaboost/#the-limits-of-bagging)  
    * [AdaBoost (Shewchuk)](https://people.eecs.berkeley.edu/~jrs/189/lec/24.pdf)  
    * [Boosting (MIT Lecture)](https://www.youtube.com/watch?v=UHBmv7qCey4)  
    <br>

    __Notes:__{: style="color: red"}  
    * order of trees matter in AdaBoost  
    <br>
            

5. **Advantages:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents45}  
    * Decreases the Bias  
    * Better accuracy over Bagging (e.g. Random Forest)  
    * Boosting can lead to learning complex non-linear decision boundaries   
    * [Why does Gradient boosting work so well for so many Kaggle problems? (Quora!)](https://www.quora.com/Why-does-Gradient-boosting-work-so-well-for-so-many-Kaggle-problems)  
    <br>

6. **Disadvantages:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents46}  
    * Reduced interpretability  
    * Harder to tune than other models, because you have so many hyperparameters and you can __easily overfit__  
    * Computationally expensive for training (sequential) and inference   
    <br>

8. **Bagging VS Boosting:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents48}   
    ![Bagging VS Boosting](https://cdn.mathpix.com/snip/images/fBJbH_Ej-9puFnO9piKuoN5ULBmPkMIbnhA6Qo64CU8.original.fullsize.png){: width="80%"}  

***

## Stacking
{: #content5}



*** 




***
***

TITLE: K-Means
LINK: research/ml/kmeans.md


[Explanation](http://www.chioka.in/explain-to-myself-k-means-algorithm/)  
[KMeans Full Treatment](https://cseweb.ucsd.edu/~dasgupta/291-unsup/lec2.pdf)  
[KMeans and EM-Algorithms](http://lear.inrialpes.fr/people/mairal/teaching/2014-2015/M2ENS/notes_cours7.pdf)  
[K-Means (code, my github)](https://github.com/AhmedBadary/Statistical-Analysis/blob/master/K-Means%20Clustering.ipynb)  



# K-Means

__K-Means:__{: style="color: red"}    
It is a method for cluster analysis. It aims to partition $$n$$ observations into $$k$$ clusters in which each observation belongs to the cluster with the nearest mean. It results in a partitioning of the data space into __Voronoi Cells__.  


__IDEA:__{: style="color: red"}  
* Minimizes the aggregate Intra-Cluster distance
* Equivalent to minimizing the Variance
* Thus, it finds k-clusters with __minimum aggregate Variance__.  


__Formal Description:__{: style="color: red"}    
Given a set of observations $$\left(\mathbf{x}_{1}, \mathbf{x} _{2}, \ldots, \mathbf{x}_{n}\right)$$, $$\mathbf{x}_ i \in \mathbb{R}^d$$, the algorithm aims to partition the $$n$$ observations into $$k$$ sets $$\mathbf{S}=\left\{S_{1}, S_{2}, \ldots, S_{k}\right\}$$ so as to minimize the __intra-cluster Sum-of-Squares__ (i.e. __variance__).  

The Objective:  
<p>$$\underset{\mathbf{S}}{\arg \min } \sum_{i=1}^{k} \sum_{\mathbf{x} \in S_{i}}\left\|\mathbf{x}-\boldsymbol{\mu}_{i}\right\|^{2}=\underset{\mathbf{S}}{\arg \min } \sum_{i=1}^{k}\left|S_{i}\right| \operatorname{Var} S_{i}$$</p>  
where $$\boldsymbol{\mu}_i$$ is the mean of points in $$S_i$$. 



__Algorithm:__{: style="color: red"}  
* Choose two random points, call them _"Centroids"_  
* Assign the closest $$N/2$$ points (Euclidean-wise) to each of the Centroids  
* Compute the mean of each _"group"/class_ of points  
* Re-Assign the centroids to the newly computed Means ‚Üë
* REPEAT!

The "assignment" step is referred to as the "expectation step", while the "update step" is a maximization step, making this algorithm a variant of the generalized expectation-maximization algorithm.


__Complexity:__{: style="color: red"}    
The original formulation of the problem is __NP-Hard__; however, __EM__ algorithms (specifically, Coordinate-Descent) can be used as efficient heuristic algorithms that converge quickly to good local minima.  
Lloyds algorithm (and variants) have $${\displaystyle \mathcal{O}(nkdi)}$$ runtime.   


__Convergence:__{: style="color: red"}    
Guaranteed to converge after a finite number of iterations  
* __Proof:__  
    The Algorithm Minimizes a __monotonically decreasing__, __Non-Negative__ _Energy function_ on a finite Domain:  
    By *__Monotone Convergence Theorem__* the objective Value Converges.  

    <button>Show Proof</button>{: .showText value="show"
    onclick="showTextPopHide(event);"}
    ![img](/main_files/ml/kmeans/2.png){: hidden=""}  


__Optimality:__{: style="color: red"}    
* __Locally optimal__: due to convergence property  
* __Non-Globally optimal:__  
    * The _objective function_ is *__non-convex__*  
    * Moreover, coordinate Descent doesn't converge to global minimum on non-convex functions.  


__Objective Function:__{: style="color: red"}    
<p>$$J(S, \mu)= \sum_{i=1}^{k} \sum_{\mathbf{x} \in S_{i}} \| \mathbf{x} -\mu_i \|^{2}$$</p>  


__Optimization Objective:__{: style="color: red"}  
<p>$$\min _{\mu} \min _{S} \sum_{i=1}^{k} \sum_{\mathbf{x} \in S_{i}}\left\|\mathbf{x} -\mu_{i}\right\|^{2}$$</p>  


__Coordinate Descent:__{: style="color: red"}  
* Fix $$S = \hat{S}$$, optimize $$\mu$$:  
    <p>$$\begin{aligned} & \min _{\mu} \sum_{i=1}^{k} \sum_{\mathbf{x} \in \hat{S}_{i}}\left\|\mu_{i}-x_{j}\right\|^{2}\\
        =&  \sum_{i=1}^{k} \min _{\mu_i} \sum_{\mathbf{x} \in \hat{S}_{i}}\left\|\mathbf{x} - \mu_{i}\right\|^{2}
    \end{aligned}$$</p>  
    * __MLE__:  
        <p>$$\min _{\mu_i} \sum_{\mathbf{x} \in \hat{S}_{i}}\left\|\mathbf{x} - \mu_{i}\right\|^{2}$$</p>  
        $$ \implies $$  
        <p>$${\displaystyle \hat{\mu_i} = \dfrac{\sum_{\mathbf{x} \in \hat{S}_ {i}} \mathbf{x}}{\vert\hat{S}_ i\vert}}$$</p>  
        <button>Show Derivation</button>{: .showText value="show"
        onclick="showTextPopHide(event);"}
        ![img](/main_files/ml/kmeans/3.png){: width="75%" hidden=""}  
* Fix $$\mu_i = \hat{\mu_i}, \forall i$$, optimize $$S$$[^1]:  
    <p>$$\arg \min _{S} \sum_{i=1}^{k} \sum_{\mathbf{x} \in S_{i}}\left\|\mathbf{x} - \hat{\mu_{i}}\right\|^{2}$$</p>  
    $$\implies$$  
    <p>$$S_{i}^{(t)}=\left\{x_{p} :\left\|x_{p}-m_{i}^{(t)}\right\|^{2} \leq\left\|x_{p}-m_{j}^{(t)}\right\|^{2} \forall j, 1 \leq j \leq k\right\}$$</p>  
    * __MLE__:  
        <button>Show Derivation</button>{: .showText value="show"
        onclick="showTextPopHide(event);"}
        ![img](/main_files/ml/kmeans/1.png){: width="75%" hidden=""}  







[^1]: The optimization here is an $$\arg \min$$ not a $$\min$$, since we are optimizing for $$i$$ over $$S_i$$.  

***
***

TITLE: Decision Trees
LINK: research/ml/decision_trees.md



## Decision Trees
{: #content1}

<button>Decision Trees (189 notes - local)</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
<iframe hidden="" src="/main_files/ml/decision_trees/n25.pdf#page=4" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>






1. **Decision Trees:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    A __decision tree__ is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains <span>conditional control statements</span>{: style="color: goldenrod"}.  

    The trees have two types of __Nodes:__  
    {: #lst-p}
    1. __Internal Nodes:__ test feature values (usually just $$1$$) and branch accordingly  
    2. __Leaf Nodes:__ specify class $$h(\mathbf{x})$$  

    ![img](https://cdn.mathpix.com/snip/images/tt_eQrxZmN0-fp6GnvjzbJm76S63axyRXQSsJN5egd0.original.fullsize.png){: width="80%"}  
    <br>

11. **CART (Classification And Regression Trees) Learning:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents111}  
    __Decision tree learning__ uses a _decision tree_ (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves).  

    > "Nonlinear method for classification and regression." - Schewchuk  

    __Classification Trees:__{: style="color: red"}  
    Tree models where the _target variable_ can take a _discrete set of values_ are called __classification trees__; in these tree structures:  
    {: #lst-p}
    * __Leaves__ represent __class labels__ and 
    * __Branches__ represent __conjunctions of features__ that lead to those class labels.  


    __Regression Trees:__{: style="color: red"}  
    Decision trees where the _target variable_ can take _continuous values_ (typically real numbers) are called __regression trees__.  

    <br>

2. **Properties:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    * __Non-Linear__
    * Used for Both, __Classification__ and __Regressions__
    * Cuts $$X$$ space into *__rectangular cells__*  
    * Works well with both __quantitative__ and __categorical__ features
    * Interpretable results (inference)
    * Decision boundary can be arbitrarily complicated (increase number of nodes to split on)  
        * Linear Classifiers VS Decision Trees ($$x$$ axis) \| Linear VS Non-Linear data ($$y$$ axis)  
            ![img](https://cdn.mathpix.com/snip/images/-MrzU9U6VLcqC8ca6cZ5mg3aUws5XJGnRG1Q-7BFrJo.original.fullsize.png){: width="50%"}  
    <br>

3. **Classification Learning Algorithm:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    Greedy, top-down learning heuristic.  
    Let $$S \subseteq\{1,2, \ldots, n\}$$ be set of sample point indices.  
    Top-level call: $$S=\{1,2, \ldots, n\} .$$  
    __Algorithm:__{: style="color: red"}  
    ![img](https://cdn.mathpix.com/snip/images/67ZJ9vY4y4bOWqkNaUm5DAt5KveQsfsg_5-_jddkSVI.original.fullsize.png){: width="80%"}  
    * [[**Algo (189)**]](https://www.youtube.com/embed/-blJnZPNwf8?start=968)  

    __(*) How to choose best split?:__{: style="color: red"}  
    {: #lst-p}
    1. Try all splits
    2. For a set $$S$$, let $$J(S)$$ be the __cost__ of $$S$$  
    3. Choose the split that *__minimizes__* $$J(S_l) + J(S_r)$$; or,   
        * The split that minimizes the __weighted average__:  
            <p>$$\dfrac{\left\vert S_{l}\right\vert  J\left(S_{l}\right)+\left\vert S_{r}\right\vert  J\left(S_{r}\right)}{\left\vert S_{l}\right\vert +\left\vert S_{r}\right\vert }$$</p>  

    * [[**Choosing Split (189)**]](https://www.youtube.com/embed/-blJnZPNwf8?start=1495)  

    * __Choosing the Split - Further Discussion:__  
        * For binary feature $$x_{i} :$$ children are $$x_{i}=0 \:\: \& \:\: x_{i}=1$$
        * If $$x_{i}$$ has $$3+$$ discrete values: split depends on application.
            * Sometimes it makes sense to use __multiway__ splits; sometimes __binary__ splits.  
        * If $$x_{i}$$ is quantitative: sort $$x_{i}$$ values in $$S$$ ; try splitting between each pair of unequal consecutive values.
            * We can __radix sort__ the points in __linear time__, and if $$n$$ is huge we should.  
        * __Efficient Splitting (clever bit):__ As you scan sorted list from left to right, you can update entropy in $$\mathcal{O}(1)$$ time per point!  
            * This is important for obtaining a fast tree-building time.  
        * [How to update # $$X$$s and $$O$$s Illustration (189)](https://www.youtube.com/-blJnZPNwf8?start=4338)  
        * [Further Discussion (189)](https://www.youtube.com/-blJnZPNwf8?start=4087)  



    __How to choose the cost $$J(S)$$:__{: style="color: red"}  
    We can accomplish this by __Measuring the Entropy__ (from _information theory_):  
    Let $$Y$$ be a random class variable, and suppose $$P(Y=C) = p_C$$.  
    * The __Self-Information ("surprise")__  of $$Y$$ being class $$C$$ (non-negative) is:  
        <p>$$- \log_2 p_C$$</p>   
        * Event w/ probability $$= 1$$  gives us __zero surprise__  
        * Event w/ probability $$= 0$$  gives us __infinite surprise__  
    * The __Entropy ("average surprisal")__ of an index set $$S$$:  
        <p>$$H(S)=-\sum_{\mathbf{C}} p_{\mathbf{C}} \log _{2} p_{\mathbf{C}}, \quad \text { where } p_{\mathbf{C}}=\dfrac{\left|\left\{i \in S : y_{i}=\mathbf{C}\right\}\right|}{|S|}$$</p>  
        The proportion of points in $$S$$ that are in class $$C$$.  
        * If all points in $S$ belong to same class? $$H(S)=-1 \log_{2} 1=0$$  
        * Half class $$C,$$ half class $$D ? H(S)=-0.5 \log_{2} 0.5-0.5 \log_{2} 0.5=1$$  
        * $$n$$ points, all different classes? $$H(S)=-\log_{2} \dfrac{1}{n}=\log_{2} n$$  
    * __Weighted avg entropy__ after split is:  
        <p>$$H_{\text {after }}=\dfrac{\left|S_{l}\right| H\left(S_{l}\right)+\left|S_{r}\right| H\left(S_{r}\right)}{\left|S_{l}\right|+\left|S_{r}\right|}$$</p>  
    * <span>Choose the split that __Maximizes *Information Gain*__</span>{: style="color: goldenrod"}:  
        <p>$$\text{Info-Gain} = H(S) - H_{\text{after}}$$</p>  
        $$\iff$$  
        <span>__Minimizing__ $$H_{\text{after}}$$</span>{: style="color: goldenrod"}.  

        * Info gain always positive except when one child is empty or  
            <p>$$\forall \mathrm{C}, P\left(y_{i}=\mathrm{C} | i \in S_{l}\right)=P\left(y_{i}=\mathrm{C} | i \in S_{r}\right)$$</p>  
            * [[**Information Gain VS other costs (189)**]](https://www.youtube.com/embed/-blJnZPNwf8?start=3566)  


    * [[**Choosing Cost - Bad Idea (189)**]](https://www.youtube.com/embed/-blJnZPNwf8?start=1740)  
    * [[**Choosing Cost - Good Idea (189)**]](https://www.youtube.com/embed/-blJnZPNwf8?start=2240)  

    <br>


4. **Algorithms and their Computational Complexity:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    [Algorithms and Complexity (189)](https://www.youtube.com/-blJnZPNwf8?start=4458)  

    __Test Point:__{: style="color: red"}  
    * __Algorithm:__ Walk down tree until leaf. Return its label.  
    * __Run Time:__  
        * __Worst-case Time:__ is $$\mathcal{O}(\text{(tree) depth})$$.  
            * For __binary features__, that‚Äôs $$\leq d$$.  
            * For __Quantitative features__, they may go deeper.  
            * Usually (not always) $$\leq \mathcal{O}(\log n)$$   
    
    __Training:__{: style="color: red"}  
    * __Algorithm:__  
        * For __Binary Features:__ try $$\mathcal{O}(d)$$ splits at each node.  
        * For __Quantitative Features:__ try $$\mathcal{O}(n'd)$$; where $$n' = \#$$ of points in node  
    * __Run Time:__  
        * __Splits/Per-Node Time__: is $$\mathcal{O}(d)$$ for both binary and quantitative.  
            > Quantitative features are asymptotically just as fast as binary features because of our clever way of computing the entropy for each split.  
        * __Points/Per-Node Amount (number)__: is $$\mathcal{O}(n)$$ points per node   
        * __Nodes/per-point Amount (number)__: is $$\mathcal{O}(\text{depth})$$ nodes per point (i.e. each point participates in $$\mathcal{O}(\text{depth})$$ nodes)   
        $$\implies$$  
        * __Worst-case Time:__  
            <p>$$\mathcal{O}(d) \times \mathcal{O}(n) \times \mathcal{O}(\text{depth}) \leq \mathcal{O}(nd  \text{ depth}) $$</p>  
    <br>  

5. **Multivariate Splits:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    [Multivariate Splits (189)](https://www.youtube.com/MPqVQy8tjU0?start=57)  
    <br>

6. **Regression Tree Learning:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    [Regression Trees (189)](https://www.youtube.com/MPqVQy8tjU0?start=420)  
    <br>

7. **Early Stopping:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    [Early Stopping (189)](https://www.youtube.com/MPqVQy8tjU0?start=760)  
    <br>

8. **Pruning:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    [Pruning (189)](https://www.youtube.com/MPqVQy8tjU0?start=1765)  
    <br>


__Notes:__{: style="color: red"}  
{: #lst-p}
* __Number of splits in a Decision Tree__: $$= dn$$  
* __Complexity of finding the split__:  
    1. __Naive:__ $$\mathcal{O}(dn^2)$$  
    2. __Fast (sort):__ $$\mathcal{O}(dn \: log n)$$  

***

## Random Forests
{: #content2}

1. **Ensemble Learning:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    [Ensemble Learning (189)](https://www.youtube.com/MPqVQy8tjU0?start=2711)  
    <br>

2. **Bagging - Bootstrap AGGregating:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    [Bagging (189)](https://www.youtube.com/MPqVQy8tjU0?start=3150)  
    <br>

3. **Random Forests:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  



***
***

TITLE: The Perceptron
LINK: research/ml/1/1.2.md


## Introduction
{: #content1}

1. **The Perceptron:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    :   The **Perceptron** is an algorithm for supervised learning of binary classifiers.

2. **Type:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} 
    :   It is a type of linear classifiers.

3. **The Problem Setup:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} 
    :   Consider $$n$$ sample points $$X_1, X_2, ..., X_n$$.
    :   For each sample point, let $${\displaystyle y_i = {\begin{cases} \:\: 1&{\text{if }}\ X_i \in C \\-1&{\text{if}}\ X_i \notin C\end{cases}}}$$
    :   > where 'C' is a given class of interest.

***

## The Perceptron Method
{: #content2}

0. **Goal:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents20} 
    :   Find weights '$$w$$' such that: $${\displaystyle {\begin{cases}X_i \cdot w \geq 0&{\text{if }}\ y_i = 1\\X_i \cdot w \leq 0&{\text{if }}\ y_i = -1\end{cases}}}$$
    :   > Where $$X_i \cdot w$$ is the signed distance.
    :   Equivalently:  
    :   $$y_iX_i \cdot w \geq 0$$
    :   > Where $$y_iX_i \cdot w \geq 0$$ is a constraint on the problem.

1. **Procedure:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} 
    :   Compute the point of greatest descent until you find a local minima and update the weights using "Gradient Descent".

2. **Decision Function:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} 
    :   $${\displaystyle f(x)={\begin{cases}1&{\text{if }}\ w\cdot X_i+\alpha>0\\0&{\text{otherwise}}\end{cases}}}$$
    :   where $$\alpha$$ is added by [The fictitious Diminsion Trick](#bodyContents28)

3. **Loss Function:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} 
    :   $${\displaystyle L(z, y_i) = {\begin{cases}0&{\text{if }}\ y_i\cdot z_i \geq 0\\-y_i z&{\text{otherwise}}\end{cases}}}$$

4. **Objective (cost) Function:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} 
    :   $$R(w) = \sum_{i=1}^n L(X_i \cdot w, y_i) = \sum_{i \in V} -y_iX_i \cdot w$$
    :   > where $$V$$ is the set of indices $$i$$ for which $$y_iX_i \cdot w < 0$$.  

    * Risk func is __Convex__ but __Non-Smooth__?  

5. **Constraints:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} 
    :   $$y_iX_i \cdot w \geq 0$$

00. **The Optimization Problem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents200} 
    :   Find weights $$w$$ that minimizes $$R(w)$$.

6. **Optimization Methods:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26}
    :   The Perceptron algorithm uses a _numerical optimization_ method.
    :   **Gradient Descent** is the most commonly used method.
    :   **Newtons Method** can also be used to optimize the objective. 

77. **The Gradient Descent Step:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents277}
    :   $$\begin{align}
            \nabla_w R(w) & \ = \\
            & \ = \nabla_w \sum_{i=1}^n L(X_i \cdot w, y_i) \\
            & \ = \nabla_w \sum_{i \in V} -y_iX_i \cdot w \\
            & \ = \sum_{i \in V} -y_iX_i
            \end{align}$$

7. **The Algorithm (Frank Rosenblatt, 1957):**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27}
    :   (1) Choose the weights $$\vec{w}$$ arbitrarily.  
    :   (2) While $$R(\vec{w}) > 0$$:  \\
        $$\:\:\:\:\:\:\:$$ (3) $$V \leftarrow$$ set of indices such that: $$ y_iX_i \cdot w < 0$$  
        $$\:\:\:\:\:\:\:$$ (4) $$ w \leftarrow w + \epsilon \cdot \sum_{i \in V} y_iX_i \;\;\;\;\;\; $$ [GD]  
        $$\:\:\:\:\:\:\:$$ (4) $$ w \leftarrow w + \epsilon \cdot y_iX_i \;\;\;\;\;\; $$ [SGD]  
    :   (5) Recurse

8. **Avoiding the constriction of the separating hyperplane to passing through the origin:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents28}   
    :   In the procedure we have just described, the separating hyperplane that this algorithm will produce will be forced to pass through the origin, since the resulting hyperplane is not translated from the origin.
    :   We can get around that by moving our problem to a higher diminsion.  
        We achieve that by adding a "fictitious" diminsion as follows:
    :   We re-write,  
    :   $$\vec{w}\cdot X_i \rightarrow \left(\begin{array}{c} w_1  & w_2  & \cdots & w_d & \alpha  \end{array} \right) \cdot \left(\begin{array}{ccccc} x_1  \\ x_2 \\ \vdots \\ x_d\\ 1 \end{array} \right)$$
    :   Now, we run the perceptron algorithm in (d + 1)-dimensional space.

12. **The Boundary:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents212}  
    :   The boundary is a hyperplane:  
    :   $$\{x \in \mathbf{R}^d : f(x) = 0\}$$  
    :   > where $$f(x) = wX_i + \alpha$$.


***

## Convergence and Complexity
{: #content3}

1. **The Perceptron Convergence Theorem I:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} 
    :   If the data is linearly separable, the perceptron algorithm will always find  a linear classifier that classifies all the data points correctly.

2. **The Perceptron Convergence Theorem II:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} 
    :   If the perceptron is guranteed to converge on a data set, then it will converge in at most $$\mathcal{O}(\dfrac{R^2}{\gamma})$$ iterations.
    :   > where $$R = \max_i \|X_i\|$$, called the _radius_ of the data, and $$\gamma = $$ max margin possible.

3. **Complexity (Runtime):**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} 
    :   $$\mathcal{O}(n*d)$$
    :   Can be made faster with 'SGD'.
    :   Although the step size/learning rate doesn‚Äôt appear in that big-O expression, it does have an effect on the
    running time, but the effect is hard to characterize.  
        The algorithm gets slower if $$\epsilon$$ is too small because it has to take lots of steps to get down the hill. But it also gets slower if $$\epsilon$$ is too big for a different reason: it jumps
        right over the region with zero risk and oscillates back and forth for a long time.

***

## Further Analysis
{: #content4}

1. **Properties:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} 
    1. It is an _Online_ Algorithm.
    2. The algorithm is quite slow.
    3. There is no way to reliably choose the learning rate.
    4. It is currently obsolete.
    5. It will not converge, nor approach any approximate solutions for non-linearly separable data.  


***
***

TITLE: Maximum Margin Classifiers
LINK: research/ml/1/1.3.md


## Introduction and Set up
{: #content1}

1. **The Margin:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}    
    The **margin** of a linear classifier is the distance from the decision boundary to the nearest sample point.  
    <br>

2. **The current Problem:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}   
    :   All the classifiers discussed thus far (i.e. Centroid, Perceptron) will converge to a correct classifier on linearly seprable data; however, the classifier they converge to is **not** unique nor the best.
    :   > _But what does it mean to be the "__best__" classifier?_
    :   We assume that if we can maximize the distance between the data points to be classified and the hyperplane that classifies them, then we have reached a boundary that allows for the "best-fit", i.e. allows for the most room for error.

3. **The Solution:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13}   
    :   We enforce a constraint that achieves a classifier that has a maximum-margin.

4. **The Signed Distance:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14}    
    The _signed distance_ is the minimum distance from a point to a hyperplane.
    We solve for the signed distance to achieve the following formula for it:
    $$d = \dfrac{\| w \cdot x_0 + b \|}{\|w\|},$$  
    where we have an n-dimensional hyperplane: $$w \cdot x + b = 0$$ and a point $$\mathbf{x}_ n$$.  
    * **Proof.**  
        * Suppose we have an affine hyperplane defined by $$w \cdot x + b$$ and a point $$\mathbf{x}_ n$$.
        * Suppose that $$\mathbf{x} \in \mathbf{R}^n$$ is a point satisfying $$w \cdot \mathbf{x} + b = 0$$, i.e. it is a point on the plane.
        * We construct the vector $$\mathbf{x}_ n‚àí\mathbf{x}$$ which points from $$\mathbf{x}$$ to $$\mathbf{x}_ n$$, and then, project (scalar projection==signed distance) it onto the unique vector perpendicular to the plane, i.e. $$w$$,  

            $$d=| \text{comp}_{w} (\mathbf{x}_ n-\mathbf{x})| = \left| \frac{(\mathbf{x}_ n-\mathbf{x})\cdot w}{\|w\|} \right| = \frac{|\mathbf{x}_ n \cdot w - \mathbf{x} \cdot w|}{\|w\|}.$$

        * Since $$\mathbf{x}$$  is a vector on the plane, it must satisfy $$w\cdot \mathbf{x}=-b$$ so we get  

            $$d=| \text{comp}_{w} (\mathbf{x}_ n-\mathbf{x})| = \frac{|\mathbf{x}_ n \cdot w +b|}{\|w\|}$$  

    Thus, we conclude that if $$\|w\| = 1$$ then the _signed distance_ from a datapoint $$X_i$$ to the hyperplane is $$\|wX_i + b\|$$.

    __(Caltech):__{: style="color: red"}  
    So, now we can characterize the margin, with its size, as the distance, $$\frac{1}{\|\mathbf{w}\|}$$, between the hyperplane/boundary and the closest point to the plane $$\mathbf{x}_ n$$, in both directions (multiply by 2) $$= \frac{2}{\|\mathbf{w}\|}$$ ; given the condition we specified earlier $$\left|\mathbf{w}^{\top} \mathbf{x}_ {n} + b\right|=1$$ for the closest point $$\mathbf{x}_ n$$.  

    Thus, we formulate the optimization problem of *__maximizing the margin__* by _maximizing the distance_, subject to the condition on how we derived the distance:  
    <p>$$\max_{\mathbf{w}} \dfrac{2}{\|\mathbf{w}\|} \:\:\: : \:\: \min _{n=1,2, \ldots, N}\left|\mathbf{w}^{\top} \mathbf{x}_{n}+b\right|=1$$</p>  
    Which we can reformulate by (1) Flipping and __Minimizing__, (2) Taking a square since it's monotonic and convex, and (3) noticing that $$\left|\mathbf{w}^{T} \mathbf{x}_ {n}+b\right|=y_{n}\left(\mathbf{w}^{T} \mathbf{x}_ {n}+b\right)$$ (since the signal and label must agree, their product will always be positive) and the $$\min$$ operator can be replaced by ensuring that for all the points the condition $$y_{n}\left(\mathbf{w}^{\top} \mathbf{x}_ {n}+b\right) \geq 1$$ holds [proof (by contradiction)](https://www.youtube.com/watch?v=eHsErlPJWUU&t=1555) as:   
    <p>$$\min_w \dfrac{1}{2} \mathbf{w}^T\mathbf{w} \:\:\: : \:\: y_{n}\left(\mathbf{w}^{\top} \mathbf{x}_ {n}+b\right) \geq 1 \:\: \forall i \in [1,N]$$</p>  
    Now when we solve the "friendly" equation above, we will get the __separating plane__ with the *__best possible margin__* (best=biggest).  

    To solve the above problem, we need something that deals with __inequality constraints__; thus, we use the __KKT method__ for solving a *__Lagrnagian under inequality constraints__*.  
    The __Lagrange Formulation__:  
    * Formulate the Lagrangian:  
        1. Take each inequality constraint and put them in the _zero-form_ (equality with Zero)  
        2. Multiply each inequality by a Lagrange Multiplier $$\alpha_n$$
        3. Add them to the objective function $$\min_w \dfrac{1}{2} \mathbf{w}^T\mathbf{w}$$  
            The sign will be $$-$$ (negative) simply because the inequality is $$\geq 0$$  
        <p>$$\min_{w, b} \max_{\alpha_n} \mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha}) = \dfrac{1}{2} \mathbf{w}^T\mathbf{w} -\sum_{n=1}^{N} \alpha_{n}\left(y_{n}\left(\mathbf{w}^{\top} \mathbf{x}_ {n}+b\right)-1\right) \:\:\: : \:\: \alpha_n \geq 0$$</p>  
    * Optimize the objective independently, for each of the unconstrained variables:  
        1. Gradient w.r.t. $$\mathbf{w}$$:   
            <p>$$\nabla_{\mathrm{w}} \mathcal{L}=\mathrm{w}-\sum_{n=1}^{N} \alpha_{n} y_{n} \mathrm{x}_ {n}=0 \\ \implies \\ \mathbf{w}=\sum_{n=1}^{N} \alpha_{n} y_{n} \mathbf{x}_ {n}$$</p>  
        2. Derivative w.r.t. $$b$$:  
            <p>$$\frac{\partial \mathcal{L}}{\partial b}=-\sum_{n=1}^{N} \alpha_{n} y_{n}=0 \\ \implies \\ \sum_{n=1}^{N} \alpha_{n} y_{n}=0$$</p>  
    * Get the *__Dual Formulation__* w.r.t. the (_tricky_) __constrained__ variable $$\alpha_n$$:  
        * Substitute with the above conditions in the original lagrangian (such that the optimization w.r.t. $$\alpha_n$$ will become free of $$\mathbf{w}$$ and $$b$$:   
            <p>$$\mathcal{L}(\boldsymbol{\alpha})=\sum_{n=1}^{N} \alpha_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} y_{n} y_{m} \alpha_{n} \alpha_{m} \mathbf{x}_{n}^{\mathrm{T}} \mathbf{x}_{m}$$</p>  
        * Notice that the first constraint $$\mathbf{w}=\sum_{n=1}^{N} \alpha_{n} y_{n} \mathbf{x}_ {n}$$ has-no-effect/doesn't-constraint $$\alpha_n$$ so it's a vacuous constraint. However, not the second constraint $$\sum_{n=1}^{N} \alpha_{n} y_{n}=0$$.   
        * Set the optimization objective and the constraints, a __quadratic function in $$\alpha_n$$__:  
        <p>$$\max_{\alpha} \mathcal{L}(\boldsymbol{\alpha})=\sum_{n=1}^{N} \alpha_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} y_{n} y_{m} \alpha_{n} \alpha_{m} \mathbf{x}_{n}^{\mathrm{T}} \mathbf{x}_{m} \\ \:\:\:\:\:\:\:\:\:\: : \:\: \alpha_n \geq 0 \:\: \forall \: n= 1, \ldots, N \:\: \wedge \:\: \sum_{n=1}^{N} \alpha_{n} y_{n}=0$$</p>  
    * Set the problem as a __Quadratic Programming__ problem:  
        * Change the _maximization_ to _minimization_ by flipping the signs:  
            <p>$$\min _{\alpha} \frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} y_{n} y_{m} \alpha_{n} \alpha_{m} \mathbf{x}_{0}^{\mathrm{T}} \mathbf{x}_{m}-\sum_{n=1}^{N} \alpha_{n}$$</p>  
        * __Isolate the Coefficients from the $$\alpha_n$$s__ and set in _matrix-form_:  
            <p>$$\min _{\alpha} \frac{1}{2} \alpha^{\top} 
                \underbrace{\begin{bmatrix}
                    y_{1} y_{1} \mathbf{x}_{1}^{\top} \mathbf{x}_{1} & y_{1} y_{2} \mathbf{x}_{1}^{\top} \mathbf{x}_{2} & \ldots & y_{1} y_{N} \mathbf{x}_{1}^{\top} \mathbf{x}_{N}  \\
                    y_{2} y_{1} \mathbf{x}_{2}^{\top} \mathbf{x}_{1} & y_{2} y_{2} \mathbf{x}_{2}^{\top} \mathbf{x}_{2} & \ldots & y_{2} y_{N} \mathbf{x}_{2}^{\top} \mathbf{x}_{N} \\
                    \ldots & \ldots & \ldots & \ldots \\
                    y_{N} y_{1} \mathbf{x}_{N}^{\top} \mathbf{x}_{1} & y_{N} y_{2} \mathbf{x}_{N}^{\top} \mathbf{x}_{2} & \ldots & y_{N} y_{N} \mathbf{x}_{N}^{\top} \mathbf{x}_{N} 
                \end{bmatrix}}_{\text{quadratic coefficients}}
            \alpha+\underbrace{\left(-1^{\top}\right)}_ {\text { linear }} \alpha \\ 
        \:\:\:\:\:\:\:\:\:\: : \:\: \underbrace{\mathbf{y}^{\top} \boldsymbol{\alpha}=0}_{\text { linear constraint }} \:\: \wedge \:\: \underbrace{0}_{\text { lower bounds }} \leq \alpha \leq \underbrace{\infty}_{\text { upper bounds }}  $$</p>  
            > The _Quadratic Programming Package_ asks you for the __Quadratic Term (Matrix)__ and the __Linear Term__, and for the __Linear Constraint__ and the __Range of $$\alpha_n$$s__; and then, gives you back an $$\mathbf{\alpha}$$.     

        Equivalently:  
        <p>$$\min _{\alpha} \frac{1}{2} \boldsymbol{\alpha}^{\mathrm{T}} \mathrm{Q} \boldsymbol{\alpha}-\mathbf{1}^{\mathrm{T}} \boldsymbol{\alpha} \quad \text { subject to } \quad \mathbf{y}^{\mathrm{T}} \boldsymbol{\alpha}=0 ; \quad \boldsymbol{\alpha} \geq \mathbf{0}$$</p>  
                
            
6. **Geometric Analysis:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16}    
    First, we notice that for any given plane $$w^Tx = 0$$, the equations, $$\gamma * w^Tx = 0$$, where $$\gamma \in \mathbf{R}$$ is a scalar, basically characterize the same plane and not many planes.  
    This is because $$w^Tx = 0 \iff \gamma * w^Tx = \gamma * 0 \iff \gamma * w^Tx = 0$$.  
    The above implies that any model that takes input $$w$$ and produces a margin, will have to be **_Scale Invariant_**.  
    To get around this and simplify the analysis, I am going to consider all the representations of the same plane, and I am going to pick one where we normalize (re-scale) the weight $$w$$ such that the signed distance (distance to the point closest to the margin) is equal to one:  
    <p>$$|w^Tx_n| > 0 \rightarrow |w^Tx_n| = 1$$</p>  
    , where $$x_n$$ is the point closest to the plane.  
    We constraint the hyperplane by normalizing $$w$$ to this equation $$|w^Tx_i| = 1$$ or with added bias, $$|w^Tx_i + b| = 1$$.  
    This implies that there exists a "slab" of width $$\dfrac{2}{\|w\|}$$.  


5. **The Margin, mathematically:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15}    
    Now, we can mathematically characterize the margin.  
    By substituting the constraints $$\: y_i(w^TX_i+ b) \geq 1, \forall i \in [1,n]$$ and the signed distance:  
    <p>$$\min_i \dfrac{1}{\|w\|} \|w^TX_i + b\| \geq \dfrac{1}{w}$$</p>  

9. **The distance of the point closest to the hyperplane:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents19}  
    :   We find the distance of the point closest to the hyperplane.
    :   Let $$X_n$$ be the point that is closest to the plane, and let $$\hat{w} = \dfrac{w}{\|w\|}$$.  
        Take any point $$X$$ on the plane, and let $$\vec{v}$$ be the vector $$\vec{v} = X_n - X$$.  
    :   Now, the distance, d is equal to 
    :   $$\begin{align}
            d & \ = \|\hat{w}\vec{v}\| \\
            & \ = \|\hat{w}(X_n - X)\| \\
            & \ = \|\hat{w}X_n - \hat{w}X)\| \\
            & \ = \dfrac{1}{\|w\|}\|wX_n + b - wX) - b\|,  & \text{we add and subtract the bias } b\\
            & \ = \dfrac{1}{\|w\|}\|(wX_n + b) - (wX + b)\| \\
            & \ = \dfrac{1}{\|w\|}\|(wX_n + b) - (0)\|,  & \text{from the eq. of the plane on a point on the plane} \\
            & \ = \dfrac{1}{\|w\|}\|(1) - (0)\|,  & \text{from the constraint on the distance of the closest point} \\
            & \ = \dfrac{1}{\|w\|}
            \end{align}
        $$

7. **Slab Existence:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17}  
    :   The analysis done above allows us to conclusively prove that there exists a slab of width $$\dfrac{2}{\|w\|}$$ containing no sample points where the hyperplane runs through (bisects) its center.

8. **Maximizing the Margin:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents18}  
    :   To maximize the margin we need to maximize the width of the slab, i.e. maximize $$\dfrac{2}{\|w\|}$$,   
    or equivalently, 
    :   $$\max_w \dfrac{2}{\|w\|} = \min_w \dfrac{\|w\|}{2} = \min_w \dfrac{1}{2}\|w\| \min_w \dfrac{1}{2}\|w\|^2$$
    :   subject to the constraint mentioned earlier $$\min_i \|wX + b\| = 1, \forall i \in [1,n]$$, or equivalently
    :   $$y_i(wX_i + b) \geq 1, \forall i \in [1,n]$$
    :   since the equation $$y_i(wX_i + b)$$ enforces the absolute value condition as was our analysis for regular linear classifiers.

11. **The Optimization Problem for Maximum Margin Classifiers:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents111}  
    :   $$\min_w \dfrac{1}{2}w^Tw \:\:\: : \:\: y_i(wX_i + b) \geq 1, \forall i \in [1,n]$$
    :   > The above problem is a Quadratic Program, in $$d + 1$$-diminsions and $$n$$-constraints, in standard form.
    :   > Notice that we use the quadratic $$w^Tw$$ instead of the linear $$w$$ as the objective because the quadratic function is smooth at zero as opposed to the linear objective which hinders the optimization.


12. **Notes:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents112}    
    * The weight vector $$\mathbf{w}$$ is orthogonal to the separating-plane/decision-boundary, defined by $$\mathbf{w}^T\mathbf{x} + b = 0$$, in the $$\mathcal{X}$$ space; Reason:  
        Since if you take any two points $$\mathbf{x}^\prime$$ and $$\mathbf{x}^{\prime \prime}$$ on the plane, and create the vector $$\left(\mathbf{x}^{\prime}-\mathbf{x}^{\prime \prime}\right)$$  parallel to the plane by subtracting the two points, then the following equations must hold:  
        <p>$$\mathbf{w}^{\top} \mathbf{x}^{\prime}+b=0 \wedge \mathbf{w}^{\top} \mathbf{x}^{\prime \prime}+b=0 \implies \mathbf{w}^{\top}\left(\mathbf{x}^{\prime}-\mathbf{x}^{\prime \prime}\right)=0$$</p>  
    * In a problem of minimizing a function:  
        * __Unconstrained problem__:  
            You set the gradient of the function to Zero and solve.  
        * __Constrained (regularization?)__:  
            The gradient becomes related to the constraint; the gradient $$\nabla E_{\mathrm{in}}$$ is __normal__ to the *__constraint__*.  
    * __Conceptual Dichotomy between Regularization and SVM__:  
        img
                
            

***
***

TITLE: Hard-Margin Support Vector Machines <br /> SVM
LINK: research/ml/1/1.4.md


## Introduction - Support Vector Machines
{: #content1}

1. **Support Vector Machines:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}  
    **Support Vector Machines** (SVMs) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.  
    The SVM is a [_Maximum Margin Classifier_](/work_files/research/ml/1_3) that aims to find the "maximum-margin hyperplane" that divides the group of points $${\displaystyle {\vec {x}}_{i}} {\vec {x}}_{i}$$ for which $${\displaystyle y_{i}=1}$$ from the group of points for which $${\displaystyle y_{i}=-1}$$.  

3. **Support Vectors:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13}  
    **Support Vectors** are the data-points that lie exactly on the margin (i.e. on the boundary of the slab).  
    They satisfy $$\|w^TX' + b\| = 1, \forall $$ support vectors $$X'$$  

2. **The Hard-Margin SVM:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}  
    The _Hard-Margin SVM_ is just a maximum-margin classifier with features and kernels (discussed later).  

***

## The Hard-Margin SVM
{: #content2}

0. **Goal:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents20}   
    :   Find weights '$$w$$' and scalar '$$b$$' that correctly classifies the data-points and, moreover, does so in the "_best_" possible way.
    :   > Where we had defined _best_ as the classifier that admits the maximum

1. **Procedure:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}   
    (1) Use a linear classifier
    (2) But, Maximize the Margin
    (3) Do so by Minimizing $$\|w\|$$  

2. **Decision Function:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22}   
    :   $${\displaystyle f(x)={\begin{cases}1&{\text{if }}\ w\cdot X_i+\alpha>0\\0&{\text{otherwise}}\end{cases}}}$$

5. **Constraints:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25}   
    :   $$y_i(wX_i + b) \geq 1, \forall i \in [1,n]$$

7. **The Optimization Problem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27}    
    Find weights '$$w$$' and scalar '$$b$$' that minimize  
    <p>$$ \dfrac{1}{2} w^Tw$$</p>  
    Subject to  
    <p>$$y_i(wX_i + b) \geq 1, \forall i \in [1,n]$$</p>  
    Formally,  
    <p>$$\min_w \dfrac{1}{2}w^Tw \:\:\: : \:\: y_i(wX_i + b) \geq 1, \forall i \in [1,n]$$</p>  

6. **Optimization Methods:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26}  
    :   The SVM optimization problem reduces to a [Quadratic Program](work_files/research/conv_opt/3_3).

## Further Analysis
{: #content3}

1. **Generalization:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31}   
    :   We notice that, geometrically, the hyperplane (the maximum margin classifier) is completely characterized by the _support vectors_ (the vectors that lie on the margin).  
    :   A very important conclusion arises.  
        The maximum margin classifier (SVM) depends **only** on the number of support vectors and **_not_** on the diminsion of the problem.  
        This implies that the computation doesn't scale up with the diminsion and, also implies, that the _kernel trick_ works very well.

2. **Properties:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32}   
    :   1. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
        2. The hyperplane is determined solely by its support vectors.
        3. The SVM always converges on linearly seprable data.
        4. The Hard-Margin SVM fails if the data is not linearly separable. 
        4. The Hard-Margin SVM is quite sensetive to outliers

***
***

TITLE: Soft-Margin Support Vector Machines <br /> The SVM
LINK: research/ml/1/1.5.md


## Introduction
{: #content1}

1. **Why another SVM? (i.e. The Problem)**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11} 
    :   The Hard-Margin SVM faces a few issues:  
        1. The Hard-Margin SVM fails if the data is not linearly separable. 
        4. The Hard-Margin SVM is quite sensetive to outliers
    :   The Soft-Margin SVM aims to fix/reconcile these problems.

3. **The solution:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}
    :   Allow some points to violate the margin, by introducing slack variables.

***

## The Soft-Margin SVM
{: #content2}

1. **Procedure:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21}  
    (1) Use a linear classifier  
    (2) But, Maximize the Margin  
    (3) Do so by Minimizing $$\|w\|$$  
    (4) But allow some points to penetrate the margin

5. **_Modified_ Constraints:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25}  
    <p>$$y_i(wX_i + b) \geq 1 - \zeta_i, \forall i \in [1,n]$$</p>  
    where the $$\zeta_i$$s are slack variables.  
    We, also, enforce the non-negativity constraint on the slack variables:  
    <p>$$\zeta_i \geq 0, \:\:\: \forall i \in [1, n]$$</p>  

    > The non-negativity constraint forces the slack variables to be zero for all points that do not violate the original constraint:  
    >   > i.e. are not inside the slab.

4. **_Modified_ Objective (cost) Function:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} 
    :   $$ R(w) = \dfrac{1}{2} w^Tw + C \sum_{i=1}^n \zeta_i$$

7. **The Optimization Problem:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27}  
    Find weights '$$w$$', scalar '$$b$$', and $$\zeta_i$$s that minimize  
    <p>$$ \dfrac{1}{2} w^Tw + C \sum_{i=1}^n \zeta_i$$</p>  
    Subject to  
    <p>$$y_i(wX_i + b) \geq 1 - \zeta_i, \zeta_i \geq 0, \forall i \in [1,n]$$</p>  
    Formally,  
    <p>$$\min_w \dfrac{1}{2}w^Tw \:\:\: : \:\: y_i(wX_i + b) \geq 1 - \zeta_i, \zeta_i \geq 0, \forall i \in [1,n]$$</p>  

6. **Optimization Methods:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26}
    :   The SVM optimization problem reduces to a [Quadratic Program](work_files/research/conv_opt/3_3) in $$d + n + 1$$-dimensions and $$2n$$-constraints.

7. **Effects of the _Regularization Hyperparameter_ ($$C$$):**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27}  
    :   | |__Small C__|__Large C__  
    | __Desire__|Maximizing Margin = $$\dfrac{1}{\|w\|}$$|keep most slack variables zero or small  
    | __Danger__|underfitting (High Misclassification)|overfitting (awesome training, awful test)  
    | __outliers__|less sensitive|very sensitive  
    | __boundary__|more "flat"|more sinuous  
    :   > The last row only applies to nonlinear decision boundaries.
    :   * We choose '$$C$$' with cross-validation.

## An Equivalent Formulation
{: #content3}

1. **Introduction:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31} 
    :   In the current SVM model, we are optimizing the objective $$R(w) = \dfrac{1}{2} w^Tw + C \sum_{i=1}^n \zeta_i$$, which looks like an $$l_2$$ regularization on the weights and an $$l_1$$ regularization on the slack variables.
    :   However, usually in function estimation we prefer the standard-form objective  to minimize (and trade-off); the loss + penalty form.

2. **Modified Loss Function:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32} 
    :   We introduce a loss function to moderate the use of the slack variables (i.e. to avoid abusing the slack variables).
    :   But first we motivate it by comparing it to the traditional $$0-1$$ Loss function.  
        Notice that the $$0-1$$ loss is actually non-convex. It has an infinite slope at $$0$$.  
        On the other hand, the hinge loss is actually convex.
    :   The hinge loss:
    :   $${\displaystyle \max \left(0, 1-y_{i}({\vec {w}}\cdot {\vec {x}}_{i}-b)\right).}$$
    :   This function is zero if the constraint, $$y_{i}({\vec {w}}\cdot {\vec {x}}_{i}-b)\geq 1$$, is satisfied, in other words, if $${\displaystyle {\vec {x}}_{i}} {\vec {x}} _ {i}$$ lies on the correct side of the margin.  
        For data on the wrong side of the margin, the function's value is proportional to the distance from the margin.
    :   All of the above suggests that this loss function is ideal for binary classification as it doesn't penalize correct classification at all.  
        > which is something we are seeking for classification as opposed to regression.

3. **Modified Objective Function:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33} 
    :   $$ R(w) = \dfrac{\lambda}{2} w^Tw +  \sum_{i=1}^n {\displaystyle \max \left(0, 1-y _ {i}({\vec {w}}\cdot {\vec {x}} _ {i}-b)\right)}$$

4. **Proof of Equivalence:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}
    :   Here we show that the two objectives to optimize for the SVM are actually equivalent.
    :   $$\begin{align}
            y_if\left(x_i\right) & \ \geq 1-\zeta_i, & \text{from 1st constraint } \\
            \implies \zeta_i & \ \geq 1-y_if\left(x_i\right) \\
            \zeta_i & \ \geq 1-y_if\left(x_i\right) \geq 0, & \text{from 2nd positivity constraint on} \zeta_i \\
            \iff \zeta_i & \ \geq \max \{0, 1-y_if\left(x_i\right)\} \\
            \zeta_i & \ = \max \{0, 1-y_if\left(x_i\right)\}, & \text{minimizing means } \zeta_i \text{reach lower bound}\\
            \implies R(w) & \ = \dfrac{\lambda}{2} w^Tw +  \sum_{i=1}^n {\displaystyle \max \left(0, 1-y _ {i}({\vec {w}}\cdot {\vec {x}} _ {i}-b)\right)}, & \text{plugging in and multplying } \lambda = \dfrac{1}{C}
            \end{align}$$

5. **The Optimization Problem:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents35} 
    :   Find weights '$$w$$' and scalar '$$b$$' that minimize
    :   $$ \dfrac{\lambda}{2} w^Tw +  \sum_{i=1}^n {\displaystyle \max \left(0, 1-y _ {i}({\vec {w}}\cdot {\vec {x}} _ {i}-b)\right)}$$
    :   Subject to
    :   $$y_i(wX_i + b) \geq 1 - \zeta_i, \zeta_i \geq 0, \forall i \in [1,n]$$
    :   Formally,
    :   $$\min_{w, b}\dfrac{\lambda}{2} w^Tw +  \sum_{i=1}^n {\displaystyle \max \left(0, 1-y _ {i}({\vec {w}}\cdot {\vec {x}} _ {i}-b)\right)}, \:\: \forall i \in [1,n]$$

## Further Analysis
{: #content4}

1. **Generalization:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents41} 
    :   We notice that, geometrically, the hyperplane (the maximum margin classifier) is completely characterized by the _support vectors_ (the vectors that lie on the margin).  
    :   A very important conclusion arises.  
        The maximum margin classifier (SVM) depends **only** on the number of support vectors and **_not_** on the dimension of the problem.  
        This implies that the computation doesn't scale up with the dimension and, also implies, that **the _kernel trick_ works very well**.

2. **Properties:**{: style="color: SteelBlue  "}{: .bodyContents4 #bodyContents42} 
    :   1. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
        2. The hyperplane is determined solely by its support vectors.
        3. The SVM always converges on linearly separable data.
        4. The Soft-Margin SVM will converge on non-linearly separable data.


---
---
---

__Extra info to be added:__{: style="color: red"}  
Requiring the Margin to be bigger, puts a restriction on the growth function (i.e. the number of possible dichotomies). So, fewer VC dimension.

* [Deep Learning with SVM](https://arxiv.org/pdf/1306.0239.pdf)  


__Why $$w$$ is Orthogonal to the plane:__   
    img



The number of non-zero paramters in a model correspond to the VC dimension



Normalizing $$w$$,  

Observations:  
1. Normalize $$w$$:  
    The Hyperplane, $$\mathbf{w}^{\top} \mathbf{x}=0$$ (which is, incidentally, the _signal_), defined by $$w$$ is scale invariant to $$w$$; since you can multiply $$w$$ by any scalar and the equation of the plane will still hold. Thus, you can normalize it by dividing by a scalar.  
    So, we choose $$w$$ (by normalizing/scaling it) such that the following equation holds for the variable $$\mathbf{x}_ n$$:  
    <p>$$\left|\mathbf{w}^{\top} \mathbf{x}_ {n}\right|=1$$</p>   



Effect of the gamma hyperparam in RBF kernel:
    The gamma parameter in SVM tuning signifies the influence of points either near or far away from the hyperplane.
    For a low gamma, the model will be too constrained and include all points of the training dataset, without really capturing the shape.
    For a higher gamma, the model will capture the shape of the dataset well.


Bias = Underfit = low complexity (model)
Variance = Overfit = high complexity

Regularization reduce overfitting = reduce variance = simplify model = increase bias

Increase C hparam in SVM = reduce underfitting

***
***

TITLE: The Centroid Method
LINK: research/ml/1/1.1.md


## The Centroid Method

1. **The Centroid:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    :   In mathematics and physics, the centroid or geometric center of a plane figure is the arithmetic mean ("average") position of all the points in the shape. 
    :   The definition extends to any object in n-dimensional space: its centroid is the mean position of all the points in all of the coordinate directions.

2. **Procedure:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} 
    :   Compute the mean ($$\mu_c$$) of all the vectors in class C and the mean ($$\mu_x$$) of all the vectors not in C.

3. **Decision Function:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} 
    :   $$f(x) = (\mu_c - \mu_x) \cdot \vec{x} - (\mu_c - \mu_x) \cdot \dfrac{\mu_c + \mu_x}{2}$$

4. **Decision Boundary:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} 
    :   The decision boundary is a Hyperplane that bisects the line segment with endpoints $$<\mu_c, \mu_x>$$.




***
***

TITLE: Introduction to <br /> NLP
LINK: research/nlp/1.md


## Introduction
{: #content1}

1. **Problems in NLP:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11} 
    :   * Question Answering (QA) 
        * Information Extraction (IE)    
        * Sentiment Analysis  
        * Machine Translation (MT)  
        * Spam Detection  
        * Parts-of-Speech (POS) Tagging  
        * Named Entity Recognition (NER)
        * Conference Resolution  
        * Word Sense Disambugation (WSD)  
        * Parsing  
        * Paraphrasing  
        * Summarization  
        * Dialog  

2. **(mostly) Solved Problems in NLP:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12} 
    :   * Spam Detection  
        * Parts-of-Speech (POS) Tagging  
        * Named Entity Recognition (NER)  


3. **Within-Reach Problems:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13} 
    :   * Sentiment Analysis  
        * Conference Resolution    
        * Word Sense Disambugation (WSD)  
        * Parsing  
        * Machine Translation (MT)  
        * Information Extraction (IE)    


4. **Open Problems in NLP:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14} 
    :   * Question Answering (QA)   
        * Paraphrasing  
        * Summarization  
        * Dialog  

5. **Issues in NLP (why nlp is hard?):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15} 
    :   * __Non-Standard English__: "Great Job @ahmed_badary! I luv u 2!! were SOO PROUD of dis."  
        * __Segmentation Issues__: "New York-New Haven" vs "New-York New-Haven"  
        * __Idioms__: "dark horse", "getting cold feet", "losing face"  
        * __Neologisms__: "unfriend", "retweet", "google", "bromance"  
        * __World Knowledge__: "Ahmed and Zach are brothers", "Ahmed and Zach are fathers"    
        * __Tricky Entity Names__: "Where is _Life of Pie_ playing tonight?", "_Let it be_ was a hit song!"  

6. **Tools we need for NLP:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16} 
    :   * Knowledge about Language.  
        * Knowledge about the World.   
        * A way to combine knowledge sources.  

7. **Methods:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17} 
    :   In general we need to construct __Probabilistic Models__ built from _language data_.    
    :   We do so by using _rough text features_.  
        > All the names models, methods, and tools mentioned above will be introduced later as you progress in the text.  


***

## Definitions
{: #content2}

1. **K-Nearest-Neighbors:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21} 
    :   

    :   __Complexity__:  
        :   * _Training_: $$\:\:\:\:\mathcal{O}(1)$$   
            * _Predict_: $$\:\:\:\:\mathcal{O}(N)$$ 








***

## Metrics
{: #content3}

1. **L1 Distance:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31} 
    :   $$d_1(I_1, I_2) = \sum_p{\|I_1^p - I_2^p\|}$$  
    :   Pixel-wise absolute value differences.  

2. **L2 Distance:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32} 
    :   $$d_2 (I_1, I_2) = \sqrt{\sum_{p} \left( I^p_1 - I^p_2 \right)^2}$$
    :   

3. **L1 vs. L2:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33} 
    :   The L2 distance penalizes errors (pixel differences) much more than the L1 metric does.  
    The L2 distnace will be small iff there are man small differences in the two vectors but will explode if there is even one big difference between them.  
    :   Another difference we highlight is that the L1 distance is dependent on the corrdinate system frame, while the L2 distance is coordinate-invariant.







***
***

TITLE: Information Extraction <br \> Named Entity Recognition
LINK: research/nlp/info_extr.md


## Introduction to Information Extraction
{: #content1}

1. **Information Extraction (IE):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   is the task of automatically extracting structured information from a (non/semi)-structured piece of text.  

2. **Structured Representations of Inforamtion:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   Usually, the extracted information is represented as:  
        * Relations (in the DataBase sense)  
        * A Knowledge Base

3. **Goals:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   1. Organize information in a way that is useful to humans. 
        2. Put information in a semantically precise form that allows further inference to be made by other computer algorithms. 

4. **Common Applications:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   * Gathering information (earning, profits, HQs, etc.) from reports  
        * Learning drug-gene product interactions from medical research literature  
        * Low-Level Information Extraction:  
            * Information about possible dates, schedules, activites gathered by companys (e.g. google, facebook)  

5. **Tasks and Sub-tasks:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    :   * __Named entity extraction__:   
            * __Named Entity Recognition__: recognition of known entity names (for people and organizations), place names, temporal expressions, and certain types of numerical expressions.     
            * __Coreference Resolution__:  detection of coreference and anaphoric links between text entities.  
            * __Relationship Extraction__: identification of relations between entities.  
        * __Semi-structured information extraction__:
            * *__Table Extraction__*: finding and extracting tables from documents.  
            * *__Comments extraction__*: extracting comments from actual content of article in order to restore the link between author of each sentence.
        * __Language and Vocabulary Analysis__:   
            * *__Terminology extraction__*: finding the relevant terms for a given corpus.          

6. **Methods:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    :   There are three standard approaches for tackling the problem of IE:  
        * __Hand-written Regular Expressions__: usually stacked.  
        * __Classifiers__:   
            * *__Generative__*:   
                * Naive Bayes Classifier
            * *__Discriminative__*:   
                * MaxEnt Models (Multinomial Logistic Regr.)
        * __Sequence Models__:   
            * Hidden Markov Models
            * Conditional Markov model (CMM) / Maximum-entropy Markov model (MEMM)
            * Conditional random fields (CRF)

***

## Named Entity Recognition (NER)
{: #content2}

1. **Named Entity Recognition:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    :   is the recognition of known entity names (for people and organizations), place names, temporal expressions, and certain types of numerical expressions;  
    this is usually done by employing existing knowledge of the domain or information extracted from other sentences.    

2. **Applications:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    :   * Named entities can be indexed, linked off, etc.
        * Sentiment can be attributed to companies or products  
        * They define a lot of the IE relations, as associations between the named entities
        * In QA, answers are often named entities  


3. **Evaluation of NER Tasks:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    :   Evaluation is usually done at the level of __Entities__ and not of __Tokens__.  
    :   One common issue with the metrics defined for text classification, namely, (Precision/Recall/F1) is that they penalize the system based on a binary evaluation on how the system did; however, let's demonstrate why that would be problamitic.  
    :   * Consider the following text:  
            "The _First **Bank of Chicago**_ announced earnings..."  
            Let the italic part of the text be the enitity we want to recognize and let the bolded part of the text, be the entitiy that our model identified.  
            The (Precision/Recall/F1) metrics would penalize the model twice, once as a false-positive (for having picked an incorrect entitiy name), and again as a false-negative (for not having picked the actual entitiy name).   
            However, we notice that our system actually picked $$3/4$$ths of the actual entity name to be recognized.  
        * This leads us seeking an evaluation metric that awards partial credit for this task.  
    :   * The __MUC Scorer__ is one, such, metric for giving partial credit.  
    :   Albeit such complications and issues with the metrics described above, the field has, unfortunately, continued using the F1-Score as a metric for NER systems due to the complexity of formulating a metric that  gives partial credit.  

***

## Sequence Models for Named Entity Recognition
{: #content3}

1. **Approach:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   * __Training__:   
            1. Collect a set of representative training documents
            2. Label each token for its entity class or other (O)
            3. Design feature extractors appropriate to the text and classes
            4. Train a sequence classifier to predict the labels from the data
    :   * __Testing__:  
            1. Receive a set of testing documents
            2. Run sequence model inference to label each token  
            3. Appropriately output the recognized entities   

2. **Encoding Classes for Sequence Labeling:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   There are two common ways to encode the classes:  
        * __IO Encoding__: this encoding will only encode only the entitiy of the token disregarding its order/position in the text (PER).  
            * For $$C$$ classes, IO produces ($$C+1$$ ) labels.  
        * __IOB Encoding__: this encoding is similar to IO encoding, however, it, also, keeps track to whether the token is the beginning of an entitiy-name (B-PER) or a continuation of such an entity-name (I-PER).  
            * For $$C$$ classes, IO produces ($$2C+1$$ ) labels.
    :   The __IO__ encoding, thus, is much lighter, and thus, allows the algorithm to run much faster than the __IOB__ encoding.  
        Moreover, in practice, the issue presented for IO encoding rarely occurs and is only limited to instances where the entities that occur next to each other, are the same entity.   
        __IOB__ encoded systems, also, tend to not learn quite as well due to the huge number of labels and are usually still prone to the same issues
    :   Thus, due to the reasons mentioned above, the __IO Encoding__ scheme is the one most commonly used.            

3. **Features for Sequence Labeling:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    :   * __Words__:   
            * Current word (like a learned dictionary)
            * Previous/Next word (context)
        * __Other kinds of Inferred Linguistic Classification__:    
            * Part-of-Speech Tags (POS-Tags)   
        * __Label-Context__:   
            * Previous (and perhaps next) label
            > This is usually what allows for sequence modeling 
    :   Other useful features:  
        * __Word Substrings__: usually there are substrings in words that are _categorical_ in nature  
            * Examples:  
                "oxa" -> Drug  
                "(noun): (noun)" -> Movie  
                "(noun)-field" -> (usually) place  
        * __Word Shapes__: the idea is to map words to simplified representations that encode attributes such as:  
           length, capitalization, numerals, Greek-Letters, internal punctuation, etc.  
           * Example:  
                The representation below shows only the first two letters and the last two letters; for everything else, it will add the capitalization and the special characters, and for longer words, it will represent them in set notation.   

                | Varicella-zoster | Xx-xxx  
                | mRNA | xXXX   
                | CPA1 | XXXd  


***

## Maximum Entropy Sequence Models
{: #content4}

1. **Maximum-Entropy (Conditional) Markov Model:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    :   is a discriminative graphical model for sequence labeling that combines features of hidden Markov models (HMMs) and maximum entropy (MaxEnt) models.  
    :   It is a discriminative model that extends a standard maximum entropy classifier by assuming that the unknown values to be learned are connected in a Markov chain rather than being conditionally independent of each other.
    :   It makes a single decision at a time, conditioned on evidence from observations and previous decisions.  
    :   > A larger space of sequences is usually explored via search.  

2. **Inference:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}  
    ![img](/main_files/nlp/1.png){: width="100%"} 

3. **Exploring the Sequence Space (Search Methods):**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}  
    :   * __Beam Inference__:  
            * *__Algorithm__*:  
                * At each position keep the top $$k$$ complete sequences.  
                * Extend each sequence in each local way.  
                * The extensions compete for the $$k$$ slots at the next position.  
            * *__Advantages__*:   
                * Fast. Beam sizes of 3-5 are almost as good as exact inference in many cases.  
                * Easy. Implementation does not require dynamic programming.  
            * *__Disadvantages__*:    
                * Inexact. The globally best sequence can fall off the beam.   
    :   * __Viterbi Inference__:  
            * *__Algorithm__*:  
                * Dynamic Programming or Memoization.  
                * Requires small window of state influence (eg. past two states are relevant)  
            * *__Advantages__*:  
                * Exact. the global best sequence is returned.  
            * *__Disadvantages__*:  
                * Hard. Harder to implement long-distance state-state interactions. 


4. **Conditional Random Fields (CRFs):**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents44}  
    :   are a type of discriminative undirected probabilistic graphical model.   
    :   They can take context into account; and are commonly used to encode known relationships between observations and construct consistent interpretations.

***
***

TITLE: Discriminative Models in NLP <br \> Maxent Models and Discriminative Estimation
LINK: research/nlp/disc.md


## Generative vs Discriminative Models
{: #content1}

Given some data $$\{(d,c)\}$$ of paired observations $$d$$ and hidden classes $$c$$:  

1. **Generative (Joint) Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   __Generative Models__ are __Joint Models__.  
    :   __Joint Models__ place probabilities $$\left(P(c,d)\right)$$ over both the observed data and the "target" (hidden) variables that can only be computed from those observed.  
    :   Generative models are typically probabilistic, specifying a joint probability distribution ($$P(d,c)$$) over observation and target (label) values,  
    and tries to __Maximize__ this __joint Likelihood__.  
        > Choosing weights turn out to be trivial: chosen as the __relative frequencies__.  
    :   __Examples:__  
        * n-gram Models
        * Naive Bayes Classifiers  
        * Hidden Markov Models (HMMs)
        * Probabilistic Context-Free Grammars (PCFGs)
        * IBM Machine Translation Alignment Models

2. **Discriminative (Conditional) Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   __Discriminative Models__ are __Conditional Models__.  
    :   __Conditional Models__ provide a model only for the "target" (hidden) variabless.  
        They take the data as given, and put a probability $$\left(P(c \| d)\right)$$ over the "target" (hidden) structures given the data.  
    :   Conditional Models seek to __Maximize__ the __Conditional Likelihood__.  
        > This (maximization) task is usually harder to do.  
    :   __Examples:__  
        * Logistic Regression
        * Conditional LogLinear/Maximum Entropy Models  
        * Condtional Random Fields  
        * SVMs  
        * Perceptrons  

3. **Generative VS Discriminative Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   Basically, _Discriminative Models_ infer outputs based on inputs,  
        while _Generative Models_ generate, both, inputs and outputs (typically given some hidden paramters).  
    :   However, notice that the two models are usually viewed as complementary procedures.  
        One does __not__ necessarily outperform the other, in either classificaiton or regression tasks.   

***

## Feature Extraction for Discriminative Models in NLP
{: #content2}

1. **Features (Intuitively):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    :   __Features__ ($$f$$) are elementary pieces of evidence that link aspects od what we observe ($$d$$) with a category ($$c$$) that we want to predict.  

2. **Features (Mathematically):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    :   A __Feature__ $$f$$ is a function with a bounded real value.  
    :   $$f : \: C \times D \rightarrow \mathbf{R}$$


3. **Models and Features:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    :   Models will assign a __weight__ to each Feature:  
        * A __Positive Weight__ votes that this configuration is likely _Correct_.  
        * A __Negative Weight__ votes that this configuration is likely _InCorrect_. 

4. **Feature Expectations:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    :   * __Empirical  Expectation (count)__:  
    :   $$E_{\text{emp}}(f_i) = \sum_{(c,d)\in\text{observed}(C,D)} f_i(c,d)$$    
    :   * __Model Expectation__:  
    :   $$E(f_i) = \sum_{(c,d)\in(C,D)} P(c,d)f_i(c,d)$$
    :   > The two Expectations represent the __Actual__ and the __Predicted__ __Counts__ of a feature __firing__, respectively.  

5. **Features in NLP:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    :   In NLP, features have a particular form.  
        They consist of:  
        * __Indicator Function__: a boolean matching function of properties of the input  
        * __A Particular Class__: specifies some class $$c_j$$  
    : $$f_i(c,d) \cong [\Phi(d) \wedge c=c_j] = \{0 \vee 1\}$$  
    :   where $$\Phi(d)$$ is a given predicate on the data $$d$$, and $$c_j$$ is a particular class.  
    :   > Basically, each feature picks out a data subset and suggests a label for it.  

***

## Feature-Based Linear Classifiers  
{: #content3}

1. **Linear Classifiers (Classification):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   * We have a __Linear Function__ from the feature sets $$\{f_i\}$$ to the classes $$\{c\}$$  
        * __Assign Weights__ $$\lambda_i$$ to each feature $$f_i$$  
        * __Consider each class__ for an observed datum $$d$$  
        * __Features Vote__ with their _weights_    :
        :   $$\text{vote}(c) = \sum \lambda_i f_i(c,d)$$  
        * __Classification__:  
            choose the class $$c$$ which __Maximizes__ the __vote__ $$\sum \lambda_i f_i(c,d)$$  


2. **Exponential Models:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   __Exponential Models__ make a probabilistic model from the linear combination $$\sum\lambda_if_i(c,d)$$
    :   * __Making the Value Positive__:    
    :   $$\sum\lambda_if_i(c,d) \rightarrow e^{\sum\lambda_if_i(c,d)}$$   
    :   * __Normalizing the Value (Making a Probability)__:  
    :  $$e^{\sum\lambda_if_i(c,d)} \rightarrow \dfrac{e^{\sum\lambda_if_i(c,d)}}{\sum_{c \in C} e^{\sum\lambda_if_i(c,d)}}$$ 
    :   $$\implies$$
    :   $$P(c \| d, \vec{\lambda}) = \dfrac{e^{\sum\lambda_if_i(c,d)}}{\sum_{c \in C} e^{\sum\lambda_if_i(c,d)}}$$
    :   The function $$P(c \| d,\vec{\lambda})$$ is referred to as the __Soft-Max__ function.  
    :   Here, the __Weights__ are the __Paramters__ of the probability model, combined via a __Soft-Max__ function.
    :   __Learning:__  
        * Given this model form, we want to choose paramters $$\{\lambda_i\}$$ that __Maximize the Conditional Likelihood__ of the data according to this model (i.e. the soft-max func.).    
    :   Exponential Models, construct _not onlt_ __classifications__ but, also, __Probability Distributions__ over the classifications.
    :   __Examples:__  
        * Log-Linear Model
        * Max Entropy (MaxEnt) Model
        * Logistic Regression  
        * Gibbs Model 

3. **Exponential Models (Training) | Maximizing the Likelihood:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    :   __The Likelihood Value__:  
        * The (log) conditional likelihood of a maxend model is a function of the iid data $$(C,D)$$ and the parameters ($$\lambda$$):  
    :   $$log P(C \| D,\lambda) = log \prod_{(c,d) \in (C,D)} P(c \| d,\lambda) = \sum_{(c,d) \in (C,D)} log P(c \| d,\lambda)$$
    :   * If there aren't many values of $$c$$, it's easy to calculate:  
    :   $$log P(c \| d,\lambda) = \sum_{(c,d) \in (C,D)} log \dfrac{e^{\sum_i \lambda_if_i(c,d)}}{\sum_c e^{\sum_i \lambda_if_i(c,d)}}$$
    :   * We can separate this into two components:  
    :   $$log P(c \| d,\lambda) = \sum_{(c,d) \in (C,D)} log e^{\sum_i \lambda_if_i(c,d)} - \sum_{(c,d) \in (C,D)} log \sum_c' e^{\sum_i \lambda_if_i(c',d)}$$
    :   $$\implies$$
    :   $$log P(C \| D, \lambda) = N(\lambda) - M(\lambda)$$
    :   * The Derivative of the Numerator is easy to calculate:  
    :   $$\dfrac{\partial N(\lambda)}{\partial \lambda_i} = \dfrac{\partial \sum_{(c,d) \in (C,D)} log e^{\sum_i \lambda_if_i(c,d)}}{\partial \lambda_i}
    \\= \dfrac{\partial \sum_{(c,d) \in (C,D)} \sum_i \lambda_if_i(c,d)}{\partial \lambda_i} 
    \\\\= \sum_{(c,d) \in (C,D)} \dfrac{\partial \sum_i \lambda_if_i(c,d)}{\partial \lambda_i} 
    \\\\= \sum_{(c,d) \in (C,D)} f_i(c,d)$$
    :   The derivative of the Numerator is __the Empirical Expectation__, $$E_{\text{emp}}(f_i)$$
    :   * The Derivative of the Denominator:  
    :   $$\dfrac{\partial M(\lambda)}{\partial \lambda_i}
    = \dfrac{\partial \sum_{(c,d) \in (C,D)} log \sum_c' e^{\sum_i \lambda_if_i(c',d)}}{\partial \lambda_i}
    \\\\= \sum_{(c,d) \in (C,D)} \sum_c' P(c' \| d, \lambda)f_i(c', d)$$
    :   The derivative of the Denominator is equal to __the Predicted Expectation (count)__, $$E(f_i, \lambda)$$
    : Thus, the derivative of the log likelihood is:  
    :   $$\dfrac{\partial log P(C \| D, \vec{\lambda})}{\partial \lambda_i} = \text{Actual Count}(f_i, C) - \text{Predicted Count}(f_i, \vec{\lambda})$$
    :   * Thus, the optimum parameters are those for which rach feature's _predicted expectation_ equals its _empirical expectation_.  
    :   * The __Optimum Distribution__ is always:  
            * Unique (parameters need not be unique)
            * Exists (if feature counts are from actual data)
    :   * These models are called __Maximum Entropy (Maxent)__ Models because we find the model having the maximum entropy, and satisfying the constraints:  
    :   $$E_p(f_j) = E_\hat{p}(f_j), \:\:\: \forall j$$
    :   * Finally, to find the optimal parameters $$\lambda_1, \dots, \lambda_d$$ one needs to optimize (maximize) the log liklehood, or equivalently, minimize the -ve loglik.  
            One can do that in variety of was using optimization methods.  
    :   * Common __Optimization Methods__:  
            * (Stochastic) Gradient Descent
            * Iterative Proportional Fitting Methods:  
                * Generalized Iterative Scaling (GIS)
                * Improved Iterative Scaling (IIS)
            * Conjugate Gradient (CG) (+ Preconditioning)
            * Quasi-Newton Methods -  Limited-Memory Variable Metric (LMVM):  
                * L-BFGS
                > This one is the most commonly used.  


***
***

TITLE: Text Classification
LINK: research/nlp/txt_clss.md


## Introduction and Definitions
{: #content1}

1. **Text Classification:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}   
    :    The task of assigning a piece of text to one or more classes or categories.  

2. **Applications:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    * __Spam Filtering__: discerning spam emails form legitimate emails.  
    * __Email Routing__: sending an email sento to a genral address to a specfic affress based on the topic.  
    * __Language Identification__: automatiacally determining the genre of a piece of text.  
    * Readibility Assessment__: determining the degree of readability of a piece of text.  
    * __Sentiment Analysis__: determining the general emotion/feeling/attitude of the author of a piece of text.  
    * __Authorship Attribution__: determining which author wrote which piece of text.  
    * __Age/Gender Identification__: determining the age and/or gender of the author of a piece of text.      

3. **Classification Methods:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   * __(Hand-Coded)Rules-Based Algorithms__: use rules based on combinations of words or other features.   
            * Can have high accuracy if the rules are carefully refined and maintained by experts.  
            * However, building and maintaining these rules is very hard.  
        * __Supervised Machine Learning__: using an ML algorithm that trains on a training set of (document, class) elements to train a classifier.  
            * _Types of Classifiers_:  
                * Naive Bayes  
                * Logistic Regression
                * SVMs
                * K-NNs  
   
***

## The Naive Bayes Classifier
{: #content2}

1. **Naive Bayes Classifiers:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}   
    :   are a family of simple probabilistic classifiers based on applying [_Bayes' Theorem_](https://en.wikipedia.org/wiki/Bayes%27_theorem) with strong (naive) independence assumptions between the features.  
    :   __The Probabilistic Model__:  
        Abstractly, naive Bayes is a conditional probability model: given a problem instance to be classified, represented by a vector $${\displaystyle \mathbf{x} =(x_{1},\dots ,x_{n})}=(x_{1},\dots ,x_{n})$$ representing some n features (independent variables), it assigns to this instance probabilities  
    :   $${\displaystyle p(C_{k}\mid x_{1},\dots ,x_{n})\,}$$
    :   for each of the $$k$$ possible outcome or classes $$C_k$$.  
    :   Now, using _Bayes' Theorem_ we decompose the conditional probability as:  
    :   $${\displaystyle p(C_{k}\mid \mathbf {x} )={\frac {p(C_{k})\ p(\mathbf {x} \mid C_{k})}{p(\mathbf {x} )}}\,}$$
    :   Or, equivalenty, and more intuitively:  
    :   $${\displaystyle {\mbox{posterior}}={\dfrac{\text{prior} \times \text{likelihood}}{\text{evidence}}}\,}$$  
    :   We can disregard the _Denomenator_ since it does __not__ depend on the classes $$C$$, making it a constant.  
    :   Now, using the _Chain-Rule_ for repeated application of the conditional probability,   the joint probability model can be rewritten as:  
    :   $$p(C_{k},x_{1},\dots ,x_{n})\, = p(x_{1}\mid x_{2},\dots ,x_{n},C_{k})p(x_{2}\mid x_{3},\dots ,x_{n},C_{k})\dots p(x_{n-1}\mid x_{n},C_{k})p(x_{n}\mid C_{k})p(C_{k})$$  
    :   Applying the naive conditional independence assumptions,  
        >   i.e. assume that each feature $${\displaystyle x_{i}}$$ is conditionally independent of every other feature $${\displaystyle x_{j}} $$ for $${\displaystyle j\neq i}$$, given the category $${\displaystyle C}$$  
    :   $$\implies \\ 
    {\displaystyle p(x_{i}\mid x_{i+1},\dots ,x_{n},C_{k})=p(x_{i}\mid C_{k})\,}.$$
    :   Thus, we can write the join probability model as:  
    :   $${\displaystyle p(C_{k}\mid x_{1},\dots ,x_{n})={\frac {1}{Z}}p(C_{k})\prod _{i=1}^{n}p(x_{i}\mid C_{k})}$$
    :   Where, $${\displaystyle Z=p(\mathbf {x} )=\sum _{k}p(C_{k})\ p(\mathbf {x} \mid C_{k})}$$ is a __constant__ scaling factor, a function of the, _known_, feature variables.  

    :   __The Decision Rule__: we commonly use the [_Maximum A Posteriori (MAP)_](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) hypothesis, as the decision rule.  
    :   Thus, __the classifier__ becomes:  
    :   $${\displaystyle {\hat {y}}={\underset {k\in \{1,\dots ,K\}}{\operatorname {argmax} }}\ p(C_{k})\displaystyle \prod _{i=1}^{n}p(x_{i}\mid C_{k}).}$$
    :   A function that assigns a class label $${\displaystyle {\hat {y}}=C_{k}}$$ for some $$k$$.

2. **Multinomial Naive Bayes:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}   
    :   With a multinomial event model, samples (feature vectors) represent the frequencies with which certain events have been generated by a multinomial $${\displaystyle (p_{1},\dots ,p_{n})}$$ where $${\displaystyle p_{i}}$$ is the probability that event $$i$$ occurs.  
    :   The likelihood of observing a feature vector (histogram) $$\mathbf{x}$$ is given by:  
    :   $${\displaystyle p(\mathbf {x} \mid C_{k})={\frac {(\sum _{i}x_{i})!}{\prod _{i}x_{i}!}}\prod _{i}{p_{ki}}^{x_{i}}}$$  
    :   The multinomial naive Bayes classifier becomes a linear classifier when expressed in log-space:  
    :   $${\displaystyle {\begin{aligned}\log p(C_{k}\mid \mathbf {x} )&\varpropto \log \left(p(C_{k})\prod _{i=1}^{n}{p_{ki}}^{x_{i}}\right)\\&=\log p(C_{k})+\sum _{i=1}^{n}x_{i}\cdot \log p_{ki}\\&=b+\mathbf {w} _{k}^{\top }\mathbf {x} \end{aligned}}}$$  
    :   where $${\displaystyle b=\log p(C_{k})}$$ and $${\displaystyle w_{ki}=\log p_{ki}}$$.  

3. **Bag-of-Words:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}   
    :   The __bag-of-words model__ (or __vector-space-model__) is a simplifying representation of text/documents.  
    :   A text is represented as the bag (Multi-Set) of its words with multiplicity, disregarding any grammatrical rules and word-orderings.

4. **The Simplifying Assumptions Used:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}   
    :   * __Bag-of-Words__: we assume that the position of the words does _not_ matter.  
        * __Naive Independence__: the feature probabilities are indpendenet given a class $$c$$.   

5. **Learning the Multi-Nomial Naive Bayes Model:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}   
    :   * __The Maximum Likelihood Estimate__: we simply use the frequencies in the date.
            * $$\hat{P}(c_j) = \dfrac{\text{doc-count}(C=c_j)}{N_\text{doc}}$$  
            > The _Prior Probability_ of a document being in class $$c_j$$, is the fraction of the documents in the training data that are in class $$c_j$$.  
            * $$\hat{P}(w_i | c_i) = \dfrac{\text{count}(w_i,c_j)}{\sum_{w \in V} \text{count}(w, c_j)}$$  
            > The _likelihood_ of the word $$w_i$$ given a class $$c_j$$, is the fraction of the occurunces of the word $$w_i$$ in class $$c_j$$ over all words in the class.    
    :   * __The Problem with Maximum Likelihood__:  
            If a certain word occurs in the test-set but __not__ in the training set, the likelihood of that word given the equation above will be set to $$0$$.  
            Now, since we are multiplying all the likelihood terms together, the MAP estimate will be set to $$0$$ as well, regardless of the other values.  

6. **Solutions to the MLE Problem:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}   
    :   Usually the problem of reducing the estimate to zero is solved by adding a regularization technique known as _smoothing_. 

7. **Lidstone Smoothing (additive smoothing):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}   
    :   is a technique used to smooth categorical data as the following:  
    :   Given an observation vector $$x = (x_1, \ldots, x_d)$$ from a multinomial distribution with $$N$$ trials, a _smoothed_ version of the data produces the estimators:  
    :   $${\hat {\theta }}_{i}={\frac {x_{i}+\alpha }{N+\alpha d}}\qquad (i=1,\ldots ,d),$$


8. **Laplace Smoothing:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}   
    :   is a special case of additive smoothing (Lidstone Smoothing) with $$\alpha = 1$$:  
    :   $${\hat {\theta }}_{i}={\frac {x_{i}+1 }{N+ d}}\qquad (i=1,\ldots ,d),$$   
9. **The Algorithm:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents29}  
    :   * Extract the _Vocabulary_ from the trianing data  
        * Calculate $$P(c_j)$$ terms  
            * For each $$c_j \in C$$ do  
                * $$\text{docs}_j \leftarrow$$ all docs with class $$=c_j$$  
                * $$P(c_j) \leftarrow \dfrac{\|\text{docs}_j\|}{\|\text{total # docs}\|}$$
        * Calculate $$P(w_k \| c_j)$$ terms  
            * $$\text{Text}_j \leftarrow$$ single doc containing all $$\text{docs}_j$$  
            * For each word $$w_k \in$$ Vocab.  
                * $$n_k \leftarrow$$ # of occurunces of $$w_k \in \text{Text}_j$$  
                * $$P(w_k \| c_j) \leftarrow \dfrac{n_k + \alpha}{n + \alpha \|Vocab.\|}$$  
10. **Summary:**{: style="color: SteelBlue"}{: .bodyContents210}  
    :   * Very fast  
        * Low storage requirements
        * Robust to Irrelevant Features  
            * Irrelevant features cancel each other out.  
        * Works well in domains with many equally important features  
            * Decision Trees_ suffer from fragmentation in such cases - especially if there is little data.  
        * It is _Optimal_ if the independence conditions hold.  

***

## Evaluation of Text Classification  
{: #content3}

1. **The $$2x2$$ Contingency Table:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   | | __correct__ (Spam) | __not correct__ (not Spam)    
        __selected__ (Spam) | tp | fp  
        __not selected__ (not Spam) | fn | tn  

2. **Accuracy:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}   
    :   $$ \text{Acc} = \dfrac{\text{tp} + \text{tn}}{\text{tp} + \text{fp} + \text{fn} + \text{tn}}$$
    :   __The Problem__:  
        Accuracy can be easily fooled (i.e. produce a very high number) in a scenario where the number of occurrences of a class we desire is much less than the data we are searching.  

3. **Precision (positive predictive value (PPV)):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    :   is the fraction of relevant instances among the retrieved instances.  
    :   __Equivalently__,  
        the % of selected items that are correct.    

    :   $${\displaystyle {\text{Precision}}={\frac {tp}{tp+fp}}\,}$$     

4. **Recall (True Positive Rate), (Sensitivity):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    :   Also referred to as the __true positive rate__ or __sensitivity__. 
    :    is the fraction of relevant instances that have been retrieved over the total amount of relevant instances.  
    :   __Equivalently__,  
        the % of correct items that are selected.  
    :   $${\displaystyle {\text{Recall}}={\frac {tp}{tp+fn}}\,}$$

5. **The Trade-Off:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}   
    :   Usually, the two measures discusses above have an inverse relation between them due to the quantities they measure.  

6. **The F-measure:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents36}   
    :   is a measure that combines _precision_ and _recall_.  
    :   It is the _harmonic mean_ of precision and recall:  
    :   $${\displaystyle F=2\cdot {\frac {\mathrm {precision} \cdot \mathrm {recall} }{\mathrm {precision} +\mathrm {recall} }}}$$ 
   
***

## General Discussion of Issues in Text Classification
{: #content4}

1. **Very Little Data:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}   
    :   * Use Naive Bayes  
            * Naive Bayes is a "high-bias" algorithm; it tends to __not__ overfit the data.  
        * Use Semi-Supervised Learning  
            * Try Bootstrapping or EM over unlabeled documents  

2. **Reasonable Amount of Data:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}   
    :   * Use:  
            * SVM  
            * Regularized Logistic Regression  
            * (try) Decision Trees  

3. **Huge Amount of Data:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}   
    :   Be careful of the run-time:  
        * SVM: slow train time  
        * KNN: slow test time  
        * Reg. Log. Regr.: somewhat faster  
        * Naive-Bayes: might be good to be used.

4. **Underflow Prevention:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents44}   
    :   __Problem:__ Due to the "_multiplicative_" nature of the algorithms we are using, we might run into a floating-point underflow problem.  
    :   __Solution__: transfer the calculations to the _log-space_ where all the multiplications are transformed into additions.  

5. **Tweaking the Performance of the Algorithms:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents45}   
    :   * Utilize __*Domain-Specific* features__ and weights  
        * __Upweighting__: counting a word as if it occurred multiple times.  
   

***
***

TITLE: Sentiment Analysis
LINK: research/nlp/sent_anlys.md


## Introduction
{: #content1}

1. **Sentiment Analysis:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   is the automated identification and quantification of affective states and subjective information in textual data.

2. **Applications:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   

3. **Formulating the Problem:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   *__Tasks to Extract__*:  
        * __Holder (source)__: of the attitude.  
        * __Target (aspect)__: of the attitude.  
        * __Type__: of the attitude.  
    :   __*Input*__:  
        * __Text__: Contains the attitude  
            * Sentence Analysis  
            * Entire-Document Analysis  
        * __main__: second   

***

## Algorithms
{: #content2}

1. **Binarized (Boolean Feature) Multinomial Naive Bayes:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    :   This algorithm works exactly the same as the [Multinomial Naive Bayes](http://ahmedbadary.ml/work_files/research/nlp/txt_clss#content2) algorithm.  
    :   However, the features (Tokens) used in this algorithm are counted based on _occurrence_ rather than _frequency_,  
        > i.e. if a certain word occurs in the text then its count is always one, regardless of the number of occurrences of the word in the text.  
    :   __Justification:__ The reason behind the binarized version is evident, intuitively, in the nature of the problem.  
        The sentiment behind a certain piece of text is usually represented in just one occurrence of a word that represents that sentiment (e.g. "Fantastic") rather than how many times did that word actually appear in the sentence.  
2. **Better Algorithms:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    :   * Max-Entropy  
        * SVMs


***

## Sentiment Lexicons
{: #content3}

1. **Sentiment Lexicons:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   Specific key-words that are related to specific polarities.  
        They are much more useful to be used instead of analyzing all of the words (tokens) in a piece of text. 


***
***

TITLE: Introduction to <br /> NLP
LINK: research/nlp/intro.md


## Introduction
{: #content1}

1. **Problems in NLP:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11} 
    :   * Question Answering (QA) 
        * Information Extraction (IE)    
        * Sentiment Analysis  
        * Machine Translation (MT)  
        * Spam Detection  
        * Parts-of-Speech (POS) Tagging  
        * Named Entity Recognition (NER)
        * Conference Resolution  
        * Word Sense Disambiguation (WSD)  
        * Parsing  
        * Paraphrasing  
        * Summarization  
        * Dialog  

2. **(mostly) Solved Problems in NLP:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12} 
    :   * Spam Detection  
        * Parts-of-Speech (POS) Tagging  
        * Named Entity Recognition (NER)  


3. **Within-Reach Problems:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13} 
    :   * Sentiment Analysis  
        * Conference Resolution    
        * Word Sense Disambiguation (WSD)  
        * Parsing  
        * Machine Translation (MT)  
        * Information Extraction (IE)    


4. **Open Problems in NLP:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14} 
    :   * Question Answering (QA)   
        * Paraphrasing  
        * Summarization  
        * Dialog  

5. **Issues in NLP (why nlp is hard?):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15} 
    :   * __Non-Standard English__: "Great Job @ahmed_badary! I luv u 2!! were SOO PROUD of dis."  
        * __Segmentation Issues__: "New York-New Haven" vs "New-York New-Haven"  
        * __Idioms__: "dark horse", "getting cold feet", "losing face"  
        * __Neologisms__: "unfriend", "retweet", "google", "bromance"  
        * __World Knowledge__: "Ahmed and Zach are brothers", "Ahmed and Zach are fathers"    
        * __Tricky Entity Names__: "Where is _Life of Pie_ playing tonight?", "_Let it be_ was a hit song!"  

6. **Tools we need for NLP:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16} 
    :   * Knowledge about Language.  
        * Knowledge about the World.   
        * A way to combine knowledge sources.  

7. **Methods:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17} 
    :   In general we need to construct __Probabilistic Models__ built from _language data_.    
    :   We do so by using _rough text features_.  
        > All the names models, methods, and tools mentioned above will be introduced later as you progress in the text.  


***
***

TITLE: Text Processing
LINK: research/nlp/txt_proc.md


## Introduction and Definitions
{: #content1}

1. **Text Normalization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   Every NLP process starts with a task called _Text Normalization_.  
    :   __Text Normaliization__ is the process of transforming text into a single canonical form that it might not have had before.  
    :   __Importance:__ Normalizing text before storing or processing it allows for _separation of concerns_, since input is guaranteed to be consistent before operations are performed on it.  
    :   __Steps:__  
        1. Segmenting/Tokenizing words in running text.  
        2. Normalizing word formats.  
        3. Segmenting sentences in running text.  

0. **Methods for Normalization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents10}  
    :   * __Case-Folding__: reducing all letters to lower case.  
            > Possibly, with the exception of capital letters mid-sentence.  
        * __Lemmatization__: reducing inflections or variant forms to base form.  
            > Basically, finding the correct dictionary headword form.  

9. **Morphology:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents19}  
    :   The study of words, how they are formed, and their relationship to other words in the same language.  
    :   * __Morphemes__: the small meaningfuk units that make up words.  
        * __Stems__: the core meaning-bearing units of words.  
        * __Affixes__: the bits and pieces that adhere to stems (often with grammatical functions).     


2. **Word Equivalence in NLP:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   Two words have the same  
        * __Lemma__, if they have the same:  
            * Stem  
            * POS  
            * Rough Word-Sense  
            > _cat_ & _cats_ -> same Lemma  
        * __Wordform__, if they have the same:  
            * full inflected surface form  
            > _cat_ & _cats_ -> different wordforms   

3. **Types and Tokens:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   * __Type__: an element of the vocabulary.  
            It is the class of all _tokens containing the same character sequence.  
        * __Token__: an instance of that type in running text.  
            It is an instance of a sequence of characters that are grouped together.  

4. **Notation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   * __N__ = Number of _Tokens_.  
        * __V__ = Vocabulary = set of _Types_.     
        * __$$\|V\|$$__ = size/cardinality of the vocabulary.  

5. **Growth of the Vocabulary:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    :   Church and Gale (1990) suggested that the size of the vocabulary grows larger than the square root of the number of tokens in a piece of text:  
    :   $$\|V\| > \mathcal{O}(N^{1/2})$$  
  

***

## Tokenization
{: #content2}

1. **Tokenization:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    :   It is the task of chopping up a character sequence and a defined document unit into pieces, called [_tokens_](#bodyContents13).  
        It may involve throwing away certain characters, such as punctuation.  

2. **Methods for Tokenization:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    :   * __Regular Expressions__  
        * __A Flag__: Specific squences of characters.  
        * __Delimiters__: pecific separating characters.  
        * __Dictionary__: exlicit definitions by a dictionary.     


3. **Categorization:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    :   Tokens are categorized by:  
        * Character Content  
        * Context  
    within a data stream.  
    :   __Categories__:  
        * _Identifiers_: names the programmer chooses  
        * _keywords_: names already in the programming language.  
        * _Operators_: symbols that operate on arguments and produce results.    
        * _Grouping Symbols_ 
        * _Data Types_
    :   _Categories_ are used for post-processing of the tokens either by the parser or by other functions in the program.  
   

***

## Word-Normalization (Stemming)
{: #content3}

1. **Stemming:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form.  
    :   The stem need __not__ map to a valid root in the language.  
    :   > Basically, Stemming is a crude chopping of [affixes](#bodyContents19)
    :   > __Example__: "automate", "automatic", "automation" -> "automat".  

2. **Porter's Algorithm:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   The most common English stemmer.  
    :   It is an iterated series of simple _replace_ rules.  


3. **Algorithms:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    :   * __The Production Technique__: we produce the lookup table, that is used by a naive stemmer, semi-automaically.  
        * __Suffix-Stripping Algorithms__: those algorithms avoid using lookup tables; instead they use a small list of rules to navigate through the text and find theroot forms from word forms.  
        * __Lemmatisation Algorithms__: the _lemmatization_ process starts determining the _part of speech_ of a word and, then, applying normalization rules to for each part-of-speech.   
        * __Stochastic Algorithms__: those algorithms are trained on a table of root form-to-inflected form relations to develop a probablistic model.  
           The model looks like a set of rules, similar to the suffic-stripping list of rules.      

***

## Sentence Segmentation
{: #content4}

1. **Sentence Segmentation:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    :   It is the problem of diving a piece of text into its component sentences.  

2. **Identifiers:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}  
    :   Identifiers such as "!", "?" are unambiguous; they usually signify the end of a sentence.  
    :   The period "." is quite ambiguous, since it can be used in other ways, such as in abbreviations and in decimal number notation.  

3. **Dealing with Ambiguous Identifiers:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}  
    :   One way of dealing with ambiguous identifiers is by building a __Binary Classifier__.  
        On a given occurrence of a period, the classifier has to decide between one of "Yes, this is the end of a sentence" or "No, this is not the end of a sentence".  
    :   __Types of Classifiers:__  
        * _Decision Trees_  
        * _Logistic Regression_  
        * _SVM_  
        * _Neural-Net_  
    :   _Decision Trees_ are a common classifier used for this problems.   


***
***

TITLE: Reinforcement Learning
LINK: research/dl/ml_research.md


## Deep Learning Generalization
{: #content1}

* [A Practical Bayesian Framework for Backpropagation Networks](https://authors.library.caltech.edu/13793/1/MACnc92b.pdf)  
* [Everything that Works Works Because it's Bayesian: Why Deep Nets Generalize?](https://www.inference.vc/everything-that-works-works-because-its-bayesian-2/)  


__Concepts, Notes, and Observations:__{: style="color: red"}  
{: #lst-p}
* Three Schools of Learning: (1) __Bayesians__   (2) __Kernel People__    (3) __Frequentists__ 
* Bayesians', INCORRECTLY, claimed that:  
    * Highly over-parametrised models fitted via maximum likelihood can't possibly work, they will overfit, won't generalise, etc.  
    * Any model with infinite parameters should be strictly better than any large, but finite parametric model.  
        (e.g. nonparametric models like kernel machines are a principled way to build models with effectively infinite number of parameters)  
* 


1. **Casting ML Algorithms as Bayesian Approximations:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    * __Classical ML__:  
        * L1 regularisation is just MAP estimation with sparsity inducing priors  
        * SVMS, support vector machines, are just the wrong way to train Gaussian processes  
        * [Herding is just Bayesian quadrature done slightly wrong](https://arxiv.org/abs/1204.1664)  
    * __DL__:  
        * [LeCun Post on Uncertainty in Neural Networks](https://www.facebook.com/yann.lecun/posts/10154058859142143)  
        * Dropout is just variational inference done wrong: [Dropout as a Bayesian Approximation](https://arxiv.org/abs/1506.02142)  
        * 


    * Deep Nets memorize 

2. **Why do Deep Nets Generalize?:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    One possibility is: "because they are really just an approximation to Bayesian machine learning." - Ferenc  

    * __SGD__:  
        SGD could be responsible for the good generalization capabilities of Deep Nets.  
        * SGD finds Flat Minima.  
            * A __Flat Minima__ is a minima where the Hessian - and consequently the inverse Fisher information matrix - has small eigenvalues.  
            * Flat might be better than sharp minima:  
                If you are in a flat minimum, there is a relatively large region of parameter space where many parameters are almost equivalent inasmuch as they result in almost equally low error. Therefore, given an error tolerance level, one can describe the parameters at the flat minimum with limited precision, using fewer bits while keeping the error within tolerance. In a sharp minimum, you have to describe the location of your minimum very precisely, otherwise your error may increase by a lot.  
        * [(Keskar et al, 2017)](https://arxiv.org/abs/1609.04836) show that deep nets genealise better with smaller batch-size when no other form of regularisation is used.  
            * And it may be because SGD biases learning towards flat minima, rather than sharp minima.  
        * [(Wilson et al, 2017)](https://arxiv.org/abs/1705.08292) show that these good generalisation properties afforded by SGD diminish somewhat when using popular adaptive SGD methods such as Adam or rmsprop.  
        * Though, there is contradictory work by [(Dinh et al, (2017)](https://arxiv.org/abs/1703.04933) who claim sharp minima can generalize well, too.  
            Also, [(Zhang et al, 2017)](https://cbmm.mit.edu/sites/default/files/publications/CBMM-Memo-067.pdf)  
        * One conclusion is: The reason deep networks work so well (and generalize at all) is not just because they are some brilliant model, but because of the specific details of how we optimize them.  
            Stochastic gradient descent does more than just converge to a local optimum, it is biased to favour local optima with certain desirable properties, resulting in better generalization.  
        * Is SGD Bayesian?  
            * Some work: 
                * [Stochastic Gradient Descent as Approximate Bayesian Inference](https://arxiv.org/pdf/1704.04289.pdf)
            * Flat Minima is Bayesian:  
                It turns out, [(Hochreiter and Schmidhuber, 1997)](http://www.bioinf.jku.at/publications/older/3304.pdf) motivated their work on seeking flat minima from a Bayesian, minimum description length perspective.  
                Even before them, [(Hinton and van Camp, 1993)](http://www.cs.toronto.edu/~fritz/absps/colt93.pdf) presented the same argument in the context of Bayesian neural networks.


                

    <button>Abu-Mostafa on Neural Network Generalization</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    <div hidden="" markdown="1">
    Indeed, the empirical evidence about generalization in deep neural networks with a huge number of weights is that they generalize better than the theory would predict. This is not just in terms of the loose bounds that the theory provides. The performance is better than even the "tight" rules of thumb that were based on the theory and worked in practice.

    This is not the first time this happens in ML. When boosting was the method of choice, generalization was better than it should be. Specifically, there was no overfitting in cases where the model complexity was going up and overfitting would be expected. In that case, a theoretical approach to explain the phenomenon based on a cost function other than ùê∏ùëñùëõ [in-sample error] was advanced. It made sense but it didn't stand up to scrutiny, as minimizing that cost function directly (instead of letting it be minimized through the specific structure of the AdaBoost algorithm for instance) suffered from the usual overfitting. There was no conclusive verdict about how AdaBoost avoids overfitting. There were bits and pieces of intuition, but it is difficult to tell whether that was explanation or rationalization.

    In the case of neural networks, there have also been efforts to explain why the performance is better. There are other approaches to generalization, e.g., based on "stability" of learning, that were invoked. However, the theoretical work on stability was based on perturbation of the training set that does not lead to a significant change in the final hypothesis. The way stability is discussed in the results I have seen in neural networks is based on perturbation of the weights that does not lead to a significant change. It thus uses the concept of stability rather than the established theoretical results to explain why generalization is good. In fact, there are algorithms that deliberately look for a solution that has this type of stability as a way to get good generalization, a regularization of sorts.

    It is conceivable that the structure of deep neural networks, similar to the case of AdaBoost, tends to result in better generalization than the general theory would indicate. To establish that, we need to identify what is it about the structure that makes this happen. In comparison, if you study SVM as a model without getting into the notion of support vectors, you will encounter "inexplicable" good generalization. Once you know about how the number of support vectors affects generalization, the mystery is gone.

    Let me conclude by emphasizing that the VC theory is not violated in any of these instances, since the theory only provides an upper bound. Those cases show a much better performance for particular models, but the performance is still within the theoretical bound. What would be a breakthrough is another, better bound that is applicable to an important class of models. For example, the number of parameters in deep neural networks is far bigger than previous models. If better generalization bounds can be proven for models with huge number of parameters, for instance, that would be quite a coup.  
    </div>



    __Notes:__{: style="color: red"}  
    * "We can connect this finding to recent work examining the generalization of large neural networks. Zhang et al. (2017) observe that deep neural networks seemingly violate the common understanding of learning theory that large models with little regularization will not generalize well. The observed disconnect between NLL and 0/1 loss suggests that these high capacity models are not necessarily immune from overfitting, but rather, overfitting manifests in probabilistic error rather than classification error." - [On Calibration of Modern Neural Networks](https://arxiv.org/pdf/1706.04599.pdf)  







***

## SECOND
{: #content2}









***

## THIRD
{: #content3}









***

***
***

TITLE: Representation Learning
LINK: research/dl/theory/representation_learning.md



* [From Deep Learning of Disentangled Representations to Higher-level Cognition (Bengio Lec)](https://www.youtube.com/watch?v=Yr1mOzC93xs)  
* [Representation Learning (CMU Lec!)](https://www.youtube.com/watch?v=754vWvIimPo)  
* [Representation Learning and Deep Learning (Bengio Talk)](https://www.youtube.com/watch?v=O6itYc2nnnM)  
* [Deep Learning and Representation Learning (Hinton Talk)](https://www.youtube.com/watch?v=7kAlBa7yhDM)  
* [Goals and Principles of Representation Learning (inFERENCe!)](https://www.inference.vc/goals-and-principles-of-representation-learning/)  
* [DALI Goals and Principles of Representation Learning (vids!)](https://www.youtube.com/playlist?list=PL-tWvTpyd1VAlbzhCpljlREd76Nlo1pOo)  



## Representation Learning
{: #content1}

1. **Representation Learning:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __Representation Learning__ (__Feature Learning__) is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.  
    This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task.  


    __Hypothesis - Main Idea:__{: style="color: red"}  
    The core __hypothesis__ for representation learning is that <span>the *unlabeled* data can be used to learn a *good* representation</span>{: style="color: goldenrod"}.  
    
    
    __Types:__{: style="color: red"}  
    Representation learning can be either [__supervised__](#bodyContents14) or [__unsupervised__](#bodyContents13).  


    __Representation Learning Approaches:__{: style="color: red"}  
    There are various ways of learning different representations:  
    {: #lst-p}
    * __Probabilistic Models__: the goal is to learn a representation that captures the probability distribution of the underlying explanatory features for the observed input. Such a learnt representation can then be used for prediction.  
    * __Deep Learning__: the representations are formed by composition of multiple non-linear transformations of the input data with the goal of yielding abstract and useful representations for tasks like classification, prediction etc.  


    __Representation Learning Tradeoff:__{: style="color: red"}  
    Most representation learning problems face a tradeoff between <span>preserving as much information about the input</span>{: style="color: purple"} as possible and <span>attaining nice properties</span>{: style="color: purple"} (such as independence).  


    __The Problem of Data (Semi-Supervised Learning\*):__{: style="color: red"}  
    We often have very large amounts of unlabeled training data and relatively little labeled training data. Training with supervised learning techniques on the labeled subset often results in severe overfitting. Semi-supervised learning offers the chance to resolve this overfitting problem by also learning from the unlabeled data. Specifically, we can learn good representations for the unlabeled data, and then use these representations to solve the supervised learning task.  


    __Learning from Limited Data:__{: style="color: red"}  
    Humans and animals are able to learn from very few labeled examples.   
    Many factors could explain improved human performance ‚Äî for example, the brain may use <span>very large ensembles of classifiers</span>{: style="color: purple"} or <span>Bayesian inference</span>{: style="color: purple"} techniques.  
    One popular hypothesis is that the brain is able to <span>leverage unsupervised or semi-supervised learning</span>{: style="color: purple"}.  

    
    __Motivation/Applications:__{: style="color: red"}  
    {: #lst-p}
    1. ML tasks such as _classification_ often require input that is mathematically and computationally convenient to process.  
        However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features.  
    2. Learning [good representations]() enables us to perform certain (specific) tasks in a more optimal manner.  
        * E.g. linked lists $$\implies$$ $$\mathcal{O}(n)$$ insertion \| red-black tree $$\implies$$ $$\mathcal{O}(\log n)$$ insertion.  
        * <button>Ex: Learning Language Representations</button>{: .showText value="show" onclick="showTextPopHide(event);"}
            * Goal: Learn Portuguese
            * For 1 month you listen to Portuguese on the radio (this is unlabeled data)
            * You develop an intuition for the language, phrases, and grammar (a model in your head)
            * It is easier to learn now from a tutor because you have a better (higher representation) of the data/language    
            {: hidden=""}
    3. Representation Learning is particularly interesting because it <span>provides (one) way to perform __unsupervised__ and __semi-supervised learning__</span>{: style="color: purple"}.  
    4. __Feature Engineering__ is hard. Representation Learning allows us to avoid having to engineer features, manually.  
    5. In general, representation learning can allow us to <span>achieve __multi-task learning__, __transfer learning__, and __domain adaptation__</span>{: style="color: purple"} through <span>shared representations</span>{: style="color: goldenrod"}.  


    __The Quality of Representations:__{: style="color: red"}  
    Generally speaking, a good representation is one that makes a subsequent learning task easier.  
    The choice of representation will usually depend on the choice of the subsequent learning task.  


    __Success of Representation Learning:__{: style="color: red"}  
    The success of representation learning can be attributed to many factors, including:  
    {: #lst-p}
    * Theoretical advantages of __*distributed* representations__ _(Hinton et al., 1986)_  
    * Theoretical advantages of __*deep* representations__ _(Hinton et al., 1986)_   
    * The __Causal Factors Hypothesis__: a general idea of underlying assumptions about the data generating process, in particular about underlying causes of the observed data.  



    __Representation Learning Domain Applications:__{: style="color: red"}  
    {: #lst-p}
    * __Computer Vision__: CNNs.  
    * __Natural Language Processing__: Word-Embeddings.  
    * __Speech Recognition__: Speech-Embeddings.  



    <button>__Representation Quality__</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    * <span>__‚ÄúWhat is a *good* representation?‚Äù__</span>{: style="color: purple"}   
        * Generally speaking, a __good representation__ is one that makes a subsequent learning task easier.  
            The choice of representation will usually depend on the choice of the subsequent learning task.  

    * <span>__‚ÄúWhat makes one representation better than another?‚Äù__</span>{: style="color: purple"}   
        * __Causal Factors Hypothesis:__  
            An __ideal representation__ is one in which <span>the __features__ within the representation _correspond_ to the underlying __causes__ of the observed data</span>{: style="color: purple"}, with __*separate* features__ or __directions__ in _feature space_ corresponding to __*different* causes__, so that <span>the __representation__ *__disentangles__* the __causes__ from one another</span>{: style="color: purple"}.  
            * __Why__:  
                * __Ease of Modeling:__ A representation that __cleanly separates the underlying causal factors__ is, also, one that is __easy to model__.  
                    * For *__many__* __AI tasks__ the two properties __coincide__: once we are able to <span>obtain the underlying explanations for the observations</span>{: style="color: purple"}, it generally becomes <span>easy to isolate individual attributes</span>{: style="color: purple"} from the others.  
                    * Specifically, __if__ a <span>__representation $$\boldsymbol{h}$$__ _represents_ many of the *__underlying causes__* of the __observed $$\boldsymbol{x}$$__</span>{: style="color: purple"}, __and__ the <span>__outputs $$\boldsymbol{y}$$__ are among the __most *salient causes*__</span>{: style="color: purple"}, __then__ it is <span>easy to __predict__ $$\boldsymbol{y}$$ from $$\boldsymbol{h}$$</span>{: style="color: purple"}.  
        * __Summary__ of the *Causal Factors Hypothesis*:  
            An __ideal representation__ is one in which <span>the __features__ within the representation _correspond_ to the underlying __causes__ of the observed data</span>{: style="color: purple"}, with __*separate* features__ or __directions__ in _feature space_ corresponding to __*different* causes__, so that <span>the __representation__ *__disentangles__* the __causes__ from one another</span>{: style="color: purple"}, especially those factors that are relevant to our applications.  

    * <span>__‚ÄúWhat is a *"salient factor"*?‚Äù__</span>{: style="color: purple"}   
        * A __*"salient factor"*__ is a causal factor (latent variable) that explains, *__well__*, the observed variations in $$X$$.  
            * <button>Illustration: Statistical Saliency</button>{: .showText value="show" onclick="showTextPopHide(event);"}
                ![img](https://cdn.mathpix.com/snip/images/TTMVZWgnTfvYTrRbqLCW5VGoOZSUPC21oMaPDhijp-c.original.fullsize.png){: width="100%" hidden=""}  
            * What makes a feature _"salient"_ for humans?  
                It could be something really simple like __correlation__ or __predictive power__.  
                Ears are a salient feature of Humans because in a majority of cases, presence of one implies presence of another.  
            * Discriminative features as salient features:  
                Note that in object detection case, the predictive power is only measured in:  
                (ear $$\rightarrow$$ person) direction, not (person $$\rightarrow$$ ear) direction.  
                E.g. if your task was to discriminate between males and females, presence of ears would not be a useful feature even though all humans have ears. Compare this to the pimples case: in human vs dog classification, pimples are a really good predictor of 'human', even though they are not a salient feature of Humans.  
                Basically I think discriminative =/= salient  
    {: hidden=""}


    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * Representation Learning can be done with both, __generative__ and __discriminative__ models.  
    * In DL, representation learning uses a composition of __transformations__ of the input data (features) to create learned features.  
    <br>

    

7. **Distributed Representation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    __Distributed Representations__ of concepts are representations composed of many elements that can be set separately from each other.  
    
    __Distributed representations of concepts__ are one of the most important tools for __representation learning__:  
    {: #lst-p}
    * Distributed representations are __powerful__ because <span>they can use $$n$$ __features__ with $$k$$ __values__ to *__describe__* $$k^{n}$$ __different concepts__</span>{: style="color: purple"}.  
    * Both __neural networks with multiple hidden units__ and __probabilistic models with multiple latent variables__ make use of the strategy of distributed representation.  
    * __Motivation for using Distributed Representations:__   
        Many __deep learning algorithms__ are motivated by the assumption that the <span>hidden units can *learn to represent* the underlying __causal factors__ that *explain* the data</span>{: style="color: goldenrod"}.  
        Distributed representations are natural for this approach, because each direction in representation space can correspond to the value of a different underlying configuration variable.  
    * __Distributed vs Symbolic Representations__:  
        * __Number of "Representable" Configurations - by example__:  
            * An example of a __distributed representation__ is a <span>vector of $$n$$ binary features</span>{: style="color: purple"}.  
                It can take $$2^{n}$$ configurations, each potentially corresponding to a different region in input space.  
                <button>Illustration</button>{: .showText value="show" onclick="showTextPopHide(event);"}
                ![img](https://cdn.mathpix.com/snip/images/aSpsvuJ06k0ls64IxpaAcv31EdfpEubzymZhuD6H5ZE.original.fullsize.png){: width="100%" hidden=""}  
            * An example of a __symbolic representation__, is <span>one-hot representation</span>{: style="color: purple"}[^6] where the input is associated with a single symbol or category.  
                If there are $$n$$ symbols in the dictionary, one can imagine $$n$$ feature detectors, each corresponding to the detection of the presence of the associated category.  
                In that case only $$n$$ different configurations of the representation space are possible, carving $$n$$ different regions in input space.  
                <button>Illustration</button>{: .showText value="show" onclick="showTextPopHide(event);"}
                ![img](https://cdn.mathpix.com/snip/images/LENWWJMJ5SNBNMLyIzxdYGLIMVqSEk9fGtkv0XQoi5s.original.fullsize.png){: width="100%" hidden=""}  
                A symbolic representation is a specific example of the broader class of non-distributed representations, which are representations that may contain many entries but without significant meaningful separate control over each entry.  
        * __Generalization__:  
            An important related concept that distinguishes a distributed representation from a symbolic one is that <span>__generalization__ arises due to *__shared attributes between different concepts__*</span>{: style="color: purple"}.  
            * <button>Discussion/Analysis</button>{: .showText value="show" onclick="showTextPopHide(event);"}
                * As pure symbols, ‚Äúcat‚Äù and ‚Äúdog‚Äù are as far from each other as any other two symbols.  
                    However, if one associates them with a meaningful distributed representation, then many of the things that can be said about cats can generalize to dogs and vice-versa.  
                    * For example, our distributed representation may contain entries such as ‚Äúhas_fur‚Äù or ‚Äúnumber_of_legs‚Äù that have the same value for the embedding of both ‚Äúcat ‚Äù and ‚Äúdog.‚Äù  
                        __Neural language models__ that operate on distributed representations of words generalize much better than other models that operate directly on one-hot representations of words _(section 12.4)_.  
                        <span>Distributed representations induce a rich similarity space, in which semantically close concepts (or inputs) are close in distance, a property that is absent from purely symbolic representations.</span>{: style="color: purple"}    
                {: hidden=""}
            <span>Distributed representations induce a rich similarity space, in which semantically close concepts (or inputs) are close in distance, a property that is absent from purely symbolic representations.</span>{: style="color: purple"}  
            [__\[Analysis: Generalization of Distributed Representations\]__](#bodyContents17gen)
    * <button>Examples of __learning algorithms__ based on __non-distributed representations__:</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        * __Clustering methods, including the $$k$$-means algorithm__: each input point is assigned to exactly one cluster.
        * __k-nearest neighbors algorithms__: one or a few templates or prototype examples are associated with a given input. In the case of $$k>1$$, there are multiple values describing each input, but they can not be controlled separately from each other, so this does not qualify as a true distributed representation.  
        * __Decision trees__: only one leaf (and the nodes on the path from root to leaf) is activated when an input is given.
        * __Gaussian mixtures and mixtures of experts__: the templates (cluster centers) or experts are now associated with a degree of activation. As with the k-nearest neighbors algorithm, each input is represented with multiple values, but those values cannot readily be controlled separately from each other.  
        * __Kernel machines with a Gaussian kernel (or other similarly local kernel)__: although the degree of activation of each ‚Äúsupport vector‚Äù or template example is now continuous-valued, the same issue arises as with Gaussian mixtures.  
        * __Language or translation models based on n-grams__: The set of contexts (sequences of symbols) is partitioned according to a tree structure of suffixes. A leaf may correspond to the last two words being w1 and w2, for example. Separate parameters are estimated for each leaf of the tree (with some sharing being possible).  
        {: hidden=""}
    > For some of these non-distributed algorithms, the output is not constant by parts but instead interpolates between neighboring regions. The relationship between the number of parameters (or examples) and the number of regions they can define remains linear.  
    

    __Generalization of Distributed Representations:__{: style="color: red"}{: #bodyContents17gen}  
    We know that for __distributed representations__, <span>__Generalization__ arises due to __*shared attributes* between different concepts__</span>{: style="color: purple"}.  

    But an important question is:  
    <span>**"When and why can there be a statistical advantage from using a distributed representation as part of a learning algorithm?"**</span>{: style="color: purple"}   
    {: #lst-p}
    * Distributed representations can have a __statistical advantage__ when an <span>__apparently complicated structure__ can be *__compactly__* __represented__ using a __*small number* of parameters__</span>{: style="color: goldenrod"}.  
    * Some traditional nondistributed learning algorithms generalize only due to the __smoothness assumption__, which states that if $$u \approx v,$$ then the target function $$f$$ to be learned has the property that $$f(u) \approx f(v),$$ in general.  
        There are many ways of formalizing such an assumption, but the end result is that if we have an example $$(x, y)$$ for which we know that $$f(x) \approx y,$$ then we choose an estimator $$\hat{f}$$ that approximately satisfies these constraints while changing as little as possible when we move to a nearby input $$x+\epsilon$$.  
        * This assumption is clearly very useful, but it _suffers_ from the __curse of dimensionality__: in order to learn a target function that increases and decreases many times in many different regions,1 we may need a number of examples that is at least as large as the number of distinguishable regions.  
            One can think of each of these regions as a category or symbol: by having a separate degree of freedom for each symbol (or region), we can learn an arbitrary decoder mapping from symbol to value.  
            However, this does not allow us to generalize to new symbols for new regions.  
    * If we are lucky, there may be some __*regularity*__ in the __target function__, besides being _smooth_.  
        For example, a __convolutional network__ with __max-pooling__ can recognize an object regardless of its location in the image, even though spatial translation of the object may not correspond to smooth transformations in the input space.  

    __Justifying Generalization in distributed representations:__{: style="color: red"}  
    {: #lst-p}
    * __Geometric justification (by analyzing binary, linear feature extractors (units)):__  
        Let us examine a special case of a distributed representation learning algorithm, that extracts __binary features__ by thresholding __linear functions__ of the input:   
        <button>Discussion</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        * Each binary feature in this representation *__divides__* $$\mathbb{R}^{d}$$ into a __pair__ of __half-spaces__.  
        * The *__exponentially large__* __number__ of __intersections__ of $$n$$ of the corresponding half-spaces _determines_ the <span>number of regions this distributed representation learner can distinguish</span>{: style="color: purple"}.  
        * The __number of regions generated by an arrangement of $$n$$ hyperplanes in $$\mathbb{R}^{d}$$__:  
            By applying a general result concerning the intersection of hyperplanes _(Zaslavsky, } 1975)_, one can show _(Pascanu et al, 2014b)_ that the number of regions this binary feature representation can distinguish is:  
            <p>$$\sum_{j=0}^{d}\left(\begin{array}{l}{n} \\ {j}\end{array}\right)=O\left(n^{d}\right)$$</p>  
        * Therefore, we see a <span>__growth__ that is *__exponential__* in the __input size__ and *__polynomial__* in the __number of hidden units__</span>{: style="color: purple"}.  
        * This provides a __geometric argument__ to _explain_ the <span>generalization power of distributed representation</span>{: style="color: purple"}:  
            with $$\mathcal{O}(n d)$$ parameters (for $$n$$ linear-threshold features in $$\mathbb{R}^{d}$$) we can distinctly represent $$\mathcal{O}\left(n^{d}\right)$$ regions in input space.  
            * If instead we made no assumption at all about the data, and used a representation with unique symbol for each region, and separate parameters for each symbol to recognize its corresponding portion of $$\mathbb{R}^{d},$$ then,  
                specifying $$\mathcal{O}\left(n^{d}\right)$$ regions would require $$\mathcal{O}\left(n^{d}\right)$$ examples.  
        * More generally, the argument in favor of the distributed representation could be extended to the case where instead of using linear threshold units we use __nonlinear__, possibly __continuous__, __feature extractors__ for each of the attributes in the distributed representation.  
            The argument in this case is that if a parametric transformation with $$k$$ parameters can learn about $$r$$ regions in input space, with $$k \ll r,$$ and if obtaining such a representation was useful to the task of interest, then we could potentially generalize much better in this way than in a non-distributed setting where we would need $$\mathcal{O}(r)$$ examples to obtain the same features and associated partitioning of the input space into $$r$$ regions.  
            Using fewer parameters to represent the model means that we have fewer parameters to fit, and thus require far fewer training examples to generalize well.  
        {: hidden=""}
    * __VC-Theory justification - Fixed Capacity__:  
        <button>Discussion</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        <div hidden="" markdown="1">
        The __capacity__ remains *__limited__* despite being able to distinctly encode so many different regions.  
        For example, the __VC-dimension__ of a __neural network of linear threshold units__ is only $$\mathcal{O}(w \log w),$$ where $$w$$ is the number of weights _(Sontag, 1998_.  

        This limitation arises because, while we can assign very many unique codes to representation space, we __cannot__:  
        {: #lst-p}
        * Use absolutely all of the code space
        * Learn arbitrary functions mapping from the representation space $$h$$ to the output $$y$$ using a linear classifier.  

        The <span>use of a __distributed representation__ _combined_ with a __linear classifier__</span>{: style="color: purple"} thus __*expresses* a prior belief__ that <span>the classes to be recognized are __linearly separable__ as a __function__ of the underlying __causal factors__ captured by $$h$$</span>{: style="color: purple"}.  

        > We will typically want to learn categories such as the set of all images of all green objects or the set of all images of cars, but not categories that require nonlinear, $$\mathrm{XOR}$$ logic. For example, we typically do not want to partition the data into the set of all red cars and green trucks as one class and the set of all green cars and red trucks as another class.  
        </div>
    * __Experimental justification__:  
        <button>Discussion</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        Though the above ideas have been abstract, they may be experimentally validated:  
        * _Zhou et al. (2015)_ find that __hidden units in a deep convolutional network__ trained on the ImageNet and Places benchmark datasets <span>learn features that are very often interpretable, corresponding to a label that humans would naturally assign</span>{: style="color: purple"}.  
            In practice it is certainly not always the case that hidden units learn something that has a simple linguistic name, but it is interesting to see this emerge near the top levels of the best computer vision deep networks. What such features have in common is that one could imagine learning about each of them without having to see all the configurations of all the others.  
        * _Radford et al. (2015)_ demonstrated that a __generative model__ can <span>learn a representation of images of faces, with separate directions in representation space capturing different underlying factors of variation</span>{: style="color: purple"}.  
            The following illustration demonstrates that one direction in representation space corresponds to whether the person is male or female, while another corresponds to whether the person is wearing glasses.  
            <button>Illustration: linear structure of latent variables</button>{: .showText value="show" onclick="showTextPopHide(event);"}
            ![img](https://cdn.mathpix.com/snip/images/zCy3LJD1DqrdXwzCvbZeuG1DyuuPwR6xdHZoClbwN4o.original.fullsize.png){: width="100%" hidden=""}  
            These features were discovered automatically, not fixed a priori.  
            There is no need to have labels for the hidden unit classifiers: gradient descent on an objective function of interest naturally learns semantically interesting features, so long as the task requires such features.  
            We can learn about the distinction between male and female, or about the presence or absence of glasses, without having to characterize all of the configurations of the $$n ‚àí 1$$ other features by examples covering all of these combinations of values.  
            <span>This form of __statistical separability__ is what allows one to generalize to new configurations of a person‚Äôs features that have never been seen during training.</span>{: style="color: goldenrod"}  
        {: hidden=""}


    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * `page 542` As a counter-example, recent research from DeepMind ([Morcos et al., 2018](https://arxiv.org/abs/1803.06959)) suggests that while some hidden units might appear to learn an interpretable feature, 'these interpretable neurons are no more important than confusing neurons with difficult-to-interpret activity'.  
        Moreover, 'networks which generalise well are much less reliant on single directions [ie. hidden units] than those which memorise'. See more in the DeepMind [blog post](https://deepmind.com/blog/understanding-deep-learning-through-neuron-deletion/).  
    * <span>Distributed representations based on latent variables can obtain all of the advantages of representation learning that we have seen with deep feedforward and recurrent networks.</span>{: style="color: goldenrod"}  
    * __Food for Thought (F2T):__  
        _"since feature engineering was made obsolete by deep learning, algorithm engineering will be made obsolete by meta-learning"_ - Sohl-Dickstein  
    <br>

8. **Deep Representations - Exponential Gain from Depth:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  

    __Exponential Gain in MLPs:__{: style="color: red"}  
    We have seen in (section 6.4.1) that multilayer perceptrons are __universal approximators__, and that some functions can be represented by __exponentially smaller *deep* networks__ compared to *__shallow__* __networks__.  
    This __decrease in model size__ leads to _improved_ __statistical efficiency__.  

    Similar results apply, more generally, to other kinds of __models with__ <span>__*distributed* hidden representations__</span>{: style="color: purple"}.  

    __Justification/Motivation:__{: style="color: red"}  
    In this and other AI tasks, <span>the __factors__ that can be *chosen almost __independently__ from each other* yet still *correspond to __meaningful inputs__* are more likely to be __*very high-level*__ and __related__ in __*highly nonlinear ways*__ to the __input__</span>{: style="color: purple"}.  
    _Goodfellow et al._ argue that this <span>demands **deep distributed representations**</span>{: style="color: purple"}, where the __higher level features__ (seen as functions of the input) or __factors__ (seen as generative causes) are *obtained through the __composition__ of many __nonlinearities__*.  
    > E.g. the example of a generative model that learned about the explanatory factors underlying images of faces, including the person‚Äôs gender and whether they are wearing glasses.  
        It would not be reasonable to expect a shallow network, such as a linear network, to learn the complicated relationship between these abstract explanatory factors and the pixels in the image.  

    __Universal Approximation property in Models (from Depth):__{: style="color: red"}  
    {: #lst-p}
    * It has been proven in many different settings that <span>__organizing computation through the composition of many nonlinearities__ and a __hierarchy of reused features__ can give an *__exponential boost__* to __statistical efficiency__</span>{: style="color: purple"}, on top of the *__exponential boost__* given by using a __distributed representation__.  
    * Many kinds of networks (e.g., with saturating nonlinearities, Boolean gates, sum/products, or RBF units) with a __single hidden layer__ can be shown to be __universal approximators__.  
        A model family that is a universal approximator can approximate a large class of functions (including all continuous functions) up to any non-zero tolerance level, given enough hidden units.  
        However, the required number of hidden units may be very large.  
    * Theoretical results concerning the __expressive power of deep architectures__ state that <span>there are families of functions that can be represented efficiently by an architecture of depth $$k$$</span>{: style="color: purple"}, but would require an *__exponential number__* of __hidden units__ (wrt. __input size__) with *__insufficient__* __depth__ (depth $$2$$ or depth $$k ‚àí 1$$).  
    
    __Exponential Gains in Structured Probabilistic Models:__{: style="color: red"}  
    {: #lst-p}
    * __PGMs as Universal Approximators__:  
        * Just like __deterministic feedforward networks__ are __universal approximators__ of __*functions*__{: style="color: goldenrod"}.  
            Many __structured probabilistic models__ with a __single hidden layer__ of __latent variables__, including restricted Boltzmann machines and deep belief networks, are __universal approximators__ of __*probability distributions*__{: style="color: goldenrod"} _(Le Roux and Bengio, 2008, 2010; Mont√∫far and Ay, 2011; Mont√∫far, 2014; Krause et al., 2013)_.  
    * __Exponential Gain from Depth in PGMs__: 
        * Just like a _sufficiently_ *__deep__* __feedforward network__ can have an *__exponential__* __advantage__ over a network that is too *__shallow__*.    
            Such results can also be obtained for other models such as __probabilistic models__.  
            * E.g. The __sum-product network (SPN)__ _(Poon and Domingos, 2011)_.  
                These models use polynomial circuits to compute the probability distribution over a set of random variables.  
                * _Delalleau and Bengio (2011)_ showed that there exist probability distributions for which a minimum depth of SPN is required to avoid needing an exponentially large model.  
                * Later, _Martens and Medabalimi (2014)_ showed that there are significant differences between every two finite depths of SPN, and that some of the constraints used to make SPNs tractable may limit their representational power.  

    __Expressiveness of Convolutional Networks:__{: style="color: red"}  
    Another interesting development is a set of theoretical results for the expressive power of families of deep circuits related to convolutional nets:  
    They highlight an *__exponential advantage__* for the deep circuit even when the <span>shallow circuit is allowed to *only* __approximate__ the function computed by the deep circuit</span>{: style="color: purple"}  (Cohen et al., 2015).  
    By comparison, previous theoretical work made claims regarding only the case where the shallow circuit must exactly replicate particular functions.  
    <br>

***

## Unsupervised Representation Learning
{: #content2}

11. **Unsupervised Representation Learning:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents211}  
    In __Unsupervised feature learning__, features are learned with *__unlabeled__* __data__.  

    The __Goal__ of unsupervised feature learning is often to <span>discover low-dimensional features that captures some structure underlying the high-dimensional input data</span>{: style="color: purple"}.  

    <button>Examples:</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    <div hidden="" markdown="1">
    * (Unsupervised) Dictionary Learning  
    * ICA/PCA  
    * AutoEncoders 
    * Matrix Factorization  
    * Clustering Algorithms
    </div>

    __Learning:__{: style="color: red"}  
    Unsupervised deep learning algorithms have a main training objective but also <span>__learn a representation__ as a *__side effect__*</span>{: style="color: purple"}.  

    __Unsupervised Learning for Semisupervised Learning:__{: style="color: red"}  xw
    When the feature learning is performed in an unsupervised way, it enables a form of __semisupervised learning__ where features learned from an unlabeled dataset are then employed to improve performance in a supervised setting with labeled data.  
    <br>

1. **Greedy Layer-Wise Unsupervised Pretraining:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    __(Greedy Layer-Wise) Unsupervised Pretraining__ 

    {: #lst-p}
    * __Greedy__: it is a __greedy algorithm__.  
        It optimizes each piece of the solution independently, one piece at a time, rather than jointly optimizing all pieces.  
    * __Layer-Wise__: the independent pieces are the layers of the network[^1].  
    * __Unsupervised__: each layer is trained with an unsupervised representation learning algorithm.  
    * __Pretraining__[^2]: it is supposed to be only a first step before a joint training algorithm is applied to fine-tune all the layers together.  

    This procedure is a canonical example of how a representation learned for one task (unsupervised learning, trying to capture the shape of the input distribution) can sometimes be useful for another task (supervised learning with the same input domain).  


    __Algorithm/Procedure:__{: style="color: red"}  
    <button>Algorithm</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/1wrL7C3u0dxHDgsAnbJi7tXljo56azdawSlLh7m1tk4.original.fullsize.png){: width="100%" hidden=""}  
    {: #lst-p}
    * __Supervised Learning Phase:__  
        It may involve:  
        1. Training a simple classifier on top of the features learned in the pretraining phase.  
        2. Supervised __fine-tuning__ of the entire network learned in the pretraining phase.  


    __Interpretation in Supervised Settings:__{: style="color: red"}  
    In the context of a __supervised learning__ task, the procedure can be viewed as:  
    {: #lst-p}
    * A __Regularizer__.  
        In some experiments, pretraining decreases test error without decreasing training error.  
    * A form of __Parameter Initialization__.  


    __Applications:__{: style="color: red"}  
    {: #lst-p}
    * __Training Deep Models__:  
        Greedy layer-wise training procedures based on unsupervised criteria have long been used to sidestep the difficulty of jointly training the layers of a deep neural net for a supervised task.  
        The deep learning renaissance of 2006 began with the discovery that this greedy learning procedure could be used to find a good initialization for a joint learning procedure over all the layers, and that this approach could be used to successfully train even fully connected architectures.  
        Prior to this discovery, only convolutional deep networks or networks whose depth resulted from recurrence were regarded as feasible to train.  
    * __Parameter Initialization__:  
        THey can also be used as initialization for other unsupervised learning algorithms, such as:  
        * __Deep Autoencoders__ _(Hinton and Salakhutdinov, 2006)_  
        * __Probabilistic mModels__ with __*many layers of latent variables*__:  
            E.g. __deep belief networks (DBNs)__ _(Hinton et al., 2006)_ and __deep Boltzmann machines (DBMs)__ _(Salakhutdinov and Hinton, 2009a)_.  

    <br>

2. **Clustering \| K-Means:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
3. **Local Linear Embeddings:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
4. **Principal Components Analysis (PCA):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
5. **Independent Components Analysis (ICA):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
6. **(Unsupervised) Dictionary Learning:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  


***

## Supervised Representation Learning
{: #content3}

11. **Supervised Representation Learning:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents311}  
    In __Supervised feature learning__, features are learned using *__labeled__* __data__.  

    __Learning:__{: style="color: red"}  
    The data label allows the system to compute an error term, the degree to which the system fails to produce the label, which can then be used as feedback to correct the learning process (reduce/minimize the error).  

    __Examples:__  
    {: #lst-p}
    * Supervised Neural Networks  
    * Supervised Dictionary Learning  

    __FFNs as Representation Learning Algorithms:__{: style="color: red"}  
    <button>Discussion</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    * We can think of __Feed-Forward Neural Networks__ trained by _supervised learning_ as performing a kind of representation learning.  
    * All the layers except the last layer (usually a linear classifier), are basically producing representations (featurizing) of the input.  
    * Training with a supervised criterion naturally leads to the representation at every hidden layer (but more so near the top hidden layer) taking on properties that make the classification task easier:  
        E.g. Making classes linearly separable in the latent space.  
    * The features in the penultimate layer should learn different properties depending on the type of the last layer.  
    * Supervised training of feedforward networks does not involve explicitly imposing any condition on the learned intermediate features.  
    * We can, however, explicitly impose certain desirable conditions.  
        <button>Example: Learning Independent Representations</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
        <p hidden="" markdown="1">
        Suppose we want to learn a representation that makes density estimation easier. Distributions with more independences are easier to model, so we could design an objective function that encourages the elements of the representation vector $$\boldsymbol{h}$$ to be independent.  
        </p>  
    {: hidden=""}

1. **Greedy Layer-Wise Supervised Pretraining:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    As discussed in section __8.7.4__, it is also possible to have greedy layer-wise supervised pretraining.  
    This builds on the __premise__ that <span>training a shallow network is easier than training a deep one</span>{: style="color: goldenrod"}, which seems to have been validated in several contexts (Erhan et al., 2010).<br>

2. **Neural Networks:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
3. **Supervised Dictionary Learning:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
  

***

## Transfer Learning and Domain Adaptation
{: #content4}

<button>Resources</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
* [ICML 2018: Advances in transfer, multitask, and semi-supervised learning (blog)](https://towardsdatascience.com/icml-2018-advances-in-transfer-multitask-and-semi-supervised-learning-2a15ef7208ec)  
* [Transfer Learning (Ruder Blog!)](http://ruder.io/transfer-learning/)  
* [Multi-Task Learning Objectives for Natural Language Processing (Ruder Blog)](http://ruder.io/multi-task-learning-nlp/)  
* [An Overview of Multi-Task Learning in Deep Neural Networks (Ruder Blog!)](http://ruder.io/multi-task/index.html)  
* [Transfer Learning Overview (paper!)](http://ftp.cs.wisc.edu/machine-learning/shavlik-group/torrey.handbook09.pdf)  
* [Transfer Learning for Deep Learning (Blog! - Resources!)](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)  
* [How transferable are features in deep neural networks? (paper)](https://arxiv.org/abs/1411.1792)  
* [On Learning Invariant Representations for Domain Adaptation (blog!)](https://blog.ml.cmu.edu/2019/09/13/on-learning-invariant-representations-for-domain-adaptation/)  
{: hidden=""}

![img](https://cdn.mathpix.com/snip/images/Qk1gQN3lvxom7rIa4o9ZUDHnlPfJZCVDiM6pAvkiq3s.original.fullsize.png){: width="50%"}  

1. **Introduction - Transfer Learning and Domain Adaptation:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    __Transfer Learning__ and __Domain Adaptation__ refer to the situation where what has been learned in one setting (i.e., distribution $$P_{1}$$) is exploited to improve generalization in another setting (say distribution $$P_{2}$$).  

    This is a generalization of __unsupervised pretraining__, where we transferred representations between an unsupervised learning task and a supervised learning task.  

    In __Supervised Learning__: __transfer learning__, __domain adaptation__, and __concept drift__ can be viewed as particular forms of __Multi-Task Learning__.  
    > However, __Transfer Learning__ is a more general term that applies to both __Supervised__ and __Unsupervised Learning__, as well as, __Reinforcement Learning__.  

    __Goal/Objective and Relation to Representation Learning:__{: style="color: red"}  
    In the cases of __Transfer Learning__, __Multi-Task Learning__, and __Domain Adaptation__: 
    The __Objective/Goal__ is to <span>take advantage of data from the first setting to extract information that may be useful when learning or even when directly making predictions in the second setting</span>{: style="color: purple"}.  

    The __core idea__ of __Representation Learning__ is that <span>the same representation may be useful in both settings</span>{: style="color: purple"}.  

    Thus, we can use <span>shared representations</span>{: style="color: goldenrod"} to accomplish Transfer Learning etc.  
    __Shared Representations__ are useful to handle multiple modalities or domains, or to transfer learned knowledge to tasks for which few or no examples are given but a task representation exists.  

    <button>Transfer Learning</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/VNQ1uX35tE4SKhEHGuTA53A03GOb9Ttb_LWI6QdOjkg.original.fullsize.png){: width="80%"}  
    <br>


2. **Transfer Learning:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}  
    __Transfer Learning__ (in ML) is the problem of storing knowledge gained while solving one problem and applying it to a different but related problem.  

    __Definition:__  
    __Formally,__ the definition of transfer learning is given in terms of:  
    {: #lst-p}
    * A __Domain $$\mathcal{D}=\{\mathcal{X}, P(X)\}$$__, $$\:\:$$ consisting of:  
        * __Feature Space $$\mathcal{X}$$__  
        * __Marginal Probability Distribution $$P(X)$$__,  
            where $$X=\left\{x_{1}, \ldots, x_{n}\right\} \in \mathcal{X}$$.  
    * A __Task $$\mathcal{T}=\{\mathcal{Y}, f(\cdot)\}$$__,  
        (given a specific domain $$\mathcal{D}=\{\mathcal{X}, P(X)\}$$) consisting of:  
        * A __label space $$\mathcal{Y}$$__   
        * An __objective predictive function $$f(\cdot)$$__  
            It is learned from the training data, which consist of pairs $$\left\{x_ {i}, y_{i}\right\}$$, where $$x_{i} \in X$$ and $$y_{i} \in \mathcal{Y}$$.  
            It can be used to predict the corresponding label, $$f(x)$$, of a new instance $$x$$.  

    Given a source domain $$\mathcal{D}_ {S}$$ and learning task $$\mathcal{T}_ {S}$$, a target domain $$\mathcal{D}_ {T}$$ and learning task $$\mathcal{T}_ {T}$$, __transfer learning__ aims to help improve the learning of the target predictive function $$f_ {T}(\cdot)$$ in $$\mathcal{D}_ {T}$$ using the knowledge in $$\mathcal{D}_ {S}$$ and $$\mathcal{T}_ {S}$$, where $$\mathcal{D}_ {S} \neq \mathcal{D}_ {T}$$, or $$\mathcal{T}_ {S} \neq \mathcal{T}_ {T}$$.  
    <br>


    In Transfer Learning, the learner must perform two or more different tasks, but we assume that many of the factors that explain the variations in $$P_1$$ are relevant to the variations that need to be captured for learning $$P_2$$. This is typically understood in a supervised learning context, where the input is the same but the target may be of a different nature.  
    <button>Example: visual features</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    <div hidden="" markdown="1">
    We may learn about one set of visual categories, such as cats and dogs, in the first setting, then learn about a different set of visual categories, such as ants and wasps, in the second setting.  
    If there is significantly more data in the first setting (sampled from $$P_1$$), then that may help to learn representations that are useful to quickly generalize from only very few examples drawn from $$P_2$$.  
    Many visual categories share low-level notions of edges and visual shapes, the effects of geometric changes, changes in lighting, etc.  
    </div>


    __Types of Transfer Learning:__{: style="color: red"}  
    {: #lst-p}
    * __Inductive Transfer Learning__:  
        $$\mathcal{D}_ {S} = \mathcal{D}_ {T} \:\:\: \text{  and  }\:\:\: \mathcal{T}_ {S} \neq \mathcal{T}_ {T}$$  
        __e.g.__ $$\left(\mathcal{D}_ {S} = \text{ Wikipedia } = \mathcal{D}_ {T}\right) \:\: \text{  and  } \:\: \left(\mathcal{T}_ {S} = \text{ Skip-Gram }\right) \neq \left(\mathcal{T}_ {T} = \text{ Classification }\right)$$  
    * __Transductive Transfer Learning (Domain Adaptation)__:  
        $$\mathcal{D}_ {S} \neq \mathcal{D}_ {T} \:\:\: \text{  and  }\:\:\: \mathcal{T}_ {S} = \mathcal{T}_ {T}$$  
        __e.g.__ $$\left(\mathcal{D}_ {S} = \text{ Reviews }\right) \neq \left(\mathcal{D}_ {T} = \text{ Tweets }\right) \:\: \text{  and  } \:\: \left(\mathcal{T}_ {S} = \text{ Sentiment Analysis } = \mathcal{T}_ {T}\right)$$  
    * __Unsupervised Transfer Learning__:  
        $$\mathcal{D}_ {S} \neq \mathcal{D}_ {T} \:\:\: \text{  and  }\:\:\: \mathcal{T}_ {S} \neq \mathcal{T}_ {T}$$  
        __e.g.__ $$\left(\mathcal{D}_ {S} = \text{ Animals}\right) \neq \left(\mathcal{D}_ {T} = \text{ Cars}\right) \: \text{  and  } \: \left(\mathcal{T}_ {S} = \text{ Recog.}\right) \neq \left(\mathcal{T}_ {T} = \text{ Detection}\right)$$  
    * <button>Transfer Learning</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](https://cdn.mathpix.com/snip/images/VNQ1uX35tE4SKhEHGuTA53A03GOb9Ttb_LWI6QdOjkg.original.fullsize.png){: width="60%" hidden=""}  

    
    __Concept Drift:__{: style="color: red"}  
    __Concept Drift__ is a phenomena where the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.  

    It can be viewed as a form of __transfer learning__ due to <span>gradual changes in the data distribution</span>{: style="color: purple"} over time.  

    <button>Concept Drift in __RL__</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    <div hidden="" markdown="1">
    Another example is in reinforcement learning. Since the agent's policy affects the environment, the agent learning and updating its policy directly results in a changing environment with shifting data distribution.  
    </div>

    
    __Unsupervised Deep Learning for Transfer Learning:__{: style="color: red"}  
    <button>Discussion</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    <div hidden="" markdown="1">
    Unsupervised Deep Learning for Transfer Learning has seen success in some machine learning competitions _(Mesnil et al., 2011; Goodfellow et al., 2011)_.  
    In the first of these competitions, the experimental setup is the following:  
    {: #lst-p}
    * Each participant is first given a dataset from the first setting (from distribution $$P_1$$), illustrating examples of some set of categories.  
    * The participants must use this to learn a good feature space (mapping the raw input to some representation), such that when we apply this learned transformation to inputs from the transfer setting (distribution $$P_2$$ ), a linear classifier can be trained and generalize well from very few labeled examples.  

    One of the most striking results found in this competition is that as an architecture makes use of deeper and deeper representations (learned in a purely unsupervised way from data collected in the first setting, $$P_1$$), the learning curve on the new categories of the second (transfer) setting $$P_2$$ becomes much better.  
    For __deep representations__, *fewer labeled examples* of the *transfer tasks* are necessary to achieve the apparently __asymptotic generalization__ performance.  
    </div><br>

3. **Domain Adaptation:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}  
    __Domain Adaptation__ is a form of *__transfer learning__* where we aim at learning from a source data distribution a well performing model on a different (but related) target data distribution.  
    
    It is a *__sequential__* process.  
    
    In __domain adaptation__, the task (and the optimal input-to output mapping) remains the same between each setting, but the input distribution is slightly different.  
    <button>Example: Sentiment Analysis</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    <div hidden="" markdown="1">
    Consider the task of sentiment analysis, which consists of determining whether a comment expresses positive or negative sentiment. Comments posted on the web come from many categories. A domain adaptation scenario can arise when a sentiment predictor trained on customer reviews of media content such as books, videos and music is later used to analyze comments about consumer electronics such as televisions or smartphones.   
    One can imagine that there is an underlying function that tells whether any statement is positive, neutral or negative, but of course the vocabulary and style may vary from one domain to another, making it more difficult to generalize across domains.  
    Simple unsupervised pretraining (with denoising autoencoders) has been found to be very successful for sentiment analysis with domain adaptation _(Glorot et al., 2011b)_.  
    </div>


4. **Multitask Learning:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents44}  
    __Multitask Learning__ is a *__transfer learning__* where multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks.  

    In particular, it is an approach to __inductive transfer__ that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.  

    It is a *__parallel__* process.  

    __Multitask vs Transfer Learning:__{: style="color: red"}  
    ![img](https://cdn.mathpix.com/snip/images/Qk1gQN3lvxom7rIa4o9ZUDHnlPfJZCVDiM6pAvkiq3s.original.fullsize.png){: width="50%"}
    1. __Multi-Task Learning__: general term for training on multiple tasks  
        1. __Joint Learning:__ by choosing mini-batches from two different tasks simultaneously/alternately
        1. __Pre-Training:__ first train on one task, then train on another  
            widely used for __word embeddings__.  
    1. __Transfer Learning__:  
        a type of multi-task learning where we are focused on one task; by learning on another task then applying those models to our main task  
    <br>


5. **Representation Learning for the Transfer of Knowledge:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents45}  
    We can use __Representation Learning__ to achieve __Multi-Task Learning__, __Transfer Learning__, and __Domain Adaptation__.  

    In general, __Representation Learning__ can be used to achieve __Multi-Task Learning__, __Transfer Learning__, and __Domain Adaptation__, <span>when there exist __features__ that are *useful for the different settings or tasks*, corresponding to __underlying factors__ that *appear in more than one setting*</span>{: style="color: purple"}.    
    This applies in two cases:  
    {: #lst-p}
    * __Shared *Input* Semantics__:   
        <button>E.g. Shared Visual Features</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        <div hidden="" markdown="1">
        We may learn about one set of visual categories, such as cats and dogs, in the first setting, then learn about a different set of visual categories, such as ants and wasps, in the second setting.  
        If there is significantly more data in the first setting (sampled from $$P_1$$), then that may help to learn representations that are useful to quickly generalize from only very few examples drawn from $$P_2$$.  
        Many visual categories share low-level notions of edges and visual shapes, the effects of geometric changes, changes in lighting, etc.  
        </div>
        In this case, we __share__ the __*lower* layers__, and have a __task-dependent__ __*upper* layers__.  
        <button>Illustration:</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](https://cdn.mathpix.com/snip/images/717fBbHzwKW71RWB9Lc6gA8gdFmVzymEWs_klK6t-w4.original.fullsize.png){: width="100%" hidden=""}  
    * __Shared *Output* Semantics__:  
        <button>E.g. __Speech Recognition Systems__</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        <div hidden="" markdown="1">
        A speech recognition system needs to produce valid sentences at the output layer, but the earlier layers near the input may need to recognize very different versions of the same phonemes or sub-phonemic vocalizations depending on which person is speaking.  
        </div>
        In cases like these, it makes more sense to __share__ the __*upper* layers__ (near the output) of the neural network, and have a __task-specific__ *__preprocessing__*.  
        <button>Illustration:</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](https://cdn.mathpix.com/snip/images/pFAT7AVYii3xvvOrZvGwn_lcnTZV4EVqCacUebAegYc.original.fullsize.png){: width="100%" hidden=""}
    <br>

6. **K-Shot Learning:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents46}  
    __K-Shot (Few-Shot) Learning__ is a _supervised_ learning setting (problem) where the goal is to learn from an extremely small number $$k$$ of labeled examples (called __shots__).  

    __General Setting:__  
    We first train a model on a __*large* dataset__ $$\mathcal{D}=\left\{\widetilde{\mathbf{x}}_ {i}, \widetilde{\gamma}_ {i}\right\}_ {i=1}^{N}$$ of __inputs__ $$\widetilde{\mathbf{x}}_ {i}$$ and __labels__ $$\widetilde{y}_ {i} \in\{1, \ldots, \widetilde{C}\}$$ that indicate which of the $$\widetilde{C}$$ __classes__ each input belongs to.  
    Then, using knowledge from the model trained on the large dataset, we perform $$\mathrm{k}$$-shot learning with a __*small* dataset__ $$\mathcal{D}=\left\{\mathbf{x}_ {i}, y_ {i}\right\}_ {i=1}^{N}$$ with $$C$$ __*new* classes__, labels $$y_ {i} \in\{\widetilde{C}+1, \widetilde{C}+C\}$$ and __*$$k$$* examples (inputs)__ from each new class.  
    During test time we classify unseen examples (inputs) $$\mathbf{x}^{* }$$ from the new classes $$C$$ and evaluate the predictions against ground truth labels $$y^{* }$$.  


    __Comparison to alternative Learning Paradigms:__{: style="color: red"}  
    ![img](https://cdn.mathpix.com/snip/images/UcWrNybNWHVrfmFVHkf69cc6UiIivyatymeq-e99MSI.original.fullsize.png){: width="60%"}  


    __As Transfer Learning:__{: style="color: red"}  
    Two extreme forms of __transfer learning__ are __One-Shot Learning__ and __Zero-Shot Learning__; they provide only *__one__* and *__zero__* labeled examples of the transfer task, respectively.  


    __One-Shot Learning:__{: style="color: red"}  
    __One-Shot Learning__ _(Fei-Fei et al., 2006)_ is a form of __k-shot learning__ where $$k=1$$.  
    
    It is possible because the representation learns to cleanly separate the underlying classes during the first stage.  
    During the transfer learning stage, only one labeled example is needed to infer the label of many possible test examples that all cluster around the same point in representation space.  
    This works to the extent that the factors of variation corresponding to these invariances have been cleanly separated from the other factors, in the learned representation space, and we have somehow learned which factors do and do not matter when discriminating objects of certain categories.  


    __Zero-Shot Learning:__{: style="color: red"}  
    __Zero-Shot Learning__ _(Palatucci et al., 2009; Socher et al., 2013b)_ or __Zero-data learning__ _(Larochelle et al., 2008)_ is a form of __k-shot learning__ where $$k=0$$.  

    __Example: Zero-Shot Learning Setting__  
    Consider the problem of having a learner read a large collection of text and then solve object recognition problems.  
    It may be possible to recognize a specific object class even without having seen an image of that object, if the text describes the object well enough.  
    For example, having read that a cat has four legs and pointy ears, the learner might be able to guess that an image is a cat, without having seen a cat before.  


    __Justification and Interpretation:__  
    Zero-Shot Learning is only possible because <span>__additional information__ has been exploited during training</span>{: style="color: purple"}.  

    We can think of think of the zero-data learning scenario as including __*three* random variables__:  
    {: #lst-p}
    1. (Traditional) __Inputs__ $$x$$  
    2. (Traditional) __Outputs__ or __Targets__ $$\boldsymbol{y}$$
    3. (Additional) __Random Variable *describing the task*__, $$T$$  

    The model is trained to estimate the conditional distribution $$p(\boldsymbol{y} \vert \boldsymbol{x}, T)$$.  
    {: #lst-p}
    * <button>Example: _updated_ zero-shot learning setting</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        <div hidden="" markdown="1">
        In the example of recognizing cats after having read about cats, the output is a binary variable $$y$$ with $$y=1$$ indicating "yes" and $$y=0$$ indicating "no".  
        The task variable $$T$$ then represents questions to be answered such as "Is there a cat in this image?".  
        If we have a training set containing unsupervised examples of objects that live in the Same space as $$T$$, we may be able to infer the meaning of unseen instances of $$T$$.  
        In our example of recognizing cats without having seen an image of the cat, it is important that we have had unlabeled text data containing sentences such as "cats have four legs" or "cats have pointy ears".  
        </div>

    __Representing the task $$T$$:__  
    Zero-shot learning requires $$T$$ to be represented in a way that <span>allows some sort of __generalization__</span>{: style="color: purple"}.  
    For example, $$T$$ cannot be just a _one-hot code_ indicating an object category.  
    > _Socher et al. (2013 b)_ provide instead a distributed representation of object categories by using a learned word embedding for the word associated with each category.  

    
    __Representation Learning for Zero-Shot Learning:__{: style="color: red"}  
    The principle, underlying __zero-shot learning__ as a form of __transfer learning__: <span>capturing a __representation__ in __*one* modality__</span>{: style="color: purple"}, <span>a __representation__ in __*another* modality__</span>{: style="color: purple"}, and the <span>__relationship__</span>{: style="color: purple"} (in general a __joint distribution__) <span>between pairs $$(\boldsymbol{x}, \boldsymbol{y})$$</span>{: style="color: purple"} consisting of _one observation $$\boldsymbol{x}$$ in one modality_ and _another observation $$\boldsymbol{y}$$ in the other modality_, _(Srivastava and Salakhutdinov, 2012)_.   
    By learning all three sets of parameters (from $$\boldsymbol{x}$$ to its representation, from $$\boldsymbol{y}$$ to its representation, and the relationship between the two representations), concepts in one representation are anchored in the other, and vice-versa, allowing one to meaningfully generalize to new pairs.  

    In particular, <span>Transfer learning between two domains $$x$$ and $$y$$ enables zero-shot learning</span>{: style="color: goldenrod"}.  

    <button>Illustration</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/FhmatT_1_acHctjXuwq9kRjPlpqPCwLyOnMP0aSHzXI.original.fullsize.png){: width="100%" hidden=""}  


    __Zero-Shot Learning in Machine Translation:__  
    <button>Discussion - Example</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    <div hidden="" markdown="1">
    A similar phenomenon happens in machine translation _(Klementiev et al., 2012; Mikolov et al., 2013b; Gouws et al., 2014)_:  
    we have words in one language, and the relationships between words can be learned from unilingual corpora; on the other hand, we have translated sentences which relate words in one language with words in the other. Even though we may not have labeled examples translating word $$A$$ in language $$X$$ to word $$B$$ in language $$Y$$, we can generalize and guess a translation for word $$A$$ because we have learned a distributed representation for words in language $$X$$, a distributed representation for words in language $$Y$$, and created a link (possibly two-way) relating the two spaces, via training examples consisting of matched pairs of sentences in both languages.  
    This transfer will be most successful if all three ingredients (the two representations and the relations between them) are learned jointly.  
    </div>

    __Relation to Multi-modal Learning:__  
    Zero-Shot Learning can be performed using __Multi-model Learning__, and vice-versa.  
    The same principle of __transfer learning__ with __representation learning__ explain how one can perform either tasks.  



    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * [K-Shot Learning (Thesis!)](https://pdfs.semanticscholar.org/8fc6/4f04a94033704453255c905796872e16f284.pdf?_ga=2.177121317.1868310785.1568811982-246730594.1555910960)  
    * [One Shot Learning and Siamese Networks in Keras (Code - Tutorial)](https://sorenbouma.github.io/blog/oneshot/)  
    * __Zero-Shot Learning__: is a form of extending supervised learning to a setting of solving for example a classification problem when not enough labeled examples are available for all classes.   
        > "Zero-shot learning is being able to solve a task despite not having received any training examples of that task." - Goodfellow  
    * __Detecting *Gravitational Waves*__ is a form of __Zero-Shot Learning__   
    * Few-shot, one-shot or zero-shot learning are encompassed in a recently emerging field known as __meta-learning__.  
        While traditionally including mainly classification, recent works in meta-learning have included regression and reinforcement learning ([Vinyals et al., 2016](https://arxiv.org/abs/1606.04080)) ([Andrychowicz et al., 2016](https://arxiv.org/abs/1606.04474)) ([Ravi & Larochelle, 2017](https://openreview.net/forum?id=rJY0-Kcll)) ([Duan et al., 2017](https://arxiv.org/abs/1611.02779)) ([Finn et al., 2017](https://arxiv.org/pdf/1703.03400.pdf)).  
        Works in this area seems to be primarily motivated by the notion of human-level AI, since humans appear to be able to require far fewer training data than most deep learning models.  
    <br>


7. **Multi-Modal Learning:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents47}  
    __Multi-Modal Learning__   


    __Representation Learning for Multi-modal Learning:__{: style="color: red"}  
    The same principle, underlying __zero-shot learning__ as a form of __transfer learning__, explains how one can perform multi-modal learning; capturing a representation in one modality, a representation in the other, and the relationship (in general a joint distribution) between pairs $$(\boldsymbol{x}, \boldsymbol{y})$$ consisting of one observation $$\boldsymbol{x}$$ in one modality and another observation $$\boldsymbol{y}$$ in the other modality _(Srivastava and Salakhutdinov, 2012)_.   
    By learning all three sets of parameters (from $$\boldsymbol{x}$$ to its representation, from $$\boldsymbol{y}$$ to its representation, and the relationship between the two representations), concepts in one representation are anchored in the other, and vice-versa, allowing one to meaningfully generalize to new pairs.  




***

## Causal Factor Learning
{: #content5}


3. **Semi-Supervised Disentangling of Causal Factors:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents53}  
    
    __Quality of Representations:__{: style="color: red"}  
    \- An important question in Representation Learning is:  
    <span>_‚Äúwhat makes one representation better than another?‚Äù_</span>{: style="color: purple"}   
    {: #lst-p}
    1. One answer to that is the __Causal Factors Hypothesis:__  
        An __ideal representation__ is one in which <span>the __features__ within the representation _correspond_ to the underlying __causes__ of the observed data</span>{: style="color: purple"}, with __*separate* features__ or __directions__ in _feature space_ corresponding to __*different* causes__, so that <span>the __representation__ *__disentangles__* the __causes__ from one another</span>{: style="color: purple"}.  
        * This hypothesis *motivates* approaches in which we first <span>seek a __*good* representation__ for $$p(\boldsymbol{x})$$</span>{: style="color: purple"}.  
            This representation may also be a *good* representation for __computing $$p(\boldsymbol{y} \vert \boldsymbol{x})$$__ if $$\boldsymbol{y}$$ is among the __most *salient*__ __causes__ of $$\boldsymbol{x}$$[^3] [^4].  
    2. __Ease of Modeling:__  
        In many approaches to representation learning, we are often concerned with a representation that is __easy to model__ (e.g. sparse entries, independent entries etc.).  
        It is not directly observed, however, that <span>a representation that __cleanly separates the underlying causal factors__ is, also, one that is __easy to model__</span>{: style="color: purple"}.  
        The answer to that is an __*extension*__ of the __Causal Factor Hypothesis:__  
        For *__many__* __AI tasks__ the two properties __coincide__: once we are able to <span>obtain the underlying explanations for the observations</span>{: style="color: purple"}, it generally becomes <span>easy to isolate individual attributes</span>{: style="color: purple"} from the others.  
        Specifically, __if__ a <span>__representation $$\boldsymbol{h}$$__ _represents_ many of the *__underlying causes__* of the __observed $$\boldsymbol{x}$$__</span>{: style="color: purple"}, __and__ the <span>__outputs $$\boldsymbol{y}$$__ are among the __most *salient causes*__</span>{: style="color: purple"}, __then__ it is <span>easy to __predict__ $$\boldsymbol{y}$$ from $$\boldsymbol{h}$$</span>{: style="color: purple"}.  

    
    <div class="borderexample" markdown="1">
    <span> The complete __Causal Factors Hypothesis__ <span>*motivates* __Semi-Supervised Learning__ via __Unsupervised Representation Learning__</span>{: style="color: goldenrod"}.</span>
    </div>


    __Analysis - When does Semi-Supervised Learning Work:__{: style="color: red"}  
    <button>Full Analysis</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    <div hidden="" markdown="1">
    * __When does Semi-Supervised Disentangling of Causal Factors *Work*?__  
        Let's start by considering two scenarios where Semi-Supervised Learning via Unsupervised Representation Learning Fails and Succeeds:  
        * <button>Failure</button>{: .showText value="show" onclick="showTextPopHide(event);"}
            <div hidden="" markdown="1">
            Let us see how semi-supervised learning can fail because <span>__unsupervised learning__ of $$p(\mathbf{x})$$ is of __*no help* to learn__ $$p(\mathbf{y} \vert \mathbf{x})$$</span>{: style="color: purple"}.  
            Consider the case where $$p(\mathbf{x})$$ is __uniformly distributed__ and we want to learn $$f(\boldsymbol{x})=\mathbb{E}[\mathbf{y} \vert \boldsymbol{x}]$$.  
            Clearly, __observing a training set__ of $$\boldsymbol{x}$$ values *__alone__* gives us __no information__ about $$p(\mathbf{y} \vert \mathbf{x})$$.  
            </div>
        * <button>Success</button>{: .showText value="show" onclick="showTextPopHide(event);"}
            <div hidden="" markdown="1">
            Consider the case where $$x$$ arises from a mixture, with one mixture component per value of $$y$$.  
            If the mixture components are __well-separated__, then modeling $$p(x)$$ reveals precisely where each component is, and <span>a __single labeled example__ of *each class* will then be *__enough__* to perfectly __learn__ $$p(\mathbf{y} \vert \mathbf{x})$$</span>{: style="color: purple"}.    
            <button>Illustration: Well-Separated Mixture</button>{: .showText value="show" onclick="showTextPopHide(event);"}
            ![img](https://cdn.mathpix.com/snip/images/ctuu5i3mZUhHsIdzpRMp4SJXHz_iWb_5_YnCfWXuCZg.original.fullsize.png){: width="100%" hidden=""}  
            </div>

        <div class="borderexample" markdown="1">
        <span>Thus, we conclude that semi-supervised learning works <span>when $$p(\mathbf{y} \vert \mathbf{x})$$ and $$p(\mathbf{x})$$ are __tied__ together</span>{: style="color: goldenrod"}.</span>  
        </div>
    * __When are $$p(\mathbf{y} \vert \mathbf{x})$$ and $$p(\mathbf{x})$$ tied?__  
        This happens when $$\mathbf{y}$$ is closely associated with one of the causal factors of $$\mathbf{x}$$, then $$p(\mathbf{x})$$ and $$p(\mathbf{y} \vert \mathbf{x})$$ will be strongly tied.  
        * Thus, unsupervised representation learning that tries to disentangle the underlying factors of variation is likely to be useful as a semi-supervised learning strategy.  

    Now, Consider the assumption that $$\mathbf{y}$$ is one of the causal factors of $$\mathbf{x}$$, and let $$\mathbf{h}$$ represent all those factors:  
    {: #lst-p}
    * The __"true" generative process__ can be conceived as <span>*__structured__* according to this __directed graphical model__</span>{: style="color: purple"}, with $$\mathbf{h}$$ as the __parent__ of $$\mathbf{x}$$:  
        <p>$$p(\mathbf{h}, \mathbf{x})=p(\mathbf{x} \vert \mathbf{h}) p(\mathbf{h})$$</p>  
        * As a consequence, the __data__ has __marginal probability__:  
            <p>$$p(\boldsymbol{x})=\mathbb{E}_ {\mathbf{h}} p(\boldsymbol{x} \vert \boldsymbol{h})$$</p>  

        From this straightforward observation, we __conclude__ that:  
        <div class="borderexample" markdown="1">
        <span>The <span>__best possible model__ of $$\mathbf{x}$$ (wrt. __generalization__) is the one that *__uncovers__* the above __"true" structure__, with $$\boldsymbol{h}$$ as a __latent variable__ that *__explains__* the __observed variations__ in $$\boldsymbol{x}$$</span>{: style="color: goldenrod"}.</span>  
        </div>  
        I.E. The __"ideal" representation learning__ discussed above should thus __recover these latent factors__.  
        If $$\mathbf{y}$$ is one of these (or closely related to one of them), then it will be very easy to learn to predict $$\mathbf{y}$$ from such a representation.  
    * We also see that the __conditional distribution__ of $$\mathbf{y}$$ given $$\mathbf{x}$$ is <span>tied by *Bayes' rule* to the __components in the above equation__</span>{: style="color: purple"}:  
        <p>$$p(\mathbf{y} \vert \mathbf{x})=\frac{p(\mathbf{x} \vert \mathbf{y}) p(\mathbf{y})}{p(\mathbf{x})}$$</p>  

        <div class="borderexample" markdown="1">
        <span>Thus the <span>__marginal__ $$p(\mathbf{x})$$ is *__intimately tied__* to the __conditional__ $$p(\mathbf{y} \vert \mathbf{x})$$, and knowledge of the structure of the former should be helpful to learn the latter</span>{: style="color: purple"}.</span>  
        </div>
    <br>
    <div class="borderexample" markdown="1">
    <span>Therefore, __in situations respecting these assumptions, semi-supervised learning should improve performance__.</span>  
    </div>  
    </div>

    __Justifying the setting where Semi-Supervised Learning Works:__  
    {: #lst-p}
    * __Semi-Supervised Learning[^5] *Works*__ when: <span>$$p(\mathbf{y} \vert \mathbf{x})$$ and $$p(\mathbf{x})$$ are *__tied together__*</span>{: style="color: goldenrod"}.  
    * __$$p(\mathbf{y} \vert \mathbf{x})$$ and $$p(\mathbf{x})$$ are *Tied*__ when: $$\mathbf{y}$$ is *__closely associated__* with one of the __causal factors__ of $$\mathbf{x}$$, or it is a __causal factor itself__.  
        * Let $$\mathbf{h}$$ represent all the __causal factors__ of $$\mathbf{x}$$, and let $$\mathbf{y} \in \mathbf{h}$$ (be a __causal factor__ of $$\mathbf{x}$$), then:  
            The __"true" generative process__ can be conceived as <span>*__structured__* according to this __directed graphical model__</span>{: style="color: purple"}, with $$\mathbf{h}$$ as the __parent__ of $$\mathbf{x}$$:  
            <p>$$p(\mathbf{h}, \mathbf{x})=p(\mathbf{x} \vert \mathbf{h}) p(\mathbf{h})$$</p>  
            * __Thus__, the __Marginal Probability of the Data $$p(\mathbf{x})$$__ is:  
                1. <span>*__Tied__* to the __conditional__ $$p(\mathbf{x} \vert \mathbf{h})$$</span>{: style="color: purple"} as:  
                    <p>$$p(\boldsymbol{x})=\mathbb{E}_ {\mathbf{h}} p(\boldsymbol{x} \vert \boldsymbol{h})$$</p>  
                    $$\implies$$  
                    * The <span>__best possible model__ of $$\mathbf{x}$$ (wrt. __generalization__)</span>{: style="color: goldenrod"} is the one that <span>*__uncovers__* the above __"true" structure__</span>{: style="color: goldenrod"}, with <span>$$\boldsymbol{h}$$ as a __latent variable__ that *__explains__* the __observed variations__ in $$\boldsymbol{x}$$</span>{: style="color: goldenrod"}.  
                        I.E. The __‚Äúideal‚Äù representation learning__ discussed above __should__ thus __*recover*__ these __latent factors__.  
                2. <span>*__(intimately) Tied__* to the __conditional__ $$p(\mathbf{y} \vert \mathbf{x})$$</span>{: style="color: purple"} (by __Bayes' rule__) as:  
                    <p>$$p(\mathbf{y} \vert \mathbf{x})=\frac{p(\mathbf{x} \vert \mathbf{y}) p(\mathbf{y})}{p(\mathbf{x})}$$</p>  

    <div class="borderexample" markdown="1">
    <span>Therefore, __in situations respecting these assumptions, semi-supervised learning should improve performance__.</span>  
    </div>


    __Encoding/Learning Causal Factors:__{: style="color: red"}  
    {: #lst-p}
    * __Problem - Number of Causal Factors:__{: style="color: DarkRed"}  
        An important research problem regards the fact that <span>most observations are formed by an _extremely_ __*large number* of underlying causes__</span>{: style="color: purple"}.  
        * Suppose $$\mathbf{y}=\mathrm{h}_ {i}$$, but the unsupervised learner does not know which $$\mathrm{h}_ {i}$$:  
            * The __brute force solution__ is for an unsupervised learner to <span>learn a representation that __captures *all* the reasonably salient generative factors__ $$\mathrm{h}_ {j}$$ and disentangles them from each other</span>{: style="color: purple"}, thus making it easy to predict $$\mathbf{y}$$ from $$\mathbf{h}$$, regardless of which $$\mathrm{h}_ {i}$$ is associated with $$\mathbf{y}$$.  
                * In practice, the brute force solution is __not feasible__ because it is _not possible to capture all or most of the factors of variation that influence an observation_.  
                    For example, in a visual scene, should the representation always encode all of the smallest objects in the background?  
                    It is a well-documented psychological phenomenon that human beings fail to perceive changes in their environment that are not immediately relevant to the task they are performing _Simons and Levin (1998)_.  
    * __Solution - Determining which causal factor to encode/learn:__{: style="color: DarkRed"}   
        An important research frontier in semi-supervised learning is determining <span>_"what to encode in each situation"_</span>{: style="color: purple"}.  
        * Currently, there are __two main strategies__ for _dealing with a large number of underlying causes_:  
            1. Use a __supervised learning signal__ at the same time as the __(*"plus"*) unsupervised learning signal__,  
                so that the model will choose to capture the most relevant factors of variation. 
            2. Use __much larger representations__ if using *purely unsupervised learning*.  
        * __New (Emerging) Strategy__ for __unsupervised learning__:  
            <span>Redefining the *definition* of "__salient__" factors</span>{: style="color: goldenrod"}.  

    

    __The definition of "*Salient*":__{: style="color: red"}  
    {: #lst-p}
    * The _current_ __definition of *"salient"* factors:__  
        In practice, we _encode_ the definition of _"salient"_ by using the __objective criterion__ (e.g. MSE).  
        > Historically, autoencoders and generative models have been trained to optimize a fixed criterion, often similar to MSE.  

        * __Problem with current definition__:  
            Since these fixed criteria determine which causes are considered salient, they will be emphasizing different factors depending on their e.g. effects on the error:    
            * E.g. MSE applied to the pixels of an image implicitly specifies that an underlying cause is only salient if it significantly changes the brightness of a large number of pixels.  
                This can be problematic if the task we wish to solve involves interacting with small objects.  
                <button>Illustration: AutoEncoder w/ MSE</button>{: .showText value="show" onclick="showTextPopHide(event);"}
                ![img](https://cdn.mathpix.com/snip/images/mEJX8pk5A1QENzLNvH4WLOzEzWaH61y-wpq6iIraGA0.original.fullsize.png){: width="100%" hidden=""}  
    * __Learned (pattern-based) "Saliency"__:  
        Certain factors could be considered *"salient"* if they follow a __*highly recognizable* pattern__.  
        E.g. if a group of pixels follow a highly recognizable pattern, even if that pattern does not involve extreme brightness or darkness, then that pattern could be considered extremely salient.  

        * This definition is _implemented_ by __Generative Adversarial Networks (GANs)__.  
            In this approach, a generative model is trained to fool a feedforward classifier. The feedforward classifier attempts to recognize all samples from the generative model as being fake, and all samples from the training set as being real.  
            In this framework, <span>any *__structured pattern__* that the feedforward network can *recognize* is __highly salient__</span>{: style="color: purple"}.  
            They <span>*__learn__* how to determine what is salient</span>{: style="color: goldenrod"}.  
            <button>Example: Advantages of Adversarial Framework in Learning Ears</button>{: .showText value="show" onclick="showTextPopHide(event);"}
            <div hidden="" markdown="1">
            _Lotter et al. (2015)_ showed that models trained to generate images of human heads will often neglect to generate the ears when trained with mean squared error, but will successfully generate the ears when trained with the adversarial framework.  
            Because the ears are not extremely bright or dark compared to the surrounding skin, they are not especially salient according to mean squared error loss, but their highly recognizable shape and consistent position means that a feedforward network can easily learn to detect them, making them highly salient under the generative adversarial framework.    
            <button>Illustration: Generating Faces w/ Ears</button>{: .showText value="show" onclick="showTextPopHide(event);"}
            ![img](https://cdn.mathpix.com/snip/images/f23z_MZr459udxGiWj0dqEV7ai-QcU9ru7a67utkDJ8.original.fullsize.png){: width="100%" hidden=""}  
            </div>
    
    Generative adversarial networks are only one step toward __determining *which factors should be represented*__.  
    We expect that __future research__{: style="color: goldenrod"} will discover <span>better ways of determining __which factors to represent__</span>{: style="color: purple"}, and develop <span>mechanisms for __*representing* different factors__ *depending on the task*</span>{: style="color: purple"}.  

    __Robustness to Change - Causal Invariance:__{: style="color: red"}  
    A __benefit__ of learning the underlying causal factors _(Sch√∂lkopf et al. (2012))_ is that:  
    if the __true generative process__ has <span>$$\mathbf{x}$$ as an *__effect__*</span>{: style="color: purple"} and <span>$$\mathbf{y}$$ as a *__cause__*</span>{: style="color: purple"}, then __modeling $$p(\mathbf{x} \vert \mathbf{y})$$__ is <span>robust to changes in $$p(\mathbf{y})$$</span>{: style="color: goldenrod"}.  
    > If the cause-effect relationship was reversed, this would not be true, since by Bayes' rule, $$p(\mathbf{x} \vert \mathbf{y})$$ would be sensitive to changes in $$p(\mathbf{y})$$.  

    Very often, when we consider changes in distribution due to __different domains__, __temporal non-stationarity__, or __changes in the nature of the task__, __*the causal mechanisms remain invariant*__{: style="color: goldenrod"} (the laws of the universe are constant) while the __marginal distribution over the underlying causes__ can *__change__*.  
    Hence, better __generalization and robustness__ to all kinds of changes can be expected via __learning a generative model that attempts to recover the causal factors__{: style="color: goldenrod"} $$\mathbf{h}$$ and $$p(\mathbf{x} \vert \mathbf{h})$$.  
    <br>

4. **Providing Clues to Discover Underlying Causes:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents54}  
    __Quality of Representations:__{: style="color: red"}  
    The answer to the following question:  
    <span>_‚Äúwhat makes one representation better than another?‚Äù_</span>{: style="color: purple"}   
    was the __Causal Factors Hypothesis:__  
    An __ideal representation__ is one in which <span>the __features__ within the representation _correspond_ to the underlying __causes__ of the observed data</span>{: style="color: purple"}, with __*separate* features__ or __directions__ in _feature space_ corresponding to __*different* causes__, so that <span>the __representation__ *__disentangles__* the __causes__ from one another</span>{: style="color: purple"}, especially those factors that are relevant to our applications.  

    __Clues for Finding the Causal Factors of Variation:__{: style="color: red"}  
    Most strategies for representation learning are based on:  
    <span>Introducing clues that help the learning to find these underlying factors of variations</span>{: style="color: purple"}.  
    The clues can help the learner separate these observed factors from the others.  
    
    __Supervised learning__ provides a *__very strong__* __clue__: a __label__ $$\boldsymbol{y},$$ presented with each $$\boldsymbol{x},$$ that usually specifies the value of at least one of the factors of variation directly.  

    More generally, to make use of abundant unlabeled data, representation learning makes use of other, less direct, hints about the underlying factors.  
    These hints take the form of __implicit prior beliefs__ that we, the designers of the learning algorithm, impose in order to <span>guide the learner</span>{: style="color: purple"}.  

    __Clues in the form of Regularization:__{: style="color: red"}  
    Results such as the __no free lunch theorem__ show that <span>__regularization__ strategies are necessary to obtain __good generalization__</span>{: style="color: purple"}.  
    While it is impossible to find a universally superior regularization strategy, one goal of deep learning is to find a __set of fairly generic regularization strategies__ that are *applicable to a wide variety of AI tasks*, similar to the tasks that people and animals are able to solve.  

    We can <span>use _generic_ __regularization strategies__ to _encourage_ learning algorithms to discover __features__ that *__correspond__* to __underlying factors__</span>{: style="color: goldenrod"}, E.G. _(Bengio et al. (2013d))_:  
    {: #lst-p}
    * __Smoothness__: This is the assumption that $$f(\boldsymbol{x}+\epsilon \boldsymbol{d}) \approx f(\boldsymbol{x})$$ for unit $$\boldsymbol{d}$$ and small $$\epsilon$$. This assumption allows the learner to generalize from training examples to nearby points in input space. Many machine learning algorithms leverage this idea, but it is insufficient to overcome the curse of dimensionality.  
    * __Linearity__: Many learning algorithms assume that relationships between some variables are linear. This allows the algorithm to make predictions even very far from the observed data, but can sometimes lead to overly extreme predictions. Most simple machine learning algorithms that do not make the smoothness assumption instead make the linearity assumption. These are in fact different assumptions‚Äî<span>linear functions with large weights applied to high-dimensional spaces may not be very smooth</span>{: style="color: goldenrod"}[^7].
    * __Multiple explanatory factors__: Many representation learning algorithms are motivated by the assumption that the data is generated by multiple underlying explanatory factors, and that most tasks can be solved easily given the state of each of these factors. Section 15.3 describes how this view motivates semisupervised learning via representation learning. Learning the structure of $$p(\boldsymbol{x})$$ requires learning some of the same features that are useful for modeling $$p(\boldsymbol{y} \vert \boldsymbol{x})$$ because both refer to the same underlying explanatory factors. Section 15.4 describes how this view motivates the use of distributed representations, with separate directions in representation space corresponding to separate factors of variation. 
    * __Causal factors__: the model is constructed in such a way that it treats the factors of variation described by the learned representation $$\boldsymbol{h}$$ as the causes of the observed data $$\boldsymbol{x}$$, and not vice-versa. As discussed in section 15.3, this is advantageous for semi-supervised learning and makes the learned model more robust when the distribution over the underlying causes changes or when we use the model for a new task. 
    * __Depth, or a hierarchical organization of explanatory factors__: High-level, abstract concepts can be defined in terms of simple concepts, forming a hierarchy. From another point of view, the use of a deep architecture expresses our belief that the task should be accomplished via a multi-step program, with each step referring back to the output of the processing accomplished via previous steps.  
    * __Shared factors across tasks__: In the context where we have many tasks, corresponding to different $$y_{i}$$ variables sharing the same input $$\mathbf{x}$$ or where each task is associated with a subset or a function $$f^{(i)}(\mathbf{x})$$ of a global input $$\mathbf{x},$$ the assumption is that each $$\mathbf{y}_ {i}$$ is associated with a different subset from a common pool of relevant factors $$\mathbf{h}$$. Because these subsets overlap, learning all the $$P\left(y_{i} \vert \mathbf{x}\right)$$ via a shared intermediate representation $$P(\mathbf{h} \vert \mathbf{x})$$ allows sharing of statistical strength between the tasks.  
    * __Manifolds__: Probability mass concentrates, and the regions in which it concentrates are locally connected and occupy a tiny volume. In the continuous case, these regions can be approximated by low-dimensional manifolds with a much smaller dimensionality than the original space where the data lives. Many machine learning algorithms behave sensibly only on this manifold (Goodfellow et al., 2014b). Some machine learning algorithms, especially autoencoders, attempt to explicitly learn the structure of the manifold. 
    * __Natural clustering__: Many machine learning algorithms assume that each connected manifold in the input space may be assigned to a single class. The data may lie on many disconnected manifolds, but the class remains constant within each one of these. This assumption motivates a variety of learning algorithms, including tangent propagation, double backprop, the manifold tangent classifier and adversarial training. 
    * __Temporal and spatial coherence__: Slow feature analysis and related algorithms make the assumption that the most important explanatory factors change slowly over time, or at least that it is easier to predict the true underlying explanatory factors than to predict raw observations such as pixel values. See section 13.3 for further description of this approach. 
    * __Sparsity__: Most features should presumably not be relevant to describing most inputs‚Äîthere is no need to use a feature that detects elephant trunks when representing an image of a cat. It is therefore reasonable to impose a prior that any feature that can be interpreted as ‚Äúpresent‚Äù or ‚Äúabsent‚Äù should be absent most of the time. 
    * __Simplicity of Factor Dependencies__: In good high-level representations, the factors are related to each other through simple dependencies. The simplest possible is marginal independence, $$P(\mathbf{h})=\prod_{i} P\left(\mathbf{h}_ {i}\right)$$, but linear dependencies or those captured by a shallow autoencoder are also reasonable assumptions. This can be seen in many laws of physics, and is assumed when plugging a linear predictor or a factorized prior on top of a learned representation.  
        * <span>_Consciousness Prior_</span>{: style="color: goldenrod"}:    
            * <button>Discussion</button>{: .showText value="show" onclick="showTextPopHide(event);"}
                * __Key Ideas:__  
                    (1) Seek Objective Functions defined purely in abstract space (no decoders)   
                    (2) _"Conscious"_ thoughts are *__low-dimensional__*.  
                    * Conscious thoughts are very *__low-dimensional__* objects compared to the full state of the (unconscious) brain  
                    * Yet they have unexpected predictive value or usefulness  
                        $$\rightarrow$$ strong constraint or prior on the underlying representation  
                        > e.g. we can plan our lives by only thinking of simple/short sentences at a time, that can be expressed with few variables (words); short-term memory is only 7 words (underutilization? no, rather, __prior__).  

                        * __Thought__: composition of few selected factors / concepts (key/value) at the highest level of abstraction of our brain.  
                        * Richer than but closely associated with short verbal expression such as a __sentence__ or phrase, a __rule__ or __fact__ (link to classical symbolic Al & knowledge representation)  
                    * Thus, <span>*__true__* __statements__ about the __very complex__ world, could be conveyed with very *__low-dimensional__* representations</span>{: style="color: goldenrod"}.  
                * __How to select a few *relevant* abstract concepts making a thought__:  
                    <span>Content-based __Attention__</span>{: style="color: goldenrod"}.  
                    * Thus, <span>__Abstraction__ is *related to* __Attention__:</span>{: style="color: purple"}  
                        <button>Relation between __Abstraction__ and __Attention__</button>{: .showText value="show" onclick="showTextPopHide(event);"}
                        ![img](https://cdn.mathpix.com/snip/images/f-kwoXKJhtC3oGzLt8HQfdcl3-x8T9-aULdanqZaqnI.original.fullsize.png){: width="100%" hidden=""}  
                * __Two Levels of Representations__:  
                    * High-dimensional abstract representation space (all known concepts and factors) $$h$$  
                    * Low-dimensional conscious thought $$c,$$ extracted from $$h$$  
                        * $$c$$ includes names (keys) and values of factors  
                    ![img](https://cdn.mathpix.com/snip/images/TwwbkSLzeI_DT6oFP1Y1DWkIQXjQOsHGs9bcT3f4cfE.original.fullsize.png){: width="50%"}  
                    * The __Goal__ of <span>using attention on the unconscious states</span>{: style="color: purple"}:  
                        is to put pressure (constraint) on the _mapping between input and representations (Encoder)_ and the _unconscious states representations $$h$$_ such that the Encoder is encouraged to learn representations that have the property that <span>that if I pick just a few elements of it, I can make a true statement or very highly probable statement about the world, (e.g. a highly probable prediction)</span>{: style="color: purple"}.  
                {: hidden=""}

    * __Causal/Mechanism Independence__:  
        * <span>_Controllable Factors_</span>{: style="color: purple"}.  

    
    The concept of representation learning ties together all of the many forms of deep learning.  
    Feedforward and recurrent networks, autoencoders and deep probabilistic models all learn and exploit representations. Learning the best possible representation remains an exciting avenue of research.  
    <br>





[^1]: It proceeds one layer at a time, training the k -th layer while keeping the previous ones fixed. In particular, the lower layers (which are trained first) are not adapted after the upper layers are introduced.  
[^2]: Commonly, ‚Äúpretraining‚Äù to refer not only to the pretraining stage itself but to the entire two phase protocol that combines the pretraining phase and a supervised learning phase.  
[^3]: This idea has guided a large amount of deep learning research since at least the 1990s _(Becker and Hinton, 1992; Hinton and Sejnowski, 1999)_, in more detail.  
[^4]: For other arguments about when <span>__semi-supervised__ learning can outperform pure __supervised__ learning</span>{: style="color: purple"}, we refer the reader to _section 1.2_ of _Chapelle et al. (2006)_.
[^5]: Using __unsupervised representation learning__ that tries to *__disentangle__* the __underlying factors of variation__.  
[^6]: It is also called a one-hot representation, since it can be captured by a binary vector with $$n$$ bits that are mutually exclusive (only one of them can be active).  
[^7]: See _Goodfellow et al. (2014b)_ for a further discussion of the limitations of the linearity assumption.  

***
***

TITLE: Manifold Learning
LINK: research/dl/theory/manifold_learning.md



* [A BIT ABOUT MANIFOLDS (Blog!)](http://mcneela.github.io/math/2019/01/30/A-Bit-About-Manifolds.html)  


## FIRST
{: #content1}

1. **The Manifold Hypothesis:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    The __Manifold Hypothesis__ states that real-world high dimensional data (such as images) lie on low-dimensional manifolds embedded in the high-dimensional space.  

    Intuitively, the existence of a low-dimensional representation makes sense, because if we had to learn the entirety of the space of input-images $$256\times256\times3$$ in a _capacity limited_ NN, we will fail; but in practice we very easily can.  
    Basically, learning an arbitrary $$256\times256\times3$$ function would be intractable.  

    * <button>Example: Lines in $$\mathbb{R}^2$$</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/dl/theory/manifold_learning/1.png){: hidden=""}  

    * Any shape 









  * __Theoretical Motivation:__ A lot of the natural transformations you might want to perform on an image, like translating or scaling an object in it, or changing the lighting, would form continuous curves in image space if you performed them continuously.  
  * __Experimental Motivation:__ Carlsson et al. found that local patches of images form a klein bottle. [On the Local Behavior of Spaces of Natural Images](http://math.uchicago.edu/~shmuel/AAT-readings/Data%20Analysis%20/mumford-carlsson%20et%20al.pdf)

***

## SECOND
{: #content2}




***
***

TITLE: Probability Theory <br /> Mathematics of Deep Learning
LINK: research/dl/theory/probability.md


* [COUNT BAYESIE: PROBABLY A PROBABILITY BLOG](https://www.countbayesie.com)  
[Review of Probability Theory (Stanford)](http://cs229.stanford.edu/section/cs229-prob.pdf)  
[A First Course in Probability (Book: _Sheldon Ross_)](http://julio.staff.ipb.ac.id/files/2015/02/Ross_8th_ed_English.pdf)  
[Statistics 110: Harvard](https://www.youtube.com/playlist?list=PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo)  
[Lecture Series on Probability (following DL-book)](https://www.youtube.com/playlist?list=PLR6O_WZHBlOELxOrXlzB1LCXd2cUIXkkm)  
[Probability Quora FAQs](https://www.quora.com/What-is-the-probability-statistics-topic-FAQ)  
[Math review for Stat 110](https://projects.iq.harvard.edu/files/stat110/files/math_review_handout.pdf)  
[Deep Learning Probability](https://jhui.github.io/2017/01/05/Deep-learning-probability-and-distribution/)  
[Probability as Extended Logic](http://bjlkeng.github.io/posts/probability-the-logic-of-science/)  
[CS188 Probability Lecture (very intuitive)](https://www.youtube.com/watch?v=sMNbLXsvRig&list=PL7k0r4t5c108AZRwfW-FhnkZ0sCKBChLH&index=13&t=0s)  


## Motivation
{: #content1}

1. **Uncertainty in General Systems and the need for a Probabilistic Framework:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    1. __Inherent stochasticity in the system being modeled:__  
        Take Quantum Mechanics, most interpretations of quantum mechanics describe the dynamics of sub-atomic particles as being probabilistic.  
    2. __Incomplete observability__:  
        Deterministic systems can appear stochastic when we cannot observe all the variables that drive the behavior of the system.  
        > i.e. Point-of-View determinism (Monty-Hall)  
    3. __Incomplete modeling__:  
        Building a system that makes strong assumptions about the problem and discards (observed) information result in uncertainty in the predictions.    
    <br />

2. **Bayesian Probabilities and Frequentist Probabilities:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    __Frequentist Probabilities__ describe the predicted number of times that a __repeatable__ process will result in a given output in an absolute scale.  

    __Bayesian Probabilities__ describe the _degree of belief_ that a certain __non-repeatable__ event is going to result in a given output, in an absolute scale.      
    
    We assume that __Bayesian Probabilities__ behaves in exactly the same way as __Frequentist Probabilities__.  
    This assumption is derived from a set of _"common sense"_ arguments that end in the logical conclusion that both approaches to probabilities must behave the same way - [Truth and probability (Ramsey 1926)](https://socialsciences.mcmaster.ca/econ/ugcm/3ll3/ramseyfp/ramsess.pdf).

3. **Probability as an extension of Logic:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    "Probability can be seen as the extension of logic to deal with uncertainty. Logic provides a set of formal rules for determining what propositions are implied to be true or false given the assumption that some other set of propositions is true or false. Probability theory provides a set of formal rules for determining the likelihood of a proposition being true given the likelihood of other propositions." - deeplearningbook p.54


***

## Basics
{: #content2}

0. **Elements of Probability:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents20}  
    * __Sample Space $$\Omega$$__: The set of all the outcomes of a stochastic experiment; where each _outcome_ is a complete description of the state of the real world at the end of the experiment.  
    * __Event Space $${\mathcal {F}}$$__: A set of _events_; where each event $$A \in \mathcal{F}$$ is a subset of the sample space $$\Omega$$ - it is a collection of possible outcomes of an experiment.  
    * __Probability Measure $$\operatorname {P}$$__: A function $$\operatorname {P}: \mathcal{F} \rightarrow \mathbb{R}$$ that satisfies the following properties:  
        * $$\operatorname {P}(A) \geq 0, \: \forall A \in \mathcal{f}$$, 
        * $$\operatorname {P}(\Omega) = 1$$, $$\operatorname {P}(\emptyset) = 0$$[^1]  
        * $${\displaystyle \operatorname {P}(\bigcup_i A_i) = \sum_i \operatorname {P}(A_i) }$$, where $$A_1, A_2, ...$$ are [_disjoint_ events](#bodyContents102)  

    __Properties:__{: style="color: red"}  
    * $${\text { If } A \subseteq B \Longrightarrow P(A) \leq P(B)}$$,   
    * $${P(A \cap B) \leq \min (P(A), P(B))} $$,  
    * __Union Bound:__ $${P(A \cup B) \leq P(A)+P(B)}$$  
    * $${P(\Omega \backslash A)=1-P(A)}$$.  
    * __Law of Total Probability (LOTB):__ $$\text { If } A_{1}, \ldots, A_{k} \text { are a set of disjoint events such that } \cup_{i=1}^{k} A_{i}=\Omega, \text { then } \sum_{i=1}^{k} P\left(A_{k}\right)=1$$  
    * __Inclusion-Exclusion Principle__:  
        <p>$$\mathbb{P}\left(\bigcup_{i=1}^{n} A_{i}\right)=\sum_{i=1}^{n} \mathbb{P}\left(A_{i}\right)-\sum_{i< j} \mathbb{P}\left(A_{i} \cap A_{j}\right)+\sum_{i< j < k} \mathbb{P}\left(A_{i} \cap A_{j} \cap A_{k}\right)-\cdots+(-1)^{n-1} \sum_{i< \ldots< n} \mathbb{P}\left(\bigcap_{i=1}^{n} A_{i}\right)$$</p>  
        * [**Example 110**](https://www.youtube.com/embed/LZ5Wergp_PA?start=2057){: value="show" onclick="iframePopA(event)"}
        <a href="https://www.youtube.com/embed/LZ5Wergp_PA?start=2057"></a>
            <div markdown="1"> </div>    
              

    * [**Properties and Proofs 110**](https://www.youtube.com/embed/LZ5Wergp_PA?start=1359){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/LZ5Wergp_PA?start=1359"></a>
        <div markdown="1"> </div>    

[^1]: Corresponds to "wanting" the probability of events that are __certain__ to have p=1 and events that are __impossible__ to have p=0  
                

1. **Random Variables:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    A __Random Variable__ is a variable that can take on different values randomly.  
    Formally, a random variable $$X$$ is a _function_ that maps outcomes to numerical quantities (labels), typically real numbers:
    <p>$${\displaystyle X\colon \Omega \to \mathbb{R}}$$</p>  

    Think of a R.V.: as a numerical "summary" of an aspect of the experiment.  

    __Types__:
    * *__Discrete__*: is a variable that has a finite or countably infinite number of states  
    * *__Continuous__*: is a variable that is a real value  

    __Examples:__  
    * __Bernoulli:__ A r.v. $$X$$ is said to have a __Bernoulli__ distribution if $$X$$ has only $$2$$ possible values, $$0$$ and $$1$$, and $$P(X=1) = p, P(X=0) = 1-p$$; denoted $$\text{Bern}(p)$$.    
    * __Binomial__: The distr. of #successes in $$n$$ independent __$$\text{Bern}(p)$$__ trials and its distribution is $$P(X=k) = \left(\begin{array}{l}{n} \\ {k}\end{array}\right) p^k (1-p)^{n-k}$$; denoted $$\text{Bin}(n, p)$$.          
    <br>

2. **Probability Distributions:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    A __Probability Distribution__ is a function that describes the likelihood that a random variable (or a set of r.v.) will take on each of its possible states.  
    Probability Distributions are defined in terms of the __Sample Space__.  
    * __Classes__:  
        * *__Discrete Probability Distribution:__* is encoded by a discrete list of the probabilities of the outcomes, known as a __Probability Mass Function (PMF)__.  
        * *__Continuous Probability Distribution:__* is described by a __Probability Density Function (PDF)__.  
    * __Types__:  
        * *__Univariate Distributions:__* are those whose sample space is $$\mathbb{R}$$.  
        They give the probabilities of a single random variable taking on various alternative values 
        * *__Multivariate Distributions__* (also known as *__Joint Probability distributions__*):  are those whose sample space is a vector space.   
        They give the probabilities of a random vector taking on various combinations of values.  


    A __Cumulative Distribution Function (CDF)__: is a general functional form to describe a probability distribution:  
    <p>$${\displaystyle F(x)=\operatorname {P} [X\leq x]\qquad {\text{ for all }}x\in \mathbb {R} .}$$</p>  
    > Because a probability distribution P on the real line is determined by the probability of a scalar random variable X being in a half-open interval $$(‚àí\infty, x]$$, the probability distribution is completely characterized by its cumulative distribution function (i.e. one can calculate the probability of any event in the event space)  
    
 

3. **Probability Mass Function:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    A __Probability Mass Function (PMF)__ is a function (probability distribution) that gives the probability that a discrete random variable is exactly equal to some value.  
    __Mathematical Definition__:  
    Suppose that $$X: S \rightarrow A, \:\:\: (A {\displaystyle \subseteq }  \mathbb{R})$$ is a discrete random variable defined on a sample space $$S$$. Then the probability mass function $$f_X: A \rightarrow [0, 1]$$ for $$X$$ is defined as:   
    <p>$$p_{X}(x)=P(X=x)=P(\{s\in S:X(s)=x\})$$</p>  
    The __total probability for all hypothetical outcomes $$x$$ is always conserved__:  
    <p>$$\sum _{x\in A}p_{X}(x)=1$$</p>
    __Joint Probability Distribution__ is a PMF over many variables, denoted $$P(\mathrm{x} = x, \mathrm{y} = y)$$ or $$P(x, y)$$.  

    A __PMF__ must satisfy these properties:  
    * The domain of $$P$$ must be the set of all possible states of $$\mathrm{x}$$.  
    * $$\forall x \in \mathrm{x}, \: 0 \leq P(x) \leq 1$$. Impossible events has probability $$0$$. Guaranteed events have probability $$1$$.  
    * $${\displaystyle \sum_{x \in \mathrm{x}} P(x) = 1}$$, i.e. the PMF must be normalized.  
    <br>
            
4. **Probability Density Function:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    A __Probability Density Function (PDF)__ is a function (probability distribution) whose value at any given sample (or point) in the sample space can be interpreted as providing a relative likelihood that the value of the random variable would equal that sample.  
    The __PDF__ is defined as the _derivative_ of the __CDF__:  
    <p>$$f_{X}(x) = \dfrac{dF_{X}(x)}{dx}$$</p>  
    A Probability Density Function $$p(x)$$ does not give the probability of a specific state directly; instead the probability of landing inside an infinitesimal region with volume $$\delta x$$ is given by $$p(x)\delta x$$.  
    We can integrate the density function to find the actual probability mass of a set of points. Specifically, the probability that $$x$$ lies in some set $$S$$ is given by the integral of $$p(x)$$ over that set.  
    > In the __Univariate__ example, the probability that $$x$$ lies in the interval $$[a, b]$$ is given by $$\int_{[a, b]} p(x)dx$$  


    A __PDF__ must satisfy these properties:  
    * The domain of $$P$$ must be the set of all possible states of $$x$$.  
    * $$\forall x \in \mathrm{x}, \: 0 \leq P(x) \leq 1$$. Impossible events has probability $$0$$. Guaranteed events have probability $$1$$.  
    * $$\int p(x)dx = 1$$, i.e. the integral of the PDF must be normalized.  
    <br>


44. **Cumulative Distribution Function:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents244}  
    A __Cumulative Distribution Function (CDF)__ is a function (probability distribution) of a real-valued random variable $$X$$, or just distribution function of $$X$$, evaluated at $$x$$, is the probability that $$X$$ will take a value less than or equal to $$x$$.    
    <p>$$F_{X}(x)=\operatorname {P} (X\leq x)$$ </p>  
    The probability that $$X$$ lies in the semi-closed interval $$(a, b]$$, where $$a  <  b$$, is therefore  
    <p>$${\displaystyle \operatorname {P} (a<X\leq b)=F_{X}(b)-F_{X}(a).}$$</p>  
    
    __Properties__:    
    * $$0 \leq F(x) \leq 1$$, 
    * $$\lim_{x \rightarrow -\infty} F(x) = 0$$, 
    * $$\lim_{x \rightarrow \infty} F(x) = 1$$, 
    * $$x \leq y \implies F(x) \leq F(y)$$.  
    <br>

5. **Marginal Probability:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    The __Marginal Distribution__ of a subset of a collection of random variables is the probability distribution of the variables contained in the subset.  
    __Two-variable Case__:  
    Given two random variables $$X$$ and $$Y$$ whose joint distribution is known, the marginal distribution of $$X$$ is simply the probability distribution of $$X$$ averaging over information about $$Y$$.
    * __Discrete__:    
        <p>$${\displaystyle \Pr(X=x)=\sum_ {y}\Pr(X=x,Y=y)=\sum_ {y}\Pr(X=x\mid Y=y)\Pr(Y=y)}$$</p>    
    * __Continuous__:    
        <p>$${\displaystyle p_{X}(x)=\int _{y}p_{X,Y}(x,y)\,\mathrm {d} y=\int _{y}p_{X\mid Y}(x\mid y)\,p_{Y}(y)\,\mathrm {d} y}$$</p>  
    * *__Marginal Probability as Expectation__*:    
    <p>$${\displaystyle p_{X}(x)=\int _{y}p_{X\mid Y}(x\mid y)\,p_{Y}(y)\,\mathrm {d} y=\mathbb {E} _{Y}[p_{X\mid Y}(x\mid y)]}$$</p>  
    <button>Intuitive Explanation</button>{: .showText value="show"  
     onclick="showTextPopHide(event);"}
    ![img](/main_files/math/prob/1.png){: width="100%" hidden=""}  

    __Marginalization:__{: style="color: red"} the process of forming the marginal distribution with respect to one variable by summing out the other variable  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * __Marginal Distribution of a variable__: is just the prior distr of the variable  
    * __Marginal Likelihood__: also known as the evidence, or model evidence, is the denominator of the Bayes equation. Its only role is to guarantee that the posterior is a valid probability by making its area sum to 1.  
        ![Example](https://cdn.mathpix.com/snip/images/UPUhBUhhUivvvIHO3nt5S52UcqPkSMS_eZEg3mhDXhk.original.fullsize.png)  
    * __both terms above are the same__  
    * __Marginal Distr VS Prior__:  
        * [Discussion](https://stats.stackexchange.com/questions/249275/whats-the-difference-between-prior-and-marginal-probabilities?rq=1)  
        * __Summary__:  
            Basically, it's a conceptual difference.  
            The prior, denoted $$p(\theta)$$, denotes the probability of some event ùúî even before any data has been taken.  
            A marginal distribution is rather different. You hold a variable value and integrate over the unknown values.  
            But, in some contexts they are the same.  

            


            


6. **Conditional Probability:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    __Conditional Probability__ is a measure of the probability of an event given that another event has occurred.  
    Conditional Probability is only defined when $$P(x) > 0$$ - We cannot compute the conditional probability conditioned on an event that never happens.   
    __Definition__:  
    <p>$$P(A|B)={\frac {P(A\cap B)}{P(B)}} = {\frac {P(A, B)}{P(B)}}$$</p>  

    > Intuitively, it is a way of updating your beliefs/probabilities given new evidence. It's inherently a sequential process.  



7. **The Chain Rule of Conditional Probability:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  
    Any joint probability distribution over many random variables may be decomposed into conditional distributions over only one variable.  
    The chain rule permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities:    
    <p>$$\mathrm {P} \left(\bigcap _{k=1}^{n}A_{k}\right)=\prod _{k=1}^{n}\mathrm {P} \left(A_{k}\,{\Bigg |}\,\bigcap _{j=1}^{k-1}A_{j}\right)$$</p>  

8. **Independence and Conditional Independence:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}  
    Two random variables $$x$$ and $$y$$ (or events ) are __independent__ if their probability distribution can be expressed as a product of two factors, one involving only $$x$$ and one involving only $$y$$:  
    <p>$$\mathrm{P}(A \cap B) = \mathrm{P}(A)\mathrm{P}(B)$$</p>  

    Two random variables $$A$$ and $$B$$ are __conditionally independent__ _given a random variable $$Y$$_ if the conditional probability distribution over $$A$$ and $$B$$ factorizes in this way for every value of $$Y$$:  
    <p>$$\Pr(A\cap B\mid Y)=\Pr(A\mid Y)\Pr(B\mid Y)$$</p>  
    or equivalently,  
    <p>$$\Pr(A\mid B\cap Y)=\Pr(A\mid Y)$$</p>  
    > In other words, $$A$$ and $$B$$ are conditionally independent given $$Y$$ if and only if, given knowledge that $$Y$$ occurs, knowledge of whether $$A$$ occurs provides no information on the likelihood of $$B$$ occurring, and knowledge of whether $$B$$ occurs provides no information on the likelihood of $$A$$ occurring.  


    __Pairwise VS Mutual Independence:__{: style="color: red"}  
    * __Pairwise__:  
        <p>$$\mathrm{P}\left(A_{m} \cap A_{k}\right)=\mathrm{P}\left(A_{m}\right) \mathrm{P}\left(A_{k}\right)$$</p>  
    * __Mutual Independence:__
        <p>$$\mathrm{P}\left(\bigcap_{i=1}^{k} B_{i}\right)=\prod_{i=1}^{k} \mathrm{P}\left(B_{i}\right)$$</p>  
        for *__all subsets__* of size $$k \leq n$$  

    __Pairwise__ independence does __not__ imply __mutual__ independence, but the other way around is TRUE (by definition).  



    __Notation:__  
    * *__$$A$$ is Independent from $$B$$__*:  $$A{\perp}B$$
    * *__$$A$$ and $$B$$ are conditionally Independent given $$Y$$__*:  $$A{\perp}B \:\vert Y$$  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * Unconditional Independence is very rare (there is usually some hidden factor influencing the interaction between the two events/variables)  
    * _Conditional Independence_ is the most basic and robust form of knowledge about uncertain environments  
            
    <br>
                
9. **Expectation:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents29}  
    The __expectation__, or __expected value__, of some function $$f(x)$$ with respect to a probability distribution $$P(x)$$ is the _"theoretical"_ average, or mean value, that $$f$$ takes on when $$x$$ is drawn from $$P$$.  
    > The Expectation of a R.V. is a weighted average of the values $$x$$ that the R.V. can take -- $$\operatorname {E}[X] = \sum_{x \in X} x \cdot p(x)$$  
    * __Discrete case__:  
        <p>$${\displaystyle \operatorname {E}_{x \sim P} [f(X)]=f(x_{1})p(x_{1})+f(x_{2})p(x_{2})+\cdots +f(x_{k})p(x_{k})} = \sum_x P(x)f(x)$$</p>             
    * __Continuous case__:  
    <p>$${\displaystyle \operatorname {E}_ {x \sim P} [f(X)] = \int p(x)f(x)dx}$$</p>   
    __Linearity of Expectation:__  
    <p>$${\displaystyle {\begin{aligned}\operatorname {E} [X+Y]&=\operatorname {E} [X]+\operatorname {E} [Y],\\[6pt]\operatorname {E} [aX]&=a\operatorname {E} [X],\end{aligned}}}$$</p>   
    __Independence:__   
    If $$X$$ and $$Y$$ are independent $$\implies \operatorname {E} [XY] = \operatorname {E} [X] \operatorname {E} [Y]$$  
    <br>

10. **Variance:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents210}  
    __Variance__ is the expectation of the squared deviation of a random variable from its mean.  
    It gives a measure of how much the values of a function of a random variable $$x$$ vary as we sample different values of $$x$$ from its probability distribution:  
    <p>$$\operatorname {Var} (f(x))=\operatorname {E} \left[(f(x)-\mu )^{2}\right] = \sum_{x \in X} (x - \mu)^2 \cdot p(x)$$</p>  
    __Variance expanded__:  
    <p>$${\displaystyle {\begin{aligned}\operatorname {Var} (X)&=\operatorname {E} \left[(X-\operatorname {E} [X])^{2}\right]\\
        &=\operatorname {E} \left[X^{2}-2X\operatorname {E} [X]+\operatorname {E} [X]^{2}\right]\\
        &=\operatorname {E} \left[X^{2}\right]-2\operatorname {E} [X]\operatorname {E} [X]+\operatorname {E} [X]^{2}\\
        &=\operatorname {E} \left[X^{2}\right]-\operatorname {E} [X]^{2}\end{aligned}}}$$  </p>   
    __Variance as Covariance__: 
    Variance can be expressed as the covariance of a random variable with itself: 
    <p>$$\operatorname {Var} (X)=\operatorname {Cov} (X,X)$$</p>   
    
    __Properties:__  
    * $$\operatorname {Var} [a] = 0, \forall a \in \mathbb{R}$$ (constant $$a$$)  
    * $$\operatorname {Var} [af(X)] = a^2 \operatorname {Var} [f(X)]$$ (constant $$a$$)
    * $$\operatorname {Var} [X + Y] = a^2 \operatorname {Var} [X] + \operatorname {Var} [Y] + 2 \operatorname {Cov} [X, Y]$$.  
    <br>

11. **Standard Deviation:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents211}  
    The __Standard Deviation__ is a measure that is used to quantify the amount of variation or dispersion of a set of data values.  
    It is defined as the square root of the variance:  
    <p>$${\displaystyle {\begin{aligned}\sigma &={\sqrt {\operatorname {E} [(X-\mu )^{2}]}}\\&={\sqrt {\operatorname {E} [X^{2}]+\operatorname {E} [-2\mu X]+\operatorname {E} [\mu ^{2}]}}\\&={\sqrt {\operatorname {E} [X^{2}]-2\mu \operatorname {E} [X]+\mu ^{2}}}\\&={\sqrt {\operatorname {E} [X^{2}]-2\mu ^{2}+\mu ^{2}}}\\&={\sqrt {\operatorname {E} [X^{2}]-\mu ^{2}}}\\&={\sqrt {\operatorname {E} [X^{2}]-(\operatorname {E} [X])^{2}}}\end{aligned}}}$$</p>  
    
    __Properties:__  
    * 68% of the data-points lie within $$1 \cdot \sigma$$s from the mean
    * 95% of the data-points lie within $$2 \cdot \sigma$$s from the mean
    * 99% of the data-points lie within $$3 \cdot \sigma$$s from the mean
    <br>

12. **Covariance:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents212}  
    __Covariance__ is a measure of the joint variability of two random variables.  
    It gives some sense of how much two values are linearly related to each other, as well as the scale of these variables:  
    <p>$$\operatorname {cov} (X,Y)=\operatorname {E} { {\big[ }(X-\operatorname {E} [X])(Y-\operatorname {E} [Y]){ \big] } }$$ </p>  
    __Covariance expanded:__  
    <p>$${\displaystyle {\begin{aligned}\operatorname {cov} (X,Y)&=\operatorname {E} \left[\left(X-\operatorname {E} \left[X\right]\right)\left(Y-\operatorname {E} \left[Y\right]\right)\right]\\&=\operatorname {E} \left[XY-X\operatorname {E} \left[Y\right]-\operatorname {E} \left[X\right]Y+\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]\right]\\&=\operatorname {E} \left[XY\right]-\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]-\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]+\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]\\&=\operatorname {E} \left[XY\right]-\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right].\end{aligned}}}$$ </p>  
    > when $${\displaystyle \operatorname {E} [XY]\approx \operatorname {E} [X]\operatorname {E} [Y]} $$, this last equation is prone to catastrophic cancellation when computed with floating point arithmetic and thus should be avoided in computer programs when the data has not been centered before.  

    __Covariance of Random Vectors__:  
    <p>$${\begin{aligned}\operatorname {cov} (\mathbf {X} ,\mathbf {Y} )&=\operatorname {E} \left[(\mathbf {X} -\operatorname {E} [\mathbf {X} ])(\mathbf {Y} -\operatorname {E} [\mathbf {Y} ])^{\mathrm {T} }\right]\\&=\operatorname {E} \left[\mathbf {X} \mathbf {Y} ^{\mathrm {T} }\right]-\operatorname {E} [\mathbf {X} ]\operatorname {E} [\mathbf {Y} ]^{\mathrm {T} },\end{aligned}}$$ </p>  

    __The Covariance Matrix__ of a random vector $$x \in \mathbb{R}^n$$ is an $$n \times n$$ matrix, such that:    
    <p>$$ \operatorname {cov} (X)_ {i,j} = \operatorname {cov}(x_i, x_j) \\
        \operatorname {cov}(x_i, x_j) = \operatorname {Var} (x_i)$$</p>   
    __Interpretations__:  
    * High absolute values of the covariance mean that the values change very much and are both far from their respective means at the same time.
    * __The sign of the covariance__:   
        The sign of the covariance shows the tendency in the linear relationship between the variables:  
        * *__Positive__*:  
            the variables tend to show similar behavior
        * *__Negative__*:  
            the variables tend to show opposite behavior  
        * __Reason__:  
        If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, (i.e., the variables tend to show similar behavior), the covariance is positive. In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, (i.e., the variables tend to show opposite behavior), the covariance is negative.  

    __Covariance and Variance:__  
    <p>$$\operatorname{Var}[X+Y]=\operatorname{Var}[X]+\operatorname{Var}[Y]+2 \operatorname{Cov}[X, Y]$$</p>  

    __Covariance and Independence:__  
    If $$X$$ and $$Y$$ are independent $$\implies \operatorname{cov}[X, Y]=\mathrm{E}[X Y]-\mathrm{E}[X] \mathrm{E}[Y] = 0$$.  
    * Independence $$\Rightarrow$$ Zero Covariance  
    * Zero Covariance $$\nRightarrow$$ Independence

    __Covariance and Correlation:__  
    If $$\operatorname{Cov}[X, Y]=0 \implies $$ $$X$$ and $$Y$$ are __Uncorrelated__.  

    * [**Covariance/Correlation Intuition**](https://www.youtube.com/embed/KDw3hC2YNFc){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/KDw3hC2YNFc"></a>
        <div markdown="1"> </div>    
    * [**Covariance and Correlation (Harvard Lecture)**](https://www.youtube.com/embed/IujCYxtpszU){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/IujCYxtpszU"></a>
        <div markdown="1"> </div>    
    * [**Covariance as slope of the Regression Line**](https://www.youtube.com/embed/ualmyZiPs9w){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/ualmyZiPs9w"></a>
        <div markdown="1"> </div>    

    <br>  

13. **Mixtures of Distributions:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents213}  
    It is also common to define probability distributions by combining other simpler probability distributions. One common way of combining distributions is to construct a __mixture distribution__.    
    A __Mixture Distribution__ is the probability distribution of a random variable that is derived from a collection of other random variables as follows: first, a random variable is selected by chance from the collection according to given probabilities of selection, and then the value of the selected random variable is realized.    
    On each trial, the choice of which component distribution should generate the sample is determined by sampling a component identity from a multinoulli distribution:  
    <p>$$P(x) = \sum_i P(x=i)P(x \vert c=i)$$</p>    
    where $$P(c)$$ is the multinoulli distribution over component identities.    

14. **Bayes' Rule:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents214}  
    __Bayes' Rule__ describes the probability of an event, based on prior knowledge of conditions that might be related to the event.    
    <p>$${\displaystyle P(A\mid B)={\frac {P(B\mid A)\,P(A)}{P(B)}}}$$</p>  
    where,   
    <p>$$P(B) =\sum_A P(B \vert A) P(A)$$</p>  


15. **Common Random Variables:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents215}  
    __Discrete RVs:__{: style="color: red"}    
    {: #lst-p}
    * __Bernoulli__:  
    ![img](/main_files/math/prob/2.png){: width="90%"}  
    * __Binomial__:  
    ![img](/main_files/math/prob/3.png){: width="90%"}  
    * __Geometric__:  
    ![img](/main_files/math/prob/4.png){: width="90%"}  
    * __Poisson__:  
    ![img](/main_files/math/prob/5.png){: width="90%"}  

    __Continuous RVs:__{: style="color: red"}  
    {: #lst-p}
    * __Uniform__:  
    ![img](/main_files/math/prob/6.png){: width="90%"}  
    * __Exponential__:  
    ![img](/main_files/math/prob/7.png){: width="90%"}  
    * __Normal/Gaussian__:  
    ![img](/main_files/math/prob/8.png){: width="90%"}  
            
            
16. **Summary of Distributions:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents216}  
    ![img](/main_files/math/prob/9.png){: width="80%"}  


17. **Formulas:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents217}  
    * $$\overline{X} = \hat{\mu}$$,  
    * $$\operatorname {E}[\overline{X}]=\operatorname {E}\left[\frac{X_{1}+\cdots+X_{n}}{n}\right] = \mu$$,  
    * $$\operatorname{Var}[\overline{X}]=\operatorname{Var}\left[\frac{X_{1}+\cdots+X_{n}}{n}\right] = \dfrac{\sigma^2}{n}$$,    
    * $$\operatorname {E}\left[X_{i}^{2}\right]=\operatorname {Var} [X]+\operatorname {E} [X]^{2} = \sigma^{2}+\mu^{2}$$,  
    * $$\operatorname {E}\left[\overline{X}^{2}\right]=\operatorname {E}\left[\hat{\mu}^{2}\right]=\frac{\sigma^{2}}{n}+\mu^{2}\:$$, [^2]  
    <br>


18. **Correlation:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents218}  
    In the broadest sense __correlation__ is any statistical association, though it commonly refers to the degree to which a pair of variables are linearly related.  

    There are several correlation coefficients, often denoted $${\displaystyle \rho }$$ or $$r$$, measuring the degree of correlation:  

    __Pearson Correlation Coefficient [\[wiki\]](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient):__{: style="color: red"}  
    It is a measure of the __linear correlation__ between two variables $$X$$ and $$Y$$.  
    <p>$$\rho_{X, Y}=\frac{\operatorname{cov}(X, Y)}{\sigma_{X} \sigma_{Y}}$$</p>  
    where, $${\displaystyle \sigma_{X}}$$ is the standard deviation of $${\displaystyle X}$$ and $${\displaystyle \sigma_{Y}}$$  is the standard deviation of $${\displaystyle Y}$$, and $$\rho \in [-1, 1]$$.   



    __Correlation and Independence:__{: style="color: red"}  
    1. Uncorrelated $$\nRightarrow$$ Independent  
    2. Independent $$\implies$$ Uncorrelated  

    Zero correlation will indicate no linear dependency, however won't capture non-linearity. Typical example is uniform random variable $$x$$, and $$x^2$$ over $$[-1,1]$$ with zero mean. Correlation is zero but clearly not independent.  
    <br> 

19. **Probabilistic Inference:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents219}  
    __Probabilistic Inference:__ compute a desired probability from other known probabilities (e.g. conditional from joint).  

    __We generally compute Conditional Probabilities:__  
    {: #lst-p}
    * $$p(\text{sun} \vert T=\text{12 pm}) = 0.99$$  
    * These represent the agents beliefs given the evidence  

    __Probabilities change with new evidence:__  
    {: #lst-p} 
    * $$p(\text{sun} \vert T=\text{12 pm}, C=\text{Stockholm}) = 0.85$$  
    $$\longrightarrow$$  
    * $$p(\text{sun} \vert T=\text{12 pm}, C=\text{Stockholm}, M=\text{Jan}) = 0.40$$  
    * Observing new evidence causes beliefs to be updated

    __Inference by Enumeration:__{: style="color: red"}  
    {: #lst-p}
    * [**CS188 Lec. 10-2**](https://www.youtube.com/embed/sMNbLXsvRig?start=3508){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/sMNbLXsvRig?start=3508"></a>
        <div markdown="1"> </div>    

    __Problems:__  
    * Worst-case time complexity $$\mathrm{O}\left(\mathrm{d}^{n}\right)$$
    * Space complexity $$\mathrm{O}\left(\mathrm{d}^{n}\right)$$ to store the joint distribution  

    __Inference with Bayes Theorem:__{: style="color: red"}  
    * __Diagnostic Probability from Causal Probability:__  
        <p>$$P(\text { cause } | \text { effect })=\frac{P(\text { effect } | \text { cause }) P(\text { cause })}{P(\text { effect })}$$</p>  




[^2]: Comes from $$\operatorname{Var}[\overline{X}]=\operatorname {E}\left[\overline{X}^{2}\right]-\{\operatorname {E}[\overline{X}]\}^{2}$$  


***

## Discrete Distributions
{: #content9}



2. **Bernoulli Distribution:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents92}  
    :   A distribution over a single binary random variable.  
        It is controlled by a single parameter $$\phi \in [0, 1]$$, which fives the probability of the r.v. being equal to $$1$$.  
        > It models the probability of a single experiment with a boolean outcome (e.g. coin flip $$\rightarrow$$ {heads: 1, tails: 0})  
    :   __PMF:__  
    :   $${\displaystyle P(x)={\begin{cases}p&{\text{if }}p=1,\\q=1-p&{\text{if }}p=0.\end{cases}}}$$  
    :   __Properties:__  
        <p>$$P(X=1) = \phi$$</p>
        <p>$$P(X=0) = 1 - \phi$$</p>
        <p>$$P(X=x) = \phi^x (1 - \phi)^{1-x}$$</p>
        <p>$$\operatorname {E}[X] = \phi$$</p>
        <p>$$\operatorname {Var}(X) = \phi (1 - \phi)$$</p>

3. **Binomial Distribution:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents93}     
    > $${\binom {n}{k}}={\frac {n!}{k!(n-k)!}}$$ is the number of possible ways of getting $$x$$ successes and $$n-x$$ failures

    
***

## Notes, Tips and Tricks
{: #content10}

* It is more practical to use a simple but uncertain rule rather than a complex but certain one, even if the true rule is deterministic and our modeling system has the fidelity to accommodate a complex rule.  
    For example, the simple rule ‚ÄúMost birds fly‚Äù is cheap to develop and is broadly useful, while a rule of the form, ‚ÄúBirds fly, except for very young birds that have not yet learned to fly, sick or injured birds that have lost the ability to fly, flightless species of birds including the cassowary, ostrich and kiwi. . .‚Äù is expensive to develop, maintain and communicate and, after all this effort, is still brittle and prone to failure.

* __Disjoint Events (Mutually Exclusive):__{: .bodyContents10 #bodyContents102} are events that cannot occur together at the same time
    Mathematically:  
    * $$A_i \cap A_j = \varnothing$$ whenever $$i \neq j$$  
    * $$p(A_i, A_j) = 0$$,  

* __Complexity of Describing a Probability Distribution__:  
    A description of a probability distribution is _exponential_ in the number of variables it models.  
    The number of possibilities is __exponential__ in the number of variables.  

* __Probability VS Likelihood__:  
    __Probabilities__ are the areas under a fixed distribution  
    $$pr($$data$$|$$distribution$$)$$  
    i.e. probability of some _data_ (left hand side) given a distribution (described by the right hand side)  
    __Likelihoods__ are the y-axis values for fixed data points with distributions that can be moved..  
    $$L($$distribution$$|$$observation/data$$)$$  

    > Likelihood is, basically, a specific probability that can only be calculated after the fact (of observing some outcomes). It is not normalized to $$1$$ (it is __not__ a probability). It is just a way to quantify how likely a set of observation is to occur given some distribution with some parameters; then you can manipulate the parameters to make the realization of the data more _"likely"_ (it is precisely meant for that purpose of estimating the parameters); it is a _function_ of the __parameters__.  
    Probability, on the other hand, is absolute for all possible outcomes. It is a function of the __Data__.  

* __Maximum Likelihood Estimation__:  
    A method that tries to find the _optimal value_ for the _mean_ and/or _stdev_ for a distribution *__given__* some observed measurements/data-points.

* __Variance__:  
    When $$\text{Var}(X) = 0 \implies X = E[X] = \mu$$. (not interesting)  

* __Reason we sometimes prefer Biased Estimators__:  
        

***
***

TITLE: The Theory of Learning
LINK: research/dl/theory/theory_of_learning.md


[Learning (wiki)](https://en.wikipedia.org/wiki/Learning)  
[Hebbian Theory (wiki)](https://en.wikipedia.org/wiki/Hebbian_theory)  
[Learning Exercises](https://neuronaldynamics-exercises.readthedocs.io/en/latest/exercises/hopfield-network.html)  
[Neuronal Dynamics - From single neurons to networks and models of cognition](https://neuronaldynamics.epfl.ch/online/index.html)  


## Learning
{: #content1}

1. **Learning:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __Learning__ is the process of acquiring new, or modifying existing, knowledge, behaviors, skills, values, or preferences.  
    <br>



***

## Types of Learning
{: #content2}

1. **Hebbian (Associative) Learning:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    __Hebbian/Associative Learning__ is the process by which a person or animal learns an association between two stimuli or events, in which, simultaneous activation of cells leads to pronounced increases in synaptic strength between those cells.  

    __Hebbian Learning in Artificial Neural Networks:__{: style="color: red"}  
    From the pov of ANNs, Hebb's principle can be described as a method of __determining how to alter the weights between model neurons__.  
    {: #lst-p}
    * The weight between two neurons:  
        * Increases if the two neurons activate simultaneously,  
        * Reduces if they activate separately.  
    * Nodes that tend to be either both positive or both negative at the same time have strong positive weights, 
        while those that tend to be opposite have strong negative weights.  

    __Hebb's Rule:__  
    The change in the $i$ th synaptic weight $w_{i}$ is equal to a learning rate $\eta$ times the $i$ th input $x_{i}$ times the postsynapic response $y$:  
    <p>$$\Delta w_{i}=\eta x_{i} y$$</p>  
    where in the case of a linear neuron:  
    <p>$$y=\sum_{j} w_{j} x_{j}$$</p>  


    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * It is regarded as the neuronal basis of __unsupervised learning__.  
    <br>


2. **Supervised Learning:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}
    __Supervised Learning__: the task of learning a function that maps an input to an output based on example input-output pairs.  
    ![img](/main_files/dl/theory/caltech/4.png){: width="70%"}  
    <br>

3. **Unsupervised Learning:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    __Unsupervised Learning__: the task of making inferences, by learning a better representation, from some datapoints that do not have any labels associated with them.  
    ![img](/main_files/dl/theory/caltech/5.png){: width="70%"}  
    > Unsupervised Learning is another name for [Hebbian Learning](https://en.wikipedia.org/wiki/Hebbian_theory)  

    <button>Unsupervised Learning Algorithms</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    * Clustering
        * hierarchical clustering
        * k-means
        * mixture models
        * DBSCAN
    * Anomaly Detection: Local Outlier Factor
    * Neural Networks
        * Autoencoders
        * Deep Belief Nets
        * Hebbian Learning
        * Generative Adversarial Networks
        * Self-organizing map
    * Approaches for learning latent variable models such as
        * Expectation‚Äìmaximization algorithm (EM)
        * Method of moments
        * Blind signal separation techniques
            * Principal component analysis
            * Independent component analysis
            * Non-negative matrix factorization
            * Singular value decomposition
    {: hidden=""}

    <br>

4. **Reinforcement Learning:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    __Reinforcement Leaning__: the task of learning how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward.  
    ![img](/main_files/dl/theory/caltech/6.png){: width="70%"}  
    <br>

5. **Semi-supervised Learning:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  


6. **Zero-Shot Learning:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  

7. **Transfer Learning:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  

8. **Multitask Learning:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}  

9. **Domain Adaptation:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents29}  

***

## Theories of Learning
{: #content3}



## Reasoning and Inference
{: #content4}

1. **Logical Reasoning:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    ![img](https://cdn.mathpix.com/snip/images/vIoP1W7b3o_HHfP_qSux2x4EbCcJGoF2TPL2moWAUeQ.original.fullsize.png){: width="80%"}  

    * __Inductive Learning__ is the process of using observations to draw conclusions  
        * It is a method of reasoning in which the __premises are viewed as supplying <span>some</span>{: style="color: goldenrod"} evidence__ for the truth of the conclusion.  
        * It goes from <span>specific</span>{: style="color: goldenrod"} to <span>general</span>{: style="color: goldenrod"} (_"bottom-up logic"_).    
        * The truth of the conclusion of an inductive argument may be __probable__{: style="color: goldenrod"}, based upon the evidence given.  
    * __Deductive Learning__ is the process of using conclusions to form observations.  
        * It is the process of reasoning from one or more statements (premises) to reach a logically certain conclusion.  
        * It goes from <span>general</span>{: style="color: goldenrod"} to <span>specific</span>{: style="color: goldenrod"} (_"top-down logic"_).    
        * The conclusions reached ("observations") are necessarily __True__.  
    * __Abductive Learning__ is a form of __inductive learning__ where we use observations to draw the *__simplest__* and __most__ *__likely__* conclusions.  
        It can be understood as "__inference to the best explanation__".  
        It is used by _Sherlock Holmes_.  


    __In Mathematical Modeling (ML):__{: style="color: red"}  
    In the context of __Mathematical Modeling__ the three kinds of reasoning can be described as follows:  
    {: #lst-p}
    * The <span>construction/creation of the structure of the model</span>{: style="color: purple"} is __abduction__.  
    * <span>Assigning values (or probability distributions) to the parameters of the model</span>{: style="color: purple"} is __induction__.  
    * <span>Executing/running the model</span>{: style="color: purple"} is __deduction__.  

    <br>


2. **Inference:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}  
    __Inference__ has two definitions:  
    {: #lst-p}
    1. A conclusion reached on the basis of evidence and reasoning.  
    2. The process of reaching such a conclusion.  
    <br>


3. **Transductive Inference/Learning (Transduction):**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}  

    The __Goal__ of Transductive Learning is to ‚Äúsimply‚Äù <span>add labels to the unlabeled data by exploiting labelled samples</span>{: style="color: purple"}.  
    While, the goal of _inductive learning_ is to infer the correct mapping from $$X$$ to $$Y$$.  

    __Transductive VS Semi-supervised Learning:__{: style="color: red"}  
    __Transductive Learning__ is only concerned with the *__unlabeled__* data.   

    __Transductive Learning:__  
    ![img](https://cdn.mathpix.com/snip/images/vJc503Mbxku0OQ6C3ctRjIGvz5QVc56Z5ksmjxn_gi4.original.fullsize.png){: width="50%"}  
    __Inductive Learning:__  
    ![img](https://cdn.mathpix.com/snip/images/HcXQ1dT1_y6Vdxsm8TZL7jU408bHTuHLn492WVe3B-Q.original.fullsize.png){: width="50%"}  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * [Transductive Inference (wiki)](https://en.wikipedia.org/wiki/Transduction_(machine_learning))  
    <br>





***
***

TITLE: Statistical Learning Theory
LINK: research/dl/theory/stat_lern_thry.md


* [Principles of Risk Minimization for Learning Theory (original papers)](http://papers.nips.cc/paper/506-principles-of-risk-minimization-for-learning-theory.pdf)  
* [Statistical Learning Theory from scratch (paper)](http://www.tml.cs.uni-tuebingen.de/team/luxburg/publications/StatisticalLearningTheory.pdf)    
* [The learning dynamics behind generalization and overfitting in Deep Networks](https://www.youtube.com/watch?v=pFWiauHOFpY)  
* [Notes on SLT](http://maxim.ece.illinois.edu/teaching/SLT/SLT.pdf)  
* [Mathematics of Learning (w/ proofs & necessary+sufficient conditions for learning)](http://web.mit.edu/9.s915/www/classes/dealing_with_data.pdf)  
* [Generalization Bounds for Hypothesis Spaces](https://courses.cs.washington.edu/courses/cse522/11wi/scribes/lecture4.pdf)  
* [Generalization Bound Derivation](https://mostafa-samir.github.io/ml-theory-pt2/)  
* [Overfitting isn‚Äôt simple: Overfitting Re-explained with Priors, Biases, and No Free Lunch](http://mlexplained.com/2018/04/24/overfitting-isnt-simple-overfitting-re-explained-with-priors-biases-and-no-free-lunch/)  
* [9.520/6.860: Statistical Learning Theory and Applications, Fall 2017](http://www.mit.edu/~9.520/fall17/)  


__Fundamental Theorem of Statistical Learning (binary classification):__{: style="color: red"}  
Let $$\mathcal{H}$$ be a hypothesis class of functions from a domain $$X$$ to $$\{0,1\}$$ and let the loss function be the $$0-1$$ loss.  
The following are equivalent:  
<p>$$\begin{array}{l}{\text { 1. } \mathcal{H} \text { has uniform convergence. }} \\ {\text { 2. The ERM is a PAC learning algorithm for } \mathcal{H} \text { . }} \\ {\text { 3. } \mathcal{H} \text { is } PAC \text { learnable. }} \\ {\text { 4. } \mathcal{H} \text { has finite } VC \text { dimension. }}\end{array}$$</p>  
This can be extended to __regression__ and __multiclass classification__.   

<button>Proof.</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
<div hidden="" markdown="1">
* 1 $$\Rightarrow 2$$ We have seen uniform convergence implies that $$\mathrm{ERM}$$ is $$\mathrm{PAC}$$ learnable
* 2 $$\Rightarrow 3$$ Obvious.
* 3 $$\Rightarrow 4$$ We just proved that PAC learnability implies finite $$\mathrm{VC}$$ dimension.
* 4 $$\Rightarrow 1$$ We proved that finite $$\mathrm{VC}$$ dimension implies uniform convergence.
</div>

__Notes:__{: style="color: red"}  
{: #lst-p}
* VC dimension fully determines <span>learnability</span>{: style="color: goldenrod"} for binary classification.  
* The VC dimension doesn‚Äôt just determine __learnability__, it also gives a <span>bound on the sample complexity</span>{: style="color: goldenrod"} (which can be shown to be __tight__{: style="color: goldenrod"}).  
* [Lecture Slides (ref)](https://www.cs.toronto.edu/~jlucas/teaching/csc411/lectures/lec23_24_handout.pdf)  
* <button>Extra Notes (what you should know)</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/9bYxbit2n1mvyrttH3CzgI-2CgdrrDNVkejd1fP5-AU.original.fullsize.png){: width="100%" hidden=""}  


## Statistical Learning Theory
{: #content1}

1. **Statistical Learning Theory:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __Statistical Learning Theory__ is a framework for machine learning drawing from the fields of statistics and functional analysis. Under certain assumptions, this framework allows us to study the question:  
    > __How can we affect performance on the test set when we can only observe the training set?__{: style="color: blue"}  

    It is a _statistical_ approach to __Computational Learning Theory__.  

2. **Formal Definition:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    Let:  
    * $$X$$: $$\:$$ the vector space of all possible __inputs__  
    * $$Y$$: $$\:$$ the vector space of all possible __outputs__  
    * $$Z = X \times Y$$: $$\:$$ the __product space__ of (input,output) pairs  
    * $$n$$: $$\:$$ the number of __samples__ in the __training set__  
    * $$S=\left\{\left(\vec{x}_{1}, y_{1}\right), \ldots,\left(\vec{x}_{n}, y_{n}\right)\right\}=\left\{\vec{z}_{1}, \ldots, \vec{z}_{n}\right\}$$: $$\:$$ the __training set__  
    * $$\mathcal{H} = f : X \rightarrow Y$$: $$\:$$ the __hypothesis space__ of all functions  
    * $$V(f(\vec{x}), y)$$: $$\:$$ an __error/loss function__  

    __Assumptions:__  
    {: #lst-p}
    * The training and test data are generated by an *__unknown, joint__* __probability distribution over datasets__ (over the product space $$Z$$, denoted: $$p_{\text{data}}(z)=p(\vec{x}, y)$$) called the __data-generating process__.  
        * $$p_{\text{data}}$$ is a __joint distribution__ so that it allows us to model _uncertainty in predictions_ (e.g. from noise in data) because $$y$$ is not a deterministic function of $$\vec{x}$$, but rather a _random variable_ with __conditional distribution__ $$p(y \vert \vec{x})$$ for a fixed $$\vec{x}$$.  
    * The __i.i.d. assumptions:__  
        * The examples in each dataset are __independent__ from each other  
        * The _training set_ and _test set_ are __identically distributed__ (drawn from the same probability distribution as each other)  

        > A collection of random variables is __independent and identically distributed__ if each random variable has the same probability distribution as the others and all are mutually independent.  
        > _Informally,_ it says that all the variables provide the same kind of information independently of each other.  
        > * [Discussion on the importance if i.i.d assumptions](https://stats.stackexchange.com/questions/213464/on-the-importance-of-the-i-i-d-assumption-in-statistical-learning/214220)  


    __The Inference Problem__{: style="color: red"}    
    Finding a function $$f : X \rightarrow Y$$ such that $$f(\vec{x}) \sim y$$.  

    __The Expected Risk:__  
    <p>$$I[f]=\mathbf{E}[V(f(\vec{x}), y)]=\int_{X \times Y} V(f(\vec{x}), y) p(\vec{x}, y) d \vec{x} d y$$</p>  

    __The Target Function:__  
    is the best possible function $$f$$ that can be chosen, is given by:  
    <p>$$f=\inf_{h \in \mathcal{H}} I[h]$$</p>  

    __The Empirical Risk:__  
    Is a *__proxy measure__* for the __expected risk__, based on the training set.  
    It is necessary because the probability distribution $$p(\vec{x}, y)$$ is _unknown_.  
    <p>$$I_{S}[f]=\frac{1}{n} \sum_{i=1}^{n} V\left(f\left(\vec{x}_{i}\right), y_{i}\right)$$</p>  

3. **Empirical risk minimization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    __Empirical Risk Minimization (ERM)__ is a principle in _statistical learning theory_ that is based on approximating the __Generalization Error (True Risk)__ by measuring the __Training Error (Empirical Risk)__, i.e. the performance on training data.  

    A _learning algorithm_ that chooses the function $$f_{S}$$ that minimizes the _empirical risk_ is called __empirical risk minimization__:  
    <p>$$R_{\mathrm{emp}}(h) = I_{S}[f]=\frac{1}{n} \sum_{i=1}^{n} V\left(f\left(\vec{x}_{i}\right), y_{i}\right)$$</p>    
    <p>$$f_{S} = \hat{h} = \arg \min _{h \in \mathcal{H}} R_{\mathrm{emp}}(h)$$</p>  

    __Complexity:__{: style="color: red"}  
    <button>Show</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
    * Empirical risk minimization for a classification problem with a _0-1 loss function_ is known to be an __NP-hard__ problem even for such a relatively simple class of functions as linear classifiers.  
        * [Paper Proof](https://arxiv.org/abs/1012.0729)  
    * Though, it can be solved efficiently when the minimal empirical risk is zero, i.e. data is linearly separable.  
    * __Coping with Hardness:__  
        * Employing a __convex approximation__ to the 0-1 loss: _Hinge Loss_, _SVM_  
        * Imposing __Assumptions on the data-generating distribution__ and thus, stop being an __agnostic learning algorithm__.  
    {: hidden=""}  


4. **Definitions:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    * __Generalization Error:__{: style="color: red"}  
        AKA: __Expected Risk/Error__, __Out-of-Sample Error__[^2], __$$E_{\text{out}}$$__   
        It is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.  
    * __Generalization Gap:__{: style="color: red"}  
        It is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.  

        __Formally:__  
        The generalization gap is the __difference between the expected and empirical error__:  
        <p>$$G =I\left[f_{n}\right]-I_{S}\left[f_{n}\right]$$</p>  

        An Algorithm is said to __Generalize__ (achieve __Generalization__) if:  
        <p>$$\lim _{n \rightarrow \infty} G_n = \lim _{n \rightarrow \infty} I\left[f_{n}\right]-I_{S}\left[f_{n}\right]=0$$</p>  
        Equivalently:  
        <p>$$E_{\text { out }}(g) \approx E_{\text { in }}(g)$$</p>  
        or 
        <p>$$I\left[f_{n}\right] \approx I_{S}\left[f_{n}\right]$$</p>  

        __Computing the Generalization Gap:__  
        Since $$I\left[f_{n}\right]$$ cannot be computed for an unknown distribution, the generalization gap __cannot be computed__ either.  
        Instead the goal of __statistical learning theory__ is to _bound_ or _characterize_ the generalization gap in probability:  
        <p>$$P_{G}=P\left(I\left[f_{n}\right]-I_{S}\left[f_{n}\right] \leq \epsilon\right) \geq 1-\delta_{n}$$</p>  
        That is, the goal is to characterize the probability $${\displaystyle 1-\delta _{n}}$$ that the generalization gap is less than some error bound $${\displaystyle \epsilon }$$ (known as the __learning rate__ and generally dependent on $${\displaystyle \delta }$$ and $${\displaystyle n}$$).  
    * __The Empirical Distribution:__{: style="color: red"}  
        _AKA **Data-Generating Distribution**_  
        is the __discrete__ uniform distribution over the _sample points_.   
    * __The Approximation-Generalization Tradeoff:__{: style="color: red"}  
        * __Goal__:  
            Small $$E_{\text{out}}$$: Good approximation of $$f$$ *__out of sample__* (not in-sample).  
        * The tradeoff is characterized by the __complexity__ of the __hypothesis space $$\mathcal{H}$$__:  
            * __More Complex $$\mathcal{H}$$__: Better chance of approximating $$f$$  
            * __Less Complex $$\mathcal{H}$$__: Better chance of generalizing out-of-sample  
        * [**Abu-Mostafa**](https://www.youtube.com/embed/zrEyxfl2-a8?start=358){: value="show" onclick="iframePopA(event)"}
        <a href="https://www.youtube.com/embed/zrEyxfl2-a8?start=358"></a>
            <div markdown="1"> </div>    
        * [Lecture-Slides on Approximation-Generalization](https://mdav.ece.gatech.edu/ece-6254-spring2017/notes/13-bias-variance-marked.pdf)  
    * __Excess Risk (Generalization-Gap) Decomposition \| Estimation-Approximation Tradeoff:__{: style="color: red"}  
        __Excess Risk__ is defined as the difference between the expected-risk/generalization-error of any function $$\hat{f} = g^{\mathcal{D}}$$ that we learn from the data (exactly just bias-variance), and the expected-risk of the __target function__ $$f$$ (known as the __bayes optimal predictor__)
        * [**Excess Risk Decomposition Video**](https://www.youtube.com/embed/YA_CE9jat4I){: value="show" onclick="iframePopA(event)"}
        <a href="https://www.youtube.com/embed/YA_CE9jat4I"></a>
            <div markdown="1"> </div>    
        * [Excess Risk & Bias-Variance Lecture Slides](https://www.ics.uci.edu/~smyth/courses/cs274/readings/xing_singh_CMU_bias_variance.pdf)  
    <br>

8. **Notes:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    * __Choices of Loss Functions__:  
        * __Regression__:  
            * __MSE__: $$\: V(f(\vec{x}), y)=(y-f(\vec{x}))^{2}$$ 
            * __MAE__: $$\: V(f(\vec{x}), y)=\vert{y-f(\vec{x})}\vert$$  
        * __Classification__:  
            * __Binary__: $$\: V(f(\vec{x}), y)=\theta(-y f(\vec{x}))$$  
                where $$\theta$$ is the _Heaviside Step Function_.  
    * __Training Data, Errors, and Risk__:  
        * __Training-Error__ is the __Empirical Risk__  
            * It is a __proxy__ for the __Generalization Error/Expected Risk__  
            * This is what we minimize
        * __Test-Error__ is an *__approximation__* to the __Generalization Error/Expected Risk__ 
            * This is what we (can) compute to ensure that minimizing Training-Err/Empirical-Risk (ERM) also minimized the Generalization-Err/Expected-Risk (which we can't compute directly)  
    * __Why the goal is NOT to minimize $$E_{\text{in}}$$ completely (intuition)__:  
        Basically, if you have noise in the data; then fitting the (finite) training-data completely; i.e. minimizing the in-sample-err completely will underestimate the out-of-sample-err.  
        Since, if noise existed AND you fit training-data completely $$E_{\text{in}} = 0$$ THEN you inherently have fitted the noise AND your performance on out-sample will be lower.   


***

## The Vapnik-Chervonenkis (VC) Theory
{: #content2}

***

## The Bias-Variance Decomposition Theory
{: #content3}

11. **The Bias-Variance Decomposition Theory:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents311}    
    __The Bias-Variance Decomposition Theory__ is a way to quantify the __Approximation-Generalization Tradeoff__.  

    __Assumptions:__  
    {: #lst-p}
    * The analysis is done over the __entire data-distribution__  
    * The target function $$f$$ is already __known__; and you're trying to answer the question:  
        "How can $$\mathcal{H}$$ approximate $$f$$ over all? not just on your sample."  
    * Applies to __real-valued targets__ (can be extended)  
    * Use __Square Error__ (can be extended)  
    <br>

1. **The Bias-Variance Decomposition:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}    
    The __Bias-Variance Decomposition__ is a way of analyzing a learning algorithm's *__expected out-of-sample error__*[^1] as a _sum of three terms:_  
    * __Bias:__ is an error from erroneous assumptions in the learning algorithm.  
    * __Variance:__ is an error from sensitivity to small fluctuations in the training set.  
    * __Irreducible Error__ (resulting from noise in the problem itself)  

    Equivalently, __Bias__ and __Variance__ measure _two different sources of errors in an estimator:_   
    * __Bias:__ measures the expected deviation from the true value of the function or parameter.  
        > AKA: __Approximation Error__[^3]  (statistics)  How well can $$\mathcal{H}$$ approximate the target function '$$f$$'  
    * __Variance:__ measures the deviation from the expected estimator value that any particular sampling of the data is likely to cause.  
        > AKA: __Estimation (Generalization) Error__ (statistics) How well we can zoom in on a good $$h \in \mathcal{H}$$  


    __Bias-Variance Decomposition Formula:__  
    For any function $$\hat{f} = g^{\mathcal{D}}$$ we select, we can decompose its *__expected (out-of-sample) error__* on an _unseen sample $$x$$_ as:  
    <p>$$\mathrm{E}\left[(y-\hat{f}(x))^{2}\right]=(\operatorname{Bias}[\hat{f}(x)])^{2}+\operatorname{Var}[\hat{f}(x)]+\sigma^{2}$$</p>  
    Where:  
    * __Bias__:  
        <p>$$\operatorname{Bias}[\hat{f}(x)]=\mathrm{E}[\hat{f}(x)]-f(x)$$</p>  
    * __Variance__:  
        <p>$$\operatorname{Var}[\hat{f}(x)]=\mathrm{E}\left[\hat{f}(x)^{2}\right]-\mathrm{E}[\hat{f}(x)]^{2}$$</p>  
    and the expectation ranges over different realizations of the training set $$\mathcal{D}$$.  

2. **The Bias-Variance Tradeoff:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    is the property of a set of predictive models whereby, models with a _lower bias_ (in parameter estimation) have a _higher variance_ (of the parameter estimates across samples) and vice-versa.  

    __Effects of Bias:__{: style="color: black"}  
    {: #lst-p}
    * __High Bias__: simple models, lead to *__underfitting__*{: style="color: red"}.  
    * __Low Bias__: complex models, lead to *__overfitting__*{: style="color: red"}.  
    
    __Effects of Variance:__{: style="color: black"}  
    {: #lst-p}
    * __High Variance__: complex models, lead to *__overfitting__*{: style="color: red"}.  
    * __Low Variance__: simple models, lead to *__underfitting__*{: style="color: red"}.  
    <br>

3. **Derivation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    <p>$${\displaystyle {\begin{aligned}\mathbb{E}_{\mathcal{D}} {\big [}I[g^{(\mathcal{D})}]{\big ]}&=\mathbb{E}_{\mathcal{D}} {\big [}\mathbb{E}_{x}{\big [}(g^{(\mathcal{D})}-y)^{2}{\big ]}{\big ]}\\
    &=\mathbb{E}_{x} {\big [}\mathbb{E}_{\mathcal{D}}{\big [}(g^{(\mathcal{D})}-y)^{2}{\big ]}{\big ]}\\
    &=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}}{\big [}(g^{(\mathcal{D})}- f -\varepsilon)^{2}{\big ]}{\big ]}
    \\&=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}} {\big [}(f+\varepsilon -g^{(\mathcal{D})}+\bar{g}-\bar{g})^{2}{\big ]}{\big ]}\\
    &=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}} {\big [}(\bar{g}-f)^{2}{\big ]}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}(\bar{g}-f)\varepsilon {\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}\varepsilon (g^{(\mathcal{D})}-\bar{g}){\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})(\bar{g}-f){\big ]}{\big ]}\\
    &=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}+2(\bar{g}-f)\mathbb{E}_{\mathcal{D}} [\varepsilon ]\: +2\: \mathbb{E}_{\mathcal{D}} [\varepsilon ]\: \mathbb{E}_{\mathcal{D}} {\big [}g^{(\mathcal{D})}-\bar{g}{\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}g^{(\mathcal{D})}-\bar{g}{\big ]}(\bar{g}-f){\big ]}\\
    &=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}{\big ]}\\
    &=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
    &=\mathbb{E}_{x}{\big [}\operatorname {Bias} [g^{(\mathcal{D})}]^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
    &=\mathbb{E}_{x}{\big [}\operatorname {Bias} [g^{(\mathcal{D})}]^{2}+\sigma ^{2}+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
\end{aligned}}}$$</p>  
    where:  
    $$\overline{g}(\mathbf{x})=\mathbb{E}_{\mathcal{D}}\left[g^{(\mathcal{D})}(\mathbf{x})\right]$$ is the __average hypothesis__ over all realization of $$N$$ data-points $$\mathcal{D}_ i$$, and $${\displaystyle \varepsilon }$$ and $${\displaystyle {\hat {f}}} = g^{(\mathcal{D})}$$ are __independent__.    

    <button>Derivation with Wikipedia Notation</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
    <p hidden="">$${\displaystyle {\begin{aligned}\operatorname {E}_ {\mathcal{D}} {\big [}(y-{\hat {f}})^{2}{\big ]}&=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}})^{2}{\big ]}\\&=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}}+\operatorname {E} [{\hat {f}}]-\operatorname {E} [{\hat {f}}])^{2}{\big ]}\\&=\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])^{2}{\big ]}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])\varepsilon {\big ]}+2\operatorname {E} {\big [}\varepsilon (\operatorname {E} [{\hat {f}}]-{\hat {f}}){\big ]}+2\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})(f-\operatorname {E} [{\hat {f}}]){\big ]}\\&=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2(f-\operatorname {E} [{\hat {f}}])\operatorname {E} [\varepsilon ]+2\operatorname {E} [\varepsilon ]\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}+2\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}(f-\operatorname {E} [{\hat {f}}])\\&=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}\\&=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\&=\operatorname {Bias} [{\hat {f}}]^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\&=\operatorname {Bias} [{\hat {f}}]^{2}+\sigma ^{2}+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\\end{aligned}}}$$</p>


4. **Results and Takeaways of the Decomposition:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    Match the __"Model Complexity"__ to the *__Data Resources__*, NOT to the _Target Complexity_.  
    ![img](/main_files/dl/theory/stat_lern_thry/1.png){: width="80%"}  

    <button>Analogy to the Approximation-Generalization Tradeoff</button>{: .showText value="show"
    onclick="showText_withParent_PopHide(event);"}
    <p hidden="">Pretty much like I'm sitting in my office, and I want a document of some kind, an old letter. Someone has asked me for a letter of recommendation, and I don't want to rewrite it from scratch. So I want to take the older letter, and just see what I wrote, and then add the update to that.  <br>
    Before everything was archived in the computers, it used to be a piece of paper. So I know the letter of recommendation is somewhere. Now I face the question, should I write the letter of recommendation from scratch? Or should I look for the letter of recommendation? The recommendation is there. It's much easier when I find it. However, finding it is a big deal. So the question is not that the target function is there. The question is, can I find it?<br>  
    (Therefore, when I give you 100 examples, you choose the hypothesis set to match the 100 examples. If the 100 examples are terribly noisy, that's even worse. Because their information to guide you is worse.)  <br>
    <strong style="color: red">The data resources you have is, "what do you have in order to navigate the hypothesis set?". Let's pick a hypothesis set that we can afford to navigate. That is the game in learning. Done with the bias and variance.</strong></p>


5. **Measuring the Bias and Variance:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}  
    * __Training Error__: reflects Bias, NOT variance
    * __Test Error__: reflects Both


6. **Reducing the Bias and Variance, and Irreducible Err:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents36}  
    * __Adding Good Feature__:  
        * Decrease Bias  
    * __Adding Bad Feature__:  
        * Doesn't affect (increase) Bias much  
    * __Adding ANY Feature__:  
        * Increases Variance  
    * __Adding more Data__:  
        * Decreases Variance
        * (May) Decreases Bias: if $$h$$ can fit $$f$$ exactly.  
    * __Noise in Test Set__:  
        * Affects ONLY Irreducible Err
    * __Noise in Training Set__:  
        * Affects BOTH and ONLY Bias and Variance  
    * __Dimensionality Reduction__:  
        * Decrease Variance (by simplifying models)  
    * __Feature Selection__:  
        * Decrease Variance (by simplifying models)  
    * __Regularization__:  
        * Increase Bias
        * Decrease Variance
    * __Increasing # of Hidden Units in ANNs__:  
        * Decrease Bias
        * Increase Variance  
    * __Increasing # of Hidden Layers in ANNs__:  
        * Decrease Bias  
        * Increase Variance  
    * __Increasing $$k$$ in K-NN__:  
        * Increase Bias
        * Decrease Variance  
    * __Increasing Depth in Decision-Trees__:  
        * Increase Variance  
    * __Boosting__:  
        * Decreases Bias  
    * __Bagging__:  
        * Reduces Variance              

    * We __Cannot Reduce__ the __Irreducible Err__  
            
            



7. **Application of the Decomposition to Classification:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents37}  
    A similar decomposition exists for:  
    * Classification w/ $$0-1$$ loss  
    * Probabilistic Classification w/ Squared Error  

8. **Bias-Variance Decomposition and Risk (Excess Risk Decomposition):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents38}  
    The __Bias-Variance Decomposition__ analyzes the behavior of the *__Expected Risk/Generalization Error__* for any function $$\hat{f}$$:  
    <p>$$R(\hat{f}) = \mathrm{E}\left[(y-\hat{f}(x))^{2}\right]=(\operatorname{Bias}[\hat{f}(x)])^{2}+\operatorname{Var}[\hat{f}(x)]+\sigma^{2}$$</p>  
    Assuming that $$y = f(x) + \epsilon$$.  

    The __Bayes Optimal Predictor__ is $$f(x) = \mathrm{E}[Y\vert X=x]$$.  

    The __Excess Risk__ is:  
    <p>$$\text{ExcessRisk}(\hat{f}) = R(\hat{f}) - R(f)$$</p>  

    __Excess Risk Decomposition:__{: style="color: red"}  
    We add and subtract the __target function $$f_{\text{target}}=\inf_{h \in \mathcal{H}} I[h]$$__ that minimizes the __(true) expected risk__:  
    <p>$$\text{ExcessRisk}(\hat{f}) = \underbrace{\left(R(\hat{f}) - R(f_{\text{target}})\right)}_ {\text { estimation error }} + \underbrace{\left(R(f_{\text{target}}) - R(f)\right)}_ {\text { approximation error }}$$</p>  



    The __Bias-Variance Decomposition__ for *__Excess Risk:__*  
    * Re-Writing __Excess Risk__:  
        <p>$$\text{ExcessRisk}(\hat{f}) = R(\hat{f}) - R(f) = \mathrm{E}\left[(y-\hat{f}(x))^{2}\right] - \mathrm{E}\left[(y-f(x))^{2}\right]$$</p>  
        which is equal to:  
        <p>$$R(\hat{f}) - R(f) = \mathrm{E}\left[(f(x)-\hat{f}(x))^{2}\right]$$</p>  
    <p>$$\mathrm{E}\left[(f(x)-\hat{f}(x))^{2}\right] = (\operatorname{Bias}[\hat{f}(x)])^{2}+\operatorname{Var}[\hat{f}(x)]$$</p>  

    * if you dont want to mess with stat-jargon; lemme rephrase:  
        is the minimizer $${\displaystyle f=\inf_{h \in \mathcal{H}} I[h]}$$ where $$I[h]$$ is the expected-risk/generalization-error (assume MSE);  
        is it $$\overline{f}(\mathbf{x})=\mathbb{E}_ {\mathcal{D}}\left[f^{(\mathcal{D})}(\mathbf{x})\right]$$ the average hypothesis over all realizations of $$N$$ data-points $$\mathcal{D}_ i$$??  


***

## Generalization Theory
{: #content4}

* [Generalization Bounds: PAC-Bayes, Rademacher, ERM, etc. (Notes!)](https://www.cs.princeton.edu/courses/archive/fall17/cos597A/lecnotes/generalize.pdf)  
* [Deep Learning Generalization (blog!)](http://www.offconvex.org/2017/12/08/generalization1/)  

1. **Generalization Theory:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    

    __Approaches to (Notions of) Quantitative Description of Generalization Theory:__{: style="color: red"}  
    {: #lst-p}
    * VC Dimension
    * Rademacher Complexity
    * PAC-Bayes Bound  



    __Prescriptive vs Descriptive Theory:__{: style="color: red"}  
    {: #lst-p}
    * __Prescriptive__: only attaches a label to the problem, without giving any insight into how to solve the problem.  
    * __Descriptive__: describes the problem in detail (e.g. by providing cause) and allows you to solve the problem.  

    Generalization Theory Notions consist of attaching a descriptive label to the basic phenomenon of lack of generalization. They are hard to compute for today‚Äôs complicated ML models, let alone to use as a guide in designing learning systems.  
    __Generalization Bounds as Descriptive Labels:__{: style="color: red"}  
    {: #lst-p}
    * __Rademacher Complexity__:  
        <button>Assumptions</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
        * labels and loss are 0,1,  
        * the badly generalizing $$h$$ predicts perfectly on the training sample $$S$$ and  
            is completely wrong on the heldout set $$S_2$$, meaning:  
            <p>$$\Delta_{S}(h)-\Delta_{S_{2}}(h) \approx-1$$</p>  
        {: hidden=""}
    <br>
        


4. **Overfitting:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents44}  
    One way to summarize __Overfitting__ is:  
    <span>discovering patterns in data that do not exist in the intended application</span>{: style="color: purple"}.  
    The typical case of overfitting "<span>decreasing loss only on the training set and increasing the loss on the validation set</span>{: style="color: purple"}" is only one example of this.  

    The question then is how we prevent overfitting from occurring.  
    The problem is that <span>we cannot know apriori what patterns will generalize from the dataset</span>{: style="color: purple"}.  


    __No-Free-Lunch (NFL) Theorem:__{: style="color: red"}  
    <button>__Explanation through a thought experiment:__</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    * Suppose you see someone toss a coin and get heads. What is the probability distribution over the next result of the coin toss?  
        Did you think heads 50% and tails 50%? If so, you‚Äôre wrong: the answer is that we don‚Äôt know. For all we know, the coin could have heads on both sides. Or, it might even obey some strange laws of physics and come out as tails every second toss. The point is, there is no way we can extract only patterns that will generalize: there will always be some scenarios where those patterns will not exist. This is the essence of what is called the No Free Lunch Theorem: any model that you create will always ‚Äúoverfit‚Äù and be completely wrong in some scenarios.  
    {: hidden=""}  


    <br>



[^1]: with respect to a particular problem.  
[^2]: Note that Abu-Mostafa defines _out-sample error $$E_{\text{out}}$$_ as the _expected error/risk $$I[f]$$_; thus making $$G = E_{\text{out}} - E_{\text{in}}$$.  
[^3]: can be viewed as a measure of the __average network approximation error__ _over all possible training data sets $$\mathcal{D}$$_   

***
***

TITLE: Regularization
LINK: research/dl/theory/regularization.md



__Resources:__{: style="color: red"}  
{: #lst-p}
* [Regularization in FFN](/work_files/research/dl/nlp/dl_book_pt1#bodyContents133)  
* [Regularization Concept](/concepts_#bodyContents616)  
* [Regularization Ch.7 Summary](https://medium.com/inveterate-learner/deep-learning-book-chapter-7-regularization-for-deep-learning-937ff261875c)  
* [How Regularization Reduces Variance from bias-var-decomp](http://cs229.stanford.edu/notes-spring2019/addendum_bias_variance.pdf)  
* [Probabilistic Interpretation of Regularization (MAP)](http://bjlkeng.github.io/posts/probabilistic-interpretation-of-regularization)  
* [The Math of Regularization](https://www.wikiwand.com/en/Regularization_(mathematics))  
* [Regularization from excess risk](https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/20.pdf)  
* [Bayesian Interpretation of Regularization (wikipedia)](https://en.wikipedia.org/wiki/Bayesian_interpretation_of_kernel_regularization)  
* [Regularization Methods Papers Explained](http://www.isikdogan.com/blog/regularization.html)  
* [Improving Model Generalization (book)](https://srdas.github.io/DLBook/ImprovingModelGeneralization.html)  



## Regularization Basics and Definitions
{: #content1}

1. **Regularization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}    
    __Regularization__ can be, loosely, defined as: any modification we make to a learning algorithm that is intended to _reduce_ its _generalization error_ but not its _training error_.  

    Formally, it is a set of techniques that impose certain restrictions on the hypothesis space (by adding information) in order to solve an __ill-posed__ problem or to prevent __overfitting__.[^1]  
    <br>


2. **Theoretical Justification for Regularization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}    
    A theoretical justification for regularization is that it attempts to impose Occam's razor on the solution.  
    From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters.  

    __Regularization from the NFL Theorem(s):__{: style="color: red"}  
    {: #lst-p}
    * The __No Free Lunch Theorem__:  
        The following statement is __FALSE__:  
        <span>"Given a number of points (and a confidence), we can always achieve a prescribed error."</span>{: style="color: purple"}  
    * __Interpretation:__ inference from finite samples can be effectively performed if and only if the problem satisfies some a priori condition.  
    * __Implications__:  
        * This implies that there is no silver bullet: we shouldn't expect any single optimization method to be perfect for all problems. Rather, we should try to design optimization methods that are tailored to the problem we're trying to solve.  
            e.g. if you want to use local search, you'll probably need to define a neighborhood relation that is informed by the problem domain.  
        * A practical implication is that machine learning won't work if there is no structure at all on the space of possible models/hypotheses.  
            Instead, we need some kind of prior that makes some models more likely than others.  
    * __Implying Regularization__:  
        One of the most used __priors__ is __Occam's Razor__{: style="color: goldenrod"} which assumes that "simpler" models are more likely than complex one's.  
        This leads to use of regularization in machine learning, as it effectively applies Occam's razor to candidate models.[^6]  
    * __Summary:__ NFL Theorem shows that regularization strategies are _necessary_ to obtain <span>good generalization</span>{: style="color: purple"}.  
    * [Ref](https://cs.stackexchange.com/questions/88192/are-the-no-free-lunch-theorems-useful-for-anything)  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * Regularization emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting. - [wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network#Generalization_and_statistics)
    <br>

    <br>

3. **Regularization in Deep Learning:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    In the context of DL, most regularization strategies are based on __regularizing estimators__, which usually works by _trading increased bias for reduced variance_.  

    An effective regularizer is one that makes a profitable trade, reducing variance significantly while not overly increasing the bias.
    <br>

4. **Regularization and Data Domains in DL - A Practical Motivation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    Most applications of DL are to domains where the true data-generating process is almost certainly outside the model family (hypothesis space). Deep learning algorithms are typically applied to extremely complicated domains such as images, audio sequences and text, for which the true generation process essentially involves simulating the entire universe.  

    Thus, controlling the complexity of the mdoel is not a simple matter of finding the model of the right size, with the right number of parameters; instead, the best fitting model (wrt. generalization error) is a large model that has been regularized appropriately.  


[^1]: Where we (Hadamard) define __Well-Posed Problems__ as having the properties (1) A Solution Exists (2) It is Unique (3) It's behavior changes continuously with the initial conditions.  

***

## Parameter Norm Penalties
{: #content2}

1. **Parameter Norms:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    Many regularization approaches are based on limiting the capacity of models by adding a parameter norm penalty $$\Omega(\boldsymbol{\theta})$$ to the objective function $$J$$. We denote the regularized objective function by $$\tilde{J}$$:  
    <p>$$\tilde{J}(\boldsymbol{\theta} ; \boldsymbol{X}, \boldsymbol{y})=J(\boldsymbol{\theta} ; \boldsymbol{X}, \boldsymbol{y})+\alpha \Omega(\boldsymbol{\theta}) \tag{7.1}$$</p>  
    where $$\alpha \in[0, \infty)$$ is a HP that weights the relative contribution of the norm penalty term, $$\Omega$$, relative to the standard objective function $$J$$.  
    * __Effects of $$\alpha$$__:  
        * $$\alpha = 0$$ results in NO regularization
        * Larger values of $$\alpha$$ correspond to MORE regularization

    The __effect of minimizing the regularized objective function__ is that it will *__decrease__*, both, _the original objective $$J$$_ on the training data and some _measure of the size of the parameters $$\boldsymbol{\theta}$$_.  

    Different choices for the parameter norm $$\Omega$$ can result in different solutions being preferred.  
    <br>


2. **Parameter Penalties and the Bias parameter:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    In NN, we usually penalize __only the weights__ of the affine transformation at each layer and we leave the __biases unregularized__.  
    Biases typically require less data than the weights to fit accurately. The reason is that _each weight specifies how TWO variables interact_ so fitting the weights well, requires observing both variable sin a variety of conditions. However, _each bias controls only a single variable_, thus, we dont induce too much _variance_ by leaving the biases unregularized. If anything, regularizing the bias can introduce a significant amount of _underfitting_.  
    <br>


3. **Note on the $$\alpha$$ parameter for different hidden layers:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    In the context of neural networks, it is sometimes desirable to use a separate penalty with a different $$\alpha$$ coefficient for each layer of the network. Because it can be expensive to search for the correct value of multiple hyperparameters, it is still reasonable to use the same weight decay at all layers just to reduce the size of search space.  
    <br>


4. **$$L^2$$ Parameter Regularization (Weight Decay):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    It is a regularization strategy that _drives the weights closer to the origin_[^2] by adding a regularization term:  
    <p>$$\Omega(\mathbf{\theta}) = \frac{1}{2}\|\boldsymbol{w}\|_ {2}^{2}$$</p>  
    to the objective function.  
    
    In statistics, $$L^2$$ regularization is also known as __Ridge Regression__ or __Tikhonov Regularization__.  

    __Analyzing Weight Decay:__{: style="color: red"}  
    <button>Show Analysis</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    * __What happens in a Single Step__:  
        We can gain some insight into the behavior of weight decay regularization by studying the gradient of the regularized objective function.  
        Take the models objective function:  
        <p>$$\tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=\frac{\alpha}{2} \boldsymbol{w}^{\top} \boldsymbol{w}+J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y}) \tag{7.2}$$</p>  
        with the corresponding _parameter gradient_:  
        <p>$$\nabla_{\boldsymbol{w}} \tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=\alpha \boldsymbol{w}+\nabla_{\boldsymbol{w}} J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y}) \tag{7.3}$$</p>  
        The gradient descent update:  
        <p>$$\boldsymbol{w} \leftarrow \boldsymbol{w}-\epsilon\left(\alpha \boldsymbol{w}+\nabla_{\boldsymbol{w}} J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})\right) \tag{7.4}$$</p>  
        Equivalently:  
        <p>$$\boldsymbol{w} \leftarrow(1-\epsilon \alpha) \boldsymbol{w}-\epsilon \nabla_{\boldsymbol{w}} J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y}) \tag{7.5}$$</p>    

        Observe that the addition of the weight decay term has modified the learning rule to __multiplicatively shrink the weight vector by  a constant factor on each step__, just before performing the usual gradient update.  

    * __What happens over the Entire course of training__:  
        We simplify the analysis by making a quadratic (2nd-order Taylor) approximation to the objective function in the neighborhood of the optimal wight-parameter of the unregularized objective $$\mathbf{w}^{\ast} = \arg \min_{\boldsymbol{w}} J(\boldsymbol{w})$$.[^3]  
        The approximation $$\hat{J}$$:  
        <p>$$\hat{J}(\boldsymbol{\theta})=J\left(\boldsymbol{w}^{\ast}\right)+\frac{1}{2}\left(\boldsymbol{w}-\boldsymbol{w}^{\ast}\right)^{\top} \boldsymbol{H}(J(\boldsymbol{w}^{\ast}))\left(\boldsymbol{w}-\boldsymbol{w}^{\ast}\right)  \tag{7.6}$$</p>  
        where $$\boldsymbol{H}$$ is the Hessian matrix of $$J$$ with respect to $$\mathbf{w}$$ evaluated at $$\mathbf{w}^{\ast}$$.  

        __Notice:__  
        * There is no first-order term in this quadratic approximation, because $$\boldsymbol{w}^{\ast}$$  is defined to be a minimum, where the gradient vanishes.  
        * Because $$\boldsymbol{w}^{\ast}$$ is the location of a minimum of $$J$$, we can conclude that $$\boldsymbol{H}$$ is __positive semidefinite__.  

        The __gradient__ of $$\hat{J} + \Omega(\mathbf{\theta})$$:  
        <p>$$\nabla_{\boldsymbol{w}} \hat{J}(\boldsymbol{w})=\boldsymbol{H}(J(\boldsymbol{w}^{\ast}))\left(\tilde{\boldsymbol{w}}-\boldsymbol{w}^{\ast}\right) + \alpha \tilde{\boldsymbol{w}} \tag{7.7}$$</p>  
        And the __minimum__ is achieved at $$\nabla_{\boldsymbol{w}} \hat{J}(\boldsymbol{w}) = 0$$:  
        <p>$$\tilde{\boldsymbol{w}}=(\boldsymbol{H}+\alpha \boldsymbol{I})^{-1} \boldsymbol{H} \boldsymbol{w}^{\ast} \tag{7.10}$$</p>  

        __Effects:__  
        * As $$\alpha$$ approaches $$0$$: the regularized solution $$\tilde{\boldsymbol{w}}$$ approaches $$\boldsymbol{w}^{\ast}$$.  
        * As $$\alpha$$ grows: we apply __spectral decomposition__ to the __real and symmetric__ $$\boldsymbol{H} = \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{\top}$$:  
            <p>$$\begin{aligned} \tilde{\boldsymbol{w}} &=\left(\boldsymbol{Q} \mathbf{\Lambda} \boldsymbol{Q}^{\top}+\alpha \boldsymbol{I}\right)^{-1} \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{\top} \boldsymbol{w}^{\ast} \\ &=\left[\boldsymbol{Q}(\boldsymbol{\Lambda}+\alpha \boldsymbol{I}) \boldsymbol{Q}^{\top}\right]^{-1} \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{\top} \boldsymbol{w}^{\ast} \\ &=\boldsymbol{Q}(\boldsymbol{\Lambda}+\alpha \boldsymbol{I})^{-1} \boldsymbol{\Lambda} \boldsymbol{Q}^{\top} \boldsymbol{w}^{\ast} \end{aligned} \tag{7.13}$$</p>  

        Thus, we see that the effect of weight decay is to rescale $$\boldsymbol{w}^{\ast}$$ along the axes defined by the eigenvector of $$\boldsymbol{H}$$. Specifically, the component of $$\boldsymbol{w}^{\ast}$$ that is aligned with the $$i$$-th eigenvector of $$\boldsymbol{H}$$  is rescaled by a factor of $$\frac{\lambda_{i}}{\lambda_{i}+\alpha}$$.  

        ![img](/main_files/dl_book/regularization/1.png){: width="100%"}   

        __Summary:__  

        \vert __Condition__|__Effect of Regularization__ \vert   
        \vert $$\lambda_{i}>>\alpha$$ \vert Not much \vert  
        \vert $$\lambda_{i}<<\alpha$$ \vert The weight value almost shrunk to $$0$$ \vert  

    * __Applying $$L^2$$ regularization to *Linear Regression* :__  
        * <button>Application to Linear Regression</button>{: .showText value="show" onclick="showTextPopHide(event);"}
            ![img](/main_files/dl_book/regularization/2.png){: width="100%" hidden=""}   
    * [Weight Decay Analysis (blog)](https://h1man5hu.github.io/regularization/understanding-the-mathematics-of-weight-decay/)  
    {: hidden=""}  
    <br>

    __$$L^2$$ Regularization Derivation:__{: style="color: red"}  
    $$L^2$$ regularization is equivalent to __MAP Bayesian inference with a Gaussian prior on the weights__.  

    __The MAP Estimate:__  
    <button>Show MAP Estimate Derivation</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    <p hidden="">$$\begin{aligned} \hat{\theta}_ {\mathrm{MAP}} &=\arg \max_{\theta} P(\theta \vert y) \\ &=\arg \max_{\theta} \frac{P(y \vert \theta) P(\theta)}{P(y)} \\ &=\arg \max_{\theta} P(y \vert \theta) P(\theta) \\ &=\arg \max_{\theta} \log (P(y \vert \theta) P(\theta)) \\ &=\arg \max_{\theta} \log P(y \vert \theta)+\log P(\theta) \end{aligned}$$</p>  
    
    We place a __Gaussian Prior__ on the weights, with __zero mean__ and __equal variance $$\tau^2$$__:  
    <p>$$\begin{aligned} \hat{\theta}_ {\mathrm{MAP}} &=\arg \max_{\theta} \log P(y \vert \theta)+\log P(\theta) \\ &=\arg \max _{\boldsymbol{w}}\left[\log \prod_{i=1}^{n} \dfrac{1}{\sigma \sqrt{2 \pi}} e^{-\dfrac{\left(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_i\right)^{2}}{2 \sigma^{2}}}+\log \prod_{j=0}^{p} \dfrac{1}{\tau \sqrt{2 \pi}} e^{-\dfrac{w_{j}^{2}}{2 \tau^{2}}} \right] \\ &=\arg \max _{\boldsymbol{w}} \left[-\sum_{i=1}^{n} \dfrac{\left(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_i\right)^{2}}{2 \sigma^{2}}-\sum_{j=0}^{p} \dfrac{w_{j}^{2}}{2 \tau^{2}}\right] \\ &=\arg \min_{\boldsymbol{w}} \dfrac{1}{2 \sigma^{2}}\left[\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_i\right)^{2}+\dfrac{\sigma^{2}}{\tau^{2}} \sum_{j=0}^{p} w_{j}^{2}\right] \\ &=\arg \min_{\boldsymbol{w}} \left[\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_i\right)^{2}+\lambda \sum_{j=0}^{p} w_{j}^{2}\right] \\ &= \arg \min_{\boldsymbol{w}} \left[ \|XW - \boldsymbol{y}\|^2 + \lambda {\|\boldsymbol{w}\|_ 2}^2\right]\end{aligned}$$</p>  
    <button>Different Notation</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](/main_files/dl_book/regularization/4.png){: width="100%" hidden=""}   
    <br>

    __Properties:__{: style="color: red"}  
    {: #lst-p}
    * Notice that L2-regularization has a rotational invariance. This actually makes it more sensitive to irrelevant features.  [\[Ref\]](https://www.cs.ubc.ca/~schmidtm/Courses/540-W18/L6.pdf)  
        > [Paper](https://icml.cc/Conferences/2004/proceedings/papers/354.pdf)  
    * Adding L2-regularization to a convex function gives a strongly-convex function. So L2-regularization can make gradient descent converge much faster.  (^ same ref)      

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * [L2-reg and Adversarial Examples (New Angle)](https://thomas-tanay.github.io/post--L2-regularization/)  
    * [Weight Decay Analysis (blog)](https://h1man5hu.github.io/regularization/understanding-the-mathematics-of-weight-decay/)  
    <br>

5. **$$L^1$$ Regularization:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    $$L^1$$ Regularization is another way to regulate the model by _penalizing the size of its parameters_; the technique adds a regularization term:  
    <p>$$\Omega(\boldsymbol{\theta})=\|\boldsymbol{w}\|_{1}=\sum_{i}\left|w_{i}\right| \tag{7.18}$$</p>  
    which is a sum of absolute values of the individual parameters.  

    The regularized objective function is given by:  
    <p>$$\tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=\alpha\|\boldsymbol{w}\|_ {1}+J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y}) \tag{7.19}$$</p>  
    with the corresponding (sub) gradient:  
    <p>$$\nabla_{\boldsymbol{w}} \tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=\alpha \operatorname{sign}(\boldsymbol{w})+\nabla_{\boldsymbol{w}} J(\boldsymbol{X}, \boldsymbol{y} ; \boldsymbol{w}) \tag{7.20}$$</p>  

    Notice that the regularization contribution to the gradient, __no longer scales linearly with each $$w_i$$__; instead it is a __constant factor with a sign = $$\text{sign}(w_i)$$__.  

    \[Analysis\]  

    __Sparsity of the $$L^1$$ regularization:__  
    In comparison to $$L^2$$, $$L^1$$ regularization results in a solution that is more __sparse__.  
    The _sparsity property_ has been used extensively as a __feature selection__ mechanism.  
    * __LASSO__: The Least Absolute Shrinkage and Selection Operator integrates an $$L^1$$ penalty with a _linear model_ and a _least-squares cost function_.  
        The $$L^1$$ penalty causes a subset of the weights to become __zero__, suggesting that the corresponding features may safely be discarded.  

    __$$L^1$$ Regularization Derivation:__{: style="color: red"}  
    $$L^1$$ regularization is equivalent to (the log-prior term in) __MAP Bayesian inference with an isotropic Laplace distribution prior on the weights__:  
    <p>$$\log p(\boldsymbol{w})=\sum_{i} \log \operatorname{Laplace}\left(w_{i} ; 0, \frac{1}{\alpha}\right)=-\alpha\|\boldsymbol{w}\|_ {1}+n \log \alpha-n \log 2 \tag{7.24}$$</p>  
    note that we can ignore the terms $$\log \alpha-\log 2$$ because they do not depend on $$\boldsymbol{w}$$.      
    <button>Derivation</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    <p hidden="">$$\begin{aligned} \hat{\theta}_ {\mathrm{MAP}} &=\arg \max_{\theta} \log P(y \vert \theta)+\log P(\theta) \\  &=\arg \max _{\boldsymbol{w}}\left[\log \prod_{i=1}^{n} \dfrac{1}{\sigma \sqrt{2 \pi}} e^{-\dfrac{\left(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_i\right)^{2}}{2 \sigma^{2}}}+\log \prod_{j=0}^{p} \dfrac{1}{2 b} e^{-\dfrac{\left|\theta_{j}\right|}{2 b}} \right] \\    &=\arg \max _{\boldsymbol{w}} \left[-\sum_{i=1}^{n} \dfrac{\left(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_i\right)^{2}}{2 \sigma^{2}}-\sum_{j=0}^{p} \dfrac{\left|w_{j}\right|}{2 b}\right] \\    &=\arg \min_{\boldsymbol{w}} \dfrac{1}{2 \sigma^{2}}\left[\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_i\right)^{2}+\dfrac{\sigma^{2}}{b} \sum_{j=0}^{p}\left|w_{j}\right|\right] \\    &=\arg \min_{\boldsymbol{w}} \left[\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_i\right)^{2}+\lambda \sum_{j=0}^{p}\left|w_{j}\right|\right] \\    &= \arg \min_{\boldsymbol{w}} \left[ \|XW - \boldsymbol{y}\|^2 + \lambda \|\boldsymbol{w}\|_ 1\right]\end{aligned}$$</p>

    __Properties:__{: style="color: red"}  
    {: #lst-p}
    * $$L^1$$ regularization can occasionally produce non-unique solutions. A simple example is provided in the figure when the space of possible solutions lies on a 45 degree line. 
    <br>


6. **$$L^1$$ VS $$L^2$$ Regularization:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    
    
    * __Feature Correlation and Sparsity__:  
        * __Identical features__:   
            * $$L^1$$ regularization spreads weight arbitrarily (all weights same sign) 
            * $$L^2$$ regularization spreads weight evenly 
        * __Linearly related features__:   
            * $$L^1$$ regularization chooses variable with larger scale, $$0$$ weight to others  
            * $$L^2$$ prefers variables with larger scale ‚Äî spreads weight proportional to scale  
        > [Reference](https://www.youtube.com/watch?v=KIoz_aa1ed4&list=PLnZuxOufsXnvftwTB1HL6mel1V32w0ThI&index=7)  

    
    __Interpreting Sparsity with an Example:__{: style="color: red"}  
    Let's imagine we are estimating two coefficients in a regression. In $$L^2$$ regularization, the solution $$\boldsymbol{w} =(0,1)$$ has the same weight as $$\boldsymbol{w}=(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})$$  so they are both treated equally. In $$L^1$$ regularization, the same two solutions favor the sparse one:  
    <p>$$\|(1,0)\|_{1}=1<\left\|\left(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}\right)\right\|_{1}=\sqrt{2}$$</p>  
    So $$L^2$$ regularization doesn't have any specific built in mechanisms to favor zeroed out coefficients, while $$L^1$$ regularization actually favors these sparser solutions.  
    > [Extensive Discussions on Sparsity (Quora)](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when)  

    <br>


__Notes:__{: style="color: red"}  
* __Elastic Net Regularization:__  
    <p>$$\Omega = \lambda\left(\alpha\|w\|_{1}+(1-\alpha)\|w\|_{2}^{2}\right), \alpha \in[0,1]$$</p>  
    * Combines both $$L^1$$ and $$L^2$$  
    * Used to __produce sparse solutions__, but to avoid the problem of $$L^1$$ solutions being sometimes __Non-Unique__  
        * The problem mainly arises with __correlated features__  
    * Elastic net regularization tends to have a grouping effect, where correlated input features are assigned equal weights.  



[^2]: More generally, we could regularize the parameters to be near any specific point in space and, surprisingly, still get a regularization effect, but better results will be obtained for a value closer to the true one, with zero being a default value that makes sense when we do not know if the correct value should be positive or negative.  

[^3]: The approximation is perfect if the objective function is truly quadratic, as in the case of __linear regression w/ MSE__.  

***

## Advanced Regularization Techniques
{: #content3}

1. **Regularization and Under-Constrained Problems:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    In some cases, regularization is necessary for machine learning problems to be properly define.  

    Many linear models (e.g. Linear Regression, PCA) depend on __inverting $$\boldsymbol{X}^{\top}\boldsymbol{X}$$__. This is not possible if $$\boldsymbol{X}^{\top}\boldsymbol{X}$$ is singular. In this case, many forms of regularization correspond to solving inverting $$\boldsymbol{X}^{\top} \boldsymbol{X}+\alpha \boldsymbol{I}$$ instead. This regularized matrix is __guaranteed to be invertible__.  
    * $$\boldsymbol{X}^{\top}\boldsymbol{X}$$ can be singular if:  
        * The data-generating function truly has no variance in some direction.  
        * No Variance is _observed_ in some direction because there are fewer examples (rows of $$\boldsymbol{X}$$) than input features (columns).  

    Models with no closed-form solution can, also, be _underdetermined_:  
    Take __logistic regression on a linearly separable dataset__, if a weight vector $$\boldsymbol{w}$$ is able to achieve perfect classification, then so does $$2\boldsymbol{w}$$ but with even __higher likelihood__. Thus, an iterative optimization procedure (sgd) will continually increase the magnitude of $$\boldsymbol{w}$$ and, in theory, will __never halt__.  
    We can use regularization to guarantee the convergence of iterative methods applied to underdetermined problems: e.g. __weight decay__ will cause gradient descent to _quit increasing the magnitude of the weights when the **slope of the likelihood is equal to the weight decay coefficient**_.  

    __Linear Algebra Perspective:__  
    Given that the __Moore-Penrose pseudoinverse__ $$\boldsymbol{X}^{+}$$ of a matrix $$\boldsymbol{X}$$ can solve underdetermined linear equations:  
    <p>$$\boldsymbol{X}^{+}=\lim_{\alpha \searrow 0}\left(\boldsymbol{X}^{\top} \boldsymbol{X}+\alpha \boldsymbol{I}\right)^{-1} \boldsymbol{X}^{\top} \tag{7.29}$$</p>  
    we can now recognize the equation as __performing linear regression with weight-decay__.  
    Specifically, $$7.29$$ is the limit of eq $$7.17$$ as the _regularization coefficient shrinks to zero_.  
    We can thus interpret the pseudoinverse as __stabilizing underdetermined problems using regularization__.  

    __The Pseudoinverse:__{: style="color: red"}  
    When applied to underdetermined systems w/ non-unique solutions; It finds the minimum norm solution to a linear system.  
    This "OLS" solution implies that not all linear functions are the same for OLS. It restricts the space of all possible non-unique linear functions that satisfy the equation to a subset of minimal norm.  
    From __SLT__ perspective, the pseudoinverse introduces __bias__ towards certain solutions.  
    <br>

2. **Dataset Augmentation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    Having more data is the most desirable thing to improving a machine learning model‚Äôs performance. In many cases, it is relatively easy to artificially generate data.  
    * __Applications__: for certain problems like __classification__ this approach is readily usable. E.g. for a classification task, we require the model to be _invariant to certain types of transformations_, of which we can generate data by applying them on our current dataset.  
        The most successful application of data-augmentation has been in __object recognition__.  
    * __Non-Applicable__: this approach is not applicable to many problems, especially those that require us to learn the true data-distribution first E.g. Density Estimation.  

    __Noise Injection as Data-Augmentation:__{: style="color: red"}  
    Injecting noise in the _input_ to a NN _(Siestma and Dow, 1991)_ can also be seen as a form of data augmentation.  
    * __Motivation:__  
        * For many classification and (some) regression tasks: the task should be possible to solve even if small random noise is added to the input [(Local Constancy)](/work_files/research/dl/theory/dl_book_pt1#bodyContents32)  
        * Moreover, NNs prove not to be very robust to noise.  

    __Injecting Noise in the Hidden Units:__  
    It can be seen as doing data-augmentation at *__multiple levels of abstraction__*. This approach can be highly effective provided that the magnitude of the noise is carefully tuned _(Poole et al. 2014)_.  
    > __Dropout__ can be seen as a process of constructing new inputs by _multiplying_ by noise.  

    <br>

3. **Noise Robustness:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    We can apply __Noise Injection__ to different components of the model as a way to regularize the model:  
    __Injecting Noise in the Input Layer:__{: style="color: red"}  
    {: #lst-p}
    * __Motivation__:  
        We have motivated the injection of noise, to the inputs, as a dataset augmentation strategy.        
    * __Interpretation__:  
        For some models, the addition of noise with infinitesimal variance at the input of the model is equivalent to __imposing a penalty on the norm of the weights__ _(Bishop, 1995a,b)_.  

    __Injecting Noise in the Hidden Layers:__{: style="color: red"}  
    {: #lst-p}
    * __Motivation__:  
        We can motivate it as a variation of data augmentation.  
    * __Interpretation__:  
        It can be seen as doing __data-augmentation__ at *__multiple levels of abstraction__*.  
    * __Applications__:  
        The most successful application of this type of noise injection is __Dropout__.  
        It can be seen as a process of constructing new inputs by _multiplying_ by noise.  

    __Injecting Noise in the Weight Matrices:__{: style="color: red"}  
    {: #lst-p}
    * __Interpretation__:  
        1. It can be interpreted as a stochastic implementation of Bayesian inference over the weights.  
            * __The Bayesian View__:  
                The Bayesian treatment of learning would consider the model weights to be _uncertain and representable via a probability distribution that reflects this uncertainty_. Adding noise to the weights is a practical, stochastic way to reflect this uncertainty.  
        2. It can, also, be interpreted as equivalent a more traditional form of regularization, _encouraging stability of the function to be learned_.  
            * <button>Analysis</button>{: .showText value="show" onclick="showTextPopHide(event);"}
                ![img](/main_files/dl_book/regularization/3.png){: width="100%" hidden=""}   
    * __Applications__:  
        This technique has been used primarily in the context of __recurrent neural networks__ _(Jim et al., 1996; Graves, 2011)_.  

    __Injecting Noise in the Output Layer:__{: style="color: red"}  
    {: #lst-p}
    * __Motivation__:  
        * Most datasets have some number of mistakes in the $$y$$ labels. It can be harmful to maximize $$\log p(y \vert \boldsymbol{x})$$ when $$y$$ is a mistake. One way to prevent this is to explicitly model the noise on the labels.  
        One can assume that for some small constant $$\epsilon$$, the training set label $$y$$ is correct with probability $$1-\epsilon$$.  
            This assumption is easy to incorporate into the cost function analytically, rather than by explicitly drawing noise samples (e.g. __label smoothing__).  
        * MLE with a softmax classifier and hard targets may never converge - the softmax can never predict a probability of exactly $$0$$ or $$1$$, so it will continue to learn larger and larger weights, making more extreme predictions forever.{: #bodyContents33mle}  
    * __Interpretation__:  
        For some models, the addition of noise with infinitesimal variance at the input of the 
    * __Applications__:  
        __Label Smoothing__ regularizes a model based on a softmax with $$k$$ output values by replacing the hard $$0$$ and $$1$$ classification targets with targets of $$\dfrac{\epsilon}{k-1}$$ and $$1-\epsilon$$, respectively.   
        * [__Applied to MLE problem:__](#bodyContents33mle) Label smoothing, compared to weight-decay, has the advantage of preventing the pursuit of hard probabilities without discouraging correct classification.  
        * Application in modern NN: _(Szegedy et al. 2015)_ 

    <br>

4. **Semi-Supervised Learning:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    __Semi-Supervised Learning__ is a class of ML tasks and techniques that makes use of both unlabeled examples from $$P(\mathbf{x})$$ and labeled examples from $$P(\mathbf{x}, \mathbf{y})$$ to estimate $$P(\mathbf{y} \vert \mathbf{x})$$ or predict $$\mathbf{y}$$ from $$\mathbf{x}$$.  

    In the context of Deep Learning, Semi-Supervised Learning usually refers to _learning a representation $$\boldsymbol{h}=f(\boldsymbol{x})$$_; the goal being to learn a representation such that __examples from the same class have similar representations__.   
    Usually, __Unsupervised Learning__ provides us clues (e.g. clustering) that influence the representation of the data.  
    > __PCA__, as a preprocessing step before applying a classifier, is a long-standing variant of this approach.  

    __Approach:__  
    Instead of separating the supervised and unsupervised criteria, we can instead have a generative model of $$P(\mathbf{x})$$ (or $$P(\mathbf{x}, \mathbf{y})$$) which shares parameters with a discriminative model $$P(\mathbf{y} \vert \mathbf{x})$$.  
    The idea is to share the unsupervised/generative criterion with the supervised criterion to _express a prior belief that the structure of $$P(\mathbf{x})$$ (or $$P(\mathbf{x}, \mathbf{y})$$) is connected to the structure of $$P(\mathbf{y} \vert \mathbf{x})$$_, which is captured by the _shared parameters_.  
    By controlling how much of the generative criterion is included in the total criterion, one can find a better trade-off than with a purely generative or a purely discriminative training criterion _(Lasserre et al., 2006; Larochelle and Bengio, 2008)_.  

    <br>    


5. **Multitask Learning:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}  
    __Multitask Learning__ is a way to improve generalization by pooling the examples (which can be seen as soft constraints imposed on the parameters) arising out of several tasks. In the same way that additional data put more pressure on the parameters of the model toward values that generalize well, when part of a model is shared across tasks, that part of the model is more constrained toward good values (assuming the sharing is justified), often yielding better generalization.  

    Improved generalization due to improved statistical strength of the shared parameters (in proportion w/ increased \# of examples). This happens only is some assumptions about the statistical relationship of the different tasks are valid; i.e. they share something.  

    From the point of view of deep learning, the underlying prior belief is the following:  
    <span>Among the factors that explain the variations observed in the data associated with the different tasks, some are shared across two or more tasks.</span>{: style="color: goldenrod"}  


    __Types:__{: style="color: red"}  
    {: #lst-p}
    * __Task-specific__: These parameters benefit only from that particular task for generalization.  
        These are the *__later layers__* in the NN.  
    * __Generic, (shared across all tasks)__: benefit from the pooled data of all the tasks.  
        These are the *__earlier layers__* in the NN.  
    <br>

6. **Early Stopping:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents36}  
    __Early Stopping:__ is a regularization method that aims to obtain a model with better validation set error ($$\implies$$ generalization) by saving the model parameters at every epoch and returning to the parameter setting at the point in time with the lowest validation set error.  


    __Premise of Early Stopping:__{: style="color: red"}  
    For a model with high representational capacity, after a certain point of time during training, the training error continues to decrease but the validation error begins to increase (overfitting). In such a scenario, a better idea would be to return back to the point where the validation error was the least.  
    ![img](https://cdn.mathpix.com/snip/images/nP6B2kfVhfI_4XZH9mE7IZUJYysgSD7xJwCHWWI1Zd8.original.fullsize.png){: width="50%" .center-image}  


    __Algorithm:__{: style="color: red"}  
    Early stopping requires the use of a __validation set__. Thus, we are not using the entire dataset for training. Choosing the ideal number of steps before we stop training can be done with cross-validation. Then, to utilize the entire dataset, a second phase of training can be done where the complete training set is used. There are two choices here:  
    {: #lst-p}
    1. Train from scratch for the same number of steps as in the Early Stopping case.  
    2. Use the weights learned from the first phase of training and _continue training_ using the complete data (not as well behaved).  


    __As Regularization:__{: style="color: red"}  
    Early Stopping affects the optimization procedure by restricting it to a small volume of the parameter space, in the neighbourhood of the initial parameter value $$\boldsymbol{\theta}_ {O}$$.  
    ![img](https://cdn.mathpix.com/snip/images/OV_Sn63r2qP3Ke35-9vIewoMdlOaLcm7OjmDy2k-awc.original.fullsize.png){: width="45%" .center-image}
    ![img](https://cdn.mathpix.com/snip/images/3GPG44-IDdIae6EJcN4XqM1xU8Tx3igHCBzrqbWFbJ4.original.fullsize.png){: width="48%" .center-image}  
    Let $$\tau$$ be the number of optimization steps taken ($$\tau$$ training iterations) with lr $$\epsilon$$. The product $$\epsilon\tau$$ can be seen as a __measure of effective capacity__{: style="color: goldenrod"}. Assuming the gradient is bounded, restricting both the number of iterations and the learning rate limits the volume of parameter space reachable from $$\boldsymbol{\theta}_ {O}$$; thus, $$\epsilon\tau$$ behaves as the reciprocal of the regularization coefficient for __weight decay__: $$\epsilon\tau \approx \dfrac{1}{\lambda}$$ $$\implies$$    
    The number of training iterations has a role *inversely proportional* to the __weight decay__ coefficient:  
    <p>$$\tau \approx \dfrac{1}{\epsilon\lambda}$$</p>    
    * When the number of iterations $$\tau$$ is small $$\iff \lambda$$ is large: Regularization is large and the capacity is small  
    * When the number of iterations $$\tau$$ is large $$\iff \lambda$$ is small: Regularization is small and the capacity is large  
    Parameter values corresponding to directions of significant curvature (of the objective function) are regularized less than directions of less curvature. Of course, in the context of early stopping, this really means that parameters that correspond to directions of significant curvature tend to learn early relative to parameters corresponding to directions of less curvature.  

    __Equivalence to Weight Decay for Linear Models:__  
    To compare with classical $$L^{2}$$ regularization, we examine a simple setting where the only parameters are linear weights $$(\boldsymbol{\theta}=\boldsymbol{w})$$. We can model the cost function $$J$$ with a quadratic approximation in the neighborhood of the empirically optimal value of the weights $$\boldsymbol{w}^{ * }$$:  
    <p>$$\hat{J}(\boldsymbol{\theta})=J\left(\boldsymbol{w}^{ * }\right)+\frac{1}{2}\left(\boldsymbol{w}-\boldsymbol{w}^{ * }\right)^{\top} \boldsymbol{H}\left(\boldsymbol{w}-\boldsymbol{w}^{ * }\right)$$</p>  
    where $$\boldsymbol{H}$$ is the Hessian matrix of $$J$$ with respect to $$\boldsymbol{w}$$ evaluated at $$\boldsymbol{w}^{ * } .$$ Given the assumption that $$\boldsymbol{w}^{ * }$$ is a minimum of $$J(\boldsymbol{w}),$$ we know that $$\boldsymbol{H}$$ is positive semidefinite.  

    <button>Derivation</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}  

    ![img](https://cdn.mathpix.com/snip/images/huUJWa3v29VPPcKVQ8YFGh87GqYWSGjlHXdJBHLLHKw.original.fullsize.png){: width="100%"}  
    ![img](https://cdn.mathpix.com/snip/images/PcTAhXy0F88H4S4L9wWe6TWynvWUd2Bqq9nig3sKujQ.original.fullsize.png){: width="100%"}  
    {: hidden=""}  



    __Properties:__{: style="color: red"}  
    {: #lst-p}
    * Early stopping is an __unobtrusive form of regularization__, in that it requires almost no change in the underlying training procedure, the objective function, or the set of allowable parameter values:  
        * This means that it is easy to use early stopping without damaging the learning dynamics. This is in contrast to weight decay, where one must be careful not to use too much weight decay and trap the network in a bad local minimum corresponding to a solution with pathologically small weights.  
    * Early Stopping does not add any extra hyperparameters, making it easy to incorporate without the need for extra tuning.  
    * It __reduces the computation cost of training__ by:  
        1. Lowering the number of training steps  
        2. Regularizing the model without having to add additional penalty terms (computation of gradients of additional terms)   
    * Early stopping may be used either alone or in conjunction with other regularization strategies. Even when using regularization strategies that modify the objective function to encourage better generalization, it is rare for the best generalization to occur at a local minimum of the training objective.  
    * It is a form of __spectral regularization__  
    * [ON EARLY STOPPING IN GRADIENT DESCENT LEARNING (paper)](http://web.mit.edu/lrosasco/www/publications/earlystop.pdf)  



    __Intuition:__{: style="color: red"}  
    (1) One way to think of early stopping is as a very efficient <span>hyperparameter selection</span>{: style="color: goldenrod"} algorithm. In this view, the _number of training steps is just another hyperparameter_. We can see that this hyperparameter has a $$U$$-shaped validation set performance curve. Most hyperparameters that control model capacity have such a $$U$$-shaped validation set performance curve. In the case of early stopping, we are <span>controlling the effective capacity of the model by determining how many steps it can take to fit the training set</span>{: style="color: goldenrod"}.   

    (2) Early stopping can be viewed as <span>regularization in time</span>{: style="color: goldenrod"}. Intuitively, a training procedure like gradient descent will tend to learn more and more complex functions as the number of iterations increases. By regularizing on time, the complexity of the model can be controlled, improving generalization.  
    
    (3) With a bounded step size, the number of steps dictates the radius ball (around your initial point) of points that is reachable. By stopping early, you limit this radius. "Weight decay is equivalent to a Lagrangian relaxation of a constraint on the weight norm. Early stopping (by previous argument) gives you the same constraint (assuming you start from 0). If you don't start from 0, you might be able to still use triangle inequality to prove it."  
    <br>

7. **Parameter Tying and Parameter Sharing:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents37}  
    Unlike the the methods focused on bringing the weights to a fixed point, e.g. 0 in the case of norm penalty, there might be situations where we might have some prior knowledge on the kind of dependencies that the model should encode.   

    __Parameter Tying:__{: style="color: red"}  
    If two tasks are similar enough (similar input/output distributions), we might want to have the parameters of both models be close to each other in value. We do this with __Parameter Tying:__ to use regularization to have two sets of parameters close to each other.  
    One way to achieve that (in an application) is to regularize the parameters of one model ($$L^2$$ norm), trained as a classifier in a supervised paradigm, to be close to the parameters of another model, trained in an unsupervised paradigm (to capture the distribution of the observed input data).[^5]  
    

    __Parameter Sharing:__{: style="color: red"}  
    The more popular way is __Parameter Sharing:__ to use constraints, <span>to force sets of parameters to be equal</span>{: style="color: goldenrod"}.  

    A significant advantage of parameter sharing over regularizing the parameters to be close (via a norm penalty) is that only a subset of the parameters (the unique set) needs to be stored in memory, e.g. in CNNs this can lead to significant reduction in the memory footprint of the model.  

    Parameter sharing has enabled CNNs to dramatically lower the number of unique model parameters and to significantly increase network sizes without requiring a corresponding increase in training data.  

    <br>

8. **Sparse Representations:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents38}  
    Another strategy is to place a __penalty on the activations__ of the units in a neural network, encouraging their activations to be sparse. This indirectly imposes a complicated penalty on the model parameters.  
    > Unlike Weight decay which acts by placing a penalty directly on the model parameters.  

    $$L^1$$ regularization induces <span>sparse parametrization</span>{: style="color: goldenrod"} i.e. sparse weights.  
    __Representational sparsity__, on the other hand, induces <span>sparse</span>{: style="color: goldenrod"} __representations__{: style="color: goldenrod"} i.e. describes a representation where many of the elements of the representation are zero (or close to zero).  

    Norm penalty regularization of representations is performed by adding to the loss function $J$ a norm penalty on the representation. This penalty is denoted $$\Omega(\boldsymbol{h}) .$$ As before, we denote the regularized loss function by $$\tilde{J}$$:  
    <p>$$\tilde{J}(\boldsymbol{\theta} ; \boldsymbol{X}, \boldsymbol{y})=J(\boldsymbol{\theta} ; \boldsymbol{X}, \boldsymbol{y})+\alpha \Omega(\boldsymbol{h})$$</p>  
    where $$\alpha \in[0, \infty)$$ weights the relative contribution of the norm penalty term, with larger values of $$\alpha$$ corresponding to more regularization.  

    The __regularizer__ $$\Omega$$:  
    {: #lst-p}
    * $$L^1$$ norm can be used  
    * Penalty derived from a Student $t$ _prior_ distribution on the representation  
    * KL-divergence penalties: especially useful for representations with elements constrained to lie on the unit interval  
    Other approaches obtain representational sparsity with a hard constraint on the activation values. For example, __orthogonal matching pursuit (OMP)__ (Pati et al, 1993) encodes an input $$x$$ with the representation $$h$$ that solves the constrained optimization problem:  
    <p>$$\underset{\boldsymbol{h},\|\boldsymbol{h}\|_ {0}< k}{\arg \min }\|\boldsymbol{x}-\boldsymbol{W} \boldsymbol{h}\|^{2}$$</p>  
    where $$\|\boldsymbol{h}\|_ {0}$$ is the number of nonzero entries of $$\boldsymbol{h}$$ . This problem can be solved efficiently when $$\boldsymbol{W}$$ is constrained to be orthogonal. This method is often called __OMP-$$k$$__, with the value of $$k$$ specified to indicate the number of nonzero features allowed. Coates and $$\mathrm{Ng}(2011)$$ demonstrated that $$\mathrm{OMP}-1$$ can be a very effective feature extractor for deep architectures.  
    <br>

9. **Ensemble Learning:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents39}  
    __Ensemble Learning__ is a set of __ensemble methods__ that use multiple learning algorithms, (models) and a strategy called __model averaging__ to combine the outcomes of those model, to obtain better predictive performance.  

    __Model averaging__ is an extremely powerful and reliable method for reducing generalization error.  

    __Motivation:__{: style="color: red"}  
    The reason that model averaging works is that different models will usually not make all the same errors on the test set.  
    Consider for example a set of $$k$$ regression models. Suppose that each model makes an error $$\epsilon_{i}$$ on each example, with the errors drawn from a zero-mean multivariate normal distribution with variances $$\mathbb{E}\left[\epsilon_{i}^{2}\right]=v$$ and covariances $$\mathbb{E}\left[\epsilon_{i} \epsilon_{j}\right]=c$$. Then the error made by the average prediction of all the ensemble models is $$\frac{1}{k} \sum_{i} \epsilon_{i}$$. The expected squared error of the ensemble predictor is:  
    <p>$$\begin{aligned} \mathbb{E}\left[\left(\frac{1}{k} \sum_{i} \epsilon_{i}\right)^{2}\right] &=\frac{1}{k^{2}} \mathbb{E}\left[\sum_{i}\left(\epsilon_{i}^{2}+\sum_{j \neq i} \epsilon_{i} \epsilon_{j}\right)\right] \\ &=\frac{1}{k} v+\frac{k-1}{k} c \end{aligned}$$</p>  
    * When the errors are *__perfectly correlated $$c=v$$__*: the MSE reduces to $$v$$, so the model averaging does not help at all.  
    * When the errors are *__perfectly uncorrelated $$c=0$$__*, the expected squared error of the ensemble is only $$\dfrac{1}{k} v$$.  
        Thus, the <span>expected squared error of the ensemble is inversely proportional to the ensemble size</span>{: style="color: goldenrod"}.  
    In other words, on average, the ensemble will perform at least as well as any of its members, and if the members make independent errors, the ensemble will perform significantly better than its members.  

    __As Regularization:__{: style="color: red"}  
    Regularization has two general definitions:  
    (1) Any modification we make to a learning algorithm that is intended to <span>_reduce_ its _generalization error_</span>{: style="color: goldenrod"} but <span>not its _training error_</span>{: style="color: goldenrod"}.  
    (2) Regularization is a (more general) way of __controlling a models capacity__ by <span>allowing us to express preference for one function over another in the same hypothesis space</span>{: style="color: goldenrod"}; instead of including or excluding members from the hypothesis space completely.  

    From those perspectives, the analysis (motivation) above shows that __ensemble methods__ satisfy both criteria in gold.  


    __Bagging:__{: style="color: red"}  
    __Bagging__ is an ensemble method that aims to reduce the generalization error.  
    It reduces __variance__ which corresponds to __generalization error__, thus decreasing __capacity__.  


    __Boosting:__{: style="color: red"}  
    _Not all techniques for constructing ensembles are designed to make the ensemble more regularized than the individual models_. For example, a technique called __boosting__ (Freund and Schapire, 1996b,a) constructs an ensemble with _higher capacity than the individual models_.   
    It reduces __bias__ which corresponds to __approximation error__, thus increasing __capacity__.  
    <br>

10. **Dropout:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents310}  
    __Dropout__ _(Srivastava et al., 2014)_ provides a computationally inexpensive but powerful method of regularizing a broad family of models.  


    __Dropout as (an approximation to) Bagging Ensemble Method for Neural Networks:__{: style="color: red"}  
    To a first approximation, dropout can be thought of as a method of making __bagging__ practical for ensembles of very many large neural networks.  
    Bagging involves training multiple models and evaluating multiple models on each test example. This seems impractical when each model is a large neural network, since training and evaluating such networks is costly in terms of runtime and memory. It is common to use ensembles of five to ten neural networks‚ÄîSzegedy et al. (2014a) used six to win the ILSVRC‚Äî but more than this rapidly becomes unwieldy.  
    Dropout provides an inexpensive approximation to training and evaluating a bagged ensemble of exponentially many neural networks. Specifically, dropout trains the ensemble consisting of all subnetworks that can be formed by removing nonoutput units from an underlying base network. In modern NNs, we can remove a unit by "multiplying" its output by zero.  

    Recall that to learn with bagging, we define $$k$$ different models, construct $$k$$ different datasets by sampling from the training set with replacement, and then train model $$i$$ on dataset $$i$$. Dropout aims to approximate this process, but with an exponentially large number of neural networks.  

    __Dropout Algorithm:__  
    To train with dropout, we:  
    {: #lst-p}
    * Use a minibatch-based learning algorithm that makes small steps, such as stochastic gradient descent.  
    * Each time we load an example into a minibatch, we randomly sample a different binary mask to apply to all the input and hidden units in the network.  
        * The mask for each unit is sampled __independently__ from all the others.  
        * The __probability of sampling a mask value of one__ (causing a unit to be included) is a _hyperparameter_ fixed before training begins. It is not a function of the current value of the model parameters or the input example.  
    * Typically, an input unit is included with probability $$0.8$$, and a hidden unit is included with probability $$0.5$$.  
    * We then run forward propagation, back-propagation, and the learning update as usual.  

    __Formally,__ suppose that a mask vector $$\mu$$ specifies which units to include, and $$J(\boldsymbol{\theta}, \boldsymbol{\mu})$$ defines the cost of the model defined by parameters $$\boldsymbol{\theta}$$ and mask $$\boldsymbol{\mu}$$ Then dropout training consists of minimizing $$\mathbb{E}_ {\boldsymbol{\mu}} J(\boldsymbol{\theta}, \boldsymbol{\mu})$$. The expectation contains exponentially many terms, but we can obtain an unbiased estimate of its gradient by sampling values of $$\boldsymbol{\mu}$$.  

    __Differences:__  
    {: #lst-p}
    * __Training (compare each number)__:  
        * __Bagging:__  
            1. The models are all independent.  
            2. Each model is trained to convergence on its respective training set.  
        * __Dropout__:  
            1. The models share parameters, with each model inheriting a different subset of parameters from the parent neural network.  
                This parameter sharing makes it possible to represent an exponential number of models with a tractable amount of memory.  
            2. Most models are not explicitly trained at all‚Äîusually, the model is large enough that it would be _infeasible to sample all possible subnetworks_ within the lifetime of the universe. Instead, a tiny fraction of the possible subnetworks are each trained for a single step, and the parameter sharing causes the remaining subnetworks to arrive at good settings of the parameters.  

        > Beyond these difference, dropout follows the bagging algorithm; e.g. training set encountered by each subnetwork is indeed a subset of the original training set sampled with replacement.  

    * __Inference (Prediction)__:  
        So far, our description of bagging and dropout has not required that the model be explicitly probabilistic. Now, we _assume_ that the model‚Äôs role is to _output a **probability distribution**_.  
        * __Bagging:__ Each model $$i$$ produces a probability distribution $$p^{(i)}(y \vert \boldsymbol{x})$$.  
            The prediction of the ensemble is given by the __arithmetic mean__ of all these distributions:  
            <p>$$\frac{1}{k} \sum_{i=1}^{k} p^{(i)}(y \vert \boldsymbol{x})$$</p>  
        * __Dropout__: Each submodel defined by mask vector $$\boldsymbol{\mu}$$ defines a probability distribution $$p(y \vert x, \mu)$$.  
            The __arithmetic mean__ over all masks is given by:  
            <p>$$\sum_{\mu} p(\boldsymbol{\mu}) p(y \vert \boldsymbol{x}, \boldsymbol{\mu})$$</p>  
            where $$p(\boldsymbol{\mu})$$ is the probability distribution that was used to sample $$\boldsymbol{\mu}$$ at training time.  

            * __Issues with direct inference with the arithmetic mean__:  
                Because this sum includes an exponential number of terms, it is intractable to evaluate except when the structure of the model permits some form of simplification.  
            * __Solution - Geometric Mean + Weight Scaling Inference__:  
                The following approach allows us to obtain a good approximation to the predictions of the entire ensemble, at the cost of only one forward propagation.  
                The __geometric mean__ of multiple probability distributions is _not guaranteed to be a probability distribution_. We guarantee it by imposing the requirement that _none of the submodels assigns probability $$0$$ to any event_, and we _renormalize the resulting distribution_.  
                __The *Unnormalized* distribution defined by the geometric mean:__  
                <p>$$\tilde{p}_{\text {ensemble }}(y \vert \boldsymbol{x})=2 \sqrt[2 d]{\prod_{\boldsymbol{\mu}} p(y \vert \boldsymbol{x}, \boldsymbol{\mu})}$$</p>  
                where $$d$$ is the number of units that may be dropped[^4].  
                __The *Normalized* distribution:__  
                <p>$$p_{\text {ensemble }}(y \vert \boldsymbol{x})=\frac{\tilde{p}_{\text {ensemble }}(y \vert \boldsymbol{x})}{\sum_y \tilde{p}_{\text {ensemble }}\left(y^{\prime} \vert \boldsymbol{x}\right)}$$</p>  

                __Weight Scaling Inference Rule:__ that approximates $$p_{\text {ensemble}}$$ by evaluating $$p(y \vert \boldsymbol{x})$$ in one model: the model with all units, but with the weights going out of unit $$i$$ multiplied by the probability of including unit $$i$$.  


                * __Analysis and Justification:__  
                    * _Warde-Farley et al. (2014)_ present arguments and empirical evidence that the geometric mean performs comparably to the arithmetic mean in this context.
                    * A key insight _(Hinton et al., 2012c)_ involved in dropout is that we can approximate $$p_{\text {ensemble}}$$ by evaluating $$p(y \vert \boldsymbol{x})$$ in one model: the model with all units, but with the weights going out of unit $$i$$ multiplied by the probability of including unit $$i$$ - The __Weight Scaling Inference Rule:__. The __motivation__ for this modification is to *__capture the right expected value of the output from that unit__*.   
                        There is not yet any theoretical argument for the accuracy of this approximate inference rule in deep nonlinear networks, but empirically it performs very well.  
                        The __goal__ is to make sure that <span>the expected total input to a unit at test time is roughly the same as the expected total input to that unit at train time</span>{: style="color: goldenrod"}.  
                        is a method that aims to make sure that <span>the expected total input to a unit at test time is roughly the same as the expected total input to that unit at train time</span>{: style="color: goldenrod"}.  
                        For many classes of models that do not have nonlinear hidden units, the weight scaling inference rule is exact; e.g. softmax regression classifier _(proof page 260 dlbook)_.  
                        The weight scaling rule is also exact in other settings, including __regression networks with conditionally normal outputs__ as well as __deep networks that have hidden layers without nonlinearities__.  
                        However, the weight scaling rule is only __an approximation for deep models that have *nonlinearities*__. Though the approximation has not been theoretically characterized, it often works well, empirically.  
                        _Goodfellow et al. (2013a)_ found experimentally that the weight scaling approximation can work better (in terms of classification accuracy) than Monte Carlo approximations to the ensemble predictor. This held true even when the Monte Carlo approximation was allowed to sample up to 1,000 subnetworks. _Gal and Ghahramani (2015)_ found that some models obtain better classification accuracy using twenty samples and the Monte Carlo approximation.  
                        It appears that <span>the optimal choice of inference approximation is problem dependent</span>{: style="color: goldenrod"}.  
                        * One other, non-efficient, way to do inference is to approximate it with sampling by averaging together the output from many masks. Even 10‚Äì20 masks are often sufficient to obtain good performance.  
                            Weight Scaling Rule is far superior.  



    __Properties and Advantages:__{: style="color: red"}  
    {: #lst-p}
    * Srivastava et al. (2014) showed that dropout is more effective than other standard computationally inexpensive regularizers, such as weight decay, filter norm constraints, and sparse activity regularization.  
    * Dropout may be combined with other forms of regularization to yield a further improvement.  
    * Dropout is very computationally cheap.  
        Using it in training requires only $$\mathcal{O}(n)$$ computation per example per update, to generate $$n$$ random binary numbers and multiply them by the state.  
    * Dropout does not significantly limit the type of model or training procedure that can be used.  
        It works well with nearly any model that uses a distributed representation and can be trained with stochastic gradient descent; e.g. feedforward neural networks, probabilistic models such as restricted Boltzmann machines, and RNNs.  
    * Stochasticity is Neither _necessary_ Nor _sufficient_ for the regularization effects of dropout:  
        * The stochasticity used while training with dropout is NOT necessary for the approaches success. It is just a means of approximating the sum over all submodels.   
            __Fast Dropout__ is an analytical approximations to this marginalization.  
        * Applying the same stochastic masks in a method analogous to __boosting__, where the stochastically sampled ensemble members are not trained independently, show almost no regularization effect compared to when the ensemble members are trained to perform well independently of each other.  
            __Dropout boosting__ is one method that trains the entire ensemble to jointly maximize the log-likelihood on the training set; and experiments have shown that it displays no regularization compared to training the entire network as a single model.   
    * The sharing of the weights means that every model is very strongly regularized.  
        This regularization is much better than L2 or L1 penalties since instead of pulling the weights towards zero, we are pulling the weights towards the correct value of the weights.  
    * [Dropout - Hinton](https://www.youtube.com/watch?v=vAVOY8frLlQ&t=0s)  
    * Running the __stochastic__ model several times on the same input (instead of all the weights halved), <span>gives an idea of the uncertainty in the answer</span>{: style="color: goldenrod"}.  
    * We can use dropout in the __input layer__, but with a higher probability of keeping an input unit:  
        *__Denoising Autoencoders__* use this.    


    __Practical Disadvantages/Issues:__{: style="color: red"}  
    {: #lst-p}
    * Typically the optimal validation set error is much lower when using dropout, but this comes at the cost of a much larger model and many more iterations of the training algorithm. For very large datasets, regularization confers little reduction in generalization error. In these cases, the computational cost of using dropout and larger models may outweigh the benefit of regularization.  
    * When extremely few labeled training examples are available, dropout is less effective. Bayesian neural networks (Neal, 1996) outperform dropout on the Alternative Splicing Dataset (Xiong et al., 2011), where fewer than 5,000 examples are available (Srivastava et al., 2014). When additional unlabeled data is available, unsupervised feature learning can gain an advantage over dropout.  


    __Dropout as Regularization - Effectiveness of Dropout:__{: style="color: red"}  
    {: #lst-p}
    * Dropout can be viewed as a means of performing <span>efficient, approximate __bagging__</span>{: style="color: goldenrod"}.  
    * Dropout trains not just a bagged ensemble of models, but an <span>ensemble of models that __share hidden units__</span>{: style="color: goldenrod"}.  
        This means each hidden unit must be able to perform well regardless of which other hidden units are in the model. Hidden units must be prepared to be swapped and interchanged between models.  
        Dropout, thus, <span>regularizes each hidden unit to be not merely a good feature but a feature that is __good in many contexts__</span>{: style="color: goldenrod"}.  
        This <span>prevents *__Co-Adaptation__* between hidden-units on the Training Data</span>{: style="color: goldenrod"}.  
    * A large portion of the power of dropout arises from the fact that the __masking noise is applied to the *hidden units*__. This can be seen as a form of <span>highly intelligent, adaptive __destruction of the information content__</span>{: style="color: goldenrod"} of the input rather than destruction of the raw values of the input.  
        This views dropout as __noise-injection__ in the _hidden units_. Which can be seen as doing <span>__data-augmentation__ at *__multiple levels of abstraction__*</span>{: style="color: goldenrod"}.  
        <button>Analysis</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        <span hidden="">For example, if the model learns a hidden unit that detects a face by finding the $$h_i$$ nose, then dropping corresponds to erasing the information that there is a nose $$h_i$$ in the image. The model must learn another $$h_i$$, that either redundantly encodes the presence of a nose or detects the face by another feature, such as the mouth. Traditional noise injection techniques that add unstructured noise at the input are not able to randomly erase the information about a nose from an image of a face unless the magnitude of the noise is so great that nearly all the information in the image is removed. Destroying extracted features rather than original values allows the destruction process to make use of all the knowledge about the input distribution that the model has acquired so far.</span>  
    * When viewed as __noise-injection__, an important aspect of dropouts success is that it uses <span>multiplicative noise</span>{: style="color: goldenrod"}.  
        Multiplicative noise does not allow for _pathological solutions_ to the noise robustness problem: If the noise were additive with fixed scale, then a rectified linear hidden unit $$h_{i}$$ with added noise $$\epsilon$$ could simply learn to have $$h_{i}$$ become very large in order to make the added noise $$\epsilon$$ insignificant by comparison.  


    __Dropout and Batch Normalization:__{: style="color: red"}  
    Another deep learning algorithm, __batch normalization__, reparametrizes the model in a way that introduces both __additive__ and __multiplicative noise__ on the _hidden units_ at training time. The primary purpose of batch normalization is to improve optimization, but the noise can have a regularizing effect, and sometimes makes dropout unnecessary.  


    __Modifying Dropout and source of Regularization:__{: style="color: red"}  
    One of the key insights of dropout is that <span>training a network with stochastic behavior and making predictions by averaging over multiple stochastic decisions implements a form of bagging with parameter sharing</span>{: style="color: goldenrod"}.  
    Earlier, we described dropout as bagging an ensemble of models formed by including or excluding units. Yet this model averaging strategy does not need to be based on inclusion and exclusion. In principle, any kind of random modification is admissible.  
    In practice, we must choose modification families that:  
    (1) neural networks are able to learn to resist.  
    (2) Ideally, we should also use model families that allow a fast approximate inference rule.  
    We can think of any form of modification parametrized by a vector $$\mu$$ as training an ensemble consisting of $$p(y \vert \boldsymbol{x}, \boldsymbol{\mu})$$ for all possible values of $$\boldsymbol{\mu}$$ There is no requirement that $$\boldsymbol{\mu}$$ have a finite number of values. For example, $$\boldsymbol{\mu}$$ can be real valued. Srivastava et al. $$(2014)$$ showed that multiplying the weights by $$\boldsymbol{\mu} \sim \mathcal{N}(\mathbf{1}, I)$$ can outperform dropout based on binary masks. Because $$\mathbb{E}[\boldsymbol{\mu}]=\mathbf{1}$$, the standard network automatically implements approximate inference in the ensemble, without needing any weight scaling.   


    __Dropout as Weight Decay in linear models:__{: style="color: red"}  
    _Wager et al. (2013)_ showed that, when applied to linear regression, dropout is equivalent to __L2 weight decay__, with a __different weight decay coefficient for each input feature__. The __magnitude of each feature‚Äôs weight decay coefficient is determined by its variance__. Similar results hold for other linear models.  
    For deep models, dropout is NOT equivalent to weight decay.  
    <br>


11. **Adversarial Training:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents311}  
    __Adversarial training__ refers to training on examples that are adversarially generated and it has been shown to reduce the error rate.  

    __Adversarial Examples:__ are examples that are intentionally constructed by using an optimization procedure to search for an input $$\boldsymbol{x}^{\prime}$$ near a data point $$\boldsymbol{x}$$ such that the model output is very different at $$\boldsymbol{x}^{\prime} .$$ In many cases, $$\boldsymbol{x}^{\prime}$$ can be so similar to $$\boldsymbol{x}$$ that a human observer cannot tell the difference between the original example and the adversarial example, but the network can make highly different predictions.  
    _Szegedy et al. (2014b)_ found that even neural networks that perform at human level accuracy have a nearly $$100\%$$ error rate on __adversarial examples__.  
    ![img](https://cdn.mathpix.com/snip/images/_TFBraSdZd9ZN5OC9dLMcF4oAZOmY2gY2_hVcnwVM5s.original.fullsize.png){: width="50%"}  


    __Application in Regularization:__{: style="color: red"}  
    __Adversarial Examples__ are interesting in the context of regularization because one can _reduce the error rate on the original test set via **adversarial training**_‚Äîtraining on adversarially perturbed examples from the training set (Szegedy et al., 2014b; Goodfellow et al., 2014b).  
    _Goodfellow et al. (2014b)_ showed that one of the primary causes of these adversarial examples is excessive linearity. Neural networks are built out of primarily linear building blocks. In some experiments the overall function they implement proves to be highly linear as a result. These linear functions are easy to optimize. Unfortunately, the value of a linear function can change very rapidly if it has numerous inputs. If we change each input by $$\epsilon$$ , then a linear function with weights $$w$$ can change by as much as $$\epsilon \|w\|_ 1$$, which can be a very large amount if $$w$$ is high-dimensional.  
    Adversarial training <span>discourages this highly sensitive locally linear behavior by encouraging the network to be locally constant in the neighborhood of the training data</span>{: style="color: goldenrod"}. This can be seen as a way of __explicitly introducing a local constancy prior__{: style="color: goldenrod"} into supervised neural nets.  

    __As Semi-supervised Learning:__  
    __Virtual Adversarial Examples__ are adversarial examples generated using not the true label but a label provided by a trained model _(Miyato et al., 2015)_.  
    Using virtual examples, we can train a classifier to assign the same label to $$x$$ and $$x^{\prime}$$. This encourages the classifier to learn a function that is robust to small changes anywhere along the manifold where the unlabeled data lie. The assumption motivating this approach is that different classes usually lie on disconnected manifolds, and a small perturbation should not be able to jump from one class manifold to another class manifold.  
    > At a point $$\boldsymbol{x}$$ that is not associated with a label in the dataset, the model itself assigns some label $$\hat{y}$$ . The model's label $$\hat{y}$$ may not be the true label, but if the model is high quality, then $$\hat{y}$$ has a high probability of providing the true label. We can seek an adversarial example $$\boldsymbol{x}^{\prime}$$ that causes the classifier to output a label $$y^{\prime}$$ with $$y^{\prime} \neq \hat{y}$$.  

    __Motivation:__  
    The assumption motivating this approach is that <span>different classes usually lie on disconnected manifolds</span>{: style="color: goldenrod"}, and a small perturbation should not be able to jump from one class manifold to another class manifold.  
    <br>


12. **Tangent Distance, Tangent Prop and Manifold Tangent Classifiers:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents312}  
    __Tangent Distance Algorithm:__{: style="color: red"}  
    Many ML models assume the data to lie on a low dimensional manifold to overcome the curse of dimensionality. The inherent assumption which follows is that small perturbations that cause the data to move along the manifold (it originally belonged to), shouldn't lead to different class predictions. The idea of the __tangent distance__ algorithm to find the K-nearest neighbors using the distance metric as the distance between manifolds. A manifold $$M_i$$ is approximated by the tangent plane at $$x_i$$, hence, this technique needs tangent vectors to be specified.  


    __Tangent Prop Algorithm:__{: style="color: red"}  
    The __tangent prop__ algorithm proposed to learn a neural network based classifier, $$f(x)$$, which is invariant to known transformations causing the input to move along its manifold. Local invariance would require that $$\bigtriangledown_x f(x)$$ is perpendicular to the tangent vectors $$V^{(i)}$$. This can also be achieved by adding a penalty term that minimizes the directional directive of $$f(x)$$ along each of the $$V(i)$$:  
    <p>$$\Omega(f)=\sum_{i}\left(\left(\nabla_{\boldsymbol{x}} f(\boldsymbol{x})\right)^{\top} \boldsymbol{v}^{(i)}\right)^{2}$$</p>  
    __Tangent Propagation__ is similar to data augmentation in that both of them use prior knowledge of the domain to specify various transformations that the model should be invariant to. However, tangent prop only resists infinitesimal perturbations while data augmentation causes invariance to much larger perturbations.  

    __Drawbacks:__  
    While this analytical approach is intellectually elegant, it has two major drawbacks.  
    1. It only regularizes the model to resist infinitesimal perturbation.  
        Explicit dataset augmentation confers resistance to larger perturbations. 
    2. The infinitesimal approach poses difficulties for models based on rectified linear units.  
        These models can only shrink their derivatives by turning units off or shrinking their weights. They are not able to shrink their derivatives by saturating at a high value with large weights, as sigmoid or tanh units can.  
        Dataset augmentation works well with rectified linear units because different subsets of rectified units can activate for different transformed versions of each original input.  


    __Manifold Tangent Classifier:__{: style="color: red"}  
    The manifold tangent classifier (Rifai et al., 2011c), eliminates the need to know the tangent vectors a priori. Autoencoders can estimate the manifold tangent vectors.  

    Manifold Tangent Classifier works in two parts:  
    1. Use Autoencoders to learn the manifold structures using Unsupervised Learning.
    1. Use these learned manifolds with tangent prop.  

    ![img](https://cdn.mathpix.com/snip/images/ouitJqqotxhESaIP5V6enbeHG7ELd2Ye7u3GfWW9RPg.original.fullsize.png){: width="65%"}  
    <br>



__Notes:__{: style="color: red"}  
{: #lst-p}
* A practical rule for choosing a regularizer:  
    * Stochastic noise is "high frequency"  
    * Deterministic noise is also non-smooth  

    Thus, we should constrain learning towards smoother hypotheses. I.E. fit the signal more than you fit the noise (which is non-smooth). We end up harming both, but harming the irregular, non-smooth noise more.  
* __Regularization does two things - reduce fit to noise AND reduce overfitting__:  
    ![img](/main_files/dl_book/regularization/5.png){: width="100%"}   


[^5]: The architectures were constructed such that many of the parameters in the classifier model could be paired to corresponding parameters in the unsupervised model.
[^4]: Here we use a uniform distribution over $$\mu$$ to simplify the presentation, but nonuniform distributions are also possible.  
[^6]: So, you can think of the NFL theorems as providing some kind of theoretical justification for regularization or theoretical understanding that helps us see what the role of regularization is and provides some partial explanation for the empirical observation that it seems to often be effective.  

***
***

TITLE: ML Models 
LINK: research/dl/theory/ml_models.md


* [Latent Variable Model Intuition (slides!)](http://mlvis2016.hiit.fi/latentVariableGenerativeModels.pdf)  
* [Radford Neal's Research: Latent Variable Models (publications)](http://www.cs.toronto.edu/~radford/res-latent.html)  


## Statistical Models
{: #content1}

22. **Statistical Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents122}  
    A __Statistical Model__ is a, non-deterministic, mathematical model that embodies a set of _statistical assumptions_ concerning the generation of sample data.  
    It is specified as a mathematical relationship between one or more _random variables_ and other non-random variables.  

    __Formal Definition:__{: style="color: red"}  
    A __Statistical Model__ consists of a pair $$(S, \mathcal{P})$$ where $$S$$ is the _set of possible observations_ (the _sample space_) and $$\mathcal{P}$$ is a <span>_**set**_ of __probability distributions__</span>{: style="color: purple"} on $$S$$.  

    The set $$\mathcal{P}$$ can be (and is usually) __parametrized__:  
    <p>$$\mathcal{P}=\left\{P_{\theta} : \theta \in \Theta\right\}$$</p>  
    The set $$\Theta$$ defines the __parameters__ of the model.  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * It is important that a statistical model consists of a __set__ of probability distributions,  
        while a _probability model_ is just one *__known__* distribution.  
    <br>

1. **Parametric Model:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    A __parametric model__ is a set of probability distributions indexed by a parameter $$\theta \in \Theta$$. We denote this as:  
    <p>$$\{p(y ; \theta) | \theta \in \Theta\},$$</p>  
    where $$\theta$$ is the __parameter__ and $$\Theta$$ is the __Parameter-Space__.  
    <br>

2. **Non-Parametric Model:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    __Non-parametric models__ differ from parametric models in that the model structure is not specified a priori but is instead _determined from data_. The term _non-parametric_ is not meant to imply that such models completely lack parameters but that the number and nature of the parameters are flexible and not fixed in advance.  

    __Examples:__  
    {: #lst-p}
    * A __histogram__ is a simple nonparametric estimate of a probability distribution.
    * __Kernel density estimation__ provides better estimates of the density than histograms.
    * __Nonparametric regression and semiparametric regression__ methods have been developed based on kernels, splines, and wavelets.  
    * __Data envelopment analysis__ provides efficiency coefficients similar to those obtained by multivariate analysis without any distributional assumption.
    * __KNNs__ classify the unseen instance based on the K points in the training set which are nearest to it.
    * A __support vector machine (SVM)__ (with a Gaussian kernel) is a nonparametric large-margin classifier.  
    * __Method of moments__ (statistics) with polynomial probability distributions.  
    <br>


3. **Other classes of Statistical Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    Given $$\mathcal{P}=\left\{P_{\theta} : \theta \in \Theta\right\}$$, the _set of probability distributions on $$S$$_.  
    * A model is __"parametric"__ if all the parameters are in finite-dimensional parameter spaces; i.e. $$\Theta$$ has __*finite dimension*__  
    * A model is __"non-parametric"__ if all the parameters are in infinite-dimensional parameter spaces  
    * A __"semi-parametric"__ model contains finite-dimensional parameters of interest and infinite-dimensional nuisance parameters  
    * A __"semi-nonparametric"__ model has both finite-dimensional and infinite-dimensional unknown parameters of interest  
    <br>

4. **Types of Statistical Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    * Linear Model
    * GLM - General Linear Model
    * GiLM - Generalized Linear Model
    * Latent Variable Model
    <br>


11. **The Statistical Model for Linear Regression:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents111}  
    Given a (random) sample $$\left(Y_{i}, X_{i 1}, \ldots, X_{i p}\right), i=1, \ldots, n$$ the relation between the _observations_ $$Y_i$$ and the _independent variables_ $$X_{ij}$$ is formulated as:  
    <p>$$Y_{i}=\beta_{0}+\beta_{1} \phi_{1}\left(X_{i 1}\right)+\cdots+\beta_{p} \phi_{p}\left(X_{i p}\right)+\varepsilon_{i} \qquad i=1, \ldots, n$$</p>  
    where $${\displaystyle \phi_{1},\ldots ,\phi_{p}}$$ may be nonlinear functions. In the above, the quantities $$\varepsilon_i$$ are random variables representing errors in the relationship.   

    __The Linearity of the Model:__{: style="color: red"}  
    The _"linear"_ part of the designation relates to the appearance of the regression coefficients, $$\beta_j$$ in a linear way in the above relationship.  
    Alternatively, one may say that the predicted values corresponding to the above model, namely:  
    <p>$$\hat{Y}_{i}=\beta_{0}+\beta_{1} \phi_{1}\left(X_{i 1}\right)+\cdots+\beta_{p} \phi_{p}\left(X_{i p}\right) \qquad(i=1, \ldots, n)$$</p>  
    are __linear functions__ of the __coefficients__ $$\beta_j$$.  

    __Estimating the Parameters $$\beta_j$$:__  
    Assuming an estimation on the basis of a __least-squares__ analysis, estimates of the unknown parameters $$\beta_j$$ are determined by _minimizing a sum of squares function:_  
    <p>$$S=\sum_{i=1}^{n}\left(Y_{i}-\beta_{0}-\beta_{1} \phi_{1}\left(X_{i 1}\right)-\cdots-\beta_{p} \phi_{p}\left(X_{i p}\right)\right)^{2}$$</p>  

    __Effects of Linearity:__  
    {: #lst-p}
    * The function to be minimized is a quadratic function of the $$\beta_j$$ for which minimization is a relatively simple problem  
    * The derivatives of the function are linear functions of the $$\beta_j$$ making it easy to find the minimizing values  
    * The minimizing values $$\beta_j$$ are linear functions of the observations $$Y_i$$  
    * The minimizing values $$\beta_j$$ are linear functions of the random errors $$\varepsilon_i$$  which makes it relatively easy to determine the statistical properties of the estimated values of $$\beta_j$$.  
    <br>


5. **Latent Variable Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    __Latent Variable Models__ are statistical models that relate a set of observable variables (so-called manifest variables) to a set of latent variables.  

    __Core Assumption - Local Independence:__{: style="color: red"}  
    __Local Independence:__  
    The observed items are conditionally independent of each other given an individual score on the latent variable(s). This means that the latent variable *__explains__* why the observed items are related to another.  

    In other words, the targets/labels on the observations are the result of an individual's position on the latent variable(s), and that the observations have nothing in common after controlling for the latent variable.  

    <p>$$p(A,B\vert z) = p(A\vert z) \times (B\vert z)$$</p>  


    <button>Example of Local Independence</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/wnxPRKkVBA88V1k3i4HdWBTtn0NQFBi5gdNkTLcCeFk.original.fullsize.png){: width="100%" hidden=""}  


    __Methods for inferring Latent Variables:__{: style="color: red"}  
    {: #lst-p}
    * Hidden Markov models (HMMs)
    * Factor analysis
    * Principal component analysis (PCA)
    * Partial least squares regression
    * Latent semantic analysis and probabilistic latent semantic analysis
    * EM algorithms
    * Pseudo-Marginal Metropolis-Hastings algorithm
    * Bayesian Methods: LDA




    __Notes:__{: style="color: red"}    
    {: #lst-p}
    * Latent Variables *__encode__*  information about the data  
        e.g. in compression, a 1-bit latent variable can encode if a face is Male/Female.  
    * __Data Projection:__  
        You *__"hypothesis"__* how the data might have been generated (by LVs).  
        Then, the LVs __generate__ the data/observations.  
        <button>Visualisation with Density (Generative) Models</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](https://cdn.mathpix.com/snip/images/ctljXHCOfIzpttSIOCsFbQxjFmjrEcf4a5Dr9KbWnTI.original.fullsize.png){: width="100%" hidden=""}  
    * [**Latent Variable Models/Gaussian Mixture Models**](https://www.youtube.com/embed/I9dfOMAhsug){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/I9dfOMAhsug"></a>
        <div markdown="1"> </div>    
    * [**Expectation-Maximization/EM-Algorithm for Latent Variable Models**](https://www.youtube.com/embed/lMShR1vjbUo){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/lMShR1vjbUo"></a>
        <div markdown="1"> </div>    


8. **Three ways to build classifiers:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    1. Generative models (e.g. LDA) [We‚Äôll learn about LDA next lecture.]
        * Assume sample points come from probability distributions, different for each class.  
        * Guess form of distributions  
        * For each class $$C$$, fit distribution parameters to class $$C$$ points, giving $$P(X\vert Y = C)$$  
        * For each $$C$$, estimate $$P(Y = C)$$   
        * Bayes‚Äô Theorem gives $$P(Y\vert X)$$  
        * If $$0-1$$ loss, pick class $$C$$ that maximizes $$P(Y = C\vert X = x)$$ [posterior probability] equivalently, maximizes $$P(X = x\vert Y = C) P(Y = C)$$  
    2. Discriminative models (e.g. logistic regression) [We‚Äôll learn about logistic regression in a few weeks.]  
        * Model $$P(Y\vert X)$$ directly  
    3. Find decision boundary (e.g. SVM)  
        * Model $$r(x)$$ directly (no posterior)  

    Advantage of (1 & 2): $$P(Y\vert X)$$ tells you probability your guess is wrong  
        [This is something SVMs don‚Äôt do.]  
    Advantage of (1): you can diagnose outliers: $$P(X)$$ is very small  
    Disadvantages of (1): often hard to estimate distributions accurately;  
        real distributions rarely match standard ones.  


***

## Regression Models
{: #content2}

1. **Linear Models:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    A __Linear Model__ takes an input $$x$$ and computes a signal $$s = \sum_{i=0}^d w_ix_i$$ that is a _linear combination_ of the input with weights, then apply a scoring function on the signal $$s$$.  
    * __Linear Classifier as a Parametric Model__:  
        Linear classifiers $$f(x, W)=W x+b$$  are an example of a parametric model that sums up the knowledge of the training data in the parameter: weight-matrix $$W$$.  
    * __Scoring Function__:  
        * *__Linear Classification__*:  
            $$h(x) = sign(s)$$  
        * *__Linear Regression__*:  
            $$h(x) = s$$  
        * *__Logistic Regression__*:  
            $$h(x) = \sigma(s)$$  



***
***

TITLE: Introduction and Basics of Deep Learning
LINK: research/dl/theory/dl_book_pt_basics_of_dl.md


## Introduction: Deep Feedforward Neural Networks
{: #content1}

1. **(Deep) FeedForward Neural Networks:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    For a classifier; given $$y=f^{\ast}(\boldsymbol{x})$$, maps an input $$\boldsymbol{x}$$ to a category $$y$$.  
    An __FNN__ defines a mapping $$\boldsymbol{y}=f(\boldsymbol{x} ; \boldsymbol{\theta})$$  and learns the value of the parameters $$\boldsymbol{\theta}$$  that result in the best function approximation.  

    * FNNs are called __networks__ because they are typically represented by composing together many different functions
    * The model is associated with a __DAG__ describing how the functions are composed together. 
    * Functions connected in a __chain structure__ are the most commonly used structure of neural networks.  
        > E.g. we might have three functions $$f^{(1)}, f^{(2)},$$ and $$f^{(3)}$$ connected in a chain, to form $$f(\boldsymbol{x})=f^{(3)}\left(f^{(2)}\left(f^{(1)}(\boldsymbol{x})\right)\right)$$; being called the $$n$$-th __Layer__ respectively.   
    * The overall length of the chain is the __depth__ of the model.  
    * During training, we drive $$f(\boldsymbol{x})$$ to match $$f^{\ast}(\boldsymbol{x})$$.   
        The training data provides us with noisy, approximate examples of $$f^{\ast}(\boldsymbol{x})$$ evaluated at different training points.  
    <br>

2. **FNNs from Linear Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    Consider linear models biggest limitation: model capacity is limited to linear functions.  
    To __extend__ linear models to represent non-linear functions of $$\boldsymbol{x}$$ we can:  
    {: #lst-p}
    * Apply the linear model not to $$\boldsymbol{x}$$ itself but <span>_to a transformed input_ $$\phi(\boldsymbol{x})$$</span>{: style="color: purple"}, where $$\phi$$ is a <span>__nonlinear transformation__</span>{: style="color: purple"}.  
    * Equivalently, apply the __kernel trick__ to obtain nonlinear learning algorithm based on implicitly applying the $$\phi$$ mapping.  

    <span>We can think of $$\phi$$ as providing a __set of features *describing*__ $$\boldsymbol{x}$$, or as providing a __new representation__ for $$\boldsymbol{x}$$.</span>{: style="color: goldenrod"}  
    __Choosing the mapping $$\phi$$:__  
    {: #lst-p}
    1. Use a very generic $$\phi$$, s.a. infinite-dimensional (RBF) kernel.  
        If $$\phi(\boldsymbol{x})$$ is of _high enough dimension_, we can _always have enough capacity_ to fit the training set, but _generalization_ to the test set often _remains poor_.  
        Very generic feature mappings are usually _based only_ on the _principle of local smoothness_ and do not encode enough prior information to solve advanced problems.  
    2. Manually Engineer $$\phi$$.  
        Requires decades of human effort and the results are usually poor and non-scalable.  
    3. The _strategy_ of __deep learning__ is to <span>__learn $$\phi$$__</span>{: style="color: purple"}.  
        * We have a __model__:  
            <p>$$y=f(\boldsymbol{x} ; \boldsymbol{\theta}, \boldsymbol{w})=\phi(\boldsymbol{x} ; \boldsymbol{\theta})^{\top} \boldsymbol{w}$$</p>  
            We now have __parameters $$\theta$$__ that we <span>use to __learn $$\phi$$__ from a <span>*__broad class of functions__*</span>{: style="color: purple"}</span>{: style="color: purple"}, and __parameters $$\boldsymbol{w}$$__ that <span>__map__ from _$$\phi(\boldsymbol{x})$$_ to the _desired output_</span>{: style="color: purple"}.  
            This is an example of a __deep FNN__, with $$\phi$$ defining a __hidden layer__.   
        * This approach is the _only one_ of the three that _gives up_ on the _convexity_ of the training problem, but the _benefits outweigh the harms_.  
        * In this approach, we parametrize the representation as $$\phi(\boldsymbol{x}; \theta)$$ and use the optimization algorithm to find the $$\theta$$ that corresponds to a good representation.  
        * __Advantages:__  
            * __Capturing the benefit of the *first* approach__:  
                by being highly generic ‚Äî we do so by using a very broad family $$\phi(\boldsymbol{x};\theta)$$.  
            * __Capturing the benefit of the *second* approach__:  
                Human practitioners can encode their knowledge to help generalization by designing families $$\phi(\boldsymbol{x}; \theta)$$ that they expect will perform well.  
                The __advantage__ is that the human designer only needs to find the right general function family rather than finding precisely the right function.  

    Thus, we can _motivate_ __Deep NNs__ as a way to do <span>__automatic, *non-linear* feature extraction__</span>{: style="color: goldenrod"} from the __inputs__.  

    This general principle of <span>improving models by learning features</span>{: style="color: purple"} extends beyond the feedforward networks to all models in deep learning.  
    FFNs are the application of this principle to learning __deterministic mappings__ from $$\boldsymbol{x}$$ to $$\boldsymbol{y}$$ that __lack__ *__feedback connections__*.  
    Other models, apply these principles to learning __stochastic mappings__, __functions with feedback__, and __probability distributions__ _over a single vector_.  


    __Advantage and Comparison of Deep NNs:__{: style="color: red"}  
    {: #lst-p}
    * __Linear classifier:__  
        * __Negative:__ Limited representational power
        * __Positive:__ Simple
    * __Shallow Neural network (Exactly one hidden layer):__  
        * __Positive:__ Unlimited representational power
        * __Negative:__ Sometimes prohibitively wide 
    * __Deep Neural network:__  
        * __Positive:__ Unlimited representational power
        * __Positive:__ Relatively small number of hidden units needed
    <br>

3. **Interpretation of Neural Networks:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    It is best to think of feedforward networks as __function approximation machines__ that are designed to achieve <span>_statistical generalization_</span>{: style="color: purple"}, occasionally drawing some insights from what we know about the brain, rather than as models of brain function.  




***

## Gradient-Based Learning
{: #content2}

1. **Stochastic Gradient Descent and FNNs:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    __Stochastic Gradient Descent__ applied to _nonconvex_ loss functions has _no convergence guarantees_ and is _sensitive_ to the _values_ of the _initial parameters_.  

    Thus, for FNNs (since they have _nonconvex loss functions_):  
    {: #lst-p}
    * *__Initialize all weights to small random values__*.  
    * The *__biases__* may be *__initialized to zero or to small positive values__*.  
    <br>

2. **Learning Conditional Distributions with Maximum Likelihood:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    When __Training__ using __Maximum Likelihood__:  
    The *__cost function__* is, simply, the *__negative log-likelihood__*.  
    Equivalently, the *__cross-entropy__* _between_ the _training data_ and the _model distribution_.  

    $$J(\boldsymbol{\theta})=-\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim \hat{p}_{\text {data }}} \log p_{\text {model }}(\boldsymbol{y} | \boldsymbol{x})  \tag{6.12}$$  

    * The specific form of the cost function changes from model to model, depending on the specific form of $$\log p_{\text {model}}$$.  
    * The expansion of the above equation typically yields some terms that do not depend on the model parameters and may be discarded.  

    __Maximum Likelihood and MSE:__  
    The equivalence between _maximum likelihood estimation with an output distribution_ and _minimization of mean squared error_ holds not just for a linear model, but in fact, the equivalence holds regardless of the $$f(\boldsymbol{x} ; \boldsymbol{\theta})$$ used to predict the mean of the Gaussian.  
    <button>Example: MSE from MLE with gaussian distr.</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](/main_files/dl_book/22.png){: hidden=""}  

    <p hidden>If $$p_{\text {model }}(\boldsymbol{y} | \boldsymbol{x})=\mathcal{N}(\boldsymbol{y} ; f(\boldsymbol{x} ; \boldsymbol{\theta}), \boldsymbol{I})$$ , then we recover the mean squared error cost, </p> 
    <p hidden>$$J(\theta)=\frac{1}{2} \mathbb{E}_{\mathbf{x}, \mathbf{y} \sim \hat{p}_{\text {data }}}\|\boldsymbol{y}-f(\boldsymbol{x} ; \boldsymbol{\theta})\|^{2}+\mathrm{const} \tag{6.13}$$ </p>  
    <p hidden>up to a scaling factor of $$1/2$$ and a term that does not depend on $$\theta$$.  
    The discarded constant is based on the variance of the Gaussian distribution, which in this case we chose not to parametrize.</p>

    __Why derive the cost function from Maximum Likelihood?__  
    It removes the burden of designing cost functions for each model.  
    Specifying a model $$p(\boldsymbol{y} | \boldsymbol{x})$$ automatically determines a cost function $$\log p(\boldsymbol{y} | \boldsymbol{x})$$.  

    __Cost Function Design - Desirable Properties:__  
    * The *__gradient__* of the cost function must be *__large__* and *__predictable__* enough to serve as a good guide.  
        Functions that *__saturate__* (become very flat) undermine this objective because they make the gradient become very small. In many cases this happens because the activation functions used to produce the output of the hidden units or the output units saturate.  
        The negative log-likelihood helps to avoid this problem for many models. Several output units involve an exp function that can saturate when its argument is very negative. The log function in the negative log-likelihood cost function undoes the exp of some output units.  

    <button>Making cross-entropy-cost-based training coherent</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](/main_files/dl_book/11.png){: hidden=""}  


3. **Learning Conditional Statistics:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    Instead of learning a full probability distribution $$p(\boldsymbol{y} | \boldsymbol{x} ; \boldsymbol{\theta})$$, we often want to learn just one conditional statistic of $$\boldsymbol{y}$$ given $$\boldsymbol{x}$$.  
    > For example, we may have a predictor $$f(\boldsymbol{x} ; \boldsymbol{\theta})$$  that we wish to employ to predict the mean of $$\boldsymbol{y}$$.  

    __The Cost Function as a *Functional:*__  
    If we use a sufficiently powerful neural network, we can think of the neural network as being able to represent any function $$f$$ from a wide class of functions, with this class being limited only by features such as continuity and boundedness rather than by having a specific parametric form. From this point of view, we can view the cost function as being a __functional__ rather than just a function.  
    A __Functional__ is a mapping from functions to real numbers.  
    We can thus think of _learning as choosing a function_ rather than merely choosing a set of parameters.  
    We can design our cost functional to have its minimum occur at some specific function we desire.  
    > For example, we can design the cost functional to have its minimum lie on the function that maps $$\boldsymbol{x}$$  to the expected value of $$\boldsymbol{y}$$ given $$\boldsymbol{x}$$.  

    Solving an optimization problem with respect to a function requires a mathematical tool called __calculus of variations__, described in _section 19.4.2._  

4. **Important Results in Optimization:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    The __calculus of variations__ can be used to derive the following two important results in Optimization:  
    1. Solving the optimization problem  
        <p>$${\displaystyle f^{\ast}=\underset{f}{\arg \min } \: \mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p_{\text {data }}}\|\boldsymbol{y}-f(\boldsymbol{x})\|^{2}} \tag{6.14}$$</p>   
        yields  
        <p>$${\displaystyle f^{\ast}(\boldsymbol{x})=\mathbb{E}_{\mathbf{y} \sim p_{\text {data }}(\boldsymbol{y} | \boldsymbol{x})}[\boldsymbol{y}] \tag{6.15}}$$</p>  
        so long as this function lies within the class we optimize over.  
        In words: if we could train on infinitely many samples from the true data distribution, *__minimizing the MSE cost function__* would give a __*function* that *predicts* the *mean of $$\boldsymbol{y}$$* for *each* value of *$$\boldsymbol{x}$$*__.  
        > Different cost functions give different statistics.  
    2. Solving the optimization problem (commonly known as __Mean Absolute Error__)  
        <p>$$f^{\ast}=\underset{f}{\arg \min } \: \underset{\mathbf{x}, \mathbf{y} \sim p_{\mathrm{data}}}{\mathbb{E}}\|\boldsymbol{y}-f(\boldsymbol{x})\|_ {1} \tag{6.16}$$</p>  
        yields a __*function* that predicts the *median* value of *$$\boldsymbol{y}$$* for each $$\boldsymbol{x}$$__, as long as such a function may be described by the family of functions we optimize over.   

    * [Derivations (linear? prob not)](http://www.stat.cmu.edu/~larry/=stat401/lecture-01.pdf)  


    __Drawbacks of MSE and MAE (mean absolute error):__  
    They often lead to poor results when used with *__gradient-based__* optimization.  
    Some output units that saturate produce very small gradients when combined with these cost functions.  
    This is one reason that the __cross-entropy__ cost function is _more popular_ than __MSE__ or __MAE__, even when it is not necessary to estimate an entire distribution $$p(\boldsymbol{y} | \boldsymbol{x})$$.  



***

## Output Units
{: #content3}

1. **Introduction:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    The choice of cost function is tightly coupled with the choice of output unit. Most of the time, we simply use the cross-entropy between the data distribution and the model distribution.  
    Thus, the choice of _how to represent the output_ then determines the form of the cross-entropy function.  

    Throughout this analysis, we suppose that:  
    _The FNN provides a set of hidden features defined by_ $$\boldsymbol{h}=f(\boldsymbol{x} ; \boldsymbol{\theta})$$.  
    The _role of the output layer_, thus, is to _provide some additional transformation from the features to complete the task_ the FNN is tasked with.  


2. **Linear Units:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    __Linear Units__ are a simple kind of output units, based on an _affine transformation_ with no non-linearity.  
    __Mathematically,__ given features $$\boldsymbol{h}$$, a layer of linear output units produces a vector $$\hat{\boldsymbol{y}}=\boldsymbol{W}^{\top} \boldsymbol{h}+\boldsymbol{b}$$.   
    
    __Application:__ used for *__Gaussian Output Distributions__*.  
    Linear output layers are often used to __produce the mean of a conditional Gaussian Distributions__:  
    <p>$$p(\boldsymbol{y} | \boldsymbol{x})=\mathcal{N}(\boldsymbol{y} ; \hat{\boldsymbol{y}}, \boldsymbol{I}) \tag{6.17}$$</p>   
    In this case, _maximizing the log-likelihood_ is equivalent to _minimizing the MSE_.  

    __Learning the Covariance of the Gaussian:__  

    The MLE framework makes it straightforward to:  
    {: #lst-p}
    * Learn the covariance of the Gaussian too
    * Make the covariance of the Gaussian be a function of the input  
    However, the covariance must be constrained to be a _positive definite matrix_ for all inputs.  
    > It is difficult to satisfy such constraints with a linear output layer, so typically other output units are used to parametrize the covariance.  
    > > Approaches to modeling the covariance are described shortly, in _section 6.2.2.4._  


    __Saturation:__  
    Because linear units do not saturate, they pose little difficulty for gradient- based optimization algorithms and may be used with a wide variety of optimization algorithms.  



3. **Sigmoid Units:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    __Sigmoid Units__ 

    __Binary Classification:__ is a classification problem over two classes. It requires predicting the value of a _binary variable $$y$$_. It is one of many tasks requiring that.  
    __The MLE approach__ is to define a *__Bernoulli distribution__* over $$y$$ conditioned on $$\boldsymbol{x}$$.  
    A __Bernoulli distribution__ is defined by just _a single_ number.  
    The Neural Network needs to predict only $$P(y=1 \vert \boldsymbol{x})$$.  




***
***

TITLE: Model Compression
LINK: research/dl/theory/model_compression.md



* [Pruning Neural Networks: Two Recent Papers (inFERENCe)](https://www.inference.vc/pruning-neural-networks-two-recent-papers/)  
* [The Lottery Ticket Hypothesis - Paper Recommendation (inFERENCe)](https://www.inference.vc/the-lottery-ticket-hypothesis/)  
* [Model Compression (blog medium)](https://medium.com/inveterate-learner/compression-in-the-ai-world-mobilenets-pruning-quantisation-ca6bf1bce265)  


## FIRST
{: #content1}

1. **Knowledge Distillation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    

    __Distilling the Knowledge in a Neural Network *(Hinton et al.)*:__{: style="color: red"}  
    {: #lst-p}
    * [Distilling the Knowledge in a Neural Network (paper)](https://arxiv.org/pdf/1503.02531.pdf)  
    * [Distilling Model Knowledge (thesis)](https://arxiv.org/pdf/1510.02437.pdf)  
    


    <br>






***
***

TITLE: Information Theory
LINK: research/dl/theory/infothry.md


[A Short Introduction to Entropy, Cross-Entropy and KL-Divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8)  
[Deep Learning Information Theory (Cross-Entropy and MLE)](https://jhui.github.io/2017/01/05/Deep-learning-Information-theory/)  
[Further Info (Lecture)](https://www.youtube.com/watch?v=XL07WEc2TRI)  



## Information Theory
{: #content1}

1. **Information Theory:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __Information theory__ is a branch of applied mathematics that revolves around quantifying how much information is present in a signal.    
    In the context of machine learning, we can also apply information theory to continuous variables where some of these message length interpretations do not apply, instead, we mostly use a few key ideas from information theory to characterize probability distributions or to quantify similarity between probability distributions.  
    <br>

2. **Motivation and Intuition:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. A message saying ‚Äúthe sun rose this morning‚Äù is so uninformative as to be unnecessary to send, but a message saying ‚Äúthere was a solar eclipse this morning‚Äù is very informative.  
    Thus, information theory quantifies information in a way that formalizes this intuition:    
    * Likely events should have low information content - in the extreme case, guaranteed events have no information at all  
    * Less likely events should have higher information content
    * Independent events should have additive information. For example, finding out that a tossed coin has come up as heads twice should convey twice as much information as finding out that a tossed coin has come up as heads once.  
    <br>

33. **Measuring Information:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents133}  
    In Shannons Theory, to __transmit $$1$$ bit of information__ means to __divide the recipients *Uncertainty* by a factor of $$2$$__.  

    Thus, the __amount of information__ transmitted is the __logarithm__ (base $$2$$) of the __uncertainty reduction factor__.   

    The __uncertainty reduction factor__ is just the __inverse of the probability__ of the event being communicated.  

    Thus, the __amount of information__ in an event $$\mathbf{x} = x$$, called the *__Self-Information__*  is:  
    <p>$$I(x) = \log (1/p(x)) = -\log(p(x))$$</p>  

    __Shannons Entropy:__  
    It is the __expected amount of information__ of an uncertain/stochastic source. It acts as a measure of the amount of *__uncertainty__* of the events.  
    Equivalently, the amount of information that you get from one sample drawn from a given probability distribution $$p$$.  
    <br>

3. **Self-Information:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    The __Self-Information__ or __surprisal__ is a synonym for the surprise when a random variable is sampled.  
    The __Self-Information__ of an event $$\mathrm{x} = x$$:    
    <p>$$I(x) = - \log P(x)$$</p>  
    Self-information deals only with a single outcome.
    <br>

4. **Shannon Entropy:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    To quantify the amount of uncertainty in an entire probability distribution, we use __Shannon Entropy__.    
    __Shannon Entropy__ is defined as the average amount of information produced by a stochastic source of data.  
    <p>$$H(x) = {\displaystyle \operatorname {E}_{x \sim P} [I(x)]} = - {\displaystyle \operatorname {E}_{x \sim P} [\log P(X)] = -\sum_{i=1}^{n} p\left(x_{i}\right) \log p\left(x_{i}\right)}$$</p>  
    __Differential Entropy__ is Shannons entropy of a __continuous__ random variable $$x$$
    <br>

5. **Distributions and Entropy:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    Distributions that are nearly deterministic (where the outcome is nearly certain) have low entropy; distributions that are closer to uniform have high entropy.
    <br>

6. **Relative Entropy \| KL-Divergence:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    The __Kullback‚ÄìLeibler divergence__ (__Relative Entropy__) is a measure of how one probability distribution diverges from a second, expected probability distribution.    
    __Mathematically:__    
    <p>$${\displaystyle D_{\text{KL}}(P\parallel Q)=\operatorname{E}_{x \sim P} \left[\log \dfrac{P(x)}{Q(x)}\right]=\operatorname{E}_{x \sim P} \left[\log P(x) - \log Q(x)\right]}$$</p>  
    * __Discrete__:    
    <p>$${\displaystyle D_{\text{KL}}(P\parallel Q)=\sum_{i}P(i)\log \left({\frac {P(i)}{Q(i)}}\right)}$$  </p>  
    * __Continuous__:    
    <p>$${\displaystyle D_{\text{KL}}(P\parallel Q)=\int_{-\infty }^{\infty }p(x)\log \left({\frac {p(x)}{q(x)}}\right)\,dx,}$$ </p>  

    __Interpretation:__    
    * __Discrete variables__:  
        it is the extra amount of information needed to send a message containing symbols drawn from probability distribution $$P$$, when we use a code that was designed to minimize the length of messages drawn from probability distribution $$Q$$.  
    * __Continuous variables__:  
            
    __Properties:__   
    * Non-Negativity:  
            $${\displaystyle D_{\mathrm {KL} }(P\|Q) \geq 0}$$  
    * $${\displaystyle D_{\mathrm {KL} }(P\|Q) = 0 \iff}$$ $$P$$ and $$Q$$ are:
        * *__Discrete Variables__*:  
                the same distribution 
        * *__Continuous Variables__*:  
                equal "almost everywhere"  
    * Additivity of _Independent Distributions_:  
            $${\displaystyle D_{\text{KL}}(P\parallel Q)=D_{\text{KL}}(P_{1}\parallel Q_{1})+D_{\text{KL}}(P_{2}\parallel Q_{2}).}$$  
    * $${\displaystyle D_{\mathrm {KL} }(P\|Q) \neq D_{\mathrm {KL} }(Q\|P)}$$  
        > This asymmetry means that there are important consequences to the choice of the ordering   
    * Convexity in the pair of PMFs $$(p, q)$$ (i.e. $${\displaystyle (p_{1},q_{1})}$$ and  $${\displaystyle (p_{2},q_{2})}$$ are two pairs of PMFs):  
            $${\displaystyle D_{\text{KL}}(\lambda p_{1}+(1-\lambda )p_{2}\parallel \lambda q_{1}+(1-\lambda )q_{2})\leq \lambda D_{\text{KL}}(p_{1}\parallel q_{1})+(1-\lambda )D_{\text{KL}}(p_{2}\parallel q_{2}){\text{ for }}0\leq \lambda \leq 1.}$$  

    __KL-Div as a Distance:__     
    Because the KL divergence is non-negative and measures the difference between two distributions, it is often conceptualized as measuring some sort of distance between these distributions.  
    However, it is __not__ a true distance measure because it is __*not symmetric*__.  
    > KL-div is, however, a *__Quasi-Metric__*, since it satisfies all the properties of a distance-metric except symmetry  

    __Applications__      
    Characterizing:  
    * Relative (Shannon) entropy in information systems
    * Randomness in continuous time-series 
    * It is a measure of __Information Gain__; used when comparing statistical models of inference  
    ![img](/main_files/math/prob/11.png){: width="100%"}    
    
    __Example Application and Direction of Minimization__    
    Suppose we have a distribution $$p(x)$$ and we wish to _approximate_ it with another distribution $$q(x)$$.  
    We have a choice of _minimizing_ either:  
    1. $${\displaystyle D_{\text{KL}}(p\|q)} \implies q^\ast = \operatorname {arg\,min}_q {\displaystyle D_{\text{KL}}(p\|q)}$$  
        Produces an approximation that usually places high probability anywhere that the true distribution places high probability.  
    2. $${\displaystyle D_{\text{KL}}(q\|p)} \implies q^\ast \operatorname {arg\,min}_q {\displaystyle D_{\text{KL}}(q\|p)}$$  
        Produces an approximation that rarely places high probability anywhere that the true distribution places low probability.  
        > which are different due to the _asymmetry_ of the KL-divergence  

    <button>Choice of KL-div Direction</button>{: .showText value="show"  
     onclick="showTextPopHide(event);"}
    ![img](/main_files/math/infothry/1.png){: width="100%" hidden=""}
    <br>

7. **Cross Entropy:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    The __Cross Entropy__ between two probability distributions $${\displaystyle p}$$ and $${\displaystyle q}$$ over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an "unnatural" probability distribution $${\displaystyle q}$$, rather than the "true" distribution $${\displaystyle p}$$.    
    <p>$$H(p,q) = \operatorname{E}_{p}[-\log q]= H(p) + D_{\mathrm{KL}}(p\|q) =-\sum_{x }p(x)\,\log q(x)$$</p>  
    
    It is similar to __KL-Div__ but with an additional quantity - the entropy of $$p$$.    
    
    Minimizing the cross-entropy with respect to $$Q$$ is equivalent to minimizing the KL divergence, because $$Q$$ does not participate in the omitted term.  
    
    We treat $$0 \log (0)$$ as $$\lim_{x \to 0} x \log (x) = 0$$.    
    <br>

8. **Mutual Information:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    The __Mutual Information (MI)__ of two random variables is a measure of the mutual dependence between the two variables. More specifically, it quantifies the "amount of information" (in bits) obtained about one random variable through observing the other random variable.  
    It Can be seen as a way of measuring the reduction in uncertainty (information content) of measuring a part of the system after observing the outcome of another parts of the system; given two R.Vs, knowing the value of one of the R.Vs in the system gives a corresponding reduction in (the uncertainty (information content) of) measuring the other one.  

    __As KL-Divergence:__  
    Let $$(X, Y)$$ be a pair of random variables with values over the space $$\mathcal{X} \times \mathcal{Y}$$ . If their joint distribution is $$P_{(X, Y)}$$ and the marginal distributions are $$P_{X}$$ and $$P_{Y},$$ the mutual information is defined as:  
    <p>$$I(X ; Y)=D_{\mathrm{KL}}\left(P_{(X, Y)} \| P_{X} \otimes P_{Y}\right)$$</p>  

    __In terms of PMFs for discrete distributions:__  
    The mutual information of two jointly discrete random variables $$X$$ and $$Y$$ is calculated as a double sum:  
    <p>$$\mathrm{I}(X ; Y)=\sum_{y \in \mathcal{Y}} \sum_{x \in \mathcal{X}} p_{(X, Y)}(x, y) \log \left(\frac{p_{(X, Y)}(x, y)}{p_{X}(x) p_{Y}(y)}\right)$$</p>  
    where $${\displaystyle p_{(X,Y)}}$$ is the joint probability mass function of $${\displaystyle X}$$ X and $${\displaystyle Y}$$, and $${\displaystyle p_{X}}$$ and $${\displaystyle p_{Y}}$$ are the marginal probability mass functions of $${\displaystyle X}$$ and $${\displaystyle Y}$$ respectively.  

    __In terms of PDFs for continuous distributions:__  
    In the case of jointly continuous random variables, the double sum is replaced by a double integral:  
    <p>$$\mathrm{I}(X ; Y)=\int_{\mathcal{Y}} \int_{\mathcal{X}} p_{(X, Y)}(x, y) \log \left(\frac{p_{(X, Y)}(x, y)}{p_{X}(x) p_{Y}(y)}\right) d x d y$$</p>  
    where $$p_{(X, Y)}$$ is now the joint probability density function of $$X$$ and $$Y$$ and $$p_{X}$$ and $$p_{Y}$$ are the marginal probability density functions of $$X$$ and $$Y$$ respectively.  


    __Intuitive Definitions:__{: style="color: red"}  
    {: #lst-p}
    * Measures the information that $$X$$ and $$Y$$ share:  
        It measures how much knowing one of these variables reduces uncertainty about the other.  
        * __$$X, Y$$ Independent__  $$\implies I(X; Y) = 0$$: their MI is zero  
        * __$$X$$ deterministic function of $$Y$$ and vice versa__ $$\implies I(X; Y) = H(X) = H(Y)$$ their MI is equal to entropy of each variable   
    * It's a Measure of the inherent dependence expressed in the joint distribution of  $$X$$ and  $$Y$$ relative to the joint distribution of $$X$$ and $$Y$$ under the assumption of independence.  
        i.e. The price for encoding $${\displaystyle (X,Y)}$$ as a pair of independent random variables, when in reality they are not.  

    __Properties:__{: style="color: red"}  
    {: #lst-p}
    * The KL-divergence shows that $$I(X; Y)$$ is equal to zero precisely when <span>the joint distribution conicides with the product of the marginals i.e. when </span>{: style="color: goldenrod"} __$$X$$ and $$Y$$ are *independent*__{: style="color: goldenrod"}.  
    * The MI is __non-negative__: $$I(X; Y) \geq 0$$  
        * It is a measure of the price for encoding $${\displaystyle (X,Y)}$$ as a pair of independent random variables, when in reality they are not.  
    * It is __symmetric__: $$I(X; Y) = I(Y; X)$$  
    * __Related to conditional and joint entropies:__  
        <p>$${\displaystyle {\begin{aligned}\operatorname {I} (X;Y)&{}\equiv \mathrm {H} (X)-\mathrm {H} (X|Y)\\&{}\equiv \mathrm {H} (Y)-\mathrm {H} (Y|X)\\&{}\equiv \mathrm {H} (X)+\mathrm {H} (Y)-\mathrm {H} (X,Y)\\&{}\equiv \mathrm {H} (X,Y)-\mathrm {H} (X|Y)-\mathrm {H} (Y|X)\end{aligned}}}$$</p>  
        where $$\mathrm{H}(X)$$ and $$\mathrm{H}(Y)$$ are the marginal entropies, $$\mathrm{H}(X | Y)$$ and $$\mathrm{H}(Y | X)$$ are the conditional entopries, and $$\mathrm{H}(X, Y)$$ is the joint entropy of $$X$$ and $$Y$$.  
        * Note the _analogy to the **union, difference, and intersection of two sets**_:  
            ![img](https://cdn.mathpix.com/snip/images/aT2_JfK4TlRP9b5JawVqQigLD7dzxOrFjDIapoSF-F4.original.fullsize.png){: width="35%" .center-image}  
    * __Related to KL-div of conditional distribution:__  
        <p>$$\mathrm{I}(X ; Y)=\mathbb{E}_{Y}\left[D_{\mathrm{KL}}\left(p_{X | Y} \| p_{X}\right)\right]$$</p>  
    * [MI (tutorial #1)](https://www.youtube.com/watch?v=U9h1xkNELvY)  
    * [MI (tutorial #2)](https://www.youtube.com/watch?v=d7AUaut6hso)  


    __Applications:__{: style="color: red"}  
    {: #lst-p}
    * In search engine technology, mutual information between phrases and contexts is used as a feature for k-means clustering to discover semantic clusters (concepts)  
    * Discriminative training procedures for hidden Markov models have been proposed based on the maximum mutual information (MMI) criterion.
    * Mutual information has been used as a criterion for feature selection and feature transformations in machine learning. It can be used to characterize both the relevance and redundancy of variables, such as the minimum redundancy feature selection.
    * Mutual information is used in determining the similarity of two different clusterings of a dataset. As such, it provides some advantages over the traditional Rand index.
    * Mutual information of words is often used as a significance function for the computation of collocations in corpus linguistics.  
    * Detection of phase synchronization in time series analysis
    * The mutual information is used to learn the structure of Bayesian networks/dynamic Bayesian networks, which is thought to explain the causal relationship between random variables  
    * Popular cost function in decision tree learning.
    * In the infomax method for neural-net and other machine learning, including the infomax-based Independent component analysis algorithm



    __Independence assumptions and low-rank matrix approximation (alternative definition):__{: style="color: red"}  
    <button>show analysis</button>{: .showText value="show"  
     onclick="showTextPopHide(event);"}
     ![img](https://cdn.mathpix.com/snip/images/jzmGBSoIKS4x2IrykIaQR3P21Y2z8RS_VmZNKkOfjZQ.original.fullsize.png){: width="100%" hidden=""}  


    __As a Metric (relation to Jaccard distance):__{: style="color: red"}  
    <button>show analysis</button>{: .showText value="show"  
     onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/dOB7qr575sMswJ1MPEbHmFvlqvJp8ncf9ulYlR4aKDY.original.fullsize.png){: width="100%" hidden=""}
    <br>


9. **Pointwise Mutual Information (PMI):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents19}  
    The PMI of a pair of outcomes $$x$$ and $$y$$ belonging to discrete random variables $$X$$ and $$Y$$ quantifies the discrepancy between the probability of their coincidence given their joint distribution and their individual distributions, assuming independence. Mathematically:  
    <p>$$\operatorname{pmi}(x ; y) \equiv \log \frac{p(x, y)}{p(x) p(y)}=\log \frac{p(x | y)}{p(x)}=\log \frac{p(y | x)}{p(y)}$$</p>  
    In contrast to mutual information (MI) which builds upon PMI, it refers to single events, whereas MI refers to the average of all possible events.  
    The mutual information (MI) of the random variables $$X$$ and $$Y$$ is the expected value of the PMI (over all possible outcomes).  


***
***

TITLE: Elements of Machine Learning
LINK: research/dl/theory/dl_book_pt_elmnts_of_ml.md


## Machine Learning Basics
{: #content1}

1. **Introduction and Definitions:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}
    * __Two Approaches to Statistics__:  
        * Frequentest Estimators  
        * Bayesian Inference  
    * __The Design Matrix__:  
        A common way for describing a dataset where it is a matrix containing a different example in each row. Each column of the matrix corresponds to a different feature.  
            

2. **Learning Algorithms:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}
    * __Learning__:  
        A computer program is said to learn from *__experience__* $$E$$ with respect to some class of *__tasks__* $$T$$ and *__performance measure__* $$P$$, if its performance at tasks in $$T$$, as measured by $$P$$, improves with experience $$E$$.  
    * __The Task $$T$$__: 
        * *__Classification__*:  
            A task where the computer program is asked to specify which of $$k$$ categories some input belongs to.  
            To solve this task, the learning algorithm is usually asked to produce a function $$f:\mathbb{R}^n \rightarrow {1, . . . , k}$$.  
            When $$y=f(x)$$, the model assigns an input described by vector $$x$$ to a category identified by numeric code $$y$$.  
            > e.g. Object Recognition
        * *__Classification with Missing Inputs__*:  
            Classification becomes more challenging if the computer program is not guaranteed that every measurement in its input vector will always be provided.  
            To solve this task, rather than providing a single classification function (as in the normal classification case), the learning algorithm must learn a set of functions, each corresponding to classifying $$x$$ with a different subset of its inputs missing.  

            One way to efficiently define such a large set of functions is to learn a probability distribution over all the relevant variables, then solve the classification task by marginalizing out the missing variables.  
            With $$n$$ input variables, we can now obtain all $$2^n$$ different classification functions needed for each possible set of missing inputs, but the computer program needs to learn only a single function describing the joint probability distribution.  
            > e.g. Medical Diagnosis (where some tests weren't conducted for any reason)  
        * *__Regression__*:  
            A computer is asked to predict a numerical value given some input.  
            To solve this task, the learning algorithm is asked to output a function $$f:\mathbb{R}^n \rightarrow R$$  
            > e.g. Object Localization
        * *__Transcription__*:  
            In this type of task, the machine learning system is asked to observe a relatively unstructured representation of some kind of data and transcribe the information into discrete textual form.  
            > e.g. OCR
        * *__Machine Translation__*:  
            In a machine translation task, the input already consists of a sequence of symbols in some language, and the computer program must convert this into a sequence of symbols in another language.  
            > e.g. Google Translate  
        * *__Structured Output__*:  
            Structured output tasks involve any task where the output is a vector (or other data structure containing multiple values) with important relationships between the different elements.  
            This is a broad category and subsumes the transcription and translation tasks described above, as well as many other tasks.  
            These tasks are called structured output tasks because the program must output several values that are all tightly interrelated. For example, the words produced by an image captioning program must form a valid sentence.  
            > e.g. Syntax Parsing, Image Segmentation  
        * *__Anomaly Detection__*:  
            In this type of task, the computer program sifts through a set of events or objects and Ô¨Çags some of them as being unusual or atypical.  
            > e.g. Insider Trading Detection
        * *__Synthesis and Sampling__*:  
            In this type of task, the machine learning algorithm is asked to generate new examples that are similar to those in the training data.  
            This is a kind of structured output task, but with the added qualification that there is no single correct output for each input, and we explicitly desire a large amount of variation in the output, in order for the output to seem more natural and realistic.  
            > e.g. Image Synthesis, Speech Synthesis
        * *__Imputation__*:  
            In this type of task, the machine learning algorithm is given a new example $$x \in \mathbb{R}^n$$, but with some entries $$x_i$$ of $$x$$ missing. The algorithm must provide a prediction of the values of the missing entries.  
        * *__Denoising__*:  
            In this type of task, the machine learning algorithm is given as input a corrupted example $$\tilde{x} \in \mathbb{R}^n$$ obtained by an unknown corruption process from a clean example $$x \in \mathbb{R}^n$$. The learner must predict the clean example $$x$$ from its corrupted version $$\tilde{x}$$, or more generally predict the conditional probability distribution $$p(x |\tilde{x})$$.  
            > e.g. Signal Reconstruction, Image Artifact Removal  
        * *__Density (Probability Mass Function) Estimation__*:  
            In the density estimation problem, the machine learning algorithm is asked to learn a function $$p_\text{model}: \mathbb{R}^n \rightarrow R$$, where $$p_\text{model}(x)$$ can be interpreted as a probability density function (if $$x$$ is continuous) or a probability mass function (if $$x$$ is discrete) on the space that the examples were drawn from.  
            To do such a task well, the algorithm needs to learn the structure of the data it has seen. It must know where _examples cluster tightly_ and where they are _unlikely to occur_.  
            Most of the tasks described above require the learning algorithm to at least implicitly capture the structure of the probability distribution (i.e. it can be computed but we don't have an equation for it). Density estimation enables us to explicitly capture that distribution.  
            In principle,we can then perform computations on that distribution to solve the other tasks as well.  
            For example, if we have performed density estimation to obtain a probability distribution p(x), we can use that distribution to solve the missing value imputation task. Equivalently, if a value $$x_i$$ is missing, and all the other values, denoted $$x_{‚àíi}$$, are given, then we know the distribution over it is given by $$p(x_i| x_{‚àíi})$$.  
                In practice, density estimation does not always enable us to solve all these related tasks, because in many cases the required operations on p(x) are computationally intractable.  
            > e.g. Language Modeling  
        * *__A Lot More__*:  
            Their are many more tasks that could be defined for and solved by Machine Learning. However, this is a list of the most common problems, which have a well-known set of methods for handling them.  

    * __The Performance Measure $$P$$__:  
        A quantitative measure of the performance of a machine learning algorithm.  
        We often use __accuracy__ or __error rate__.

        
11. **Learning vs Optimization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents111}  
    __Generalization:__ is the ability to perform well on previously unobserved inputs.  
    __Generalization (Test) Error:__ is defined as the expected value of the error on a new input.  

    __Learning vs Optimization:__{: style="color: red"}  
    {: #lst-p}
    * The problem of Reducing the __training error__ on the __training set__ is one of *__optimization__*.  
    * The problem of Reducing the __training error__, as well as, the __generalization (test) error__ is one of *__learning__*.  
    <br>

22. **Statistical Learning Theory:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents122}  
    It is a framework that, under certain assumptions, allows us to study the question of "
    How can we affect performance on the test set when we can observe only the training set?"  

    __Assumptions:__  
    {: #lst-p}
    * The training and test data are generated by a _probability distribution over datasets_ called the __data-generating process__.  
    * The __i.i.d. assumptions:__  
        * The examples in each dataset are __independent__ from each other  
        * The _training set_ and _test set_ are __identically distributed__ (drawn from the same probability distribution as each other)  

        This assumption enables us to describe the data-generating process with a probability distribution over a single example. The same distribution is then used to generate every train example and every test example.  
    * We call that shared underlying distribution the __data-generating distribution__, denoted $$p_{\text {data }}$$  

    This probabilistic framework and the i.i.d. assumptions enable us to mathematically study the relationship between training error and test error.  

33. **Capacity, Overfitting, and Underfitting:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents133}  
    The __ML process:__  
    We sample the training set, then use it to choose the parameters to reduce training set error, then sample the test set.  
    Under this process, the __expected test error is greater than or equal to the expected value of training error__.  

    The factors determining how well a machine learning algorithm will perform are its ability to:  
    {: #lst-p}
    1. Make the training error small
    2. Make the gap between training and test error small  
    
    These two factors correspond to the two central challenges in machine learning: __underfitting__ and __overfitting__.  
    __Underfitting:__  occurs when the model is not able to obtain a sufficiently low error value on the training set.  
    __Overfitting:__ occurs when the gap between the training error and test error is too large.  

    We can control whether a model is more likely to overfit or underfit by altering its __capacity__.  
    __Capacity:__ a models capacity is its ability to fit a wide variety of functions.  
    {: #lst-p}
    * Models with __low capacity__ may struggle to fit the training set. 
    * Models with __high capacity__ can overfit by memorizing properties of the training set that do not serve them well on the test set. 

    One way to control the __capacity__ of a learning algorithm is by choosing its __hypothesis space__.  
    __Hypothesis Space:__ the set of functions that the learning algorithm is allowed to select as being the solution.  

    Statistical learning theory provides various means of quantifying model capacity.Among these, the most well known is the __Vapnik-Chervonenkis (VC) dimension__.  
    __The VC Dimension:__ is defined as being the largest possible value of $$m$$ for which there exists a training set of $$m$$ different $$\mathbf{x}$$ points that the classifier can label arbitrarily.  
    It measure the *__capacity of a binary classifier__*.  

    Quantifying the capacity of the model enables statistical learning theory to make quantitative predictions. The most important results in statistical learning theory show that the *__discrepancy between training error and generalization error is bounded from above by a quantity that grows as the model capacity grows but shrinks as the number of training examples increases__*.  

    ![img](/main_files/dl_book/10.png){: width="90%"}   



    > Effective capacity and Representational Capacity...  


33. **Regularization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents133}  
    __Regularization__ is a (more general) way of controlling a models capacity by allowing us to express _preference_ for one function over another in the same hypothesis space; instead of including or excluding members from the hypothesis space completely.  
    > We can think of excluding a function from a hypothesis space as expressing an infinitely strong preference against that function.  

    __Regularization__ can be defined as any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.  

    __Example: Weight Decay__  
    It is a regularization form that adds the $$L^2$$ norm of the __weights__ to the cost function; allowing us to express preference for smaller weights. It is controlled by a hyperparameter $$\lambda$$.  
    <p>$$J(\boldsymbol{w})=\mathrm{MSE}_ {\mathrm{train}}+\lambda \boldsymbol{w}^{\top} \boldsymbol{w}$$</p>   
    This gives us solutions that have a smaller slope, or that put weight on fewer of the features.  

    More generally, the __regularizer__ penalty of __weight decay__ is:  
    <p>$$\Omega(\boldsymbol{w})=\boldsymbol{w}^{\top} \boldsymbol{w}$$</p>   

    <br>

3. **Estimators:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    A __Point Estimator__ or __statistic__ is any function of the data:  
    <p>$$\hat{\boldsymbol{\theta}}_{m}=g\left(\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(m)}\right)$$</p>  
    such that a good estimator is a function whose output is close to the true underlying $$ \theta $$ that generated the training data.  
    > We assume that the true $$\boldsymbol{\theta}$$ is fixed, and that $$\hat{\boldsymbol{\theta}}$$ is a function of the data, which is drawn from a random process, making $$\hat{\boldsymbol{\theta}}$$ a __random variable__.  


    __Function Estimation/Approximation__ refers to estimation of the relationship between _input_ and _target data_.  
    I.E. We are trying to predict a variable $$y$$ given an input vector $$x$$, and we assume that there is a function $$f(x)$$ that describes the approximate relationship between $$y$$ and $$x$$.  
    If we assume that: $$y = f(x) + \epsilon$$, where $$\epsilon$$ is the part of $$y$$ that is not predictable from $$x$$; then we are interested in approximating $$f$$ with a model or estimate $$ \hat{f} $$.  
    > Function estimation is really just the same as estimating a parameter $$\boldsymbol{\theta}$$; the function estimator $$ \hat{f} $$ is simply a point estimator in function space.  

    * [**Estimators as statistics and their probability distributions**](https://www.youtube.com/embed/lr5WH-JVT5I){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/lr5WH-JVT5I"></a>
        <div markdown="1"> </div>    

    <br>


4. **Properties of Estimators - Bias and Variance:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    The __Bias__ of an estimator is:  
    <p>$$ \operatorname{bias}\left(\hat{\boldsymbol{\theta}}_{m}\right)=\mathbb{E}\left(\hat{\boldsymbol{\theta}}_{m}\right)-\boldsymbol{\theta} $$</p>  
    where the expectation is over the data (seen as samples from a random variable) and $$ \theta $$ is the true underlying value of $$ \theta $$ used to define the data-generating distribution.  
    * __Unbiased Estimators:__ An estimator $$\hat{\boldsymbol{\theta}}_{m}$$ is said to be __unbiased__ if $$\operatorname{bias}\left(\hat{\boldsymbol{\theta}}_{m}\right)=\mathbf{0}$$, which implies that $$ \mathbb{E}\left(\hat{\boldsymbol{\theta}}_{m}\right)=\boldsymbol{\theta} $$.  
    * __Asymptotically Unbiased Estimators:__ An estimator is said to be __asymptotically unbiased__ if $$ \lim _{m \rightarrow \infty} \operatorname{bias}\left(\hat{\boldsymbol{\theta}}_{m}\right)=\mathbf{0},$$ which implies that $$\lim _{m \rightarrow \infty} \mathbb{E}\left(\hat{\boldsymbol{\theta}}_{m}\right)=\boldsymbol{\theta} $$  

    The __Variance__ of an estimator is a way to measure how much we expect the estimator to vary as a function of the data sample, defined, simply, as the variance over the training set random variable $$\hat{\theta}$$:  
    <p>$$ \operatorname{Var}(\hat{\theta}) $$</p>  

    The __Standard Error__ $$\operatorname{SE}(\hat{\theta})$$, of an estimator, is the square root of the variance.  
    * E.g. __The Standard Error of the (Sample) Mean:__  
        $$\operatorname{SE}\left(\hat{\mu}_{m}\right)=\sqrt{\operatorname{Var}\left[\frac{1}{m} \sum_{i=1}^{m} x^{(i)}\right]}=\frac{\sigma}{\sqrt{m}}$$  
        Where $$\sigma^2$$ is the true variance of the samples $$x^i$$.  

        <button>Derivation</button>{: .showText value="show"
        onclick="showText_withParent_PopHide(event);"}
        <p hidden="">$$\begin{aligned} \operatorname{Var}\left[\overline{X}_{n}\right] &=\operatorname{Var}\left[\frac{1}{n} \sum_{i=1}^{n} X_{i}\right] \\ &=\frac{1}{n^{2}} \operatorname{Var}\left[\sum_{i=1}^{n} X_{i}\right] \\ &=\frac{1}{n^{2}} \sum_{i=1}^{n} \operatorname{Var}\left[X_{i}\right] \\ &=\frac{1}{n^{2}} \sum_{i=1}^{n} \sigma^{2} \\ &=\frac{1}{n^{2}} n \sigma^{2}=\frac{\sigma^{2}}{n} \end{aligned}$$</p>  

    > Unfortunately, neither the square root of the sample variance nor the square root of the unbiased estimator of the variance provide an unbiased estimate of the standard deviation.  

    * [Properties of the Sample Mean Estimator (Derivations)](https://www.statlect.com/fundamentals-of-statistics/mean-estimation)  
    <br>

5. **Generalization Error from Standard Error (of the mean):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    We often estimate the __generalization error__ by computing the __sample mean__ of the error on the test set.  
    Taking advantage of the central limit theorem, which tells us that the mean will be approximately distributed with a normal distribution, we can use the standard error to compute the probability that the true expectation falls in any chosen interval.  
    For example, the 95 percent confidence interval centered on the mean $$\hat{\mu}_ {m}$$ is:  
    <p>$$\left(\hat{\mu}_{m}-1.96 \mathrm{SE}\left(\hat{\mu}_{m}\right), \hat{\mu}_{m}+1.96 \mathrm{SE}\left(\hat{\mu}_{m}\right)\right)$$</p>  
    under the normal distribution with mean $$\hat{\mu}_{m}$$ and variance $$\mathrm{SE}\left(\hat{\mu}_{m}\right)^{2}$$.  
    We say that algorithm $$\boldsymbol{A}$$ is __better than__ algorithm $$\boldsymbol{B}$$ if the _upper bound_ of the $$95$$ percent confidence interval for the error of algorithm $$\boldsymbol{A}$$ is __less than__ the _lower bound_ of the $$95$$ percent confidence interval for the error of algorithm $$\boldsymbol{B}$$.  

6. **The Bias Variance Trade-off:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    Bias and variance measure two different sources of error in an estimator:  
    * __Bias__: measures the expected deviation from the true value of the function or parameter  
    * __Variance__: provides a measure of the deviation from the expected estimator value that any particular sampling of the data is likely to cause  

    __Evaluating Models - Trading off Bias and Variance:__  
    * The most common way to negotiate this trade-off is to use __cross-validation__
    * Alternatively, we can also compare the __mean squared error (MSE)__ of the estimates:  
        <p>$$\begin{aligned} \mathrm{MSE} &=\mathbb{E}\left[\left(\hat{\theta}_{m}-\theta\right)^{2}\right] \\ &=\operatorname{Bias}\left(\hat{\theta}_{m}\right)^{2}+\operatorname{Var}\left(\hat{\theta}_{m}\right) \end{aligned}$$</p>  
        The __MSE__ measures the overall expected deviation ‚Äî in a squared error sense ‚Äî between the estimator and the true value of the parameter $$\theta$$.  

    <button>Capacity and Bias/Variance</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/dl_book/1.png){: hidden=""}  


7. **Properties of Estimators - Consistency:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    __Consistency__ is a property implying that as the number of data points $$m$$ in our dataset increases, our point estimates converge to the true value of the corresponding parameters. Formally:  
    <p>$$\mathrm{plim}_{m \rightarrow \infty} \hat{\theta}_{m}=\theta$$</p>  
    Where:    
    $${\text { The symbol plim indicates convergence in probability, meaning that for any } \epsilon>0,}$$ 
    <p>$${P\left(\vert\hat{\theta}_{m}-\theta \vert>\epsilon\right) \rightarrow 0 \text { as } m \rightarrow \infty}$$</p>  
    > Sometimes referred to as __Weak Consistency__  

    __Strong Consistency__ applies to *__almost sure convergence__* of $$\hat{\theta}$$ to $$\theta$$.  

    __Consistency and Asymptotic Bias:__  
    * Consistency ensures that the bias induced by the estimator diminishes as the number of data examples grows.  
    * However, asymptotic unbiasedness does __not__ imply consistency

    <br>

8. **Maximum Likelihood Estimation (MLE):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    __MLE__ is a method/principle from which we can derive specific functions that are *__good estimators__* for different models.  

    Let $$\mathbb{X}=\left\{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(m)}\right\}$$ be a set of $$m$$ examples drawn _independently_ from the true but unknown data-generating distribution $$p_{\text { data }}(\mathbf{x})$$, and let $$p_{\text { model }}(\mathbf{x} ; \boldsymbol{\theta})$$ be a _parametric_ family of probability distributions over the same space indexed by $$\boldsymbol{\theta}$$[^4],  
    The Maximum Likelihood Estimator for $$\boldsymbol{\theta}$$ is:  
    <p>$$\begin{aligned} \boldsymbol{\theta}_{\mathrm{ML}} &=\underset{\boldsymbol{\theta}}{\arg \max } p_{\text { model }}(\mathbb{X} ; \boldsymbol{\theta}) \\ &=\underset{\boldsymbol{\theta}}{\arg \max } \prod_{i=1}^{m} p_{\text { model }}\left(\boldsymbol{x}^{(i)} ; \boldsymbol{\theta}\right) \end{aligned}$$</p>  
    We take the $$log$$ for _numerical stability_:  
    <p>$$\boldsymbol{\theta}_{\mathrm{ML}}=\underset{\boldsymbol{\theta}}{\arg \max } \sum_{i=1}^{m} \log p_{\text { model }}\left(\boldsymbol{x}^{(i)} ; \boldsymbol{\theta}\right) \tag{5.58}$$</p>  
    Because the $$\text { arg max }$$ does not change when we rescale the cost function, we can divide by $$m$$ to obtain a version of the criterion that is expressed as an __expectation with respect to the empirical distribution $$\hat{p}_ {\text { data }}$$__  defined by the training data:  
    <p>$$\boldsymbol{\theta}_{\mathrm{ML}}=\underset{\boldsymbol{\theta}}{\arg \max } \mathbb{E}_{\mathbf{x} \sim \hat{p} \text { data }} \log p_{\text { model }}(\boldsymbol{x} ; \boldsymbol{\theta}) \tag{5.59}$$</p>  

    __MLE as Minimizing KL-Divergence between the Empirical dist. and the model dist.:__{: style="color: red"}  
    We can interpret maximum likelihood estimation as _minimizing the dissimilarity_ between the __empirical distribution $$\hat{p}_ {\text { data }}$$__, defined by the training set, and the __model distribution__, with the degree of dissimilarity between the two measured by the __KL divergence__.  
    * The __KL-divergence__ is given by:  
        <p>$$D_{\mathrm{KL}}\left(\hat{p}_{\text { data }} \| p_{\text { model }}\right)=\mathbb{E}_{\mathbf{x} \sim \hat{p}_{\text { data }}}\left[\log \hat{p}_{\text { data }}(\boldsymbol{x})-\log p_{\text { model }}(\boldsymbol{x})\right] \tag{5.60}$$</p>  
    The term on the left is a function only of the data-generating process, not the model. This means when we train the model to minimize the KL divergence, we need only minimize:  
    <p>$$-\mathbb{E}_{\mathbf{x} \sim \hat{p}_{\text { data }}}\left[\log p_{\text { model }}(\boldsymbol{x})\right] \tag{5.61}$$</p>  
    which is of course the same as the _maximization_ in equation $$5.59$$.  

    Minimizing this KL-divergence corresponds exactly to __minimizing the cross-entropy between the distributions__.  
    > Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution defined by the training set and theprobability distribution defined by model.  
    > E.g. __MSE__ is the _cross-entropy_ between the __empirical distribution__ and a __Gaussian model__.  

    We can thus see maximum likelihood as an attempt to _make the model distribution match the empirical distribution $$\hat{p} _ {\text { data }}$$_[^5].  

    Maximum likelihood thus becomes minimization of the negative log-likelihood(NLL), or equivalently, minimization of the cross-entropy[^6].  

    * [MLE as Minimizing KL-div](http://www.jessicayung.com/maximum-likelihood-as-minimising-kl-divergence/)  


    __Conditional Log-Likelihood (MLE for Supervised Learning):__{: style="color: red"}  
    The maximum likelihood estimator can readily be generalized to estimate a _conditional probability $$P(\mathbf{y} | \mathbf{x} ; \boldsymbol{\theta})$$_ in order to predict $$\mathbf{y}$$  given $$\mathbf{x}$$. If $$X$$ represents all our inputs and $$Y$$ all our observed targets, then the conditional maximum likelihood estimator is:  
    <p>$$\boldsymbol{\theta}_ {\mathrm{ML}}=\underset{\boldsymbol{\theta}}{\arg \max } P(\boldsymbol{Y} | \boldsymbol{X} ; \boldsymbol{\theta}) \tag{5.62}$$</p>  
    and the log-likelihood estimator is:  
    <p>$$\boldsymbol{\theta}_{\mathrm{ML}}=\underset{\boldsymbol{\theta}}{\arg \max } \sum_{i=1}^{m} \log P\left(\boldsymbol{y}^{(i)} | \boldsymbol{x}^{(i)} ; \boldsymbol{\theta}\right) \tag{5.63}$$</p>  


    __Properties of Maximum Likelihood Estimator:__{: style="color: red"}  
    The main appeal of the maximum likelihood estimator is that it can be shown to be the _best estimator asymptotically_, as the number of examples $$m \rightarrow \infty$$, in terms of its _rate of convergence_ as $$m$$ increases.  
    * __Consistency__: as the number of training examples approaches infinity, the maximum likelihood estimate of a parameter converges to the true value of the parameter, under the following conditions:  
        * The true distribution $$p_{\text { data }}$$ must lie within the model family $$p_{\text { model }}(\cdot ; \boldsymbol{\theta})$$. Otherwise, no estimator can recover $$p_{\text { data }}$$.  
        * The true distribution $$p_{\text { data }}$$ must correspond to exactly one value of $$\boldsymbol{\theta}$$. Otherwise, maximum likelihood can recover the correct $$p_{\text { data }}$$ but will not be able to determine which value of $$\boldsymbol{\theta}$$ was used by the data-generating process.   
    * __Statistical Efficiency__: meaning that one consistent estimator may obtain lower generalization error for a fixed number of samples $$m$$, or equivalently, may require fewer examples to obtain a fixed level of _generalization error_.[^7]  
        The __Cram√©r-Rao lower bound__ shows that _no consistent estimator has a lower MSE than the maximum likelihood estimator._  
    <br>

9. **Maximum A Posteriori (MAP) Estimation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents19}  
    The __MAP estimate__ chooses the point of _maximal posterior probability_ by allowing the prior to influence the choice of the point estimate:  
    <p>$$\boldsymbol{\theta}_ {\mathrm{MAP}}=\underset{\boldsymbol{\theta}}{\arg \max } p(\boldsymbol{\theta} | \boldsymbol{x})=\underset{\boldsymbol{\theta}}{\arg \max } \log p(\boldsymbol{x} | \boldsymbol{\theta})+\log p(\boldsymbol{\theta}) \tag{5.79}$$</p>  

    Many regularized estimation strategies, such as maximum likelihood learning regularized with weight decay, can be interpreted as making the MAP approximation to Bayesian inference.  
    > E.g. MAP Bayesian inference with a __Gaussian prior__ on the weights corresponds to __weight decay__ Regularization:  
        consider a linear regression model with a Gaussian prior on the weights $$\mathbf{w}$$. If this prior is given by $$\mathcal{N}\left(\boldsymbol{w} ; \mathbf{0}, \frac{1}{\lambda} \boldsymbol{I}^{2}\right)$$, then the log-prior term in equation $$5.79$$ is *__proportional__* to the familiar $$\lambda w^{T} w$$ weight decay penalty, plus a constant.    

    This view applies when the regularization consists of adding an extra term to the objective function that corresponds to $$\log p(\boldsymbol{\theta})$$ (i.e. logarithm of a probability distribution).  
        


[^4]: In other words, $$p_{\text { model }}(x ; \boldsymbol{\theta})$$ maps any configuration $$x$$ to a real number estimating the true probability $$p_{\text { data }}(x)$$.  
[^5]: Ideally, we would like to match the true data-generating distribution $$p_{\text{ data }}$$, but we have no direct access to this distribution.  
[^6]: The perspective of maximum likelihood as minimum KL divergence becomes helpful in this case because the KL divergence has a known minimum value of zero. The negative log-likelihood can actually become negative when $$x$$ is real-valued.  
[^7]: Statistical efficiency (measured by the MSE between the estimated and true parameter) is typically studied in the __parametric case__ (as in linear regression), where our goal is to estimate the value of a parameter (assuming it is possible to identify the true parameter), not the value of a function.  


***

## The Mathematics of Neural Networks
{: #content2}

0. **Derivative:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents20}  
    The derivative of a function is the amount that the value of a function changes when the input changes by an $$\epsilon$$ amount:  
    <p>$$f'(a)=\lim_{h\to 0}{\frac {f(a+h)-f(a)}{h}}. \\
    \text{i.e. } f(x + \epsilon)\approx f(x)+\epsilon f'(x)
    $$</p>  
    
    __The Chain Rule__ is a way to compute the derivative of _composite functions_.  
    If $$y = f(x)$$ and $$z = g(y)$$:  
    <p>$$\dfrac{\partial z}{\partial x} = \dfrac{\partial z}{\partial y} \dfrac{\partial y}{\partial x}$$</p>       

1. **Gradient: (Vector in, Scalar out)**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    Gradients generalize derivatives to __*scalar functions*__ of several variables  
    ![Gradient](/main_files/math/calc/1.png){: width="80%"}  

    __Property:__ the gradient of a function $$\nabla f(x)$$ points in the direction of __steepest ascent__ from $$x$$.  
    <br>

2. **The Jacobian: (Vector in, Vector out)**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
:   The __Jacobian__ of $$f: \mathbb{R}^n \rightarrow \mathbb{R}^m $$ is a matrix of _first-order partial derivatives_ of a __*vector-valued function*__:  
:  ![Jacobian](/main_files/math/calc/2.png){: width="80%"}  
:   __The Chain Rule:__  
    Let $$f : \mathbb{R}^N \rightarrow \mathbb{R}^M$$ and $$g : \mathbb{R}^M \rightarrow \mathbb{R}^ K$$; and let  $$x \in \mathbb{R}^N, y \in \mathbb{R}^M$$, and $$z \in \mathbb{R}^K$$ with $$y = f(x)$$ and $$z = g(y)$$:  
:  $$\dfrac{\partial z}{\partial x} = \dfrac{\partial z}{\partial y} \dfrac{\partial y}{\partial x}$$    
:   where, $$\dfrac{\partial z}{\partial y} \in \mathbb{R}^{K \times M}$$ matrix, $$\dfrac{\partial y}{\partial x} \in \mathbb{R}^{M \times N}$$ matrix, and $$\dfrac{\partial z}{\partial x} \in \mathbb{R}^{K \times N}$$  matrix;  
    the multiplication of $$\dfrac{\partial z}{\partial y}$$  and $$\dfrac{\partial y}{\partial x}$$ is a matrix multiplication.  

2. **The Generalized Jacobian: (Tensor in, Tensor out)**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}
:   __A Tensor__ is a D-dimensional grid of number.  
:   Suppose that $$f: \mathbb{R}^{N_1 \times \cdots \times N_{D_x}} \rightarrow \mathbb{R}^{M_1 \times \cdots \times M_{D_y}} $$.  
    If $$y = f(x)$$ then the derivative $$\dfrac{\partial y}{\partial x}$$ is a __generalized Jacobian__ - an object with shape:  
:   $$(M_1 \times \cdots \times M_{D_y}) \times (N_1 \times \cdots \times N_{D_x})$$ 
:   >  we can think of the generalized Jacobian as generalization of a matrix, where each ‚Äúrow‚Äù has the same shape as $$y$$  and each ‚Äúcolumn‚Äù has the same shape as $$x$$.  
:    Just like the standard Jacobian, the generalized Jacobian tells us the relative rates of change between all elements of $$x$$  and all elements of $$y$$:  
:   $$(\dfrac{\partial y}{\partial x})_{i,j} = \dfrac{\partial y_i}{\partial x_j} \in \mathbb{R}$$
:   Just as the derivative, the generalized Jacobian gives us the relative change in $$y$$ given a small change in $$x$$:  
:   $$f(x + \delta x)\approx f(x)+ f'(x) \delta x = y + \dfrac{\partial y}{\partial x}\delta x$$  
:   where now, $$\delta x$$ is a tensor in $$\mathbb{R}{N_1 \cdots N_{d_x}}$$ and $$\dfrac{\partial y}{\partial x}$$ is a generalized matrix in $$\mathbb{R}^{(M_1 \times \cdots \times M_{D_y}) \times (N_1 \times \cdots \times N_{D_x})} $$.  
    The product $$\dfrac{\partial y_i}{\partial x_j} \delta x$$ is, therefore, a __*generalized matrix-vector multiply*__, which results in a tensor in $$\mathbb{R}^{M_1 \times \cdots \times M_{D_y}}$$.  
:   The __generalized matrix-vector multiply__ follows the same algebraic rules as a traditional matrix-vector multiply:  
:   ![matrix-vector mult](/main_files/math/calc/4.png){: width="80%"}  
:   ![matrix-vector mult-2](/main_files/math/calc/5.png){: width="100%"}  
:   __The Chain Rule:__  
:   ![chain rule](/main_files/math/calc/6.png){: width="100%"}  


3. **The Hessian:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}
:   The __Hessian__ Matrix of a _scalar function_ $$f: \mathbb{R}^d \rightarrow \mathbb{R} $$ is a matric of _second-order partial derivatives_:
:  ![Hessian](/main_files/math/calc/3.png){: width="80%"}
:   __Properties:__ 
        * The Hessian matrix is __*symmetric*__ - since we usually work with smooth/differentiable functions - due to _Clairauts Theorem_.  
        > __Clairauts Theorem:__ if the partial derivatives are continuous, the order of differentiation can be interchanged  
        * The Hessian is used in some optimization algorithms such as Newton‚Äôs method  
        * It is expensive to calculate but can drastically reduce the number of iterations needed to converge to a local minimum by providing information about the curvature of $$f$$


4. **Matrix Calculus:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}
:   __Important Identities:__  
:   $${\frac  {\partial {\mathbf  {a}}^{\top }{\mathbf  {x}}}{\partial {\mathbf  {x}}}}={\frac  {\partial {\mathbf  {x}}^{\top }{\mathbf  {a}}}{\partial {\mathbf  {x}}}}= a \\ 
    {\frac  {\partial {\mathbf  {x}}^{\top }{\mathbf  {A}}{\mathbf  {x}}}{\partial {\mathbf  {x}}}}=  ({\mathbf  {A}}+{\mathbf  {A}}^{\top }){\mathbf  {x}} \\ 
    {\frac  {\partial {\mathbf  {x}}^{\top }{\mathbf  {A}}{\mathbf  {x}}}{\partial {\mathbf  {x}}}}=  2{\mathbf  {A}}{\mathbf  {x}} \:\:\:\:\: \text{[Symmetric } A\text{]}$$
:   [Identities](https://en.wikipedia.org/wiki/Matrix_calculus)
:   __The Product Rule:__  
:   $${\displaystyle {\begin{aligned}\nabla (\mathbf {A} \cdot \mathbf {B} )&=(\mathbf {A} \cdot \nabla )\mathbf {B} +(\mathbf {B} \cdot \nabla )\mathbf {A} +\mathbf {A} \times (\nabla \times \mathbf {B} )+\mathbf {B} \times (\nabla \times \mathbf {A} )\\&=\mathbf {J} _{\mathbf {A} }^{\mathrm {T} }\mathbf {B} +\mathbf {J}_{\mathbf {B} }^{\mathrm {T} }\mathbf {A} \\&=\nabla \mathbf {A} \cdot \mathbf {B} +\nabla \mathbf {B} \cdot \mathbf {A} \ \end{aligned}}}\\ 
    \implies \\ 
    \nabla (fg) = (f')^T g + (g')^T f$$ 
:   Thus, we set our function $$h(x) = \langle f(x), g(x) \rangle = f(x)^T g(x)$$; then,  
:   $$\nabla h(x) = f'(x)^T g(x) + g'(x)^T f(x).$$






***

## Challenges in Machine Learning
{: #content3}

1. **The Curse of Dimensionality:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    It is a phenomena where many machine learning problems become exceedingly difficult when the number of dimensions in the data is high.  

    * The number of possible distinct configurations of a set of variables increases exponentially as the number of variables increases:  
        <button>Capacity and Bias/Variance</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](/main_files/dl_book/2.png){: hidden=""}  
        * __Statistical Challenge:__ the number of possible configurations of $$x$$ is much larger than the number of training examples  

2. **Local Constancy and Smoothness Regularization:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    Prior believes about the particular data-set/learning-problem can be incorporated as:  
    * Beliefs about the __distribution__ of __parameters__ 
    * Beliefs about the __properties__ of the estimating __function__  
        > expressed implicitly by choosing algorithms that are biased toward choosing some class of functions over another, even though these biases may not be expressed (or even be possible to express) in terms of a probability distribution representing our degree of belief in various functions.  

    __The Local Constancy (Smoothness) Prior:__ states that the function we learn should not change very much within a small region.  
    Mathematically, traditional ML methods are designed to encourage the learning process to learn a function $$f^\ast$$ that satisfies the condition:  
    $$\:\:\:\:\:\:\:$$ $$\:\:\:\:\:\:\:$$ $$f^{*}(\boldsymbol{x}) \approx f^{*}(\boldsymbol{x}+\epsilon)$$  
    for most configurations $$x$$ and small change $$\epsilon$$.  
    * <button>Example: K-Means</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](/main_files/dl_book/7.png){: hidden=""}  


    A [__Local Kernel__](/concepts_#bodyContents60) can be thought of as a similarity function that performs template matching, by measuring how closely a test example $$x$$ resembles each training example $$x^{(i)}$$.  
    Much of the modern motivation for Deep Learning is derived from studying the limitations of local template matching and how deep models are able to succeed in cases where local template matching fails _(Bengio et al., 2006b)_.  

    <button>Example: Decision Trees</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    _Decision trees also suffer from the limitations of exclusively smoothness-based learning, because they break the input space into as many regions as there are leaves and use a separate parameter (or sometimes many parameters for extensions of decision trees) in each region. If the target function requires a tree with at least $$n$$ leaves to be represented accurately, then at least $$n$$ training examples are required to fit the tree. A multiple of $$n$$ is needed to achieve some level of statistical confidence in the predicted output._{: hidden=""}  

    In general, to distinguish $$\mathcal{O}(k)$$ regions in input space, all these methods require $$\mathcal{O}(k)$$ examples. Typically there are $$\mathcal{O}(k)$$ parameters, with $$\mathcal{O}(1)$$ parameters associated with each of the $$\mathcal{O}(k)$$ regions.  

    
    __Key Takeaways:__   
    * *__Is there a way to represent a complex function that has many more regions to be distinguished than the number of training examples?__*  
        Clearly, assuming only smoothness of the underlying function will not allow a learner to do that.  
        The smoothness assumption and the associated nonparametric learning algorithms work extremely well as long as there are enough examples for the learning algorithm to observe high points on most peaks and low points on most valleys of the true underlying function to be learned.  
    * *__Is it possible to represent a complicated function efficiently? and if it is complicated, Is it possible for the estimated function to generalize well to new inputs?__*  
        Yes.  
        The key insight is that a very large number of regions, such as $$\mathcal{O}(2^k)$$, can be defined with $$\mathcal{O}(k)$$ examples, so long as we introduce some dependencies between the regions through additional assumptions about the underlying data-generating distribution.
        In this way, we can actually generalize non-locally _(Bengio and Monperrus, 2005; Bengio et al., 2006c)_.  
    * *__Deep Learning VS Machine Learning:__*  
        The core idea in deep learning is that we assume that the data was generated by the composition of factors, or features, potentially at multiple levels in a hierarchy.  
        These apparently mild assumptions allow an exponential gain in the relationship between the number of examples and the number of regions that can be distinguished.  
        The exponential advantages conferred by the use of deep distributed representations counter the exponential challenges posed by the curse of dimensionality.  
        > __Further Reading:__ (on the exponential gain) Sections: 6.4.1, 15.4 and 15.5.  
    

3. **Manifold Learning:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    A __Manifold__ - a connected region - is a set of points associated with a neighborhood around each point. From any given point, the manifold locally appears to be a Euclidean space.  
    ![img](/main_files/dl_book/8.png){: width="100%"}  

    __Manifolds in ML:__  
    In ML, the term is used loosely to designate a connected set of points that can be approximated well by considering only a small number of degrees of freedom, or dimensions, embedded in a higher-dimensional space. Each dimension corresponds to a local direction of variation.  
    In the context of machine learning, we allow the dimensionality of the manifold to vary from one point to another. This often happens when a manifold intersects itself. For example, a figure eight is a manifold that has a single dimension in most places but two dimensions at the intersection at the center.  
    __Manifold Assumptions:__  
    <button>Discussion</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    _Many machine learning problems seem hopeless if we expect the machine learning algorithm to learn functions with interesting variations across all of $$\mathbb{R}^n$$. Manifold learning algorithms surmount this obstacle by assuming that most of $$\mathbb{R}^n$$ consists of invalid inputs, and that interesting inputs occur only a long a collection of manifolds containing a small subset of points, with interesting variations in the output of the learned function occurring only along directions that lie on the manifold, or with interesting variations happening only when we move from one manifold to another. Manifold learning was introduced in the case of continuous-valued data and in the unsupervised learning setting, although this probability concentration idea can be generalized to both discrete data and the supervised learning setting: the key assumption remains that probability mass is highly concentrated._{: hidden=""}  
    We assume that the *__data lies along a low-dimensional manifold__*:  
    * May not always be correct or useful  
    * In the context of AI tasks (e.g. processing images, sounds, or text): At least approximately correct.  
        To show that is true we need to argue two points:  
        * The probability distribution over images, text strings, and sounds that occur in real life is highly concentrated.  
            <button>Example/proof: The Manifold of Natural Images</button>{: .showText value="show" onclick="showTextPopHide(event);"}
            ![img](/main_files/dl_book/9.png){: hidden=""}   
            > Uniform noise essentially never resembles structured inputs from these domains.   
        * We must, also, establish that the examples we encounter are connected to each other by other examples, with each example surrounded by other highly similar examples that can be reached by applying transformations to traverse the manifold:  
            _Informally,_ we can imagine such neighborhoods and transformations:  
            In the case of images, we can think of many possible transformations that allow us to trace out a manifold in image space: we can gradually dim or brighten the lights, gradually move or rotate objects in the image, gradually alter the colors on the surfaces of objects, and so forth.  
            > Multiple manifolds are likely involved in most applications. For example,the manifold of human face images may not be connected to the manifold of cat face images.  
            > Rigorous Results: _(Cayton, 2005; Narayanan and Mitter,2010; Sch√∂lkopf et al., 1998; Roweis and Saul, 2000; Tenenbaum et al., 2000; Brand,2003; Belkin and Niyogi, 2003; Donoho and Grimes, 2003; Weinberger and Saul,2004)_  

    __Benefits:__  
    When the data lies on a low-dimensional manifold, it can be most natural for machine learning algorithms to represent the data in terms of coordinates on the manifold, rather than in terms of coordinates in $$\mathbb{R}^n$$.  
    E.g. In everyday life, we can think of roads as 1-D manifolds embedded in 3-D space. We give directions to specific addresses in terms of address numbers along these 1-D roads, not in terms of coordinates in 3-D space.  
    > Learning Manifold Structure: _figure 20.6_  



***
***

TITLE: The Theory of Learning
LINK: research/dl/theory/learning_problem_caltech.md


[Lecture on Statistical Learning Theory from Risk perspective & Bayes Decision Rule](https://www.youtube.com/watch?v=rqJ8SrnmWu0&list=PLnZuxOufsXnvftwTB1HL6mel1V32w0ThI&index=4)  
[Learning Theory Andrew NG (CS229 Stanford)](https://www.youtube.com/watch?v=tojaGtMPo5U&list=PLA89DCFA6ADACE599&index=9)  
[Empirical Risk Minimization (Cornell)](https://www.youtube.com/watch?v=AkmPv2WEsHw)  


## The Learning Problem
{: #content1}

1. **When to use ML:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    When:  
    1. A pattern Exists
    2. We cannot pin the pattern down mathematically 
    3. We have Data  

    We usually can do without the first two. But the third condition we __CANNOT__ do without.  
    The Theory of Learning only depends on the data.  

    > "We have to have data. We are learning from data. So if someone knocks on my door with an interesting machine learning application, and they tell me how exciting it is, and how great the application would be, and how much money they would make, the first question I ask, __'what data do you have?'__. If you have data, we are in business. If you don't, you are _out of luck_." - Prof. Ng


2. **The ML Approach to Problem Solving:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    Consider the Netflix problem: _Predicting how a viewer will rate a movie_.   
    * __Direct Approach__:  
        ![img](/main_files/dl/theory/caltech/1.png){: width="40%"}  
        * Ask each user to give a rank/rate for the different "factors/features" (E.g. Action, Comedy, etc.)  
        * Watch each movie and assign a rank/rate for the same factors  
        * Match the factors and produce a __rating__  
    * __ML Approach__:  
        ![img](/main_files/dl/theory/caltech/2.png){: width="40%"}  
        Essentially it is a __Reversed__ approach  
        * Start with the __Ratings__ (dataset) that the users assigned to each movie  
        * Then _deduce_ the "factors/features" that are consistent with those Ratings  
            Note: we usually start with random initial numbers for the factors  
    <br>

3. **Components of Learning:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    * __Input__: $$\vec{x}$$  
    * __Output__: $$y$$ 
    * __Data__:  $${(\vec{x}_ 1, y_ 1), (\vec{x}_ 2, y_ 2), ..., (\vec{x}_ N, y_ N)}$$ 
    * __Target Function__: $$f : \mathcal{X} \rightarrow \mathcal{Y}$$  (Unknown/Unobserved)  
    * __Hypothesis__: $$g : \mathcal{X} \rightarrow \mathcal{Y}$$  
    <br>

5. **Components of the Solution:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    * __The Learning Model__:  
        * __The Hypothesis Set__:  $$\mathcal{H}=\{h\},  g \in \mathcal{H}$$  
            > E.g. Perceptron, SVM, FNNs, etc.  
        * __The Learning Algorithm__: picks $$g \approx f$$ from a hypothesis set $$\mathcal{H}$$  
            > E.g. Backprop, Quadratic Programming, etc.  

    Motivating the inclusion of a _Hypothesis Set_:  
    * __No Downsides__: There is __no loss of generality__ by including a hypothesis set, since any restrictions on the elements of the set have no effect on what the learning algorithms  
        Basically, there is no downside because from a practical POV thats what you do; by choosing an initial approach, e.g. SVM, Linear Regression, Neural Network, etc., we are already dictating a hypothesis set. If we don't choose one, then the hypothesis set has no restrictions and is the set of all possible hypothesis without loss of generalization.  
    * __Upside__: The hypothesis set plays a pivotal role in the _theory of learning_, by dictating whether we can learn or not.  
    <br>

6. **The Basic Premise/Goal of Learning:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    "Using a set of observations to uncover an underlying process"  
    Rephrased mathematically, the __Goal of Learning__ is:   
    Use the Data to find a hypothesis $$g \in \mathcal{H}$$, from the hypothesis set $$\mathcal{H}=\{h\}$$, that _approximates_ $$f$$ well.  
    <br>

7. **Types of Learning:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    * __Supervised Learning__: the task of learning a function that maps an input to an output based on example input-output pairs.  
        ![img](/main_files/dl/theory/caltech/4.png){: width="70%"}  
    * __Unsupervised Learning__: the task of making inferences, by learning a better representation, from some datapoints that do not have any labels associated with them.  
        ![img](/main_files/dl/theory/caltech/5.png){: width="70%"}  
        > Unsupervised Learning is another name for [Hebbian Learning](https://en.wikipedia.org/wiki/Hebbian_theory)
    * __Reinforcement Leaning__: the task of learning how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward.  
        ![img](/main_files/dl/theory/caltech/6.png){: width="70%"}  
    <br>

8. **The Learning Diagram:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    ![img](/main_files/dl/theory/caltech/3.png){: width="70%"}  

***

## The Feasibility of Learning
{: #content2}

The Goal of this Section is to answer the question: Can we make any statements/inferences outside of the sample data that we have?  

1. **The Problem of Learning:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    Learning a truly __Unknown__ function is __Impossible__, since outside of the observed values, the function could assume _any value_ it wants.  

2. **The Bin Analogy - A Related Experiment::**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    ![img](/main_files/dl/theory/caltech/7.png){: width="70%"}  

    $$\mu$$ is a constant that describes the actual/real probability of picking the red marble.  
    $$\nu$$, however, is random and depends on the frequency of red marbles in the particular sample that you have collected.    

    Does $$\nu$$ approximate $$\mu$$?  
    * The short answer is __NO__:  
        The Sample an be mostly green while bin is mostly red.  
    * The Long answer is __YES__:  
        The Sample frequency $$\nu$$ is likely/probably close to bin frequency $$\mu$$.  
        > Think of a presidential poll of 3000 people that can predict how the larger $$10^8$$ mil. people will vote  

    The Main distinction between the two answers is in the difference between *__Possible__* VS *__Probable__*. 

    What does $$\nu$$ say about $$\mu$$?  
    In a big sample (Large $$N$$), $$\nu$$ is _probably_ close to $$\mu$$ (within $$\epsilon$$).   
    Formally, we the __Hoeffding's Inequality__:  
    <p>$$\mathbb{P}[|\nu-\mu|>\epsilon] \leq 2 e^{-2 \epsilon^{2} N}$$</p>  
    In other words, the probability that $$\nu$$ does not approximate $$\mu$$ well (they are not within an $$\epsilon$$ of each other), is bounded by a negative exponential that dampens fast but depends directly on the tolerance $$\epsilon$$.  
    > This reduces to the statement that "$$\mu = \nu$$" is PAC (PAC: Probably, Approximately Correct).    

    Properties:  
    * It is valid for $$N$$ and $$\epsilon$$. 
    * The bound does not depend on the value of $$\mu$$.  
    * There is a __Trade-off__ between the number of samples $$N$$ and the tolerance $$\epsilon$$.  
    * Saying that $$\nu \approx \mu \implies \mu \approx \nu$$, i.e. saying $$\nu$$ is approximately the same as $$\mu$$, implies that $$\mu$$ is approximately the same as $$\nu$$ (yes, tautology).   
        The logic here is subtle:  
        * Logically, the inequality is making a statement on $$\nu$$ (the random variable), it is saying that $$\nu$$ tends to be close to $$\mu$$ (the constant, real probability).  
        * However, since the inequality is symmetric, we are using the inequality to infer $$\mu$$ from $$\nu$$.  
            But that is not the cause and effect that actually takes place. $$\mu$$, actually, affects $$\nu$$.  

    Translating to the Learning Problem:  
    ![img](/main_files/dl/theory/caltech/8.png){: width="70%"}  
    > Notice how the meaning of the accordance between $$\mu$$ and $$\nu$$  is not accuracy of the model, but rather accuracy of the TEST.  

    Back to the Learning Diagram:  
    ![img](/main_files/dl/theory/caltech/9.png){: width="70%"}  
    The marbles in the bin correspond to the input space (datapoints). This adds a NEW COMPONENT to the Learning problem - the probability of generating the input datapoints (up to this point we treated learning in an absolute sense based on some fixed datapoints).   
    To adjust the statement of the learning problem to accommodate the new component:  
    we add a probability distribution $$P$$  over the input space $$\mathcal{X}$$. This, however, doesn't restrict the argument at all; we can invoke any probability on the space, and the machinery still holds. We, also, do not, even, need to know what $$P$$ is (even though $$P$$ affects $$\mu$$), since Hoeffding's Inequality allows us to bound the LHS with no dependence on $$\mu$$.  
    Thus, now we assume that the input datapoints $$\vec{x}_1, ..., \vec{x}_N$$ are assumed to be generated by $$P$$, __independently__.  
    So this is a very benign addition, that would give us high dividends - The Feasibility of Learning.    

    However, this is not learning; it is __Verification__. Learning involves using an algorithm to search a space $$\mathcal{H}$$  and try different functions $$h \in \mathcal{H}$$. Here, we have already picked some specific function and are testing its performance on a sample, using maths to guarantee the accuracy of the test within some threshold we are willing to tolerate.  


    Extending Hoeffding's Inequality to Multiple hypotheses $$h_i$$:  
    ![img](/main_files/dl/theory/caltech/10.png){: width="70%"}  
    ![img](/main_files/dl/theory/caltech/11.png){: width="70%"}  

    Putting the right notation:  
    ![img](/main_files/dl/theory/caltech/12.png){: width="70%"}  
    ![img](/main_files/dl/theory/caltech/13.png){: width="70%"}  

    <button>Why Hoeffding Inequality doesn't apply for multiple bins</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](/main_files/dl/theory/caltech/14.png){: hidden=""}  
    > i.e. the 10 heads are not a good indication of the real probability  
    
    <button>From coins to learning</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](/main_files/dl/theory/caltech/15.png){: hidden=""}  
    Equivalently, in learning, if the hypothesis set size is 1000, and there are 10 points we test against, the probability that one of those hypothesis performing well on the 10 points, but actually being a bad hypothesis is high, and increases with the hypothesis set size.  
    > Hoeffding's inequality has a guarantee for one experiment, that gets terribly diluted as you increase the number of experiments.  

    Solution:  
    We follow the very same reasoning: we want to know the probability of at least one failing. This can be bounded by the union bound, which intuitively says that the maximum probability of at least an event occurring in N is when all the events are independent, in which case you just sum up the probabilities:  
    <p>$$\begin{aligned} \mathbb{P}\left[ | E_{\text {in }}(g)-E_{\text {out }}(g) |>\epsilon\right] \leq \mathbb{P}[ & | E_{\text {in }}\left(h_{1}\right)-E_{\text {out }}\left(h_{1}\right) |>\epsilon \\ & \text {or } | E_{\text {in }}\left(h_{2}\right)-E_{\text {out }}\left(h_{2}\right) |>\epsilon \\ & \cdots \\ & \text {or } | E_{\text {in }}\left(h_{M}\right)-E_{\text {out }}\left(h_{M}\right) |>\epsilon ] \\ \leq & \sum_{m=1}^{M} \mathbb{P}\left[ | E_{\text {in }}\left(h_{m}\right)-E_{\text {out }}\left(h_{m}\right) |>\epsilon\right] \end{aligned}$$</p>  
    Which implies:  
    <p>$$\begin{aligned} \mathbb{P}\left[ | E_{\text {in }}(g)-E_{\text {out }}(g) |>\epsilon\right] & \leq \sum_{m=1}^{M} \mathbb{P}\left[ | E_{\text {in }}\left(h_{m}\right)-E_{\text {out }}\left(h_{m}\right) |>\epsilon\right] \\ & \leq \sum_{m=1}^{M} 2 e^{-2 \epsilon^{2} N} \end{aligned}$$</p>   
    Or,  
    <p>$$\mathbb{P}\left[ | E_{\ln }(g)-E_{\text {out }}(g) |>\epsilon\right] \leq 2 M e^{-2 \epsilon^{2} N}$$</p>  
    The more sophisticated the model you use, the looser that in-sample will track the out-of-sample. Because the probability of them deviating becomes bigger and bigger and bigger.  
    The conclusion may seem both awkward and obvious, but the bigger the hypothesis set, the higher the probability of at least one function being very bad. In the event that we have an infinite hypothesis set, of course this bound goes to infinity and tells us nothing new.  

    [References](http://testuggine.ninja/notes/feasibility-of-learning#fnref:limited)


3. **The Learning Analogy:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    For an exam, the practice problems are the training set. You're going to look at the question. You're going to answer. You're going to compare it with the real answer. And then you are going to adjust your hypothesis, your understanding of the material, in order to do it better, and go through them and perhaps go through them again, until you get them right or mostly right or figure out the material (this makes you better at taking the exam).
    We don't give out the actual exams questions because __acing the final is NOT the goal__, the goal is to __learn the material (have a small $$E_{\text{out}}$$)__. The final exam is only a way of gauging how well you actually learned. And in order for it to gauge how well you actually learned, I have to give you the final at the point you have already fixed your hypothesis. You prepared. You studied. You discussed with people. You now sit down to take the final exam. So you have one hypothesis. And you go through the exam. hopefully, will reflect what your understanding will be outside.  
    > The exam measures $$E_{\text{in}}$$, and we know that it tracks $$E_{\text{out}}$$ (by Hoeffding), so it tracks well how you understand the material proper.  



8. **Notes:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}  
    * __Learning Feasibility__:  
        When learning we only deal with In-Sample Errors $$[E_{\text{in}}(\mathbf{w})]$$; we never handle the out-sample error explicitly; we take the theoretical guarantee that when you do well in-sample $$\implies$$ you do well out-sample (Generalization).  

***

## Error and Noise
{: #content3}

The Current Learning Diagram:  
![img](/main_files/dl/theory/caltech/16.png){: width="50%"}  

1. **Error Measures:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    __Error Measures__ aim to answer the question:  
    "What does it mean for $$h$$ to approximate $$f$$ ($$h \approx f$$)?"  
    The __Error Measure__: $$E(h, f)$$  
    It is almost always defined point-wise: $$\mathrm{e}(h(\mathbf{X}), f(\mathbf{X}))$$.  
    Examples:  
    * __Square Error__:  $$\:\:\:\mathrm{e}(h(\mathbf{x}), f(\mathbf{x}))=(h(\mathbf{x})-f(\mathbf{x}))^{2}$$  
    * __Binary Error__:  $$\:\:\:\mathrm{e}(h(\mathbf{x}), f(\mathbf{x}))=[h(\mathbf{x}) \neq f(\mathbf{x})]$$  (1 if true else 0)  

    The __overall error__ $$E(h,f) = $$ _average_ of pointwise errors $$\mathrm{e}(h(\mathbf{x}), f(\mathbf{x}))$$:  
    * __In-Sample Error__:  
        <p>$$E_{\mathrm{in}}(h)=\frac{1}{N} \sum_{n=1}^{N} \mathrm{e}\left(h\left(\mathbf{x}_{n}\right), f\left(\mathbf{x}_{n}\right)\right)$$</p>    
    * __Out-Sample Error__:  
        <p>$$E_{\text {out }}(h)=\mathbb{E}_ {\mathbf{x}}[\mathrm{e}(h(\mathbf{x}), f(\mathbf{x}))]$$</p>  

2. **The Learning Diagram - with pointwise error:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    ![img](/main_files/dl/theory/caltech/17.png){: width="50%"}  
    There are two additions to the diagram:  
    * The first is to realize that we are defining the error measure __on a point__. 
    * Another is that in deciding whether $$g$$  is close to $$f$$ , which is the goal of learning, we test this with a point $$x$$. And the criterion for deciding whether $$g(x)$$ is approximately the same as $$f(x)$$ is our pointwise error measure.  

3. **Defining the Error Measure:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  

    __Types:__  
    * False Positive
    * False Negative  

    There is no inherent merit to choosing one error function over another. It's not an analytic question. It's an application-domain question.  
    <button>Examples - Supermarket</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](/main_files/dl/theory/caltech/18.png){: hidden=""}  
    <button>Examples - CIA</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](/main_files/dl/theory/caltech/19.png){: hidden=""}  

    The error measure should be _specified by the user_. Since, that's not always possible, the alternatives:  
    * __Plausible Measures__: measures that have an _analytic argument_ for their merit, based on certain _assumptions_.  
        E.g. _Squared Error_ comes from the _Gaussian Noise_ Assumption.  
    * __Friendly Measures__: An _easy-to-use_ error measure, without much justification.  
        E.g. Linear Regression error leads to the easy closed-form solution, Convex Error measures are easy to optimize, etc.  
                  

4. **The Learning Diagram - with the Error Measure:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    ![img](/main_files/dl/theory/caltech/20.png){: width="50%"}  
    The __Error Measure__ provides a quantitative assessment of the statement $$g(\mathbf{x}) \approx f(\mathbf{x})$$.  


5. **Noisy Targets:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}  
    The _'Target Function'_ is not always a _function_ because two _'identical'_ input points can be mapped to two different outputs (i.e. they have different labels).  

    The solution: Replacing the target function with a __Target Distribution__.  
    Instead of $$y = f(x)$$ we use the  _conditional target distribution_: $$P(y | \mathbf{x})$$. What changes now  is that, instead of $$y$$  being deterministic of $$\mathbf{x}$$, once you generate $$\mathbf{x}$$, $$y$$  is also probabilistic-- generated by $$P(y | \mathbf{x})$$.  
    $$(\mathbf{x}, y)$$ is now generated by the __joint distribution__:  
    <p>$$P(\mathbf{x}) P(y | \mathbf{x})$$</p>  

    Equivalently, we can define a __Noisy Target__ as a _deterministic (target) function_ $$\:f(\mathbf{x})=\mathbb{E}(y | \mathbf{x})\:$$  PLUS _Noise_ $$\: y-f(x)$$.     
    This can be done WLOG since a _deterministic target_ is a special kind of a _noisy target_:  
    Define $$P(y \vert \mathbf{x})$$ to be identically Zero, except for $$y = f(x)$$.   


6. **The Learning Diagram - with the Noisy Target:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents36}  
    ![img](/main_files/dl/theory/caltech/21.png){: width="50%"}  

    Now, $$E_{\text {out}}(h) = \mathbb{E}_ {x, y}[e(h(x), y)]$$ instead of $$\mathbb{E}_ {\mathbf{x}}[\mathrm{e}(h(\mathbf{x}), f(\mathbf{x}))]$$, and  
    $$\left(\mathbf{x}_{1}, y_{1}\right), \cdots,\left(\mathbf{x}_{N}, y_{N}\right)$$ are generated independently of each (each tuple).  


7. **Distinction between $$P(y | \mathbf{x})$$ and $$P(\mathbf{x})$$:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents37}  
    The __Target Distribution__ $$P(y \vert \mathbf{x})$$ is what we are *__trying to learn__*.  
    The __Input Distribution__ $$P(\mathbf{x})$$, only, *__quantifies relative importance__*  of $$\mathbf{x}$$; we are __NOT__ trying to learn this distribution.  
    > Rephrasing: Supervised learning only learns $$P(y \vert \mathbf{x})$$ and not $$P(\mathbf{x})$$; $$P(\mathbf{x}, y)$$ is __NOT__ a target distribution for Supervised Learning.  

    Merging $$P(\mathbf{x})P(y \vert \mathbf{x})$$ as $$P(\mathbf{x}, y)$$, although allows us to generate examples $$(\mathbf{x}, y)$$, mixes the two concepts that are inherently different.  


8. **Preamble to Learning Theory:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents38}  
    __Generalization VS Learning:__  
    We know that _Learning is Feasible_.
    * __Generalization__:  
        It is likely that the following condition holds:  
        <p>$$\: E_{\text {out }}(g) \approx E_{\text {in }}(g)  \tag{3.1}$$</p>  
        This is equivalent to "good" __Generalization__.  
    * __Learning__:  
        Learning corresponds to the condition that $$g \approx f$$, which in-turn corresponds to the condition:  
        <p>$$E_{\text {out }}(g) \approx 0  \tag{3.2}$$</p>      


    __How to achieve Learning:__{: style="color: red"}    
    We achieve $$E_{\text {out }}(g) \approx 0$$ through:  
    {: #lst-p}
    1. $$E_{\mathrm{out}}(g) \approx E_{\mathrm{in}}(g)$$  
        A __theoretical__ result achieved through Hoeffding __PROBABILITY THEORY__{: style="color: red"}  .   
    2. $$E_{\mathrm{in}}(g) \approx 0$$  
        A __Practical__ result of minimizing the In-Sample Error Function (ERM) __Optimization__{: style="color: red"}  .  

    Learning is, thus, reduced to the 2 following questions:  
    {: #lst-p}
    1. Can we make sure that $$E_{\text {out }}(g)$$ is close enough to $$E_{\text {in }}(g)$$? (theoretical)  
    2. Can we make $$E_{\text {in}}(g)$$ small enough? (practical)  


    What the Learning Theory will achieve:  
    {: #lst-p}
    * Characterizing the _feasibility of learning_ for __infinite $$M$$__ (hypothesis).  
        We are going to measure the model not by the number of hypotheses, but by a single parameter which tells us the sophistication of the model. And that sophistication will reflect the out-of-sample performance as it relates to the in-sample performance (through the Hoeffding (then VC) inequalities).   
    * Characterizing the tradeoff:  
        ![img](/main_files/dl/theory/caltech/22.png){: width="50%"}   
        In words:  
        We realized that we would like our model, the hypothesis set, to be elaborate, in order to be able to fit the data. The more parameters you have, the more likely you are going to fit the data and get here. So the $$E_{\text{in}}$$ goes down if you use more complex models. However, if you make the model more complex, the discrepancy between $$E_{\text{out}}$$ and $$E_{\text{in}}$$ gets worse and worse. $$E_{\text{in}}$$ tracks $$E_{\text{out}}$$ much more loosely than it used to.  




## Linear Models I
{: #content4}

![img](/main_files/dl/theory/caltech/3.jpg){: width="100%"}    

![img](/main_files/dl/theory/caltech/4.jpg){: width="100%"}    

* __[Interpreting Linear Classifiers (cs231n)](https://cs231n.github.io/linear-classify/#interpret)__  


    * [**Linear Classifiers Demo (cs231n)**](http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/){: value="show" onclick="iframePopA(event)"}
    <a href="http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/"></a>
        <div markdown="1"> </div>  
    * [Linear Regression from Conditional Distribution (gd example)](https://stats.stackexchange.com/questions/407812/derive-linear-regression-model-from-the-conditional-distribution-of-yx)  
    * [__Linear Regression Probabilistic Development__](http://bjlkeng.github.io/posts/a-probabilistic-view-of-regression/)  
    * In a linear model, if the errors belong to a normal distribution the least squares estimators are also the maximum likelihood estimators.[^1]  
            

[^1]: [Reference: Equivalence of Generalized-LS and MLE in Exponential Family](https://www.researchgate.net/publication/254284684_The_Equivalence_of_Generalized_Least_Squares_and_Maximum_Likelihood_Estimates_in_the_Exponential_Family)  
    




## The Linear Model II
{: #content5}

1. **Linear Models - Logistic Regression:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents51}  
    ![img](/main_files/dl/theory/caltech/25.png){: width="80%"}   
    The __Logistic Regression__ applies a _non-linear transform_  on the _signal_; it's a softer approximation to the hard-threshold non-linearity applied by _Linear Classification_.  


2. **The Logistic Function $$\theta$$:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents52}  
    <p>$$\theta(s)=\frac{e^{s}}{1+e^{s}}=\frac{1}{1+e^{-s}}$$</p>  

    ![img](/main_files/dl/theory/caltech/26.png){: width="80%"}   
    * __Soft Threshold__: corresponds to uncertainty; interpreted as probabilities.  
    * __Sigmoid__: looks like a flattened out 'S'.  
        

3. **The Probability Interpretation:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents53}  
    $$h(\mathbf{x})=\theta(s)$$ is interpreted as a probability.  
    It is in-fact a __Genuine Probability__. The output of logistic regression is treated genuinely as a probability even during learning.  
    Justification:  
    Data $$(\mathbf{x}, y)$$ with binary $$y$$, (we don't have direct access to probability, but the binary $$y$$ is affected by the probability), generated by a noisy target:  
    <p>$$P(y | \mathbf{x})=\left\{\begin{array}{ll}{f(\mathbf{x})} & {\text {for } y=+1} \\ {1-f(\mathbf{x})} & {\text {for } y=-1}\end{array}\right.$$</p>  
    The target $$f : \mathbb{R}^{d} \rightarrow[0,1]$$ is the probability.  
    We learn $$\:\:\:\: g(\mathbf{x})=\theta\left(\mathbf{w}^{\top} \mathbf{x}\right) \approx f(\mathbf{x})$$.   
    > In words: So I'm going to call the probability the target function itself. The probability that someone gets heart attack is $$f(\mathbf{x})$$. And I'm trying to learn $$f$$, notwithstanding the fact that the examples that I am getting are giving me just sample values of $$y$$, that happen to be generated by $$f$$.  

    <button>Further Analysis</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    * Logistic Regression uses the __sigmoid__ function to "squash" the output feature/signal into the $$[0, 1]$$ space.  
        Although, one could interpret the _sigmoid classifier_ as just a function with $$[0,1]$$ range, it is actually, a __Genuine Probability__.  
    * To see this:  
        * A labeled, classification Data-Set, does __NOT__ (explicitly) give you the _probability_ that something is going to happen, rather, just the fact that an event either happened $$(y=1)$$ or that it did not $$(y=0)$$, without the actual probability of that event happening.  
        * One can think of this data as being generated by a (the following) noisy target:  
            $${\displaystyle P(y \vert x) ={\begin{cases}f(x)&{\text{for }}y = +1,\\1-f(x)&{\text{for }}y=-1.\\\end{cases}}}$$   
        * They have the form that a certain probability that the event occurred and a certain probability that the event did NOT occur, given their input-data.  
        * This is generated by the target we want to learn; thus, the function $$f(x)$$ is the target function to approximate.  
    * In Logistic Regression, we are trying to learn $$f(x)$$ not withstanding the fact that the data-points we are learning from are giving us just sample values of $$y$$ that happen to be generated by $$f$$.  
    * Thus, the __Target__ $$f : \mathbb{R}^d \longrightarrow [0,1]$$ is the probability.  
        <span>The output of Logistic Regression is treated genuinely as a __probability__ even _during **Learning**_.</span>{: style="color: purple"}   
    {: hidden=""}

4. **Deriving the Error Measure (Cross-Entropy) from Likelihood:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents54}  
    The error measure for logistic regression is based on __likelihood__ - it is both, plausible and friendly/well-behaved? (for optimization).  
    For each $$(\mathbf{x}, y)$$, $$y$$ is generated wit probability $$f(\mathbf{x})$$.  

    __Likelihood__: We are maximizing the *__likelihood of this hypothesis__*, under the *__data set__* that we were given, with respect to the *__weights__*. I.E. Given the data set, how likely is this hypothesis? Which means, what is the probability of that data set under the assumption that this hypothesis is indeed the target?  

    * __Deriving the Likelihood:__   
        1. We start with:  
            <p>$$P(y | \mathbf{x})=\left\{\begin{array}{ll}{h(\mathbf{x})} & {\text {for } y=+1} \\ {1-h(\mathbf{x})} & {\text {for } y=-1}\end{array}\right.$$</p>  
        2. Substitute $$h(\mathbf{x})=\theta \left(\mathbf{w}^{\top} \mathbf{x}\right)$$:  
            <p>$$P(y | \mathbf{x})=\left\{\begin{array}{ll}{\theta(\mathbf{w}^T\mathbf{x})} & {\text {for } y=+1} \\ {1-\theta(\mathbf{w}^T\mathbf{x})} & {\text {for } y=-1}\end{array}\right.$$</p>  
        3. Since we know that $$\theta(-s)=1-\theta(s)$$, we can simplify the piece-wise function:  
            <p>$$P(y | \mathbf{x})=\theta\left(y \mathbf{w}^{\top} \mathbf{x}\right)$$</p>  
        4. To get the __likelihood__ of the dataset $$\mathcal{D}=\left(\mathbf{x}_{1}, y_{1}\right), \ldots,\left(\mathbf{x}_{N}, y_{N}\right)$$:  
            <p>$$\prod_{n=1}^{N} P\left(y_{n} | \mathbf{x}_{n}\right) =\prod_{n=1}^{N} \theta\left(y_{n} \mathbf{w}^{\mathrm{T}} \mathbf{x}_ {n}\right)$$</p>  

    * __Maximizing the Likelihood (Deriving the Cross-Entropy Error):__  
        1. Maximize:  
            <p>$$\prod_{n=1}^{N} \theta\left(y_{n} \mathbf{w}^{\top} \mathbf{x}_ {n}\right)$$</p>  
        2. Take the natural log to avoid products:  
            <p>$$\ln \left(\prod_{n=1}^{N} \theta\left(y_{n} \mathbf{w}^{\top} \mathbf{x}_ {n}\right)\right)$$</p>  
            Motivation:  
            * The inner quantity is __non-negative__ and non-zero.  
            * The natural log is __monotonically increasing__ (its max, is the max of its argument)  
        3. Take the average (still monotonic):  
            <p>$$\frac{1}{N} \ln \left(\prod_{n=1}^{N} \theta\left(y_{n} \mathbf{w}^{\top} \mathbf{x}_ {n}\right)\right)$$</p>  
        4. Take the negative and __Minimize__:  
            <p>$$-\frac{1}{N} \ln \left(\prod_{n=1}^{N} \theta\left(y_{n} \mathbf{w}^{\top} \mathbf{x}_ {n}\right)\right)$$</p>  
        5. Simplify:  
            <p>$$=\frac{1}{N} \sum_{n=1}^{N} \ln \left(\frac{1}{\theta\left(y_{n} \mathbf{w}^{\tau} \mathbf{x}_ {n}\right)}\right)$$</p>  
        6. Substitute $$\left[\theta(s)=\frac{1}{1+e^{-s}}\right]$$:  
            <p>$$\frac{1}{N} \sum_{n=1}^{N} \underbrace{\ln \left(1+e^{-y_{n} \mathbf{w}^{\top} \mathbf{x}_{n}}\right)}_{e\left(h\left(\mathbf{x}_{n}\right), y_{n}\right)}$$</p>  
        7. Use this as the *__Cross-Entropy__*  __Error Measure__:  
            <p>$$E_{\mathrm{in}}(\mathrm{w})=\frac{1}{N} \sum_{n=1}^{N} \underbrace{\ln \left(1+e^{-y_{n} \mathrm{w}^{\top} \mathbf{x}_{n}}\right)}_{\mathrm{e}\left(h\left(\mathrm{x}_{n}\right), y_{n}\right)}$$</p>  


6. **The Decision Boundary of Logistic Regression:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents56}  
    __Decision Boundary:__ It is the set of $$x$$ such that:  
    <p>$$\frac{1}{1+e^{-\theta \cdot x}}=0.5 \implies 0=-\theta \cdot x=-\sum_{i=0}^{n} \theta_{i} x_{i}$$</p>  


7. **The Logistic Regression Algorithm:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents57}  
    ![img](/main_files/dl/theory/caltech/27.png){: width="80%"}    

8. **Summary of Linear Models:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents58}  
    ![img](/main_files/dl/theory/caltech/26.png){: width="80%"}    

9. **Nonlinear Transforms:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents59}  
    <p>$$\mathbf{x}=\left(x_{0}, x_{1}, \cdots, x_{d}\right) \stackrel{\Phi}{\longrightarrow} \mathbf{z}=\left(z_{0}, z_{1}, \cdots \cdots \cdots \cdots \cdots, z_{\tilde{d}}\right)$$</p>  
    <p>$$\text {Each } z_{i}=\phi_{i}(\mathbf{x}) \:\:\:\:\: \mathbf{z}=\Phi(\mathbf{x})$$</p>  
    Example: $$\mathbf{z}=\left(1, x_{1}, x_{2}, x_{1} x_{2}, x_{1}^{2}, x_{2}^{2}\right)$$  
    The Final Hypothesis $$g(\mathbf{x})$$ in $$\mathcal{X}$$ space:  
    * __Classification:__ $$\operatorname{sign}\left(\tilde{\mathbf{w}}^{\top} \Phi(\mathbf{x})\right)$$ 
    * __Regression:__ $$\tilde{\mathbf{w}}^{\top} \Phi(\mathbf{x})$$  

    __Two Non-Separable Cases:__{: style="color: red"}  
    * Almost separable with some outliers:  
        ![img](/main_files/dl/theory/caltech/23.png){: width="38%"}   
        1. Accept that $$E_{\mathrm{in}}>0$$; use a linear model in $$\mathcal{X}$$.    
        2. Insist on $$E_{\mathrm{in}}=0$$; go to a high-dimensional $$\mathcal{Z}$$.  
            This has a __worse__ chance for generalizing. 
    * Completely Non-Linear:  
        ![img](/main_files/dl/theory/caltech/24.png){: width="38%"}   
        Data-snooping example: it is hard to choose the right transformations; biggest flop is to look at the data to choose the right transformations; it invalidates the VC inequality guarantee.  
        > Think of the VC inequality as providing you with a warranty.   



## The Bias Variance Decomposition
{: #content6}

![img](/main_files/dl/theory/caltech/1.jpg){: width="100%"}    

![img](/main_files/dl/theory/caltech/2.jpg){: width="100%"}    




***
***

TITLE: Interpretable Machine Learning (Models)
LINK: research/dl/theory/interpretable_ml.md



* [The Building Blocks of Interpretability (distill)](https://distill.pub/2018/building-blocks/)  
* [Explaining a Black-box Using Deep Variational Information Bottleneck Approach](https://blog.ml.cmu.edu/2019/05/17/explaining-a-black-box-using-deep-variational-information-bottleneck-approach/)  


***
***

TITLE: Estimation
LINK: research/dl/theory/estimation.md


[MLE vs MAP Estimation](https://h1man5hu.github.io/machine%20learning/maximum-likelihood-estimation-vs-maximum-a-posteriori-estimation/)  


## Estimation
{: #content1}

***

## Maximum Likelihood Estimation (MLE)
{: #content2}

1. **Maximum Likelihood Estimation:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    __Likelihood in Parametric Models:__{: style="color: red"}  
    {: #lst-p}
    Suppose we have a parametric model $$\{p(y ; \theta) \vert \theta \in \Theta\}$$ and a sample $$D=\left\{y_{1}, \ldots, y_{n}\right\}$$:  
    * The likelihood of parameter estimate $$\hat{\theta} \in \Theta$$ for sample $$\mathcal{D}$$ is:  
        <p>$$p(\mathcal{D} ; \hat{\theta})=\prod_{i=1}^{n} p\left(y_{i} ; \hat{\theta}\right)$$</p>  
    * In practice, we prefer to work with the __log-likelihood__.  Same maximum but  
        <p>$$\log p(\mathcal{D} ; \hat{\theta})=\sum_{i=1}^{n} \log p\left(y_{i} ; \theta\right)$$</p>  
        and sums are easier to work with than products.  

    __MLE for Parametric Models:__{: style="color: red"}  
    {: #lst-p}
    The __maximum likelihood estimator (MLE)__ for $$\theta$$ in the (parametric) model $$\{p(y, \theta) \vert \theta \in \Theta\}$$ is:  
    <p>$$\begin{aligned} \hat{\theta} &=\underset{\theta \in \Theta}{\arg \max } \log p(\mathcal{D}, \hat{\theta}) \\ &=\underset{\theta \in \Theta}{\arg \max } \sum_{i=1}^{n} \log p\left(y_{i} ; \theta\right) \end{aligned}$$</p>  

    > You are finding the value of the parameter $$\theta$$ that, if used (in the model) to generate the probability of the data, would make the data most _"likely"_ to occur.  

    * __MLE Intuition__:  
        If I choose a _hypothesis_ $$h$$ underwhich the _observed data_ is very *__plausible__* then the _hypothesis_ is very *__likely__*.  
    * [**Maximum Likelihood as Empirical Risk Minimization**](https://www.youtube.com/embed/JrFj0xpGd2Q?start=2609){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/JrFj0xpGd2Q?start=2609"></a>
        <div markdown="1"> </div>    
    * Finding the MLE is an optimization problem.
    * For some model families, calculus gives a closed form for the MLE
    * Can also use numerical methods we know (e.g. SGD)  


    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * __Why maximize the natural log of the likelihood?__  
        <button>Discussion</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
        *   
            1. Numerical Stability: change products to sums  
            2. The logarithm of a member of the family of exponential probability distributions (which includes the ubiquitous normal) is polynomial in the parameters (i.e. max-likelihood reduces to least-squares for normal distributions)  
            $$\log\left(\exp\left(-\frac{1}{2}x^2\right)\right) = -\frac{1}{2}x^2$$   
            3. The latter form is both more numerically stable and symbolically easier to differentiate than the former. It increases the dynamic range of the optimization algorithm (allowing it to work with extremely large or small values in the same way).  
            4. The logarithm is a monotonic transformation that preserves the locations of the extrema (in particular, the estimated parameters in max-likelihood are identical for the original and the log-transformed formulation)  

            * Gradient methods generally work better optimizing $$log_p(x)$$ than $$p(x)$$ because the gradient of $$log_p(x)$$ is generally more __well-scaled__. [link](https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability)
                __Justification:__ the gradient of the original term will include a $$e^{\vec{x}}$$ multiplicative term that scales very quickly one way or another, requiring the step-size to equally scale/stretch in the opposite direction.  
        {: hidden=""}



***

## Maximum A Posteriori (MAP) Estimation
{: #content3}


***
***

TITLE: Learning for Machines (Concepts)
LINK: research/dl/theory/learning_concepts.md



* [Learning (wiki)](https://en.wikipedia.org/wiki/Learning)  
* [Hebbian Theory (wiki)](https://en.wikipedia.org/wiki/Hebbian_theory)  
* [Neuronal Dynamics - From single neurons to networks and models of cognition](https://neuronaldynamics.epfl.ch/online/index.html)  
* [Complete Outline of Machine "Learning" (wiki!)](https://en.wikipedia.org/wiki/Outline_of_machine_learning)  
* [Progress in AI (wiki!)](https://en.wikipedia.org/wiki/Progress_in_artificial_intelligence)  
* [Scaling Learning Algorithms for AI \| Local VS Non-Local Learning etc. (paper!)](http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_chapter2007.pdf)  

    

## Learning
{: #content1}

1. **Learning:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __Learning__ is the process of acquiring new, or modifying existing, knowledge, behaviors, skills, values, or preferences.  
    <br>



***

## Second
{: #content2}

1. **Learning:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  



***

## Learning Concepts
{: #content3}

1. **Inductive (Learning) Bias:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    




***
***

TITLE: Papers
LINK: research/dl/theory/papers.md



1. []()

## GANs
{: #content1}

0. **Lists of Papers:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents10}  
    :   1. [Awesome DL-Papers](https://github.com/terryum/awesome-deep-learning-papers)
        2. [Adversarial Nets Papers](https://github.com/zhangqianhui/AdversarialNetsPapers)
        3. [really-awesome-gan](https://github.com/nightrome/really-awesome-gan)
        4. [Tha GAN ZOO](https://deephunt.in/the-gan-zoo-79597dc8c347)
        4. [Tha GAN ZOO](https://github.com/hindupuravinash/the-gan-zoo)
        5. [Delving Deep into GANs](https://github.com/GKalliatakis/Delving-deep-into-GANs)

1. **Improving Training:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   1. [Improved Techniques for Training GANs](https://arxiv.org/pdf/1606.03498.pdf)
        2. [Improved Training of Wasserstein GANs](https://arxiv.org/pdf/1704.00028.pdf)
        3. [WGAN](https://arxiv.org/abs/1701.07875)
        4. [Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/abs/1710.10196)

2. **GAN Architectures:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   1. []()
        2. [Adversarial Autoencoders](https://arxiv.org/pdf/1511.05644.pdf)

3. **Improving GANs (Theory):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   1. [InfoGAN](https://arxiv.org/pdf/1606.03657.pdf)
        2. [Conditional GANs \| cGANs](https://arxiv.org/pdf/1411.1784.pdf)

4. **Theory:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   

5. **Code:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    :   1. [improved_wgan_training](https://github.com/igul222/improved_wgan_training)
        2. [Progressive Growing of GANs](https://github.com/tkarras/progressive_growing_of_gans)
        3. [Implementation of different GAN architectures](https://github.com/wiseodd/generative-models)
        4. [Fantastic GANs and where to find them](http://guimperarnau.com/blog/2017/03/Fantastic-GANs-and-where-to-find-them)
        4. [Fantastic GANs and where to find them II](http://guimperarnau.com/blog/2017/11/Fantastic-GANs-and-where-to-find-them-II)


***

## RNNs
{: #content2}

***

## Maths
{: #content3}

***

## Statistics
{: #content4}

***

## Optimization
{: #content5}

***

## Machine Learning
{: #content6}

***

## Computer Vision
{: #content7}

***

## NLP
{: #content8}
    :   * __Speech Recognition__:  
            
            1. [Human parity in speech recognition (xiong et al. 2016)]()
            2. [Deep Learning for Speech (Deng et al.)]()
    :   * __Timeline papers__:  
            1. [Graves &Jaitley (Tpwards End to End Speech Recog. with Neural Nets)](/)
            2. [Maas et al. (Lexicon Free Conversational Speech Recog. with Neural Nets)]()  
            3. [Chan et al. (LAS)](/)

***

## Physics
{: #content9}

***

## Medical-NLP
{: #content10}
    :   1. [Classifying medical notes into standard disease codes](https://arxiv.org/pdf/1802.00382v1.pdf)  

***

## Misc.
{: #content11}

***
***

TITLE: Computational Learning Theory
LINK: research/dl/theory/comp_learning_theory.md


* [Learning Theory (Formal, Computational or Statistical) (Blog + references!)](http://bactra.org/notebooks/learning-theory.html)  


## FIRST
{: #content1}

1. **Linear Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  








***

## SECOND
{: #content2}









***

## THIRD
{: #content3}










***
***

TITLE: Sampling and Monte Carlo Methods
LINK: research/dl/concepts/sampling.md


__Resources:__{: style="color: red"}  
{: #lst-p}
* [Importance Sampling (Tut - Ben Lambert)](https://www.youtube.com/watch?v=V8f8ueBc9sY)  
* [Random Walk Metropolis Sampling Algorithm (Tut. B-Lambert)](https://www.youtube.com/watch?v=U561HGMWjcw)  
* [Gibbs Sampling (Tut. B-Lambert)](https://www.youtube.com/watch?v=ER3DDBFzH2g)  
* [Hamiltonian Monte Carlo Intuition (Tut. B-Lambert)](https://www.youtube.com/watch?v=a-wydhEuAm0)  
* [Markov Chains (Stat 110)](https://www.youtube.com/watch?v=8AJPs3gvNlY)  
* [Markov Chain Monte Carlo Methods, Rejection Sampling and the Metropolis-Hastings Algorithm!](http://bjlkeng.github.io/posts/markov-chain-monte-carlo-mcmc-and-the-metropolis-hastings-algorithm/)  
* [MCMC Course Tutorial (Mathematical Monk Vids!)](https://www.youtube.com/watch?v=12eZWG0Z5gY)  
* [An Introduction to MCMC for Machine Learning (M Jordan!)](https://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf)  
* [MCMC Intuition for Everyone (blog)](https://towardsdatascience.com/mcmc-intuition-for-everyone-5ae79fff22b1)  



## Sampling
{: #content1}

1. **Monte Carlo Sampling:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    When a sum or an integral cannot be computed exactly we can approximate it using Monte Carlo sampling. 
    The idea is to __view the sum or integral as if it were an *expectation under some distribution*__ and to <span>approximate the expectation by a corresponding average</span>{: style="color: goldenrod"}:   
    \- __Sum:__  
    <p>$$s=\sum_{\boldsymbol{x}} p(\boldsymbol{x}) f(\boldsymbol{x})=E_{p}[f(\mathbf{x})]$$</p>  
    \- __Integral:__  
    <p>$$s=\int p(\boldsymbol{x}) f(\boldsymbol{x}) d \boldsymbol{x}=E_{p}[f(\mathbf{x})]$$</p>  
    We can approximate $$s$$ by drawing $$n$$ samples $$\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(n)}$$ from $$p$$ and then forming the __empirical average__:  
    <p>$$\hat{s}_{n}=\frac{1}{n} \sum_{i=1}^{n} f\left(\boldsymbol{x}^{(i)}\right)$$</p><br>

2. **Importance Sampling:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    There is __no unique decomposition__ of the MC approximation because $$p(\boldsymbol{x}) f(\boldsymbol{x})$$ can always be rewritten as:  
    <p>$$p(\boldsymbol{x}) f(\boldsymbol{x})=q(\boldsymbol{x}) \frac{p(\boldsymbol{x}) f(\boldsymbol{x})}{q(\boldsymbol{x})} $$</p>  
    where we now sample from $$q$$ and average $$\frac{p f}{q}$$.  

    Formally, the expectation becomes:  
    <p>$$E_{p}[f(\mathbf{x})] = \sum_{\boldsymbol{x}} p(\boldsymbol{x}) f(\boldsymbol{x}) = \sum_{\boldsymbol{x}} q(\boldsymbol{x}) \dfrac{p(\boldsymbol{x})}{q(\boldsymbol{x})} f(\boldsymbol{x}) = E_q\left[\dfrac{p(\boldsymbol{x})}{q(\boldsymbol{x})} f(\boldsymbol{x})\right]$$</p>  


    __Biased Importance Sampling:__{: style="color: red"}  
    Another approach is to use biased importance sampling, which has the advantage of not requiring normalized $$p$$ or $$q$$. In the case of discrete variables, the biased importance sampling estimator is given by  
    <p>$$\begin{aligned} \hat{s}_{B I S} &=\frac{\sum_{i=1}^{n} \frac{p\left(\boldsymbol{x}^{(i)}\right)}{q\left(\boldsymbol{x}^{(i)}\right)} f\left(\boldsymbol{x}^{(i)}\right)}{\sum_{i=1}^{n} \frac{p\left(\boldsymbol{x}^{(i)}\right)}{q\left(\boldsymbol{x}^{(i)}\right)}} \\ &=\frac{\sum_{i=1}^{n} \frac{p\left(\boldsymbol{x}^{(i)}\right)}{\tilde{q}\left(\boldsymbol{x}^{(i)}\right)} f\left(\boldsymbol{x}^{(i)}\right)}{\sum_{i=1}^{n} \frac{p(i)}{\tilde{q}(i)}} \\ &=\frac{\sum_{i=1}^{n} \frac{\tilde{p}\left(\boldsymbol{x}^{(i)}\right)}{\tilde{q}\left(\boldsymbol{x}^{(i)}\right)} f\left(\boldsymbol{x}^{(i)}\right)}{\sum_{i=1}^{n} \frac{\tilde{p}\left(\boldsymbol{x}^{(i)}\right)}{\tilde{q}\left(\boldsymbol{x}^{(i)}\right)}} \end{aligned}$$</p>  
    where $$\tilde{p}$$ and $$\tilde{q}$$ are the unnormalized forms of $$p$$ and $$q$$, and the $$\boldsymbol{x}^{(i)}$$ are the samples from $$q$$.  
    __Bias:__  
    This estimator is biased because $$\mathbb{E}[\hat{s}_ {BIS}] \neq s$$, except __asymptotically when $$n \rightarrow \infty$$__ and the __denominator of the first equation__ (above) __converges to $$1$$__. Hence this estimator is called *__asymptotically unbiased__*.  


    __Statistical Efficiency:__{: style="color: red"}  
    Although a good choice of $$q$$ can greatly improve the efficiency of Monte Carlo estimation, a poor choice of $$q$$ can make the efficiency much worse.  
    \- If there are <span>samples of $$q$$ for which $$\frac{p(\boldsymbol{x})|f(\boldsymbol{x})|}{q(\boldsymbol{x})}$$ is large, then the variance of the estimator can get very large</span>{: style="color: goldenrod"}.  
    This may happen when $$q(\boldsymbol{x})$$ is tiny while neither $$p(\boldsymbol{x})$$ nor $$f(\boldsymbol{x})$$ are small enough to cancel it.  
    The $$q$$ distribution is usually chosen to be a simple distribution so that it is easy to sample from. When $$\boldsymbol{x}$$ is high dimensional, this simplicity in $$q$$ causes it to match $$p$$ or $$p\vert f\vert $$ poorly.  
    (1) When $$q\left(\boldsymbol{x}^{(i)}\right) \gg p\left(\boldsymbol{x}^{(i)}\right)\left|f\left(\boldsymbol{x}^{(i)}\right)\right|$$, importance sampling collects useless samples (summing tiny numbers or zeros).  
    (2) On the other hand, when $$q\left(\boldsymbol{x}^{(i)}\right) \ll p\left(\boldsymbol{x}^{(i)}\right)\left|f\left(\boldsymbol{x}^{(i)}\right)\right|$$, which will happen more rarely, the ratio can be huge.  
    Because these latter events are rare, they may not show up in a typical sample, yielding typical underestimation of $$s$$, compensated rarely by gross overestimation.  
    Such very large or very small numbers are typical when $$\boldsymbol{x}$$ is high dimensional, because in high dimension the dynamic range of joint probabilities can be very large.  

    <span> A good IS sampling distribution $$q$$ is a *low variance* distribution.</span>{: style="color: goldenrod"}{: .borderexample}  


    __Applications:__{: style="color: red"}  
    In spite of this danger, importance sampling and its variants have been found very useful in many machine learning algorithms, including deep learning algorithms. They have been used to:  
    {: #lst-p}
    * Accelerate training in neural language models with a large vocabulary  
    * Accelerate other neural nets with a large number of outputs  
    * Estimate a partition function (the normalization constant of a probability distribution)
    * Estimate the log-likelihood in deep directed models, e.g. __Variational Autoencoders__  
    * Improve the estimate of the gradient of the cost function used to train model parameters with stochastic gradient descent  
        Particularly for models, such as __classifiers__, in which most of the total value of the cost function comes from a small number of misclassified examples.  
        Sampling more _difficult examples_ more frequently can __reduce the variance of the gradient__ in such cases _(Hinton, 2006)_.  


    __Approximating Distributions:__{: style="color: red"}  
    To approximate the expectation (mean) of a distribution $$p$$:  
    <p>$${\mathbb{E}}_ {p}[x]=\sum_{x} x p(x)$$</p>  
    by sampling from a distribution $$q$$.  
    Notice that:  
    (1) $${\displaystyle {\mathbb{E}}_ {p}[x]=\sum_{x} x p(x) = \sum_{x} x\frac{p(x)}{q(x)} q(x)}$$  
    (2) $${\displaystyle \sum_{x} x\frac{p(x)}{q(x)} q(x)=\mathbb{E}_ {q}\left[x \frac{p(x)}{q(x)}\right]}$$  
    We approximate the expectation over $$q$$ in (2) with the empirical distribution:  
    <p>$$\mathbb{E}_ {q}\left[x \frac{p(x)}{q(x)}\right] \approx \dfrac{1}{n} \sum_{i=1}^n x_i \dfrac{p(x_i)}{q(x_i)}$$</p>  

    __Approximating UnNormalized Distributions - *Biased* Importance Sampling:__{: style="color: red"}  
    Let $$p(x)=\frac{h(x)}{Z}$$, then  
    <p>$$\begin{aligned}\mathbb{E}_{p}[x] &= \sum_{x} x \frac{h(x)}{Z} \\ &= \sum_{x} x \frac{h(x)}{Z q(x)} q(x) \\ &\approx \frac{1}{Z} \frac{1}{n} \sum_{i=1}^{n} x_{i} \frac{h\left(x_{i}\right)}{q\left(x_{i}\right)} \end{aligned}$$</p>  
    where the samples $$x_i$$ are drawn from $$q$$.  
    To get rid of the $$\dfrac{1}{Z}$$ factor,  
    \- First, we define the importance sample __*weight:*__  
    <p>$$w_i = \frac{h\left(x_{i}\right)}{q\left(x_{i}\right)}$$</p>  
    \- then the __sample *mean weight:*__  
    <p>$$\bar{w} = \dfrac{1}{n} \sum_{i=1}^n w_i = $$</p>  
    \- Now, we decompose $$Z$$ by noticing that:  
    <p>$$\mathbb{E}_ {p}[1]=1=\sum_{x} \frac{h(x)}{Z}$$</p>  
    $$\implies$$  
    <p>$$Z = \sum_{x} h(x)$$</p>  
    \- we approximate the expectation again with IS:  
    <p>$$\begin{aligned} Z 
    &= \sum_{x} h(x) \\ &= \sum_{x} \frac{h(x)}{q(x)} q(x) \\ &\approx \dfrac{1}{n} \sum_{i=1}^{n} \frac{h\left(x_{i}\right)}{q\left(x_{i}\right)} \\ &= \bar{w} \end{aligned}$$</p>  
    Thus, the sample normalizing constant $$\hat{Z}$$ is equal to the sample _mean weight_:  
    <p>$$Z = \bar{w}$$</p>  

    Finally, 
    <p>$$\mathbb{E}_ {p}[x] \approx \frac{1}{\bar{w}} \frac{1}{n} \sum_{i=1}^{n} x_{i} w_i  = \dfrac{\overline{xw}}{\bar{w}}$$</p>  


    __Curse of Dimensionality in IS - Variance of the Estimator:__{: style="color: red"}  
    A big problem with Importance Sampling is that the __variance__ of the IS estimator can be greatly _sensitive_ to the choice of $$q$$.  
    The __Variance__ is:  
    <p>$$\operatorname{Var}\left[\hat{s}_ {q}\right]=\operatorname{Var}\left[\frac{p(\mathbf{x}) f(\mathbf{x})}{q(\mathbf{x})}\right] / n$$</p>  
    The __Minimum Variance__ occurs when $$q$$ is:  
    <p>$$q^{* }(\boldsymbol{x})=\frac{p(\boldsymbol{x})|f(\boldsymbol{x})|}{Z}$$</p>  
    where $$Z$$ is the normalization constant, chosen so that $$q^{* }(\boldsymbol{x})$$ sums or integrates to $$1$$ as appropriate.  
    \- Any choice of sampling distribution $$q$$ is __valid__ (in the sense of yielding the correct expected value), and 
    \- $$q^{ * }$$ is the __optimal one__ (in the sense of yielding minimum variance).  
    \- Sampling from $$q^{ * }$$ is usually infeasible, but other choices of $$q$$ can be feasible while still reducing the variance somewhat.<br>  


3. **Markov Chain Monte Carlo (MCMC) Methods:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    __Motivation:__{: style="color: red"}  
    In many cases, we wish to use a Monte Carlo technique but there is no tractable method for drawing exact samples from the distribution $$p_{\text {model}}(\mathbf{x})$$ or from a good (low variance) importance sampling distribution $$q(\mathbf{x})$$.  
    In the context of deep learning, this most often happens when $$p_{\text {model}}(\mathbf{x})$$ is represented by an *__undirected model__*.  
    In these cases, we introduce a mathematical tool called a __Markov chain__ to *__approximately sample__* from $$p_{\text {model}}(\mathbf{x})$$. The family of algorithms that use Markov chains to perform Monte Carlo estimates is called __Markov Chain Monte Carlo (MCMC) methods__.  

    __Idea of MCs:__  
    \- The core idea of a Markov chain is to have a state $$\boldsymbol{x}$$ that begins as an arbitrary value.  
    \- Over time, we randomly update $$\boldsymbol{x}$$ repeatedly.  
    \- Eventually $$\boldsymbol{x}$$ becomes (very nearly) a fair sample from $$p(\boldsymbol{x})$$.  

    __Definition:__  
    Formally, a __Markov chain__ is defined by:  
    * A __random state__ $$x$$ and  
    * A __transition distribution__ $$T\left(x^{\prime} \vert x\right)$$  
        specifying the probability that a random update will go to state $$x^{\prime}$$ if it starts in state $$x$$.<br>  
    Running the Markov chain means repeatedly updating the state $$x$$ to a value $$x^{\prime}$$ sampled from $$T\left(\mathbf{x}^{\prime} \vert x\right)$$.  


    __Finite, Countable States:__  
    We take the case where the random variable $$\mathbf{x}$$ has __countably many states__.  
    __Representation:__  
    We represent the state as just a positive integer $$x$$.  
    Different integer values of $$x$$ map back to different states $$\boldsymbol{x}$$ in the original problem.  

    Consider what happens when we __run *infinitely* many Markov chains in *parallel*__.  
    \- All the states of the different Markov chains are drawn from some distribution $$q^{(t)}(x)$$, where $$t$$ indicates the number of time steps that have elapsed.  
    \- At the beginning, $$q^{(0)}$$ is some distribution that we used to arbitrarily initialize $$x$$ for each Markov chain.  
    \- Later, $$q^{(t)}$$ is influenced by all the Markov chain steps that have run so far.  
    \- Our __goal__ is for <span>$$q^{(t)}(x)$$ to converge to $$p(x)$$</span>{: style="color: purple"}.  

    * __Probability of transitioning to a new state__:  
        Let's update a single Markov chain's state $$x$$ to a new state $$x^{\prime}$$.  
        The __probability of a single state landing in state $$x^{\prime}$$__ is given by:  
        <p>$$q^{(t+1)}\left(x^{\prime}\right)=\sum_{x} q^{(t)}(x) T\left(x^{\prime} \vert x\right)$$</p>  
        * __Describing $$q$$__:  
            Because we have reparametrized the problem in terms of a positive integer $$x$$, we can describe the probability distribution $$q$$ using a vector $$\boldsymbol{v}$$ with:  
            <p>$$q(\mathrm{x}=i)=v_{i}$$</p>  
        * __The Transition Operator $$T$$ as a Matrix__:  
            Using our integer parametrization, we can represent the effect of the transition operator $$T$$ using a matrix $$A$$.  
            We define $$A$$ so that:  
            <p>$$A_{i, j}=T\left(\mathbf{x}^{\prime}=i \vert \mathbf{x}=j\right)$$</p>  

        Rather than writing it in terms of $$q$$ and $$T$$ to understand how a single state is updated, we may now use $$v$$ and $$A$$ to describe how the entire distribution over all the different Markov chains (running in parallel) shifts as we apply an update.  
        Rewriting the __probability of a single state landing in state $$x^{\prime} = i$$__:  
        <p>$$\boldsymbol{v}^{(t)}=\boldsymbol{A} \boldsymbol{v}^{(t-1)}$$</p>  
        * __Matrix Exponentiation:__  
            Applying the Markov chain update repeatedly corresponds to multiplying by the matrix $$A$$ repeatedly.  
            In other words, we can think of the process as exponentiating the matrix $$\boldsymbol{A}$$.  

        Thus, $$\boldsymbol{v}^{(t)}$$ can, finally, be rewritten as  
        <p>$$\boldsymbol{v}^{(t)}=\boldsymbol{A}^{t} \boldsymbol{v}^{(0)}$$</p>  
    * __Convergence - The Stationary Distribution__:  
        Let's first examine the matrix $$A$$.  
        * __Stochastic Matrices__:  
            __Stochastic Matrices__ are ones where each of their columns represents a _probability distribution_.  
            The Matrix $$A$$ is a stochastic matrix.  
            * __Perron-Frobenius Theorem - Largest Eigenvalue__:  
                If there is a nonzero probability of transitioning from any state $$x$$ to any other state $$x$$ for some power $$t$$, then the __Perron-Frobenius theorem__ guarantees that the <span>largest eigenvalue is real and equal to $$1$$</span>{: style="color: goldenrod"}.  
            * __Unique Largest Eigenvalue__:  
                Under some additional mild conditions, $$A$$ is guaranteed to have only one eigenvector with eigenvalue $$1$$.  
        * __Exponentiated Eigenvalues__:  
            Over time, we can see that __all the eigenvalues are exponentiated__:  
            <p>$$\boldsymbol{v}^{(t)}=\left(\boldsymbol{V} \operatorname{diag}(\boldsymbol{\lambda}) \boldsymbol{V}^{-1}\right)^{t} \boldsymbol{v}^{(0)}=\boldsymbol{V} \operatorname{diag}(\boldsymbol{\lambda})^{t} \boldsymbol{V}^{-1} \boldsymbol{v}^{(0)}$$</p>  

            This process causes <span>all the eigenvalues that are not equal to $$1$$ to decay to zero</span>{: style="color: purple"}.  

        The process thus <span>converges to a __stationary distribution__ (__equilibrium distribution__)</span>{: style="color: goldenrod"}.  
        * __Convergence Condition - Eigenvector Equation:__  
            At __convergence__, the following __eigenvector equation__ holds:  
            <p>$$\boldsymbol{v}^{\prime}=\boldsymbol{A} \boldsymbol{v}=\boldsymbol{v}$$</p>  
            and this same condition *__holds for every additional step__*.  
            * __Stationary Point Condition__:  
                Thus, To be a __stationary point__, $$\boldsymbol{v}$$ must be an __eigenvector with corresponding eigenvalue $$1$$__.  
                This condition guarantees that <span>once we have reached the stationary distribution, repeated applications of the transition sampling procedure do not change the _distribution_ over the states of all the various Markov chains</span>{: style="color: purple"} (although the transition operator does change each individual state, of course).  
        * __Convergence to $$p$$__:  
            If we have chosen $$T$$ correctly, then the stationary distribution $$q$$ will be equal to the distribution $$p$$ we wish to sample from.  
            __Gibbs Sampling__ is one way to choose $$T$$.  

    __Continuous Variables:__  

    __Convergence:__  
    In general, a Markov chain with transition operator $T$ will converge, under mild conditions, to a fixed point described by the equation  
    <p>$$q^{\prime}\left(\mathbf{x}^{\prime}\right)=\mathbb{E}_ {\mathbf{x} \sim q} T\left(\mathbf{x}^{\prime} \vert \mathbf{x}\right)$$</p>  
    which is exactly what we had in the __discrete case__ defined as a *__sum__*:  
    <p>$$q^{\prime}\left(x^{\prime}\right)=\sum_{x} q^{(t)}(x) T\left(x^{\prime} \vert x\right)$$</p>  
    and in the __continuous case__ as an *__integral__*:  
    <p>$$q^{\prime}\left(x^{\prime}\right)=\int_{x} q^{\prime}(x) T\left(x^{\prime} \vert x\right)$$</p>  


    __Using the Markov Chain:__{: style="color: red"}  
    <div class="borderexample">Regardless of whether the state is continuous or discrete, all Markov chain methods consist of <span style="color: goldenrod">repeatedly applying stochastic updates until eventually the state begins to yield samples from the equilibrium distribution</span>.</div>  
    \- __Training the Markov Chain:__{: style="color: red"}  
    Running the Markov chain until it reaches its equilibrium distribution is called *__burning in__* the Markov chain.  

    \- __Sampling from the Markov Chain:__{: style="color: red"}  
    After the chain has reached equilibrium, a sequence of infinitely many samples may be drawn from the equilibrium distribution.  
    There are <span>__difficulties/drawbacks__</span>{: style="color: darkred"} with using Markov Chains for sampling:  
    {: #lst-p}
    * __Representative Samples - Independence__:  
        The samples are __identically distributed__, but _any two successive samples_ will be __highly correlated__ with each other.  
        * __Issue__:  
            A _finite sequence_ of samples may thus not be very _representative of the equilibrium distribution_.   
        * __Solutions__:  
            1. One way to mitigate this problem is to __return only every $$n$$ successive samples__, so that our estimate of the statistics of the equilibrium distribution is not as _biased by the correlation_ between an MCMC sample and the next several samples.  
                <span>Markov chains are thus expensive to use because of the time required to _burn in_ to the equilibrium distribution and the time required to transition from one sample to another reasonably decorrelated sample after reaching equilibrium</span>{: style="color: darkred"}.   
            2. To get *__truly independent samples__*, one can __run multiple Markov chains in parallel__.  
            This approach uses extra parallel computation to _eliminate latency_.  

        \- The strategy of using only a single Markov chain to generate all samples and the strategy of using one Markov chain for each desired sample are two extremes.  
        \- In __deeplearning__ we usually <span>_use a number of chains that is similar to the number of examples in a minibatch_ and then draw as many samples as are needed from this fixed set of Markov chains</span>{: style="color: goldenrod"}.  
        > A commonly used number of Markov chains is $$100$$.  
    * __Convergence to Equilibrium - Halting__:  
        The theory of Markov Chains allows us to __*guarantee* convergence to equilibrium__. However, it does not specify anything about the __convergence *criterion*__:  
        {: #lst-p}
        * <span>The theory does not allow us to know the Mixing Time in advance.</span>{: style="color: darkred"}.   
            The __Mixing Time__ is the number of steps the Markov chain must run before reaching its _equilibrium distribution_.  
        * <span>The theory, also, does not guide us on how to test/determine whether an MC has reached equilibrium.</span>{: style="color: darkred"}.   

        __Convergence Criterion Theoretical Analysis:__  
        If we analyze the Markov chain from the point of view of a matrix $$A$$ acting on a vector of probabilities $$v$$ , then we know that the chain mixes when $$A^{t}$$ has effectively lost all the eigenvalues from $$A$$ besides the unique eigenvalue of 1.  
        This means that the <span>_magnitude of the second-largest eigenvalue_ will determine the __mixing time__</span>{: style="color: purple"}.  

        __Convergence Criterion In Practice:__  
        In practice, though, we _cannot actually represent our Markov chain in terms of a matrix_.  
        \- The _number of states_ that our probabilistic model can visit is _exponentially large in the number of variables_, so it is infeasible to represent $$\boldsymbol{v}$$, $$A$$, or the eigenvalues of $$\boldsymbol{A}$$.  
        Because of these and other obstacles, we usually __do not know whether a Markov chain has mixed__.  
        Instead, we simply <span>run the Markov chain for an amount of time that we roughly estimate to be sufficient, and use heuristic methods to determine whether the chain has mixed</span>{: style="color: goldenrod"}.  
        These heuristic methods include *__manually inspecting samples__* or __*measuring correlations* between successive samples__.  
    
    <div class="borderexample" markdown="1">
    <span>This section described how to _draw samples_ from a distribution $$q(x)$$ by _repeatedly updating_ $$\boldsymbol{x} \leftarrow \boldsymbol{x}^{\prime} \sim T\left(\boldsymbol{x}^{\prime} \vert \boldsymbol{x}\right)$$.</span>{: style="color: goldenrod"} 
    </div>  


    __Finding a useful $$q(x)$$:__{: style="color: red"}  
    There are two basic approaches to ensure that $$q(x)$$ is a useful distribution:  
    {: #lst-p}
    (1) Derive $$T$$ from a given learned $$p_{\text {model}}$$. E.g. [__Gibbs Sampling__](#bodyContents14), Metropolis-Hastings, etc.     
    (2) Directly _parameterize_ $$T$$ and learn it, so that its stationary distribution implicitly defines the $$p_{\text {model}}$$ of interest. E.g. Generative Stochastic Networks, Diffusion Inversion, Approximate Bayesian Computation.  
    <br>

4. **Gibbs Sampling:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    __Gibbs Sampling__ is an MCMC algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution, when direct sampling is difficult.  

    It is a method for finding a useful distribution $$q(x)$$ by deriving $$T$$ from a given learned $$p_{\text {model}}$$; in the case of sampling from __EBMs__.  

    It is a conceptually simple and effective approach to building a Markov Chain that samples from $$p_{\text {model}}(\boldsymbol{x})$$, in which sampling from $$T\left(\mathbf{x}^{\prime} \vert \mathbf{x}\right)$$ is accomplished by selecting one variable $$\mathbf{x}_ {i}$$ and sampling it from $$p_{\text {model}}$$ conditioned on its neighbors in the undirected graph $$\mathcal{G}$$ defining the structure of the energy-based model.  

    __Block Gibbs Sampling:__{: style="color: red"}  
    We can, also, sample several variables at the same time as long as they are conditionally independent given all their neighbors.  
    __Block Gibbs Sampling__ is a Gibbs sampling approach that updates many variables simultaneously.  

    __Application - RBMs:__  
    All the hidden units of an RBM may be sampled simultaneously because they are conditionally independent from each other given all the visible units.  
    Likewise, all the visible units may be sampled simultaneously because they are conditionally independent from each other given all the hidden units.  


    __In Deep Learning:__{: style="color: red"}  
    In the context of the deep learning approach to undirected modeling, it is rare to use any approach other than Gibbs sampling. Improved sampling techniques are one possible research frontier.  



    __Summary:__{: style="color: red"}  
    {: #lst-p}
    * A method for sampling from probability distributions of $$\geq 2$$-dimensions.  
    * It is an __MCMC__ method; A __*dependent* sampling__ algorithm.  
    * It is a special case of the __Metropolis-Hastings__ Algorithm.  
        * But, accept all proposals (i.e. no rejections).  
        * It is slightly more __efficient__ than MH because of no rejections.  
        * It requires us to know the __conditional probabilities__ $$p(X_i \vert X_{0}^t, \ldots, X_{i-1}^{t}, X_{i+1}^{t-1}, \ldots, X_{n}^{t-1})$$ and be able to sample from them.  
        * It is __slow__ for *__correlated parameters__*; like MH.  
            Can be alleviated by doing *__block__* sampling (blocks of correlated variables).  
            I.E. sample $$X_j, X_k \sim p(X_j, X_k \vert X_{0}^t, \ldots, X_{n}^{t-1})$$ at the same time.  
            It is _more efficient_ than sampling from uni-dimensional conditional distributions, but generally harder.  
            <button>Sampling Paths</button>{: .showText value="show" onclick="showTextPopHide(event);"}
            ![img](https://cdn.mathpix.com/snip/images/L1FMvW_bNnZNbzbulNgszJhCWqAVxFrX0TG1f5aO6yo.original.fullsize.png){: width="100%" hidden=""}  
            * Gibbs walks in a zig-zag pattern.  
            * MH walks in the diagonal direction but frequently goes off in the orthogonal direction (which have to be rejected).  
            * Hamiltonian MC, best of both worlds: walks in diagonal direction and accept a high proportion of steps).  
    * Often used in __Bayesian Inference__.  
    * Guaranteed to __Asymptotically Converge__ to the true joint distribution.  
    * It is an alternative to deterministic algorithms for inference like EM.  
    * [Gibbs Sampling (Tut. B-Lambert)](https://www.youtube.com/watch?v=ER3DDBFzH2g)  
    <br>


5. **The Challenge of Mixing between Separated Modes in MCMC Algorithms:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    The primary difficulty involved with MCMC methods is that they have a tendency to __mix poorly__.  

    __Slow Mixing/Failure to Mix:__{: style="color: red"}  
    Ideally, __successive samples__ from a Markov chain designed to sample from $$p(\boldsymbol{x})$$ would be <span>completely _independent_</span>{: style="color: purple"} from each other and would __visit many different regions__ in $$\boldsymbol{x}$$ space __proportional to their probability__.  
    Instead, especially in *__high-dimensional__* cases, <span>MCMC samples become very *__correlated__*</span>{: style="color: purple"}. We refer to such behavior as __slow mixing__ or even __failure to mix__.  

    __Intuition - Noisy Gradient Descent:__  
    MCMC methods with slow mixing can be seen as inadvertently performing something resembling __noisy gradient descent__ _on the energy function_, or equivalently __noisy hill climbing__ _on the probability_, with respect to the state of the chain (the random variables being sampled).  
    \- The chain tends to take small steps (in the space of the state of the Markov chain), from a configuration $$\boldsymbol{x}^{(t-1)}$$ to a configuration $$\boldsymbol{x}^{(t)}$$, with the energy $$E\left(\boldsymbol{x}^{(t)}\right)$$ generally lower or approximately equal to the energy $$E\left(\boldsymbol{x}^{(t-1)}\right)$$, with a preference for moves that yield lower energy configurations.  
    \- When starting from a rather _improbable configuration_ (higher energy than the typical ones from $$p(\mathbf{x})$$), the chain tends to __gradually reduce the energy of the state__ and only occasionally move to another mode.  
    \- Once the chain has found a region of low energy (for example, if the variables are pixels in an image, a region of low energy might be a connected manifold of images of the same object), which we call a __mode__, the chain will tend to walk around that mode (following a kind of *__random walk__*).  
    \- Once in a while it will step out of that mode and generally return to it or (if it finds an escape route) move toward another mode.  
    \- The problem is that <span>successful escape routes are rare for many interesting distributions</span>{: style="color: goldenrod"}, so the Markov chain will continue to sample the same mode longer than it should.   

    __In Gibbs Sampling:__{: style="color: red"}  
    The problem is very clear when we consider the Gibbs Sampling algorithm.  
    The probability of going from one mode to a nearby mode within a given number of steps is _determined_ by the <span>_shape_ of the __‚Äúenergy barrier‚Äù__</span>{: style="color: purple"} between these modes.  
    \- Transitions between two modes that are __separated by a high energy barrier__ (a region of low probability) are _exponentially less likely_ (in terms of the height of the energy barrier).  
    <button>Gibbs Algorithm Paths</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/2qEiOUCm6i-VjZCNSi3TqQgOyQXCZIEcMoKdIZmJAOM.original.fullsize.png){: width="100%" hidden=""}  
    The problem arises when there are <span>multiple modes with high probability that are separated by regions of low probability</span>{: style="color: purple"}, especially when each Gibbs sampling step must update only a small subset of variables whose values are largely determined by the other variables.  

    __Example and Analysis:__  
    <button>Example and Analysis</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/pIdAEPT0Unuz4oVxenQhB8_WBFtvwU60iA_LD7c4Rn8.original.fullsize.png){: width="100%" hidden=""}  

    __Possible Solution - Block Gibbs Sampling:__  
    Sometimes this problem can be resolved by finding groups of highly dependent units and updating all of them simultaneously in a block. Unfortunately, when the dependencies are complicated, it can be computationally intractable to draw a sample from the group. After all, the problem that the Markov chain was originally introduced to solve is this problem of sampling from a large group of variables.  


    __In (Generative) Latent Variable Models:__{: style="color: red"}  
    In the context of models with __latent variables__, which define a __joint distribution__ $$p_{\text {model}}(\boldsymbol{x}, \boldsymbol{h}),$$ we often __draw samples__ of $$\boldsymbol{x}$$ by *__alternating__* between sampling from $$p_{\text {model}}(\boldsymbol{x} \vert \boldsymbol{h})$$ and sampling from $$p_{\text {model}}(\boldsymbol{h} \vert \boldsymbol{x})$$.  

    __Learning-Mixing Tradeoff:__  
    \- From the pov of __mixing rapidly__, we would like $$p_{\text {model}}(\boldsymbol{h} \vert \boldsymbol{x})$$ to have high entropy.  
    \- From the pov of learning a useful representation of $$\boldsymbol{h},$$ we would like $$\boldsymbol{h}$$ to <span>encode enough information</span>{: style="color: purple"} about $$\boldsymbol{x}$$ <span>to reconstruct it well</span>{: style="color: purple"}, which implies that <span>$$\boldsymbol{h}$$ and $$\boldsymbol{x}$$ and $$\boldsymbol{x}$$ should have _high_ __mutual information__</span>{: style="color: purple"}.  
    These two goals are at odds with each other. We often <span>learn generative models that very precisely _encode_ $$\boldsymbol{x}$$ into $$\boldsymbol{h}$$ but are not able to _mix_ very well</span>{: style="color: goldenrod"}.  

    __In Boltzmann Machines:__  
    This situation arises frequently with Boltzmann machines-the sharper the distribution a Boltzmann machine learns, the harder it is for a Markov chain sampling from the model distribution to mix well.  
    <button>Slow Mixing in Deep Probabilistic Models - Illustration</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/JHPt1iCCZnS-q351i2PFJL8aeJd-2iUSU5nXxEbYYaA.original.fullsize.png){: width="100%" hidden=""}  


    __Summary - Takeaways:__{: style="color: red"}  
    All this could make MCMC methods __less useful__ when the <span>distribution of interest has a __manifold structure__ with a **_separate_ manifold for each class**</span>{: style="color: purple"}: the distribution is __concentrated around many modes__, and these __modes are separated by vast regions of high energy__.  
    This type of distribution is what we expect in many __classification problems__, and it would make MCMC methods __converge very slowly__ because of *__poor mixing between modes__*.  
    <br>


6. **Solutions for the Slow Mixing Problem:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    Since, it is difficult to mix between the different modes of a distribution when the distribution has <span>sharp peaks of high probability surrounded by regions of low probability</span>{: style="color: purple"},  
    Several techniques for faster mixing are based on <span>constructing alternative versions of the target distribution in which the __peaks are not as *high*__ and the __surrounding valleys are not as *low*__</span>{: style="color: purple"}.  
    \- A particularly simple way to do so, is to use __Energy-based Models__:  
    <p>$$p(\boldsymbol{x}) \propto \exp (-E(\boldsymbol{x}))$$</p>  
    \- Energy-based models may be augmented with an extra parameter $$\beta$$ controlling __how sharply peaked__ the distribution is:  
    <p>$$p_{\beta}(\boldsymbol{x}) \propto \exp (-\beta E(\boldsymbol{x}))$$</p>  
    \- The $$\beta$$ parameter is often described as being the __reciprocal of the *temperature*__, reflecting the origin of energy-based models in statistical physics.  
    \- \- When the _temperature falls to **zero**_, and _$$\beta$$ rises to **infinity**_, the EBM becomes __deterministic__.  
    \- \- When the *temperature rises to __infinity__*, and *$$\beta$$ falls to __zero__*, the distribution (for discrete $$\boldsymbol{x}$$) becomes __uniform__.  

    Typically, a model is trained to be evaluated at $$\beta=1$$. However, we can make use of other temperatures, particularly those where $$\beta<1$$.  

    __Tempering:__{: style="color: red"}  
    __Tempering__ is a general strategy of mixing between modes of $$p_{1}$$ rapidly by drawing samples with $$\beta<1$$.  
    Markov chains based on __tempered transitions__ _(Neal, 1994)_ temporarily sample from higher-temperature distributions to mix to different modes, then resume sampling from the unit temperature distribution.  
    These techniques have been applied to models such as __RBMs__ _(Salakhutdinov, 2010)_.  

    __Parallel Tempering:__  
    Another approach is to use __parallel tempering__ _(Iba, 2001)_, in which the Markov chain simulates many different states in parallel, at different temperatures.  
    \- The highest temperature states mix slowly, while the lowest temperature states, at temperature $$1$$, provide accurate samples from the model.  
    \- The transition operator includes stochastically swapping states between two different temperature levels, so that a sufficiently high-probability sample from a high-temperature slot can jump into a lower temperature slot. This approach has also been applied to RBMs _(Desjardins et al., 2010 ; Cho et al., 2010)_.  


    __Results - In Practice:__  
    Although tempering is a promising approach, at this point it has not allowed researchers to make a strong advance in solving the challenge of sampling from complex EBMs.  
    One possible reason is that there are __critical temperatures__ around which the temperature transition must be very slow (as the temperature is gradually reduced) for tempering to be effective.  


    __Depth for Mixing (in Latent-Variable Models):__{: style="color: red"}  
    {: #lst-p}
    * __Problem - Mixing in Latent Variable Models__:  
        When drawing samples from a latent variable model $$p(\boldsymbol{h}, \boldsymbol{x}),$$ we have seen that if $$p(\boldsymbol{h} \vert \boldsymbol{x})$$ encodes $$\boldsymbol{x}$$ too well, then sampling from $$p(\boldsymbol{x} \vert \boldsymbol{h})$$ will not change $$\boldsymbol{x}$$ very much, and mixing will be poor.  
        * __Example of the problem $$(\alpha)$$__:  
            Many representation learning algorithms, such as __Autoencoders__ and __RBMs__, tend to <span>yield a marginal distribution over $$\boldsymbol{h}$$ that is more *__uniform__* and more *__unimodal__* than the original data distribution over $$\boldsymbol{x}$$</span>{: style="color: purple"}.  
        * __Reason for $$(\alpha)$$__:  
            It can be argued that this arises from <span>trying to minimize reconstruction error while using all the available representation space</span>{: style="color: purple"}, because minimizing reconstruction error over the training examples will be better achieved when different training examples are __easily distinguishable__ from each other in $$\boldsymbol{h}$$-space, and thus __well separated__.  
    * __Solution - Deep Representations__:  
        One way to resolve this problem is to make $$\boldsymbol{h}$$ a __deep representation__, encoding $$\boldsymbol{x}$$ into $$\boldsymbol{h}$$ in such a way that a Markov chain in the space of $$\boldsymbol{h}$$ can mix more easily.  
        * __Solution to the problem $$(\alpha)$$__:  
            \- _Bengio et al. (2013 a)_ observed that deeper stacks of regularized autoencoders or RBMs yield marginal distributions in the top-level $$\boldsymbol{h}$$-space that appeared more spread out and more uniform, with less of a gap between the regions corresponding to different modes (categories, in the experiments).  
            \- Training an RBM in that higher-level space allowed __Gibbs sampling__ to *__mix faster between modes__*.  
        
            > It remains unclear, however, how to exploit this observation to help better train and sample from deep generative models.  

    
    __Summary/Takeaway of MCMC methods In-Practice (DL):__{: style="color: red"}  
    Despite the difficulty of mixing, Monte Carlo techniques are useful and are often the best tool available.  
    Indeed, they are the primary tool used to confront the *__intractable partition function__* of __undirected models__.  




***


***
***

TITLE: Inference and Approximate Inference
LINK: research/dl/concepts/inference.md


__Resources:__{: style="color: red"}  
{: #lst-p}
* [Variational Bayes and The Mean-Field Approximation (blog)](http://bjlkeng.github.io/posts/variational-bayes-and-the-mean-field-approximation/)  
* [Variational Inference: Mean Field Approximation (Lecture Notes)](https://www.cs.cmu.edu/~epxing/Class/10708-17/notes-17/10708-scribe-lecture13.pdf)  
* [Graphical Models, Exponential Families, and Variational Inference (M Jordan)](https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf)  


## Inference and Approximate Inference
{: #content1}

1. **Inference:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __Inference__ usually refers to <span>computing the probability distribution over one set of variables given another</span>{: style="color: purple"}.  


    __Goals:__{: style="color: red"}  
    {: #lst-p}
    - Computing the likelihood of observed data (in models with latent variables).
    - Computing the marginal distribution over a given subset of nodes in the model.
    - Computing the conditional distribution over a subsets of nodes given a disjoint subset of nodes.
    - Computing a mode of the density (for the above distributions).

    __Approaches:__{: style="color: red"}  
    {: #lst-p}
    - __Exact inference algorithms:__  
        * Brute force
        * The elimination algorithm
        * Message passing (sum-product algorithm, belief propagation)
        * Junction tree algorithm  
    - __Approximate inference algorithms__:  
        * Loopy belief propagation
        * Variational (Bayesian) inference $$+$$ mean field approximations
        * Stochastic simulation / sampling /  MCMC

    __Inference in Deep Learning - Formulation:__{: style="color: red"}  
    In the context of __Deep Learning__, we usually have two __sets of variables__:  
    (1) Set of *__visible__* (*__observed__*) __variables__: $$\: \boldsymbol{v}$$  
    (2) Set of *__latent__* __variables__: $$\: \boldsymbol{h}$$  

    __Inference__ in DL corresponds to <span>computing the *__likelihood__* of __observed data__ $$p(\boldsymbol{v})$$</span>{: style="color: goldenrod"}.  

    When training __probabilistic models with *latent variables*__, we are usually interested in computing  
    <p>$$p(\boldsymbol{h} \vert \boldsymbol{v})$$</p>   
    where $$\boldsymbol{h}$$ are the latent variables, and $$\boldsymbol{v}$$ are the observed (visible) variables (data).  
    <br>


2. **The Challenge of Inference:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    __Motivation - The Challenge of Inference:__{: style="color: red"}  
    The __challenge of inference__ usually refers to the difficult problem of computing $$p(\boldsymbol{h} \vert \boldsymbol{v})$$ or taking expectations wrt it.  
    Such operations are often necessary for tasks like __Maximum Likelihood Learning__.  

    __Intractable Inference:__  
    In DL, intractable inference problems, usually, arise from <span>interactions between *__latent__* __variables__ in a structured graphical model</span>{: style="color: purple"}.  
    These interactions are usually due to:  
    {: #lst-p}
    * __Directed Models__: _"explaining away"_ interactions between *__mutual ancestors__* of the __same visible unit__.  
    * __Undirected Models__: direct interactions between the latent variables.  

    __In Models:__  
    {: #lst-p}
    * __Tractable Inference:__  
        * Many *__simple__* graphical models with only <span>__one hidden layer__</span>{: style="color: purple"} have tractable inference.  
            E.g. __RBMs__, __PPCA__.  
    * __Intractable Inference__:  
        * Most graphical models with <span>__multiple hidden layers__</span>{: style="color: purple"} with __hidden variables__ have intractable *__posterior distributions__*.  
            __Exact inference__ requires an __exponential time__.  
            E.g. __DBMs__, __DBNs__.  
        * Even some models with only a <span>__single__ layer</span>{: style="color: goldenrod"} can be intractable.  
            E.g. __Sparse Coding__  

    <button>Interactions in Graphical Models</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/1GkbyQub5WitledsQpfv77ARrYUn67NzFhCFy_TMeUc.original.fullsize.png){: width="100%" hidden=""}  

    __Computing the Likelihood of Observed Data:__  
    We usually want to compute the likelihood of the observed data $$p(\boldsymbol{v})$$, equivalently the log-likelihood $$\log p(\boldsymbol{v})$$.  
    This usually requires marginalizing out $$\boldsymbol{h}$$.  
    This problem is __intractable__ (difficult) if it is _costly_ to __marginalize__ $$\boldsymbol{h}$$.  
    {: #lst-p}
    * __Data Likelihood__:  (<span>intractable</span>{: style="color: purple"})  
        $$p_{\theta}(\boldsymbol{v})=\int_\boldsymbol{h} p_{\theta}(h) p_{\theta}(v \vert h) dh$$  
    * __Marginal Likelihood (evidence)__: is the data likelihood $$p_{\theta}(\boldsymbol{v})$$ (<span>intractable</span>{: style="color: purple"})    
        $$\int_\boldsymbol{h} p_{\theta}(h) p_{\theta}(v \vert h) dh$$  
    * __Prior__:  
        $$p(\boldsymbol{h})$$ 
    * (Conditional) __Likelihood__:  
        $$p_{\theta}(\boldsymbol{v} \vert h)$$  
    * __Joint__:  
        $$p_{\theta}(\boldsymbol{v}, \boldsymbol{h})$$  
    * __Posterior__: (<span>intractable</span>{: style="color: purple"})    
        $$p_{\theta}(\boldsymbol{h} \vert \boldsymbol{v})=\frac{p_{\theta}(\boldsymbol{v}, \boldsymbol{h})}{p_{\theta}(\boldsymbol{v})}=\frac{p_{\theta}(\boldsymbol{v} \vert h) p_{\theta}(h)}{\int_{\boldsymbol{h}} p_{\theta}(h) p_{\theta}(x \vert h) d h}$$  

    <br>


22. **Approximate Inference:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents122}  
    __Approximate Inference__ is an important and practical approach to confronting the __challenge of (intractable) inference__.  
    It poses __exact inference__ as an __optimization problem__, and aims to *__approximate__* the underlying optimization problem.  
    <br>  
    

3. **Inference as Optimization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    __Exact inference__ can be described as an __optimization problem__.  

    * __Inference Problem:__  
        * Compute the __log-likelihood__ of the __observed data__, $$\log p(\boldsymbol{v} ; \boldsymbol{\theta})$$.  
            Can be intractable to marginalize $$\boldsymbol{h}$$.  
    * __Inference Problem as Optimization - Core Idea__:  
        * Choose a family of distributions over the *__latent__* __variables__ $$\boldsymbol{h}$$ with its own set of variational parameters $$\boldsymbol{v}$$: $$q(\boldsymbol{h} \vert \boldsymbol{v})$$.  
        * Find the setting of the parameters that makes our approximation closest to the posterior distribution over the latent variables $$p(\boldsymbol{h} \vert \boldsymbol{v})$$.  
            I.E. __Optimization__  
        * Use learned $$q$$ in place of the posterior (as an approximation).  
    * __Optimization - Fitting $$q$$ to the posterior $$p$$__:  
        * Optimize $$q$$ to approximate $$p(\boldsymbol{h} \vert \boldsymbol{v})$$  
        * __Similarity Measure:__ use the *__KL-Divergence__* as a similarity measure between the two distributions  
            <p>$$D_{\mathrm{KL}}(q \| p) = \mathrm{E}_ {h \sim q}\left[\log \frac{q(h)}{p(h\vert {v})}\right] =\int_{h} q(h) \log \left(\frac{q(h)}{p(h\vert {v})}\right) dh$$</p>  
        * __Intractability:__ minimizing the KL Divergence (above) is an intractable problem.  
            Because the expression contains the intractable term $$p(\boldsymbol{h}\vert \boldsymbol{v})$$ which we were trying to avoid.  
    * __Evidence Lower Bound__:  
        * We rewrite the KL Divergence expression in terms of log-likelihood of the data:  
            <p>$$\begin{aligned} D_{\mathrm{KL}}(q \| p) &=\int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h \vert v)} dh \\ &=\int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh+\int_{\boldsymbol{h}} q(h) \log p(v) dh \\ &=\int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh+\log p(\boldsymbol{v}) \end{aligned}$$</p>  
            where we're using Bayes theorem on the second line and the RHS integral simplifies because it's simply integrating over the support of $$q$$ and $$p$$ is not a function of $$h$$.  
            Thus,  
            <p>$$\log p(\boldsymbol{v}) = D_{\mathrm{KL}}(q \| p) - \int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh$$</p>  
        * Notice that since the KL-Divergence is <span>_Non-Negative_</span>{: style="color: purple"}:  
            <p>$$\begin{align}
                D_{\mathrm{KL}}(q \| p) &\geq 0 \\
                D_{\mathrm{KL}}(q \| p) - \int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh &\geq - \int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh \\
                \log p(\boldsymbol{v}) &\geq - \int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh 
                \end{align}
                $$</p>   
            Thus, the term $$- \int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh$$ provides a __lower-bound__{: style="color: goldenrod"} on the __log likelihood of the data__.   
        * We rewrite the term as:  
            <p>$$\mathcal{L}(\boldsymbol{v}, \boldsymbol{\theta}, q) = - \int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh$$</p>  
            the __Evidence Lower Bound (ELBO)__{: style="color: goldenrod"} AKA <span>Variational Free Energy</span>{: style="color: goldenrod"}.  
            Thus,  
            <p>$$\log p(\boldsymbol{v}) \geq \mathcal{L}(\boldsymbol{v}, \boldsymbol{\theta}, q)$$</p>  
        * __The Evidence Lower Bound__ can also be defined as:  
            <p>$$\begin{align}
                \mathcal{L}(\boldsymbol{v}, \boldsymbol{\theta}, q) &= - \int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh \\
                &= \log p(\boldsymbol{v} ; \boldsymbol{\theta})-D_{\mathrm{KL}}(q(\boldsymbol{h} \vert \boldsymbol{v}) \| p(\boldsymbol{h} \vert \boldsymbol{v} ; \boldsymbol{\theta})) \\
                &= \mathbb{E}_ {\mathbf{h} \sim q}[\log p(\boldsymbol{h}, \boldsymbol{v})]+H(q)  
                \end{align}
                $$ </p>  
            The latter being the __canonical definition__ of the ELBO.  
    * __Inference with the Evidence Lower Bound__:  
        * For an appropriate choice of $$q, \mathcal{L}$$ is <span>__tractable__</span>{: style="color: purple"} to compute.  
        * For any choice of $$q, \mathcal{L}$$ provides a lower bound on the likelihood
        * For $$q(\boldsymbol{h} \vert \boldsymbol{v})$$ that are better approximations of $$p(\boldsymbol{h} \vert \boldsymbol{v}),$$ the lower bound $$\mathcal{L}$$ will be tighter  
            I.E. closer to $$\log p(\boldsymbol{v})$$.  
        * When $$q(\boldsymbol{h} \vert \boldsymbol{v})=p(\boldsymbol{h} \vert \boldsymbol{v}),$$ the approximation is perfect, and $$\mathcal{L}(\boldsymbol{v}, \boldsymbol{\theta}, q)=\log p(\boldsymbol{v} ; \boldsymbol{\theta})$$.  
        * <span>Maximizing the ELBO minimizes the KL-Divergence $$D_{\mathrm{KL}}(q \| p)$$</span>{: style="color: goldenrod"}.  
    * __Inference__:  
        We can thus think of inference as the procedure for finding the $$q$$ that maximizes $$\mathcal{L}$$:  
        * __Exact Inference__: maximizes $$\mathcal{L}$$ perfectly by searching over a family of functions $$q$$ that includes $$p(\boldsymbol{h} \vert \boldsymbol{v})$$.  
        * __Approximate Inference__: approximate inference uses approximate optimization to find $$q$$.  
            We can make the optimization procedure less expensive but approximate by:  
            * Restricting the family of distributions $$q$$ that the optimization is allowed to search over  
            * Using an imperfect optimization procedure that may not completely maximize $$\mathcal{L}$$ but may merely increase it by a significant amount.  
    * __Core Idea of Variational Inference__:  
        We don't need to explicitly compute the posterior (or the marginal likelihood), we can solve an optimization problem by finding the right distribution $$$$  that best fits the Evidence Lower Bound.  


    __Learning and Inference wrt the ELBO - Summary:__{: style="color: red"}  
    The <span>__ELBO__ $$\mathcal{L}(\boldsymbol{v}, \boldsymbol{\theta}, q)$$ is a lower bound on $$\log p(\boldsymbol{v} ; \boldsymbol{\theta})$$</span>{: style="color: goldenrod"}:  
    {: #lst-p}
    * __Inference__: can be viewed as <span>maximizing $$\mathcal{L}$$ with respect to $$q$$</span>{: style="color: goldenrod"}.  
    * __Learning__: can be viewed as <span>maximizing $$\mathcal{L}$$ with respect to $$\theta$$</span>{: style="color: goldenrod"}.  


    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * The difference between the ELBO and the KL divergence is the log normalizer (i.e. the evidence), which is the quantity that the ELBO bounds.  
    * Maximizing the ELBO is equivalent to Minimizing the KL-Divergence.  
    <br>

4. **Expectation Maximization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    The __Expectation-Maximization__ Algorithm is an iterative method to find _maximum likelihood_ or _maximum a posteriori (MAP)_ estimates of parameters in statistical models with _unobserved latent variables_.  

    It is based on maximizing a lower bound $$\mathcal{L}$$.  
    It is not an approach to __approximate inference__.  
    It is an approach to learning with an *__approximate__* __posterior__.  

    __The EM Algorithm:__{: style="color: red"}  
    The EM Algorithm consists of alternating between two steps until convergence:  
    {: #lst-p}
    * The __E(xpectation)-step:__  
        * Let $$\theta^{(0)}$$ denote the value of the parameters at the beginning of the step.  
        * Set $$q\left(\boldsymbol{h}^{(i)} \vert \boldsymbol{v}\right)=p\left(\boldsymbol{h}^{(i)} ; \boldsymbol{\theta}^{(0)}\right)$$ for all indices $$i$$ of the training examples $$\boldsymbol{v}^{(i)}$$ we want to train on (both batch and minibatch variants are valid).  
            By this we mean $$q$$ is defined in terms of the current parameter value of $$\boldsymbol{\theta}^{(0)}$$;  
            if we vary $$\boldsymbol{\theta},$$ then $$p(\boldsymbol{h} \vert \boldsymbol{v} ; \boldsymbol{\theta})$$ will change, but $$q(\boldsymbol{h} \vert \boldsymbol{v})$$ will remain equal to $$p\left(\boldsymbol{h} \vert \boldsymbol{v} ; \boldsymbol{\theta}^{(0)}\right)$$.  
    * The __M(aximization)-step:__  
        * Completely or partially maximize  
            <p>$$\sum_i \mathcal{L}\left(\boldsymbol{v}^{(i)}, \boldsymbol{\theta}, q\right)$$</p>  
            with respect to $$\boldsymbol{\theta}$$ using your optimization algorithm of choice.  

    __Relation to Coordinate Ascent:__{: style="color: red"}  
    The algorithm can be viewed as a __Coordinate Ascent__ algorithm to maximize $$\mathcal{L}$$.  
    On one step, we maximize $$\mathcal{L}$$ with respect to $$q,$$ and on the other, we maximize $$\mathcal{L}$$ with respect to $$\boldsymbol{\theta}$$.  
    __Stochastic Gradient Ascent__ on _latent variable models_ can be seen as a special case of the EM algorithm where the M-step consists of taking a single gradient step.  
    > Other variants of the EM algorithm can make much larger steps. For some model families, the M-step can even be performed analytically, jumping all the way to the optimal solution for $$\theta$$ given the current $$q$$.  

    __As Approximate Inference - Interpretation:__{: style="color: red"}  
    Even though the E-step involves _exact inference_, the EM algorithm can be viewed as using _approximate inference_.  
    The M-step assumes that the same value of $$q$$ can be used for all values of $$\theta$$.  
    This will introduce a gap between $$\mathcal{L}$$ and the true $$\log p(\boldsymbol{v})$$ as the M-step moves further and further away from the value $$\boldsymbol{\theta}^{(0)}$$ used in the E-step.  
    Fortunately, the E-step reduces the gap to zero again as we enter the loop for the next time.  

    
    __Insights/Takeaways:__{: style="color: red"}  
    {: #lst-p}
    1. The __Basic Structure of the Learning Process:__  
        We update the model parameters to improve the likelihood of a completed dataset, where all missing variables have their values provided by an estimate of the posterior distribution.  
        > This particular insight is not unique to the EM algorithm. For example, using gradient descent to maximize the log-likelihood also has this same property; the log-likelihood gradient computations require taking expectations with respect to the posterior distribution over the hidden units.   
    2. __Reusing $$q$$:__  
        We can continue to use one value of $$q$$ even after we have moved to a different value of $$\theta$$.  
        This particular insight is used throughout _classical machine learning_ to derive large M-step updates.  
        In the context of deep learning, most models are too complex to admit a tractable solution for an optimal large M-step update, so this second insight, which is more unique to the EM algorithm, is rarely used.  



    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * [The Expectation-Maximization Algorithm and Derivation (Blog!)](http://bjlkeng.github.io/posts/the-expectation-maximization-algorithm/)  
    * The EM algorithm enables us to make large learning steps with a fixed $$q$$  
    <br>


5. **MAP Inference:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    __MAP Inference__ is an alternative form of inference where we are interested in computing the single most likely value of the missing variables, rather than to infer the entire distribution over their possible values $$p(\boldsymbol{h} \vert \boldsymbol{v})$$.  
    In the context of __latent variable models__, we compute:  
    <p>$$\boldsymbol{h}^{* }=\underset{\boldsymbol{h}}{\arg \max } p(\boldsymbol{h} \vert \boldsymbol{v})$$</p>  

    __As Approximate Inference:__{: style="color: red"}  
    It is __not__ usually thought of as __approximate inference__, since it computes the <span>exact most likely value of $$\boldsymbol{h}^{* }$$</span>{: style="color: purple"}.  
    However, to develop a [__learning process__](#bodyContents15lp) wrt maximizing the lower bound $$\mathcal{L}(\boldsymbol{v}, \boldsymbol{h}, q),$$ then it is helpful to think of MAP inference as a procedure that provides a value of $$q$$.  
    In this sense, we can think of MAP inference as __approximate inference__, because it <span>does not provide the optimal $$q$$</span>{: style="color: purple"}.  
    We can __derive__ MAP Inference as a form of approximate inference by <span>restricting the family of distributions $$q$$ may be drawn from</span>{: style="color: goldenrod"}.  
    __Derivation:__  
    {: #lst-p}
    * We require $$q$$ to take on a __Dirac distribution__:  
        <p>$$q(\boldsymbol{h} \vert \boldsymbol{v})=\delta(\boldsymbol{h}-\boldsymbol{\mu})$$</p>  
    * This means that we can now control $$q$$ entirely via $$\boldsymbol{\mu}$$.  
    * Dropping terms of $$\mathcal{L}$$ that do not vary with $$\boldsymbol{\mu},$$ we are left with the optimization problem:  
        <p>$$\boldsymbol{\mu}^{* }=\underset{\mu}{\arg \max } \log p(\boldsymbol{h}=\boldsymbol{\mu}, \boldsymbol{v})$$</p>  
    * which is *__equivalent__* to the __MAP inference problem__:  
        <p>$$\boldsymbol{h}^{* }=\underset{\boldsymbol{h}}{\arg \max } p(\boldsymbol{h} \vert \boldsymbol{v})$$</p>  

    __The Learning Procedure with MAP Inference:__{: style="color: red"}{: #bodyContents15lp}  
    We can, thus, justify a learning procedure similar to __EM__, where we alternate between:  
    {: #lst-p}
    * Performing MAP inference to infer $$\boldsymbol{h}^{* }$$, and  
    * Updating update $$\boldsymbol{\theta}$$ to increase $$\log p\left(\boldsymbol{h}^{* }, \boldsymbol{v}\right)$$.  

    __As Coordinate Ascent:__  
    As with EM, this is a form of __coordinate ascent__ on $$\mathcal{L},$$ where we alternate between using inference to optimize $$\mathcal{L}$$ with respect to $$q$$ and using parameter updates to optimize $$\mathcal{L}$$ with respect to $$\boldsymbol{\theta}$$.  

    __Lower Bound (ELBO) Justification:__  
    The procedure as a whole can be justified by the fact that $$\mathcal{L}$$ is a lower bound on $$\log p(\boldsymbol{v})$$.  
    In the case of MAP inference, this justification is rather *__vacuous__*, because the bound is __infinitely loose__, due to the __Dirac distribution's differential entropy of negative infinity__.  
    <span>Adding noise to $$\mu$$ would make the bound meaningful again</span>{: style="color: goldenrod"}.   

    __MAP Inference in Deep Learning - Applications:__{: style="color: red"}  
    MAP Inference is commonly used in deep learning as both a <span>__feature extractor__</span>{: style="color: purple"} and a <span>__learning mechanism__</span>{: style="color: purple"}.  
    It is primarily used for __sparse coding models__.  

    __MAP Inference in Sparse Coding Models:__{: style="color: red"}{: #bodyContents15map_sc}  
    <button>Sparse Coding Discussion</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/xhMTHTt2HOc7OibASJ5OYwn0Dw0ck6ZFGalynC643GQ.original.fullsize.png){: width="100%" hidden=""}  

    __Summary:__{: style="color: red"}  
    Learning algorithms based on MAP inference enable us to <span>__learn using a *point estimate*__ of $$p(\boldsymbol{h} \vert \boldsymbol{v})$$ rather than inferring the entire distribution</span>{: style="color: goldenrod"}.  
    <br>


6. **Variational Inference and Learning:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    

    __Main Idea - Restricting family of distributions $$q$$:__{: style="color: red"}  
    The core idea behind variational learning is that we can maximize $$\mathcal{L}$$ over a restricted family of distributions $$q$$.  
    This family should be chosen so that it is easy to compute $$\mathbb{E}_ {q} \log p(\boldsymbol{h}, \boldsymbol{v})$$.  
    A typical way to do this is to introduce assumptions about how $$q$$ factorizes.  
    Mainly, we make a __Mean-Field Approximation__ to $$q$$.  


    __Mean-Field Approximation:__{: style="color: red"}  
    __Mean-Field Approximation__ is a type of _Variational Bayesian Inference_ where we assume that the unknown variables can be partitioned so that each partition is <span>__independent__</span>{: style="color: purple"} of the others.  
    The Mean-Field Approximation assumes the variational distribution over the latent variables factorizes as:  
    <p>$$q(\boldsymbol{h} \vert \boldsymbol{v})=\prod_{i} q\left(h_{i} \vert \boldsymbol{v}\right)$$</p>  
    I.E. it imposes the restriction that $$q$$ is a __factorial distribution__.  

    More generally, we can impose any graphical model structure we choose on $$q,$$ to flexibly determine how many interactions we want our approximation to capture.  
    This fully general graphical model approach is called __structured variational inference__ _(Saul and Jordan, 1996)_.  

    __The Optimal Probability Distribution $$q$$:__{: style="color: red"}  
    The beauty of the variational approach is that we do not need to specify a specific parametric form for $$q$$.  
    We specify how it should factorize, but then <span>the optimization problem determines the __*optimal* probability distribution__ within those factorization constraints</span>{: style="color: purple"}.  
    __The Inference Optimization Problem:__{: style="color: red"}  
    {: #lst-p}
    * For __*discrete* latent variables__: we use traditional optimization techniques to optimize a finite number of variables describing the $$q$$ distribution.  
    * For __*continuous* latent variables__: we use <span>__calculus of variations__</span>{: style="color: purple"} to perform optimization over a space of functions and actually determine which function should be used to represent $$q$$.  
        * __Calculus of Variations__ removes much of the responsibility from the human designer of the model, who now must specify only how $$q$$ factorizes, rather than needing to guess how to design a specific $$q$$ that can accurately approximate the posterior.  

        > Calculus of variations is the origin of the names "variational learning" and "variational inference", but the names apply in both discrete and continuous cases.    

    __KL-Divergence Optimization:__  
    {: #lst-p}
    * The Inference Optimization Problem boils down to <span>maximizing $$\mathcal{L}$$ with respect to $$q$$</span>{: style="color: purple"}.  
    * This is equivalent to <span>minimizing $$D_{\mathrm{KL}}(q(\boldsymbol{h} \vert \boldsymbol{v}) \| p(\boldsymbol{h} \vert \boldsymbol{v}))$$</span>{: style="color: purple"}.  
    * Thus, we are <span>fitting $$q$$ to $$p$$</span>{: style="color: goldenrod"}.  
    * However, we are doing so with the opposite direction of the KL-Divergence. We are, _unnaturally_, assuming that $$q$$ is constant and $$p$$ is varying.  
    * In the inference optimization problem, we choose to use $$D_{\mathrm{KL}}\left(q(\boldsymbol{h} \vert \boldsymbol{v}) \| p(\boldsymbol{h} \vert \boldsymbol{v})\right)$$ for *__computational reasons__*.  
        * Specifically, computing $$D_{\mathrm{KL}}\left(q(\boldsymbol{h} \vert \boldsymbol{v}) \| p(\boldsymbol{h} \vert \boldsymbol{v})\right)$$ involves evaluating expectations with respect to $$q,$$ so by designing $$q$$ to be simple, we can simplify the required expectations.  
        * The opposite direction of the KL divergence would require computing expectations with respect to the true posterior.  
            Because the form of the true posterior is determined by the choice of model, we cannot design a reduced-cost approach to computing $$D_{\mathrm{KL}}(p(\boldsymbol{h} \vert \boldsymbol{v}) \| q(\boldsymbol{h} \vert \boldsymbol{v}))$$ exactly.  
    * __Three Cases for Optimization__:  
        * If $$q$$ is high and $$p$$ is high, then we are happy (i.e. low KL divergence).
        * If $$q$$ is high and $$p$$ is low then we pay a price (i.e. high KL divergence).
        * If $$q$$ is low then we dont care (i.e. also low KL divergence, regardless of $$p$$).
    * __Optimization-based Inference vs Maximum Likelihood (ML) Learning__:  
        * __ML-Learning:__ fits a model to data by minimizing $$D_{\mathrm{KL}}\left(p_{\text {data }} \| p_{\text {model }}\right)$$.  
            It encourages the <span>__model__ to have __*high* probability__ everywhere that the __data__ has __*high* probability__</span>{: style="color: purple"}, 
        * __Optimization-based Inference__:   
            It encourages <span>__$$q$$__ to have __*low* probability__ everywhere the __true posterior__ has __*low* probability__</span>{: style="color: purple"}.  
    
    __Variational (Bayesian) Inference:__{: style="color: red"}  
    __Variational Bayesian Inference__ AKA __Variational Bayes__ is most often used to infer the <span>_conditional_ distribution over the latent variables given the observations</span>{: style="color: purple"}  (and parameters).  
    This is also known as the __posterior distribution over the *latent* variables__:  
    <p>$$p(z \vert x, \alpha)=\frac{p(z, x \vert \alpha)}{\int_{z} p(z, x \vert \alpha)}$$</p>  
    which is usually *__intractable__*.  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * __KL Divergence Optimization:__  
        Optimizing the KL-Divergence given by:  
        <p>$$D_{\mathrm{KL}}(q \| p) = \mathrm{E}_ {z \sim q}\left[\log \frac{q(z)}{p(z\vert x)}\right] =\int_{z} q(z) \log \left(\frac{q(z)}{p(z\vert x)}\right) dz$$</p>  
        * __Three Cases for Optimization__:  
            * If $$q$$ is high and $$p$$ is high, then we are happy (i.e. low KL divergence).
            * If $$q$$ is high and $$p$$ is low then we pay a price (i.e. high KL divergence).
            * If $$q$$ is low then we dont care (i.e. also low KL divergence, regardless of $$p$$).
    <br>

***

## Variational Inference and Learning
{: #content2}



2. **Variational Inference - Discrete Latent Variables:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    Variational Inference with Discrete Latent Variables is relatively straightforward.  
    __Representing $$q$$:__  
    We define a distribution $$q$$ where each factor of $$q$$ is just defined by a lookup table over discrete states.  
    In the simplest case, $$h$$ is binary and we make the mean field assumption that $$q$$ factorizes over each individual $$h_{i}$$.  
    In this case we can parametrize $$q$$ with a vector $$\hat{h}$$ whose entries are probabilities.  
    Then $$q\left(h_{i}=1 \vert \boldsymbol{v}\right)=\hat{h}_ {i}$$.  
    __Optimizing $$q$$:__  
    After determining how to represent $$q$$ we simply __optimize its parameters__.  
    For __discrete__ latent variables this is just a standard optimization problem e.g. *__gradient descent__*.  
    However, because this optimization must occur in the inner loop of a learning algorithm, it must be __very fast__[^1].  
    A popular choice is to <span>__iterate fixed-point equations__</span>{: style="color: purple"}; to solve:  
    <p>$$\frac{\partial}{\partial \hat{h}_ {i}} \mathcal{L}=0$$</p>  
    for $$\hat{h}_ {i}$$.  
    We repeatedly update different elements of $$\hat{\boldsymbol{h}}$$ until we satisfy a convergence criterion.  


    __Application - Binary Sparse Coding:__{: style="color: red"}  
    <br>



3. **Variational Inference - Continuous Latent Variables:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    Variational Inference and Learning with Continuous Latent Variables requires the use of the [__calculus of variations__](#bodyContents41) for maximizing $$\mathcal{L}$$ with respect to $$q(\boldsymbol{h} \vert \boldsymbol{v})$$.  

    In most cases, practitioners need not solve any calculus of variations problems themselves. Instead, there is a __general equation for the mean field fixed-point updates__.  

    __The General Equation for Mean-Field Fixed-Point Updates:__{: style="color: red"}  
    If we make the mean field approximation  
    <p>$$q(\boldsymbol{h} \vert \boldsymbol{v})=\prod_{i} q\left(h_{i} \vert \boldsymbol{v}\right)$$</p>  
    and fix $$q\left(h_{j} \vert \boldsymbol{v}\right)$$ for all $$j \neq i,$$ then the <span>optimal $$q\left(h_{i} \vert \boldsymbol{v}\right)$$ may be obtained by __normalizing the unnormalized distribution__</span>{: style="color: goldenrod"}:  
    <p>$$\tilde{q}\left(h_{i} \vert \boldsymbol{v}\right) = \exp \left(\mathbb{E}_{\mathbf{h}_{-i} \sim q\left(\mathbf{h}_ {-i} \vert \boldsymbol{v}\right)} \log \tilde{p}(\boldsymbol{v}, \boldsymbol{h})\right) = e^{\mathbb{E}_{\mathbf{h}_ {-i} \sim q\left(\mathbf{h}_ {-i} \vert \boldsymbol{v}\right)} \log \tilde{p}(\boldsymbol{v}, \boldsymbol{h})}$$</p>  
    as long as $$p$$ does not assign $$0$$ probability to any joint configuration of variables.  
    \- Carrying out the expectation inside the equation will yield the correct functional form of $$q\left(h_{i} \vert \boldsymbol{v}\right)$$.   
    \- The General Equation yields the mean field approximation for any probabilistic model.  
    \- Deriving functional forms of $$q$$ directly using calculus of variations is only necessary if one wishes to develop a new form of variational learning.  
    \- The General Equation is a __fixed-point equation__, designed to be iteratively applied for each value of $$i$$ repeatedly until convergence.  

    __Functional Form of the Optimal Distribution/Solution:__{: style="color: red"}  
    The General Equation tells us the <span>__functional form__ that the _optimal solution_ will take</span>{: style="color: purple"}, whether we arrive there by fixed-point equations or not.  
    <span>This means we can take the functional form from that equation but regard some of the values that appear in it as *__parameters__*, which we can optimize with any optimization algorithm we like.</span>{: style="color: goldenrod"}  
    <button>Example - Application:</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/YFBRwnXeoFZ7iB-yu4PE3Pszt1U5A8BGnc5ZhATfijg.original.fullsize.png){: width="100%" hidden=""}  

    For examples of real applications of variational learning with continuous variables in the context of deep learning, see _Goodfellow et al. (2013d)_.  
    <br>


4. **Interactions between Learning and Inference:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    <span>Using __approximate inference__ as part of a __learning algorithm__ affects the **learning process**</span>{: style="color: purple"}, and this in turn <span>affects the **_accuracy_ of the inference algorithm**</span>{: style="color: purple"}.  
    __Analysis:__  
    {: #lst-p}
    * The training algorithm tends to adapt the model in a way that makes the approximating assumptions underlying the approximate inference algorithm become more true.  
    * When training the parameters, variational learning increases  
        <p>$$\mathbb{E}_ {\mathbf{h} \sim q} \log p(\boldsymbol{v}, \boldsymbol{h})$$</p>  
    * For a specific $$v$$ this:  
        * increases $$p(\boldsymbol{h} \vert \boldsymbol{v})$$ for values of $$\boldsymbol{h}$$ that have high probability under $$q(\boldsymbol{h} \vert \boldsymbol{v})$$ and  
        * decreases $$p(\boldsymbol{h} \vert \boldsymbol{v})$$ for values of $$\boldsymbol{h}$$ that have low probability under $$q(\boldsymbol{h} \vert \boldsymbol{v})$$.  
    * This behavior <span>causes our approximating assumptions to become *__self-fulfilling prophecies__*</span>{: style="color: purple"}.  
        If we train the model with a unimodal approximate posterior, we will obtain a model with a true posterior that is far closer to unimodal than we would have obtained by training the model with exact inference.  

    __Computing the Effect (Harm) of using Variational Inference:__{: style="color: red"}  
    Computing the true amount of harm imposed on a model by a variational approximation is thus very difficult.  
    {: #lst-p}
    * There exist several methods for estimating $$\log p(\boldsymbol{v})$$:  
        We often estimate $$\log p(\boldsymbol{v} ; \boldsymbol{\theta})$$ after training the model and find that the gap with $$\mathcal{L}(\boldsymbol{v}, \boldsymbol{\theta}, q)$$ is small.  
        * From this, we can __conclude__ that <span>our variational approximation is __accurate for the specific value of $$\boldsymbol{\theta}$$__</span>{: style="color: purple"} that we obtained from the learning process.  
        * We should *__not__* __conclude__ that <span>our variational approximation is __accurate in general__</span>{: style="color: purple"} or that <span>the variational approximation __did *little harm* to the learning process__</span>{: style="color: purple"}.  
    * To measure the *__true amount of harm__ induced by the variational approximation*:  
        * We would need to know $$\boldsymbol{\theta}^{* }=\max_{\boldsymbol{\theta}} \log p(\boldsymbol{v} ; \boldsymbol{\theta})$$.  
        * It is possible for $$\mathcal{L}(\boldsymbol{v}, \boldsymbol{\theta}, q) \approx \log p(\boldsymbol{v} ; \boldsymbol{\theta})$$ and $$\log p(\boldsymbol{v} ; \boldsymbol{\theta}) \ll \log p\left(\boldsymbol{v} ; \boldsymbol{\theta}^{* }\right)$$ to hold simultaneously.  
        * If $$\max_{q} \mathcal{L}\left(\boldsymbol{v}, \boldsymbol{\theta}^{* }, q\right) \ll \log p\left(\boldsymbol{v} ; \boldsymbol{\theta}^{* }\right),$$ because $$\boldsymbol{\theta}^{* }$$ induces too complicated of a posterior distribution for our $$q$$ family to capture, then the learning process will never approach $$\boldsymbol{\theta}^{* }$$.  
        * Such a problem is very difficult to detect, because we can only know for sure that it happened if we have a superior learning algorithm that can find $$\boldsymbol{\theta}^{* }$$ for comparison.  
    <br>


5. **Learned Approximate Inference:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    __Motivation:__{: style="color: red"}  
    Explicitly performing optimization via iterative procedures such as _fixed-point equations_ or _gradient-based optimization_ is often __very expensive__ and __time consuming__.  
    Many approaches to inference avoid this expense by <span>learning to perform approximate inference</span>{: style="color: purple"}.  

    __Learned Approximate Inference:__  
    Learns to perform approximate inference by viewing the (multistep iterative) optimization process as a function $$f$$ that maps an input $$v$$ to an approximate distribution $$q^{* }=\arg \max_{q} \mathcal{L}(\boldsymbol{v}, q)$$, and then <span>approximates this function with a __neural network__</span>{: style="color: goldenrod"} that implements an approximation $$f(\boldsymbol{v} ; \boldsymbol{\theta})$$.  


    __Wake-Sleep:__{: style="color: red"}  
    __Motivation__:  
    {: #lst-p}
    * One of the main difficulties with training a model to infer $$h$$ from $$v$$ is that we do not have a supervised training set with which to train the model.  
    * Given a $$v$$ we do not know the appropriate $$h$$.  
    * The mapping from $$v$$ to $$h$$ depends on the choice of model family, and evolves throughout the learning process as $$\theta$$ changes.  

    __Wake-Sleep Algorithm__:  
    The __wake-sleep algorithm__ _(Hinton et al., 1995b; Frey et al., 1996)_ resolves this problem by <span>drawing samples of both $$h$$ and $$v$$ __from the *model distribution*__</span>{: style="color: purple"}.  
    * For example, in a __directed model__, this can be done cheaply by performing *__ancestral sampling__* beginning at $$h$$ and ending at $$v$$.  
        The inference network can then be trained to perform the reverse mapping: predicting which $$h$$ caused the present $\boldsymbol{v}$.  
    
    __DrawBacks__:  
    The main drawback to this approach is that we will only be able to train the inference network on values of $$\boldsymbol{v}$$ that have high probability under the model.  
    Early in learning, the <span>model distribution will not resemble the data distribution</span>{: style="color: purple"}, so <span>the inference network will not have an opportunity to _learn on samples that resemble data_</span>{: style="color: purple"}.  

    __Relation to Biological Dreaming:__  
    <button>Discussion</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/gAcJcN1TtW7H23J0TssVMLYjOufszOjoEigvRExEVeo.original.fullsize.png){: width="100%" hidden=""}  


    __Generative Modeling - Application:__{: style="color: red"}  
    Learned approximate inference has recently become one of the dominant approaches to generative modeling, in the form of the __Variational AutoEncoder__ _(Kingma, 2013; Rezende et al., 2014)_.  
    In this elegant approach, there is <span>no need to _construct explicit targets_ for the inference network</span>{: style="color: purple"}.  
    Instead, the <span>inference network is simply used to define $$\mathcal{L},$$</span>{: style="color: purple"} and then <span>the parameters of the inference network are adapted to increase $$\mathcal{L}$$</span>{: style="color: purple"}.  




***
***

## Mathematics of Approximate Inference
{: #content4}

[Directional Derivative](http://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx)  
[The Calculus of Variations (Blog!)](http://bjlkeng.github.io/posts/the-calculus-of-variations/#id1)  

1. **Calculus of Variations:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    
    Method for finding the *__stationary__* __functions__ of a functional $$I[f]$$ (function of functions) by solving a differential equation.  

    __Formally,__ calculus of variations seeks to find the function $$y=f(x)$$ such that the integral (functional):  
    <p>$$I[y]=\int_{x_{1}}^{x_{2}} L\left(x, y(x), y^{\prime}(x)\right) d x$$</p>   
    <p>$$\begin{array}{l}{\text {where}}\\{x_{1}, x_{2} \text { are constants, }} \\ {y(x) \text { is twice continuously differentiable, }} \\ {y^{\prime}(x)=d y / d x} \\ {L\left(x, y(x), y^{\prime}(x)\right) \text { is twice continuously differentiable with respect to its arguments } x, y, y^{\prime}}\end{array}$$</p>  
    is __stationary__.  


    __Euler Lagrange Equation - Finding Extrema:__{: style="color: red"}  
    Finding the extrema of functionals is similar to finding the maxima and minima of functions. The maxima and minima of a function may be located by finding the points where its derivative vanishes (i.e., is equal to zero). The extrema of functionals may be obtained by finding functions where the functional derivative is equal to zero. This leads to solving the associated Euler‚ÄìLagrange equation.  

    The __Euler Lagrange Equation__ is a second-order partial differential equation whose solutions are the functions for which a given functional is stationary:  
    <p>$$\frac{\partial L}{\partial f}-\frac{d}{d x} \frac{\partial L}{\partial f^{\prime}} = 0$$</p>  
    It is defined in terms of the __functional derivative__:  
    <p>$$\frac{\delta J}{\delta f(x)} = \frac{\partial L}{\partial f}-\frac{d}{d x} \frac{\partial L}{\partial f^{\prime}} = 0$$</p>  




    __Shortest Path between Two Points:__{: style="color: red"}  
    Find path such that the distance $$AB$$ between two points is minimized.  
    Using the *__arc length__*, we define the following __functional__:  
    <p>$$\begin{align}
        I &= \int_{A}^{B} dS \\
             &= \int_{A}^{B} \sqrt{dx^2 + dy^2} \\
             &= \int_{A}^{B} \sqrt{1 + \left(\dfrac{dy}{dx}\right)^2} dx \\ 
             &= \int_{x_1}^{x_2} \sqrt{1 + \left(\dfrac{dy}{dx}\right)^2} dx
        \end{align}
        $$</p>  
    * Now, we formulate the __variational problem__:  
        Find the extremal function $$y=f(x)$$ between two points $$A=(x_1, y_1)$$ and $$B=(x_2, y_2)$$ such that the following integral is __minimized__:  
        <p>$$I[y] = \int_{x_{1}}^{x_{2}} \sqrt{1+\left[y^{\prime}(x)\right]^{2}} d x$$</p>   
        where $$y^{\prime}(x)=\frac{d y}{d x}, y_{1}=f\left(x_{1}\right), y_{2}=f\left(x_{2}\right)$$.  
    * __Solution:__  
        We use the __Euler-Lagrange Equation__ to find the extremal function $$f(x)$$ that minimizes the functional $$I[y]$$:  
        <p>$$\frac{\partial L}{\partial f}-\frac{d}{d x} \frac{\partial L}{\partial f^{\prime}}=0$$</p>  
        where $$L=\sqrt{1+\left[f^{\prime}(x)\right]^{2}}$$.  
        * Since $$f$$ does not appear explicity in $$L,$$ the first term in the Euler-Lagrange equation vanishes for all $$f(x)$$  
            <p>$$\frac{\partial L}{\partial f} = 0$$</p>  
        * Thus,  
            <p>$$\frac{d}{d x} \frac{\partial L}{\partial f^{\prime}}=0$$</p>  
        * Substituting for $$L$$ and taking the derivative:  
            <p>$$\frac{d}{d x} \frac{f^{\prime}(x)}{\sqrt{1+\left[f^{\prime}(x)\right]^{2}}}=0$$</p>  
            for some constant $$c$$.  
        * If the derivative $$\frac{d}{dx}$$, above, is zero, then  
            <p>$$\frac{f^{\prime}(x)}{\sqrt{1+\left[f^{\prime}(x)\right]^{2}}}=c$$</p>  
            for some constant $$c$$.  
        * Square both sides:  
            <p>$$\frac{\left[f^{\prime}(x)\right]^{2}}{1+\left[f^{\prime}(x)\right]^{2}}=c^{2}$$</p>  
            where $$0 \leq c^{2}<1$$.  
        * Solving:  
            <p>$$\left[f^{\prime}(x)\right]^{2}=\frac{c^{2}}{1-c^{2}}$$</p>  
            $$\implies$$  
            <p>$$f^{\prime}(x)=m$$</p>  
            is a constant $$m$$.  
        * Integrating:  
            <p>$$f(x)=m x+b$$</p>  
            is an __equation of a (straight) line__, where $$m=\frac{y_{2}-y_{1}}{x_{2}-x_{1}} \quad$$ and $$\quad b=\frac{x_{2} y_{1}-x_{1} y_{2}}{x_{2}-x_{1}}$$.  

        In other words, the shortest distance between two points is a straight line.  
        <div class="borderexample" markdown="1">
        <span>We have found the extremal function $$f(x)$$ that minimizes the functional $$A[y]$$ so that $$A[f]$$ is a minimum.</span>{: style="color: purple"}
        </div>  

    <br>

2. **Mean Field Methods:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}

3. **Mean Field Approximations:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}



[^1]: To achieve this speed, we typically use special optimization algorithms that are designed to solve comparatively small and simple problems in few iterations.  

***
***

TITLE: The Partition Function
LINK: research/dl/concepts/partition_function.md



[A Thorough Introduction to Boltzmann Machines](http://willwolf.io/2018/10/20/thorough-introduction-to-boltzmann-machines/)  
[Strategies for Confronting the Partition Function (Blog! + code)](http://willwolf.io/2018/10/29/additional-strategies-partition-function/)  
[Approximating the Softmax (Ruder)](http://ruder.io/word-embeddings-softmax/index.html)  
[Confronting the partition function (Slides)](http://www.tsc.uc3m.es/~jcid/MLG/mlg2018/DL_Cap18.pdf)  
[Graphical Models, Exponential Families, and Variational Inference (M Jordan)](https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf)  
* [Towards Biologically Plausible Deep Learning (paper!)](https://arxiv.org/pdf/1502.04156.pdf)  



## Introduction - The Partition Function
{: #content1}

1. **The Partition Function:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __The Partition Function__ is the _normalization constant_ of an unnormalized probability distribution $$\tilde{p}(\mathbf{x} ; \boldsymbol{\theta})$$.   
    
    Formally, it is the (possibly infinite) sum over the unnormalized probability $$\tilde{p}(\mathbf{x} ; \boldsymbol{\theta})$$ of all the states/events $$\boldsymbol{x} \in X$$,  
    {: #lst-p}
    * __Discrete Variables__:  
        <p>$$Z(\boldsymbol{\theta}) = \sum_{\boldsymbol{x}} \tilde{p}(\boldsymbol{x})$$</p>  
    * __Continuous Variables__:  
        <p>$$Z(\boldsymbol{\theta}) = \int \tilde{p}(\boldsymbol{x}) d \boldsymbol{x}$$</p>  

    It is defined such that:  
    <p>$$\sum_\mathbf{x} p(\mathbf{x} ; \boldsymbol{\theta}) = \sum_\mathbf{x} \dfrac{\tilde{p}(\mathbf{x} ; \boldsymbol{\theta})}{Z(\boldsymbol{\theta})} = 1$$</p>  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * The Partition Function contains an __explicit Temperature__  
    * The Partition Function is a __generating function__  
    * [Statistical Mechanics of Learning from Examples](https://pdfs.semanticscholar.org/2498/a4e1755f047accc06a6e0fab0b0eb1b37ae0.pdf)  
        _Sompolinsky et al._ confront the partition function for a Perceptron using statistical mechanics methods developed for spin glasses and simple nets (Garder, Derrida) and applied it to Perceptrons and, later, to something like MLPs.  
    * [Unreasonable effectiveness of learning neural networks: From accessible states and robust ensembles to basic algorithmic schemes](https://www.pnas.org/content/pnas/113/48/E7655.full.pdf)  
        Uses old techniques from non-equilibrium statistical mechanics to address the modern problems of inference.  
    <br>

2. **Handling the Partition Function - Motivation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    Many __*Undirected* Probabilistic Graphical Models (PGMs)__ are defined by an unnormalized probability distribution $$\tilde{p}(\mathbf{x} ; \boldsymbol{\theta})$$.  
    To obtain a valid probability distribution, we need to *__normalize__* $$\tilde{p}$$ by dividing by a partition function $$Z(\boldsymbol{\theta})$$:  
    <p>$$p(\mathbf{x} ; \boldsymbol{\theta})=\dfrac{1}{Z(\boldsymbol{\theta})} \tilde{p}(\mathbf{x} ; \boldsymbol{\theta})$$</p>  
    Calculating the partition function can be *__intractable__* for many interesting models.  

    __The Partition Function in Deep Probabilistic Models:__{: style="color: red"}  
    Deep Probabilistic Models are usually designed with the partition function in mind. There a few approaches taken in the designs:  
    {: #lst-p}
    * Some models are designed to have a __tractable normalizing constant__.  
    * Others are designed to be used in ways (training/inference) that _avoid_ computing the normalized probability altogether.  
    * Yet, other models directly confront the challenge of intractable partition functions.  
        They use techniques, described below, for training and evaluating models with intractable $$Z$$.  

    __Handling the Partition Function:__{: style="color: red"}  
    There are a few approaches to handle the (intractable) partition function:  
    {: #lst-p}
    1. Estimate the __partition function__ as a *__learned parameter__*; __Noise-Contrastive Estimation__.  
    2. Estimate the *__gradient__* of the partition function directly; __Stochastic MLE__, __Contrastive-Divergence__.  
    3. Avoid computing quantities related to the partition function altogether; __Score Matching__, __Pseudolikelihood__.  
    4. Estimate the __partition function__ (itself) explicitly: __Annealed IS__, __Bridge Sampling__, __Linked IS__.   
    <br>

3. **The Log-Likelihood Gradient:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    __Phase Decomposition of Learning:__{: style="color: red"}  
    Learning using __MLE__ requires computing the gradient of the __NLL__, $$\nabla_{\boldsymbol{\theta}} \log p(\mathbf{x} ; \boldsymbol{\theta})$$.  
    What makes learning undirected models by maximum likelihood particularly difficult is that the __partition function depends on the parameters__; thus, the gradient of the NLL wrt the parameters involves computing the gradient of $$Z(\mathbf{\theta})$$.  
    In __undirected models__, this gradient can be written as:    
    <p>$$\nabla_{\boldsymbol{\theta}} \log p(\mathbf{x} ; \boldsymbol{\theta})=\nabla_{\boldsymbol{\theta}} \log \tilde{p}(\mathbf{x} ; \boldsymbol{\theta})-\nabla_{\boldsymbol{\theta}} \log Z(\boldsymbol{\theta})$$</p>  
    which *__decomposes__* the gradient (learning) into a <span>__positive phase__</span>{: style="color: purple"} and a <span>__negative phase__</span>{: style="color: purple"}.  

    __Difficulties in Learning wrt the Decomposition:__  
    {: #lst-p}
    * __Difficulty in the *Negative* Phase:__  
        \- For most __undirected models__ of interest, the __negative phase__ is *__difficult__* to compute. This is usually due to having to compute the unnormalized probability for __all__ the states.  
        \- __Directed models__ define many "implicit" *__conditional independencies__* between the variables, making it easier to compute the normalization due to many terms canceling out.  
        * __Example - RBMs__:  
            The quintessential example of a model with a straightforward positive phase and a difficult negative phase is the RBM.  
            It has hidden units that are conditionally independent from each other given the visible units.  

        > *__Word2vec__* is another example.  
    * __Difficulty in the *Positive* Phase__:  
        \- Latent Variable Models, generally, have intractable positive phase.  
        \- Models with no latent variables or with few interactions between latent variables typically have a tractable positive phase.  
        * __Example - VAEs__:  
            VAEs define a __continuous__ distribution (over the data) with __latent variable $$z$$__:  
            <p>$$p_{\theta}(x)=\int p_{\theta}(z) p_{\theta}(x \vert z) d z$$</p>  
            which is __intractable__ to compute for every $$z$$.  
            Due to complicated interactions between latent variables, this integral requires exponential time to compute as it needs to be evaluated over all configurations of latent variable.  
            (all $$z_i$$ variables are dependent on each other.)   

    __Positive and Negative Phases:__  
    The terms positive and negative do not refer to the sign of each term in the equation, but rather reflect their effect on the probability density defined by the model.  
    \- The __positive phase__ <span>*__increases__* the probability of training data</span>{: style="color: goldenrod"}  (by reducing the corresponding free energy)  
    \- The __negative phase__ <span>*__decreases__*  the probability of samples generated by the model</span>{: style="color: goldenrod"}.  


    __Monte Carlo Methods for Approximate LL Maximization:__{: style="color: red"}  
    To use MC methods for approximate learning, we need to rewrite the gradient of the partition function $$\nabla_{\boldsymbol{\theta}} \log Z$$ as an expectation of the __unnormalized probability__ $$\tilde{p}$$:  
    <p>$$\nabla_{\boldsymbol{\theta}} \log Z=\mathbb{E}_{\mathbf{x} \sim p(\mathbf{x})} \nabla_{\boldsymbol{\theta}} \log \tilde{p}(\mathbf{x})$$</p>  
    This identity is the basis for a variety of Monte Carlo methods for __approximately maximizing the likelihood__ of models with intractable partition functions.  
    * <button>Derivation - Discrete Variables</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        * Decomposing the gradient of $$\log Z$$:  
            <p>$$\begin{aligned} \nabla_{\boldsymbol{\theta}} \log Z =& \frac{\nabla_{\boldsymbol{\theta}} Z}{Z} \\=& \frac{\nabla_{\boldsymbol{\theta}} \sum_{\mathbf{x}} \tilde{p}(\mathbf{x})}{Z} \\=& \frac{\sum_{\mathbf{x}} \nabla_{\boldsymbol{\theta} \tilde{p}(\mathbf{x})}}{Z} \end{aligned}$$</p>  
        * For models that guarantee $$p(\mathbf{x})>0$$ for all $$\mathbf{x},$$ we can substitute $$\exp (\log \tilde{p}(\mathbf{x}))$$ for $$\tilde{p}(\mathbf{x})$$:  
            <p>$$\begin{aligned} \frac{\sum_{\mathbf{x}} \nabla_{\boldsymbol{\theta}} \exp (\log \tilde{p}(\mathbf{x}))}{Z} &= \frac{\sum_{\mathbf{x}} \exp (\log \tilde{p}(\mathbf{x})) \nabla_{\boldsymbol{\theta}} \log \tilde{p}(\mathbf{x})}{Z} \\ &=\frac{\sum_{\mathbf{x}} \tilde{p}(\mathbf{x}) \nabla_{\boldsymbol{\theta}} \log \tilde{p}(\mathbf{x})}{Z} \\ &=\sum_{\mathbf{x}} p(\mathbf{x}) \nabla_{\boldsymbol{\theta}} \log \tilde{p}(\mathbf{x}) \\ &= \mathbb{E}_{\mathbf{x} \sim p(\mathbf{x})} \nabla_{\boldsymbol{\theta}} \log \tilde{p}(\mathbf{x}) \end{aligned}$$</p>  
        {: hidden=""}

    * <button>Derivation - Continuous Variables</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        * We use __Leibniz's rule for diÔ¨Äerentiation under the integral sign__ to obtain the identity:  
            <p>$$\nabla_{\boldsymbol{\theta}} \int \tilde{p}(\mathbf{x}) d \boldsymbol{x}=\int \nabla_{\boldsymbol{\theta}} \tilde{p}(\mathbf{x}) d \boldsymbol{x}$$</p>  
            * __Applicability - Measure Theory:__  
                This identity is applicable only under certain regularity conditions on $$\tilde{p}$$ and $$\nabla_{\boldsymbol{\theta}} \tilde{p}(\mathbf{x})$$.  
                In measure theoretic terms, the conditions are:  
                1. The unnormalized distribution $$\tilde{p}$$ must be a Lebesgue-integrable function of $$\boldsymbol{x}$$  for every value of $$\boldsymbol{\theta}$$.  
                2. The gradient $$\nabla_{\boldsymbol{\theta}} \tilde{p}(\mathbf{x})$$ must exist for all $$\boldsymbol{\theta}$$ and almost all $$\boldsymbol{x}$$.  
                3. There must exist an integrable function $$R(\boldsymbol{x})$$ that bounds $$\nabla_{\boldsymbol{\theta}} \tilde{p}(\mathbf{x})$$ in the sense that $$\max_{i}\left\vert\frac{\partial}{\partial \theta_{\theta}} \tilde{p}(\mathbf{x})\right\vert \leq R(\boldsymbol{x})$$ for all $$\boldsymbol{\theta}$$ and almost all $$\boldsymbol{x}$$.   

                Fortunately, <span>most machine learning models of interest have these properties.</span>{: style="color: purple"}.  
        {: hidden=""}

    __Intuition:__  
    The Monte Carlo approach to learning provides an intuitive framework in terms of the *__phases__* of the __learning decomposition__:  
    {: #lst-p}
    * __Positive Phase__:  
        In the positive phase, we increase $$\log \tilde{p}(\mathbf{x})$$ for $$\boldsymbol{x}$$ drawn from the data.  
        * __Parametrize $$\log \tilde{p}$$ in terms of an Energy Function__:  
            We interpret the positive phase as <span>__pushing down on the energy__ of training examples</span>{: style="color: purple"}.   
    * __Negative Phase__:  
        In the negative phase, we decrease the partition function by decreasing $$\log \tilde{p}(\mathbf{x})$$ drawn from the model distribution.  
        * __Parametrize $$\log \tilde{p}$$ in terms of an Energy Function__:  
            We interpret the negative phase as <span>__pushing up on the energy__ of samples drawn from the models</span>{: style="color: purple"}.  

    <button>Illustration - Phase Learning</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/HEn1uNFj_rtWwuNKaHdn8DyPvb-SM69ZsL4zwjdXXsU.original.fullsize.png){: width="100%" hidden=""}  
    <br>


4. **Stochastic Maximum Likelihood and Contrastive Divergence:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    To __approximately maximize the log likelihood__, using the identity derived above, we need to use __MCMC__ methods.  

    __Motivation - The Naive Approach:__{: style="color: red"}  
    The naive way to compute the identity above, is to approximate it by __burning in a set of Markov Chains__ from a **_random initialization_** everytime the gradient is needed.  
    When learning is performed using __stochastic gradient descent__, this means <span>the chains must be *__burned in once per gradient step__*</span>{: style="color: purple"}.  
    <button>Naive MCMC Algorithm</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/1pNcMVQ9WFqRWUxnrWlKyHq9GCOpAFUKtQJnxze1MeI.original.fullsize.png){: width="100%" hidden=""}  
    The __high cost of burning in the Markov chains__ in the _inner loop_ makes this procedure __computationally infeasible__.  

    __Learning Intuition (from naive algorithm):__  
    {: #lst-p}
    * We can view the MCMC approach to maximum likelihood as trying to __achieve balance between two forces__:  
        * One <span>__pushing up__ on the model distribution where the data occurs</span>{: style="color: purple"}   
            Corresponds to __maximizing $$\log \tilde{p}$$__.  
        * Another <span>__pushing down__ on the model distribution where the model samples occur</span>{: style="color: purple"}.  
            Corresponds to __minimizing $$\log Z$$__.  
    * There are several __approximations__ to the __negative phase.__  
        Each of these approximations can be understood as making the negative phase __computationally cheaper__ but also making it __push down in the *wrong locations*__.  
    * __Negative Phase Intuition__:  
        * Because the negative phase involves drawing samples from the model‚Äôs distribution, we can think of it as <span>finding points that the model believes in strongly</span>{: style="color: purple"}.  
        * Because the negative phase acts to reduce the probability of those points, they are generally considered to <span>represent the model‚Äôs incorrect beliefs about the world</span>{: style="color: purple"}.  
            Referred to, in literature, as __‚Äúhallucinations‚Äù__ or __‚Äúfantasy particles‚Äù__.  
            * <button>Biological Relation to Dreaming</button>{: .showText value="show" onclick="showTextPopHide(event);"}
                * In fact, the negative phase has been proposed as a possible explanation for dreaming in humans and other animals _(Crick and Mitchison, 1983)_, the idea being that the brain maintains a probabilistic model of the world and follows the gradient of $$\log \tilde{p}$$ when experiencing real events while awake and follows the negative gradient of $$\log \tilde{p}$$ to minimize $$\log Z$$ while sleeping and experiencing events sampled from the current model. This view explains much of the language used to describe algorithms with a positive and a negative phase, but it has not been proved to be correct with neuroscientific experiments.  
                    In machine learning models, it is usually necessary to use the positive and negative phase simultaneously, rather than in separate periods of wakefulness and REM sleep.  
                    As we will see in section 19.5, other machine learning algorithms draw samples from the model distribution for other purposes, and such algorithms could also provide an account for the function of dream sleep.  
                {: hidden=""}

    __Summary:__  
    <div class="borderexample" markdown="1" Style="padding: 0;">
    The main cost of the _naive_ MCMC algorithm is the __cost of burning in the Markov chains from a random initialization at each step__.
    </div>   


    __Contrastive Divergence:__{: style="color: red"}  
    One way to avoid the high cost in Naive MCMC, is to <span>initialize the Markov chains from a distribution that is very close to the model distribution</span>{: style="color: purple"}, so that the burn in operation does not take as many steps.  
    The __Contrastive Divergence (CD)__  (or CD-$$k$$ to indicate CD with $$k$$ Gibbs steps) algorithm initializes the Markov chain at each step with samples from the __data distribution__ _(Hinton, 2000, 2010)_.  
    \- Obtaining samples from the data distribution is _free_, because they are already available in the dataset.  
    \- Initially, the data distribution is not close to the model distribution, so the negative phase is not very accurate.  
    \- Fortunately, the positive phase can still accurately increase the model‚Äôs probability of the data.  
    \- After the positive phase has had some time to act, the model distribution is closer to the data distribution, and the negative phase starts to become accurate.  
    <button>Contrastive Divergence Algorithm</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/Tq22qSUmQjXraJOE0tlRAbIrIZdYn-zv8-3bZUrKCi8.original.fullsize.png){: width="100%" hidden=""}  

    __Drawbacks:__  
    {: #lst-p}
    * __Spurious Modes__:  
        Since CD is still an approximation to the correct negative phase, it results in __spurious modes__; i.e. <span>fails to suppress regions of high probability that are far from actual training examples</span>{: style="color: purple"}.  
        __Spurious Modes:__ are those regions that have _high probability under the model_ but _low probability under the data-generating distribution_.  
        <button>Spurious Modes - Illustration</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](https://cdn.mathpix.com/snip/images/Pcv0tGJ87LYdEOkXI-4Rx37TW7_-WoaeJUj9xjfSZyU.original.fullsize.png){: width="100%" hidden=""}  
        \- Modes in the distribution that are __far from the data distribution__ will *__not be visited__* by Markov chains initialized at _training points_, unless $$k$$ is very large.  
    * __CD as a Biased Estimator in RBMs and Boltzmann Machines:__  
        \- _Carreira-Perpi√±an and Hinton (2005)_ showed experimentally that the CD estimator is *__biased__* for __RBMs__ and __fully visible Boltzmann machines__, in that it _converges to different points than the maximum likelihood estimator_.  
        \- They argue that because the bias is *__small__*, CD could be used as an <span>inexpensive way to initialize a model</span>{: style="color: purple"} that could later be fine-tuned via more expensive MCMC methods.  
        * __Interpretation:__ _Bengio and Delalleau (2009)_ show that CD can be interpreted as <span>discarding the smallest terms of the correct MCMC update gradient</span>{: style="color: purple"}, which _explains the bias_.  
    * __Random Gradients__:  
        _Sutskever and Tieleman (2010)_ showed that the CD <span>update direction is not the gradient of any function</span>{: style="color: purple"}.  
        This allows for situations where CD could cycle forever, but in practice this is not a serious problem.  
    * __Difficulty for Deep Models:__  
        * CD is useful for training __shallow models__ like __RBMs__.  
        * The __RBMs__ can be *__stacked__* to __*initialize* deeper models__ like __DBNs__ or __DBMs__.  
        * However, CD does NOT provide much help for training __deeper models__ _directly_.  
            * This is because it is <span>difficult to obtain samples of the hidden units given samples of the visible units</span>{: style="color: purple"}.  
                * Since the hidden units are __not included in the data__, initializing from training points cannot solve the problem.  
                * Even if we initialize the visible units from the data, we will still need to _burn in a Markov chain_ sampling from the distribution over the hidden units conditioned on those visible samples.  


    __Relation to Autoencoder Training:__  
    \- The CD algorithm can be thought of as <span>penalizing the model for having a Markov chain that *__changes the input rapidly__* when the *__input comes from the data__*</span>{: style="color: purple"}.  
    \- This means training with CD somewhat resembles __autoencoder training__.  
    \- Even though CD is more _biased_ than some of the other training methods, it can be useful for __pretraining shallow models__ that will later be __stacked__.  
    {: #lst-p}
    * This is because the <span>earliest models in the stack are encouraged to __copy more information__ up to their __latent variables__</span>{: style="color: purple"}, thereby <span>making it available to the later models</span>{: style="color: purple"}.  
        This should be thought of more as an often-exploitable *__side effect__* of CD training rather than a principled design advantage.  


    __Stochastic Maximum Likelihood (SML) - Persistent Contrastive Divergence (PCD, PCD-$$k$$):__{: style="color: red"}  
    __SML__ AKA __PCD__ is a method that initializes the Markov Chains, in CD, at each gradient step with their <span>states from the *__previous__* gradient step</span>{: style="color: purple"}.  
    This strategy resolves many of the problems with CD.  
    __Idea:__  
    {: #lst-p}
    * The basic idea of this approach is that, as long as the steps taken by the stochastic gradient algorithm are small, the model from the previous step will be similar to the model from the current step.  
    * It follows that the samples from the previous model‚Äôs distribution will be very close to being fair samples from the current model‚Äôs distribution, so a Markov chain initialized with these samples will not require much time to mix.  
    <button>SML/PCD Algorithm</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/wB9YjPq8Dl4HNJQT-H-G4TBa4h1Uji6nKFe8K6vPtgc.original.fullsize.png){: width="100%" hidden=""}  

    __Advantages:__  
    {: #lst-p}
    * SML is considerably <span>more resistant to forming models with __spurious modes__</span>{: style="color: purple"} than CD is:  
        Because each Markov chain is continually updated throughout the learning process, rather than restarted at each gradient step, the chains are free to wander far enough to find all the model‚Äôs modes.  
    * SML is able to <span>train deep models efficiently</span>{: style="color: purple"}:  
        * SML provides an initialization point for both the *__hidden__* and the __*visible* units__:  
            Because it is possible to store the state of all the sampled variables, whether visible or latent.  
        * CD is only able to provide an initialization for the visible units, and therefore requires burn-in for deep models.  
    * __Performance/Results - In-Practice__:  
        _Marlinet al. (2010)_ compared SML to many other criteria presented in this section. They found that:  
        * SML results in the <span>best test set log-likelihood for an __RBM__</span>{: style="color: purple"}, and that 
        * if the RBM‚Äôs *__hidden units__* are used as __features__ for an __SVM classifier__, SML results in the <span>best classification accuracy</span>{: style="color: purple"}.  


    __Mixing Evaluation:__  
    <button>Discussion</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/_cahz7TXWFoCdoaiMM2HIkj1xdkm-xzvhcWJXQ7woNs.original.fullsize.png){: width="100%" hidden=""}  

    __Sample Evaluation:__  
    <button>Discussion</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    <div hidden="">Care must be taken when evaluating the samples from a model trained with SML. It is necessary to draw the samples starting from a fresh Markov chain initialized from a random starting point after the model is done training. The samples present in the persistent negative chains used for training have been influenced by several recent versions of the model, and thus can make the model appear to have greater capacity than it actually does.</div>


    __Bias-Variance of CD and SML:__{: style="color: red"}  
    _Berglund and Raiko (2013)_ performed experiments to examine the bias and variance in the *__estimate of the gradient__* provided by CD and SML:  
    {: #lst-p}
    * __CD__ proves to have __*lower* variance__  than the estimator based on exact sampling.  
        The cause of CD's low variance is its use of the same training points in both the positive and negative phase.  
        If the negative phase is initialized from different training points, the variance rises above that of the estimator based on exact sampling.   
    * __SML__ has higher variance.  


    __Improving CD & SML:__{: style="color: red"}  
    {: #lst-p}
    * __MCMC Algorithms__:  
        All these methods based on using MCMC to draw samples from the model canin principle be used with almost any variant of MCMC. This means that techniques such as SML can be improved by using any of the enhanced MCMC techniques described in chapter 17, such as parallel tempering _(Desjardins et al., 2010; Choet al., 2010)_.  
    * __Fast PCD (FPCD)__:  
        Another approach to accelerating mixing during learning relies not on changing the Monte Carlo sampling technology but rather on changing the parametrization of the model and the cost function.  
        __FPCD__ is such a method that involves replacing the parameters $$\boldsymbol{\theta}$$ of a traditional model with an expression:  
        <p>$$\boldsymbol{\theta}=\boldsymbol{\theta}^{(\mathrm{slow})}+\boldsymbol{\theta}^{(\mathrm{fast})}$$</p>  
        <button>Discussion</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](https://cdn.mathpix.com/snip/images/LM2-pHOcdnMiHYxNHo5Uee7iwKEy__DYiik76SIhymw.original.fullsize.png){: width="100%" hidden=""}  


    __Training with Positive Phase Estimators (bound-based, variational methods):__{: style="color: red"}  
    One key benefit to the MCMC-based methods described in this section is that they provide an estimate of the gradient of $$\log Z,$$ and thus we can essentially decompose the problem into the $$\log \tilde{p}$$ contribution and the $$\log Z$$ contribution.  
    We can then use any other method to tackle $$\log \tilde{p}(\mathbf{x})$$ and just add our negative phase gradient onto the other method‚Äôs gradient.  
    In particular, this means that our <span>positive phase can make use of methods that provide only a __lower bound on $$\tilde{p}$$__</span>{: style="color: goldenrod"}.  
    Most of the other methods of dealing with $$\log Z$$ presented in this chapter are incompatible with bound-based positive phase methods.  
    <br>


5. **Pseudolikelihood:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    __Motivation:__{: style="color: red"}  
    We can sidestep the issue of approximating the intractable partition function by training the model without computing it at all.  

    __Idea:__{: style="color: red"}  
    Most of these approaches are based on the observation that it is <span>easy to compute *__ratios__* of probabilities in an __undirected model__</span>{: style="color: purple"}.  
    This is because the partition function appears in both the numerator and the denominator of the ratio and cancels out:  
    <p>$$\frac{p(\mathbf{x})}{p(\mathbf{y})}=\frac{\frac{1}{Z} \tilde{p}(\mathbf{x})}{\frac{1}{Z} \tilde{p}(\mathbf{y})}=\frac{\tilde{p}(\mathbf{x})}{\tilde{p}(\mathbf{y})}$$</p>  

    __Pseudolikelihood:__{: style="color: red"}  
    The __Pseudolikelihood__ is an objective function, based on predicting the value of feature $$x _ {i}$$ given all the other features $$\boldsymbol{x}_ {-i}$$:  
    <p>$$\sum_{i=1}^{n} \log p\left(x_{i} \vert \boldsymbol{x}_ {-i}\right)$$</p>  

    __Derivation:__  
    {: #lst-p}
    * The pseudolikelihood is based on the observation that conditional probabilities take this ratio-based form and thus can be computed without knowledge of the partition function.  
    * <button>Derivation</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/u8dif-AW7sUp2ySsRfu3DdAgIe1VSJjept2ielw2PLY.original.fullsize.png){: width="100%" hidden=""}  

    __Computational Cost:__  
    {: #lst-p}
    * If each random variable has $$k$$ different values, this requires only $$k \times n$$ evaluations of $$\tilde{p}$$ to compute,  
    * as opposed to the $$k^{n}$$ evaluations needed to compute the partition function.  

    __Justification:__  
    Estimation by maximizing the pseudolikelihood is __asymptotically consistent__ _(Mase, 1995)_.  
    When the datasets do not approach the large sample limit, pseudolikelihood may display different behavior from the maximum likelihood estimator.  

    __Generalized Pseudolikelihood Estimator:__{: style="color: red"}  
    The __Generalized Pseudolikelihood Estimator__ gives us a way to trade-off computational complexity for deviation from maximum likelihood behavior.  
    The GPE objective function:  
    <p>$$\sum_{i=1}^{m} \log p\left(\mathbf{x}_{\mathbb{S}^{(i)}} \vert \mathbf{x}_{-\mathbb{S}^{(i)}}\right)$$</p>    
    __Complexity-Consistency Tradeoff:__  
    It uses $$m$$ different sets $$\mathbb{S}^{(i)}, i=1, \ldots, m$$ of indices of variables that appear together on the left side of the conditioning bar:  
    {: #lst-p}
    * In the extreme case of $$m=1$$ and $$\mathbb{S}^{(1)}=1, \ldots, n,$$ the generalized pseudolikelihood <span>recovers the log-likelihood</span>{: style="color: purple"}.  
    * In the extreme case of $$m=n$$ and $$\mathbb{S}^{(i)}=\{i\},$$ the generalized pseudolikelihood <span>recovers the pseudolikelihood</span>{: style="color: purple"}.  


    __Performance:__{: style="color: red"}  
    The performance of pseudolikelihood-based approaches depends largely on how the model will be used:  
    {: #lst-p}
    * Pseudolikelihood tends to perform poorly on tasks that require a good model of the full joint $$p(\mathbf{x}),$$ such as density estimation and sampling. 
    * It can perform better than maximum likelihood for tasks that require only the conditional distributions used during training, such as filling in small amounts of missing values.  
    * Generalized pseudolikelihood techniques are especially powerful if the data has regular structure that allows the $$\mathbb{S}$$ index sets to be designed to capture the most important correlations while leaving out groups of variables that have only negligible correlation.  
        For example, in natural images, pixels that are widely separated in space also have weak correlation, so the generalized pseudolikelihood can be applied with each $$\mathbb{S}$$ set being a small, spatially localized window.  

    __Drawbacks - Training with Lower-Bound Maximization Methods:__{: style="color: red"}  
    {: #lst-p}
    * One weakness of the pseudolikelihood estimator is that it cannot be used with other approximations that provide only a lower bound on $$\tilde{p}(\mathbf{x}),$$ e.g. __variational inference__.  
        * This is because $$\tilde{p}$$ appears in the denominator.  
            A lower bound on the denominator provides only an upper bound on the expression as a whole, and there is no benefit to maximizing an upper bound.  
            This makes it difficult to apply pseudolikelihood approaches to deep models such as __deep Boltzmann machines__, since variational methods are one of the dominant approaches to approximately marginalizing out the many layers of hidden variables that interact with each other.  
    * Nonetheless, pseudolikelihood is still useful for deep learning, because it can be used to train single-layer models or deep models using approximate inference methods that are not based on lower bounds.  

    __Pseudolikelihood vs SML/PCD - Computational Cost:__  
    Pseudolikelihood has a much greater cost per gradient step than SML, due to its explicit computation of all the conditionals.  
    But generalized pseudolikelihood and similar criteria can still perform well if only one randomly selected conditional is computed per example _(Goodfellow et al., 2013b)_, thereby bringing the computational cost down to match that of SML.  


    __Relation to the Negative Phase:__{: style="color: red"}  
    Though the pseudolikelihood estimator does not explicitly minimize $$\log Z$$, it can still be thought of as having something resembling a negative phase.  
    The denominators of each conditional distribution result in the learning algorithm suppressing the probability of all states that have only one variable differing from a training example.  

    __Asymptotic Efficiency:__{: style="color: red"}  
    See _Marlin and de Freitas (2011)_ for a theoretical analysis of the asymptotic efficiency of pseudolikelihood.  
    <br>

6. **Score-Matching and Ratio-Matching:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    <button>PDF</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    <iframe hidden="" src="/main_files/pdf/score-ratio_matching.pdf" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>

    __Denoising Score Matching:__{: style="color: red"}  
    <button>Denoising Score Matching - Discussion</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/3yUoSuXqxsiW13WrbGbRNpvujyqSAhz5af0MLcXZrX8.original.fullsize.png){: width="100%" hidden=""}  
    <br>

7. **Noise-Contrastive Estimation (NCE):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    __Noise-Contrastive Estimation (NCE)__ is a method for computing the partition function as a learned parameter in the model; where the probability distribution estimated by the model is represented __explicitly__ as:  
    <p>$$\log p_{\text {model }}(\mathbf{x})=\log \tilde{p}_ {\text {model}}(\mathbf{x} ; \boldsymbol{\theta})+c$$</p>  
    where $$c$$ is explicitly introduced as an approximation of $$-\log Z(\boldsymbol{\theta})$$.  

    Rather than estimating only $$\boldsymbol{\theta}$$, the noise contrastive estimation procedure treats $$c$$ as just another parameter and estimates $$\boldsymbol{\theta}$$ and $$c$$ simultaneously, using the same algorithm for both.  
    The resulting $$\log p_{\text {model}}(\mathbf{x})$$ thus may not correspond exactly to a valid probability distribution, but it will become closer and closer to being valid as the estimate of $$c$$ improves.[^1]  

    __Derivation:__{: style="color: red"}  
    {: #lst-p}
    * __Problem with Maximum Likelihood Criterion:__{: style="color: red"}  
        Such an approach would not be possible using maximum likelihood as the criterion for the estimator.  
        The maximum likelihood criterion would choose to set $$c$$ arbitrarily high, rather than setting $$c$$ to create a valid probability distribution.  
    * __Solution - New Estimator of the original problem:__{: style="color: red"}  
        NCE works by reducing the unsupervised learning problem of estimating $$p(\mathrm{x})$$ to that of learning a probabilistic binary classifier in which one of the categories corresponds to the data generated by the model.  
        This supervised learning problem is constructed in such a way that maximum likelihood estimation defines an *__asymptotically consistent__* estimator of the original problem.  

        Specifically,  
        1. <span>Posit two distributions:</span>{: style="color: goldenrod"} the __model__, and a __noise distribution__.
            * The __Noise Distribution $$p_{\text{noise}}(\mathbf{x})$$:__{: style="color: red"}  
                We introduce a new distribution $$p_{\text{noise}}(\mathbf{x})$$ over the noise.  
                The noise distribution should be __tractable to evaluate and to sample from__.    
        2. <span>Construct a new *__joint model__* over both $$\boldsymbol{x}$$ and a __*binary* variable__ $$y$$</span>{: style="color: goldenrod"}:  
            * We can now construct a model over both $$\mathbf{x}$$ and a new, binary class variable $$y$$. In the new joint model, we specify that  
                (1) $$p_{\mathrm{joint}}(y=1)=\frac{1}{2}$$  
                (2) $$p_{\mathrm{joint}}(\mathbf{x} \vert y=1)=p_{\mathrm{model}}(\mathbf{x})$$  
                (3) $$p_{\mathrm{joint}}(\mathbf{x} \vert y=0)=p_{\mathrm{noise}}(\mathbf{x})$$  
                In other words, $$y$$ is a __switch variable__ that <span>determines whether we will __generate__ $$\mathbf{x}$$ from the *__model__* or from the *__noise distribution__*</span>{: style="color: purple"}.  
            * Equivalently, We can construct a similar __joint model of *training data*__.  
                Formally, 
                (1) $$p_{\text {train}}(y=1)=\frac{1}{2}$$  
                (2) $$p_{\text {train}}(\mathbf{x} \vert y=1)=p_{\text {data }}(\mathbf{x}),$$  
                (3) $$p_{\text {train}}(\mathbf{x} \vert y=0)=p_{\text {noise}}(\mathbf{x})$$  
                In this case, the __switch variable__ <span>determines whether we draw $$\mathbf{x}$$ from the *__data__* or from the *__noise distribution__*</span>{: style="color: purple"}.  
        3. <span>Construct the new supervised Binary Classification Task</span>{: style="color: goldenrod"} - __fitting $$p_{\text {joint}}$$ to $$p_{\text {train}}$$__:  
            We can now just use standard maximum likelihood learning on the supervised learning problem of fitting $$p_{\text {joint}}$$ to $$p_{\text {train}}$$, by swapping $$p_{\text {model}}$$ with $$p_{\text {joint}}$$:  
            <p>$$\boldsymbol{\theta}, c=\underset{\boldsymbol{\theta}, c}{\arg \max } \mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p_{\text {train}}} \log p_{\text {joint}}(y \vert \mathbf{x})$$</p>  
            * __Expanding $$p_{\text{joint}}(y \vert x)$$:__  
                The distribution $$p_{\text{joint}}$$ is essentially a *__logistic regression__* model applied to the difference in log probabilities of the model and the noise distribution:  
                <p>$$\begin{aligned}  
                    p_{\text {joint}}(y=1 \vert \mathbf{x}) &= \frac{p_{\text {model }}(\mathbf{x})}{p_{\text {model }}(\mathbf{x})+p_{\text {noise}}(\mathbf{x})} \\
                    &= \frac{1}{1+\frac{p_{\text {noise}}(\mathbf{x})}{p_{\text {model}} (\mathbf{x})}}  \\
                    &= \frac{1}{1+\exp \left(\log \frac{p_{\text {noise}}(\mathbf{x})}{p_{\text {model }}(\mathbf{x})}\right)} \\
                    &= \sigma\left(-\log \frac{p_{\text {noise}}(\mathbf{x})}{p_{\text {model }}(\mathbf{x})}\right) \\
                    &= \sigma\left(\log p_{\text {model }}(\mathbf{x})-\log p_{\text {noise}}(\mathbf{x})\right) 
                    \end{aligned}$$</p>     
    * <button>Different Derivation</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](https://cdn.mathpix.com/snip/images/xhM0Q_ZbT3N-zobKCCyKe2mFA3OAyDatXGYFkok0H3Q.original.fullsize.png){: width="100%" hidden=""}  

    __Summary:__  
    {: #lst-p}
    1. <span>Posit two distributions:</span>{: style="color: goldenrod"} the __model__, and a __noise distribution__.
    2. Given a data point, <span>predict from which distribution this point was generated</span>{: style="color: goldenrod"}.  

    NCE is thus simple to apply as long as  $$ \log \tilde{p}_{\text {model}}$$ is easy to back-propagate through, and, as specified above, $$p_{\text {noise}}$$ is easy to evaluate (in order to evaluate $$p_{\text {joint}}$$ and sample from (to generate the training data).  

    __The Noise Distribution:__{: style="color: red"}  
    {: #lst-p}
    * __Practical Implications and Complexity__:  
    * __Better Distributions - Parametric $$p_{\text{noise}}$$__:  
        The noise distribution is generally __non-parametric__.  
        However, there is nothing stopping us from evolving this distribution and giving it trainable parameters, then updating these parameters such that it generates increasingly __"optimal"__ samples.  
        * __Optimality__:  
            Of course, we would have to design what __"optimal"__ means.  
            * __Adversarial Contrastive Estimation__:  
                One interesting approach is called [Adversarial Contrastive Estimation](https://arxiv.org/abs/1805.03642), wherein the authors adapt the noise distribution to generate increasingly "harder negative examples, which forces the main model to learn a better representation of the data.  


    __Weaknesses/Drawbacks:__{: style="color: red"}  
    {: #lst-p}
    * __Problems with Many RVs:__  
        When NCE is applied to problems with many random variables, it becomes __less efficient__.  
        * The logistic regression classifier can reject a noise sample by identifying any one variable whose value is unlikely.  
            This means that learning slows down greatly after $$p_{\text {model}}$$ has learned the basic marginal statistics.  
        * Imagine learning a model of images of faces, using unstructured Gaussian noise as $$p_{\text {noise}}$$.  
            If $$p_{\text {model }}$$ learns about eyes, it can reject almost all unstructured noise samples without having learned anything about other facial features, such as mouths.  
    * __Noise Distribution Complexity__:  
        The constraint that $$p_{\text {noise}}$$ must be easy to evaluate and easy to sample from can be overly restrictive:  
        * For our __training data__, we <span>require the ability to sample from our noise distribution.</span>{: style="color: purple"}.  
        * For our __target__, we <span>require the ability to compute the likelihood of some data under our noise distribution</span>{: style="color: purple"}.  

        When $$p_{\text {noise}}$$ is simple, most samples are likely to be too obviously distinct from the data to force $$p_{\text {model}}$$ to improve noticeably.  
    * __Training with Lower-Bound Maximizing Methods__:  
        NCE does not work if only a lower bound on $$\tilde{p}$$ is available.  
        Such a lower bound could be used to construct a lower bound on $$p_{\text {joint}}(y=1 \vert \mathbf{x}),$$ but it can only be used to construct an upper bound on $$p_{\text {joint}}(y=0 \vert \mathbf{x}),$$ which appears in half the terms of the NCE objective.  
        Likewise, a lower bound on $$p_{\text {noise}}$$ is not useful, because it provides only an upper bound on $$p_{\text {joint}}(y=1 \vert \mathbf{x})$$.  


    __Self-Contrastive Estimation:__{: style="color: red"}  
    When the model distribution is copied to define a new noise distribution before each gradient step, NCE defines a procedure called __self-contrastive estimation__, whose <span>expected gradient is equivalent to the expected gradient of maximum likelihood</span>{: style="color: purple"} _(Goodfellow, 2014)_.  
    __Interpretation:__  
    {: #lst-p}
    * __Self-Contrastive Estimation__:  
        The special case of NCE where the noise samples are those generated by the model suggests that maximum likelihood can be interpreted as a <span>procedure that forces a model to constantly learn to __distinguish__ *__reality__* from its __own evolving *beliefs*__</span>{: style="color: goldenrod"},   
    * __NCE__:  
        However, NCE achieves some __reduced computational cost__ by <span>only forcing the model to __distinguish__ *__reality__* from a *__fixed baseline (noise model)__*</span>{: style="color: goldenrod"}.  


    __Connection to Importance Sampling:__{: style="color: red"}  
    _Jozefowicz et al. (2016)_ show that NCE and IS are not only similar as both are sampling-based approaches, but are strongly connected.  
    While NCE uses a binary classification task, they show that IS can be described similarly using a __surrogate loss function__: Instead of performing binary classification with a logistic loss function like NCE, IS then optimises a multi-class classification problem with a softmax and cross-entropy loss function.  
    They observe that as IS performs *__multi-class classification__*, it may be a better choice for __language modeling__, as the loss leads to <span>tied updates between the data and noise samples</span>{: style="color: purple"} rather than <span>independent updates</span>{: style="color: purple"} as with NCE.  
    Indeed, Jozefowicz et al. (2016) use IS for language modeling and obtain state-of-the-art performance on the 1B Word benchmark.  


    __Relation to Generative Adversarial Networks (GANs):__{: style="color: red"}  
    Noise contrastive estimation is based on the idea that a <span>good generative model should be able to __distinguish data from noise__</span>{: style="color: purple"}.  
    A closely related idea is that a <span>good generative model should be able to __generate samples that no classifier can distinguish from data__</span>{: style="color: purple"}.  
    This idea yields generative adversarial networks.  

    __Self-Normalization:__{: style="color: red"}  
    _Mnih and Teh (2012)_ and _Vaswani et al. (2013)_ fix $$c = 1$$.  
    They report does not affect the model's performance.  
    This assumption has the nice side-effect of __reducing the model's parameters__, while ensuring that the model *__self-normalises__* by not depending on the explicit normalisation in $$c$$.  
    Indeed, _Zoph et al. (2016)_ find that even when learned, $$c$$ is close to $$1$$ and has low variance.  
    <br>

8. **Negative Sampling:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    __Negative Sampling (NEG)__ can be seen as an approximation to __NCE__.  
    As we have mentioned above, NCE can be shown to approximate the loss of $$\log p_{\text{model}}$$ as the number of samples $$k$$ increase.  
    NEG simplifies NCE and does away with this guarantee, as the objective of NEG is to learn high-quality word representations rather than achieving low perplexity on a test set, as is the goal in language modeling.  

    The key difference to NCE is that NEG only approximates this probability by making it as easy to compute as possible.  
    It simplifies NCE as follows:  
    {: #lst-p}
    1. Considers noise distributions whose likelihood we cannot evaluate  
    2. To accommodate, it simply set the most expensive term $$p_{\text {noise}}(x)=1$$  
        Equivalently, $$k\:p_{\text {noise}}(x)=1$$  
        * <button>Derivation - Discrete Variables</button>{: .showText value="show" onclick="showTextPopHide(event);"}  
            * Thus, __Expanding $$p_{\text{joint}}(y \vert x)$$:__  
                The distribution $$p_{\text{joint}}$$ is essentially a *__logistic regression__* model applied to the difference in log probabilities of the model and the noise distribution:  
                <p>$$\begin{aligned}  
                    p_{\text {joint}}(y=1 \vert \mathbf{x}) &= \frac{p_{\text {model }}(\mathbf{x})}{p_{\text {model }}(\mathbf{x})+p_{\text {noise}}(\mathbf{x})} \\
                    &= \frac{1}{1+\frac{p_{\text {noise}}(\mathbf{x})}{p_{\text {model}} (\mathbf{x})}}  \\
                    &= \frac{1}{1+\exp \left(\log \frac{p_{\text {noise}}(\mathbf{x})}{p_{\text {model }}(\mathbf{x})}\right)} \\
                    &= \sigma\left(-\log \frac{p_{\text {noise}}(\mathbf{x})}{p_{\text {model }}(\mathbf{x})}\right) \\
                    &= \sigma\left(-\log \frac{1}{p_{\text {model}}(\mathbf{x})}\right) \\
                    &= \sigma\left(\log p_{\text {model}}(\mathbf{x})\right) 
                    \end{aligned}$$</p>  
            {: hidden=""}  

    __Equivalence with NCE:__{: style="color: red"}  
    {: #lst-p}
    * $$k p_{\text {noise}}=1$$ is exactly then true, when (discrete):  
        1. $$k=\vert X\vert$$ and  
        2. $$p_{\text {noise}}$$ is a __*uniform* distribution__.  
        
        In this case, NEG is equivalent to NCE.  
    * The reason we set $$k p_{\text {noise}}=1$$ and not to some other constant can be seen by rewriting the equation, as $$P(y=1 \vert  \mathbf{x})$$ can simplify the sigmoid function.  
    * In all other cases, NEG only approximates NCE, which means that it will <span>not directly optimize the likelihood $$\log p_{\text {model}}(\mathbf{x})$$</span>{: style="color: purple"}.  
    * __Asymptotic Consistency:__  
        Since NEG only approximates NCE, it lacks any asymptotic consistency guarantees.  

    __Application - Language Modeling and Word Embeddings:__  
    NEG only approximates NCE, which means that it will not directly optimise the likelihood of correct words, which is key for language modelling. While NEG may thus be useful for learning word embeddings, its lack of asymptotic consistency guarantees makes it inappropriate for language modelling.  
    <br>

9. **Self-Normalization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents19}  
    Remember from NCE that we decomposed the log likelihood of the model as:  
    <p>$$\log p_{\text {model }}(\mathbf{x})=\log \tilde{p}_ {\text {model}}(\mathbf{x} ; \boldsymbol{\theta})+c$$</p>  
    where $$c$$ is explicitly introduced as an approximation of $$-\log Z(\boldsymbol{\theta})$$.  

    If we are able to constrain our model so that it sets $$c=0$$ (i.e. $$e^c = 1$$), then we can avoid computing the normalization in $$c$$ altogether.  
    _Devlin et al. (2014)_ thus propose to add a __squared error penalty__ term to the loss function that encourages the model to <span>keep $$c$$ as close as possible to $$0$$</span>{: style="color: purple"}:  
    <p>$$\tilde{J} = J + \lambda (c-0)^{2}$$</p>  
    where $$\lambda$$ allows us to trade-off between model accuracy and mean self-normalisation.  

    At inference time, we set  
    <p>$$p_{\text {model }}(\mathbf{x})=\dfrac{\tilde{p}_ {\text {model}}(\mathbf{x} ; \boldsymbol{\theta})}{Z(\boldsymbol{\theta})} \approx \dfrac{\tilde{p}_ {\text {model}}(\mathbf{x} ; \boldsymbol{\theta})}{1} = \tilde{p}_ {\text {model}}(\mathbf{x} ; \boldsymbol{\theta})$$</p>  

    __Results - MT:__{: style="color: red"}  
    They report that self-normalisation achieves a speed-up factor of about 15, while only resulting in a small degradation of BLEU scores compared to a regular non-self-normalizing neural language model.  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * [Paper repro: ‚ÄúSelf-Normalizing Neural Networks‚Äù (Blog - Code?)](https://becominghuman.ai/paper-repro-self-normalizing-neural-networks-84d7df676902)  
    <br>


***


***

## Estimating the Partition Function
{: #content3}

1. **Estimating the Partition Function:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    <button>PDF</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    <iframe hidden="" src="/main_files/pdf/approx_part.pdf" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>

2. **Annealed Importance Sampling (AIS):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    <button>PDF</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    <iframe hidden="" src="/main_files/pdf/ais.pdf" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>

3. **Bridge Sampling:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    <button>PDF</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    <iframe hidden="" src="/main_files/pdf/bs.pdf" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>

4. **Linked Importance Sampling (LIS):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    <button>PDF</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    <iframe hidden="" src="/main_files/pdf/lis.pdf" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>

5. **Estimating the Partition Function while Training:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}  
    <button>PDF</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    <iframe hidden="" src="/main_files/pdf/est_training.pdf" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>


[^1]: NCE is also applicable to problems with a tractable partition function, where there is no need to introduce the extra parameter $$c$$. However, it has generated the most interest as a means of estimating models with difficult partition functions.  

***
***

TITLE: TensorFlow 
LINK: research/dl/concepts/var_calc.md


## Calculus of Variations
{: #content1}

1. **Calculus of Variations:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  


22. **Functional:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents122}  
    A __Functional__ 
    <br>

2. **Functional Derivative:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    The __Functional Derivative__ relates a change in a functional to a change in a function on which the functional depends.  
    In an integral $$L$$ of a functional, if a function $$f$$ is varied by adding to it another function $$\delta f$$ that is arbitrarily small, and the resulting integrand is expanded in powers of $$\delta f,$$ the coefficient of $$\delta$$ in the first order term is called the __functional derivative__.  
    Consider the __functional__  
    <p>$$J[f]=\int_{a}^{b} L\left[x, f(x), f^{\prime}(x)\right] d x$$</p>  
    where $$f^{\prime}(x) \equiv d f / d x .$$ If is varied by adding to it a function $$\delta f,$$ and the resulting integrand $$L\left(x, f+\delta f, f^{\prime}+\delta f^{\prime}\right)$$ is expanded in  powers of  $$\delta f$$, then the change in the value of $$ J$$ to first order in $$\delta f$$ can be expressed as follows:  
    <p>$$\delta J=\int_{a}^{b} \frac{\delta J}{\delta f(x)} \delta f(x) d x$$</p>  
    The coefficient of $$\delta f(x),$$ denoted as $$\delta J / \delta f(x),$$ is called the functional derivative of $$J$$ with respect to $$f$$ at the point $$x$$.  
    The functional derivative is the left hand side of the __Euler-Lagrange equation__:  
    <p>$$\frac{\delta J}{\delta f(x)}=\frac{\partial L}{\partial f}-\frac{d}{d x} \frac{\partial L}{\partial f^{\prime}}$$</p>  


    __Formal Description:__{: style="color: red"}  
    The definition of a functional derivative may be made more mathematically precise and formal by defining the space of functions more carefully:  
    {: #lst-p}
    * __Banach Space:__ the functional derivative is the <span>Fr√©chet derivative</span>{: style="color: goldenrod"}  
    * __Hilbert Space:__ (Hilbert is special case of Banach) <span>Fr√©chet derivative</span>{: style="color: goldenrod"}  
    * __General *Locally Convex* Spaces__: the functional derivative is the <span>Gateaux derivative</span>{: style="color: goldenrod"}  


    __Properties:__{: style="color: red"}  
    {: #lst-p}
    * __Linearity__:  
        <p>$$\frac{\delta(\lambda F+\mu G)[\rho]}{\delta \rho(x)}=\lambda \frac{\delta F[\rho]}{\delta \rho(x)}+\mu \frac{\delta G[\rho]}{\delta \rho(x)}$$</p>  
        where $$\lambda, \mu$$ are constants.  
    * __Product Rule__:  
        <p>$$\frac{\delta(F G)[\rho]}{\delta \rho(x)}=\frac{\delta F[\rho]}{\delta \rho(x)} G[\rho]+F[\rho] \frac{\delta G[\rho]}{\delta \rho(x)}$$</p>  
    * __Chain Rule__:  
        * If $$F$$ is a functional and $$G$$ another functional:  
            <p>$$\frac{\delta F[G[\rho]]}{\delta \rho(y)}=\int d x \frac{\delta F[G]}{\delta G(x)}_ {G=G[\rho]} \cdot \frac{\delta G[\rho](x)}{\delta \rho(y)}$$</p>  
        * If $$G$$ is an ordinary differentiable function (local functional) $$g,$$ then this reduces to:  
            <p>$$\frac{\delta F[g(\rho)]}{\delta \rho(y)}=\frac{\delta F[g(\rho)]}{\delta g[\rho(y)]} \frac{d g(\rho)}{d \rho(y)}$$</p>  

    __Formula for Determining the Functional Derivative:__{: style="color: red"}  
    We present a formula to determine functional derivatives for a common class of functionals that can be written as the integral of a function and its derivatives:  
    Given a functional $$F[\rho]=\int f(\boldsymbol{r}, \rho(\boldsymbol{r}), \nabla \rho(\boldsymbol{r})) d \boldsymbol{r}$$ and a function $$\phi(\boldsymbol{r})$$ that vanishes on the boundary of the region of integration, the functional derivative is:  
    <p>$$\frac{\delta F}{\delta \rho(\boldsymbol{r})}=\frac{\partial f}{\partial \rho}-\nabla \cdot \frac{\partial f}{\partial \nabla \rho}$$</p>  
    where $$\rho=\rho(\boldsymbol{r})$$ and $$f=f(\boldsymbol{r}, \rho, \nabla \rho)$$.  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * [Functional Derivative (wiki)](https://en.wikipedia.org/wiki/Functional_derivative)  
    <br>

3. **Euler Lagrange Equation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    
    __Generalization to Manifolds:__{: style="color: red"}  
    <button>Manifold Equations</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/T9qXN12UsjttUJnt3pNYnGOTiTca4dXex-BD2vLpRLk.original.fullsize.png){: width="100%" hidden=""}  


    __Beltrami Identity:__{: style="color: red"}  
    __Beltrami Identity__ is a special case of the Euler Lagrange Equation where $$\partial L / \partial x=0$$, defined as:  
    <p>$$L-f^{\prime} \frac{\partial L}{\partial f^{\prime}}=C$$</p>  
    where $$C$$ is a constant.  

    It is applied to many problems where the condition is satisfied like the __Brachistochrone problem__.  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * [Functional Derivative (wiki)](https://en.wikipedia.org/wiki/Functional_derivative)  
    <br>




***
***

TITLE: Loss Functions
LINK: research/dl/concepts/loss_funcs.md


[Loss Functions (blog)](https://isaacchanghau.github.io/post/loss_functions/)  
[Information Theory (Cross-Entropy and MLE, MSE, Nash, etc.)](https://jhui.github.io/2017/01/05/Deep-learning-Information-theory/)  


# Loss Functions
{: #content1}


### **Loss Functions**{: style="color: SteelBlue; font-size: 1.27em"}{: .bodyContents1 #bodyContents11}    
Abstractly, a __loss function__ or __cost function__ is a function that maps an event or values of one or more variables onto a real number, intuitively, representing some "cost" associated with the event.  

Formally, a __loss function__ is a function $$L :(\hat{y}, y) \in \mathbb{R} \times Y \longmapsto L(\hat{y}, y) \in \mathbb{R}$$  that takes as inputs the predicted value $$\hat{y}$$ corresponding to the real data value $$y$$ and outputs how different they are.  

***

# Loss Functions for Regression
{: #content2}

![img](/main_files/dl/concepts/loss_funcs/6.png){: width="61%"}  
<br>

### **Introduction**{: style="color: SteelBlue; font-size: 1.27em"}{: .bodyContents2 #bodyContents211}    
Regression Losses usually only depend on the __residual__ $$r = y - \hat{y}$$ (i.e. what you have to add to your prediction to match the target)  

__Distance-Based Loss Functions:__{: style="color: red"}  
A Loss function $$L(\hat{y}, y)$$ is called __distance-based__ if it:  
* Only depends on the __residual__:  
    <p>$$L(\hat{y}, y) = \psi(y-\hat{y})  \:\: \text{for some } \psi : \mathbb{R} \longmapsto \mathbb{R}$$</p>  
* Loss is $$0$$ when residual is $$0$$:  
    <p>$$\psi(0) = 0$$</p>  

__Translation Invariance:__{: style="color: red"}  
Distance-based losses are translation-invariant:  
<p>$$L(\hat{y}+a, y+a) = L(\hat{y}, y)$$</p>  

> Sometimes __Relative-Error__ $$\dfrac{\hat{y}-y}{y}$$ is a more _natural_ loss but it is NOT translation-invariant  


<br>

### **MSE**{: style="color: SteelBlue; font-size: 1.27em"}{: .bodyContents2 #bodyContents21}    
The __MSE__ minimizes the sum of *__squared differences__* between the predicted values and the target values.  
<p>$$L(\hat{y}, y) = \dfrac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{y}_ {i}\right)^{2}$$</p>  
![img](/main_files/dl/concepts/loss_funcs/1.png){: width="30%" .center-image}  

<button>Derivation</button>{: .showText value="show"
onclick="showTextPopHide(event);"}
![img](/main_files/dl/concepts/loss_funcs/5.png){: width="100%" hidden=""}  
<br>

### **MAE**{: style="color: SteelBlue; font-size: 1.27em"}{: .bodyContents2 #bodyContents22}    
The __MAE__ minimizes the sum of *__absolute differences__* between the predicted values and the target values.  
<p>$$L(\hat{y}, y) = \dfrac{1}{n} \sum_{i=1}^{n}\vert y_{i}-\hat{y}_ {i}\vert$$</p>  

__Properties:__{: style="color: red"}  
* Solution may be __Non-unique__  
* __Robustness__ to outliers  
* __Unstable Solutions:__{: #bodyContents22stability}    
    <button>Explanation</button>{: .showText value="show"
    onclick="showTextPopHide(event);"}
    _The instability property of the method of least absolute deviations means that, for a small horizontal adjustment of a datum, the regression line may jump a large amount. The method has continuous solutions for some data configurations; however, by moving a datum a small amount, one could ‚Äújump past‚Äù a configuration which has multiple solutions that span a region. After passing this region of solutions, the least absolute deviations line has a slope that may differ greatly from that of the previous line. In contrast, the least squares solutions is stable in that, for any small adjustment of a data point, the regression line will always move only slightly; that is, the regression parameters are continuous functions of the data._{: hidden=""}  
* __Data-points "Latching"[^3]:__  
    * __Unique Solution__:  
        If there are $$k$$ *__features__* (including the constant), then at least one optimal regression surface will pass through $$k$$ of the *__data points__*; unless there are multiple solutions.  
    * __Multiple Solutions__:  
        The region of valid least absolute deviations solutions will be __bounded by at least $$k$$ lines__, each of which __passes through at least $$k$$ data points__.  
    > [Wikipedia](https://en.wikipedia.org/wiki/Least_absolute_deviations#Other_properties)  
* 
<br>

### **Huber Loss**{: style="color: SteelBlue; font-size: 1.27em"}{: .bodyContents2 #bodyContents23}    

AKA: __Smooth Mean Absolute Error__  
<p>$$L(\hat{y}, y) = \left\{\begin{array}{cc}{\frac{1}{2}(y-\hat{y})^{2}} & {\text { if }|(y-\hat{y})|<\delta} \\ {\delta(y-\hat{y})-\frac{1}{2} \delta} & {\text { otherwise }}\end{array}\right.$$</p>  

__Properties:__{: style="color: red"}  
* It‚Äôs __less sensitive__{: style="color: green"} to outliers than the *MSE* as it treats error as square only inside an interval.  

__Code:__{: style="color: red"}  
```python
def Huber(yHat, y, delta=1.):
    return np.where(np.abs(y-yHat) < delta,.5*(y-yHat)**2 , delta*(np.abs(y-yHat)-0.5*delta))
```
<br>

### **KL-Divergence**{: style="color: SteelBlue; font-size: 1.27em"}{: .bodyContents2 #bodyContents25}  


<p>$$L(\hat{y}, y) = $$</p>  

<br>

### **Analysis and Discussion**{: style="color: SteelBlue; font-size: 1.27em"}{: .bodyContents2 #bodyContents26}  
__MSE vs MAE:__{: style="color: red"}  

| __MSE__ | __MAE__ |
| Sensitive to _outliers_ | Robust to _outliers_ |
| Differentiable Everywhere | Non-Differentiable at $$0$$ |
| Stable[^1] Solutions | Unstable Solutions |
| Unique Solution | Possibly multiple[^2] solutions |

* __Statistical Efficiency__:  
    * "For normal observations MSE is about $$12\%$$ more efficient than MAE" - Fisher  
    * $$1\%$$ Error is enough to make MAE more efficient  
    * 2/1000 bad observations, make the median more efficient than the mean  
* Subgradient methods are slower than gradient descent  
    * you get a lot better convergence rate guarantees for MSE  




<br>

### **Notes**{: style="color: SteelBlue; font-size: 1.27em"}{: .bodyContents2 #bodyContents27}  




***

# Loss Functions for Classification
{: #content3}

![img](/main_files/dl/concepts/loss_funcs/0.png){: width="65%" #losses}  
<br>

### **$$0-1$$ Loss**{: style="color: SteelBlue; font-size: 1.27em"}{: .bodyContents3 #bodyContents311}    

<p>$$L(\hat{y}, y) = I(\hat{y} \neq y) = \left\{\begin{array}{ll}{0} & {\hat{y}=y} \\ {1} & {\hat{y} \neq y}\end{array}\right.$$</p>  
<br>

### **MSE**{: style="color: SteelBlue; font-size: 1.27em"}{: .bodyContents3 #bodyContents31}    

We can write the loss in terms of the margin $$m = y\hat{y}$$:  
$$L(\hat{y}, y)=(y - \hat{y})^{2}=(1-y\hat{y})^{2}=(1-m)^{2}$$   
> Since $$y \in {-1,1} \implies y^2 = 1$$  


<p>$$L(\hat{y}, y) = (1-y \hat{y})^{2}$$</p>  


![img](/main_files/dl/concepts/loss_funcs/1.png){: width="30%" .center-image}  

<button>Derivation</button>{: .showText value="show"
onclick="showTextPopHide(event);"}
![img](/main_files/dl/concepts/loss_funcs/5.png){: width="100%" hidden=""}  

<br>

### **Hinge Loss**{: style="color: SteelBlue; font-size: 1.27em"}{: .bodyContents3 #bodyContents32}    

<p>$$L(\hat{y}, y) = \max (0,1-y \hat{y})=|1-y \hat{y}|_ {+}$$</p>  

__Properties:__{: style="color: red"}  
* Continuous, Convex, Non-Differentiable  

![img](/main_files/dl/concepts/loss_funcs/3.png){: width="30%" .center-image}  
<br>

### **Logistic Loss**{: style="color: SteelBlue; font-size: 1.27em"}{: .bodyContents3 #bodyContents33}  

AKA: __Log-Loss__, __Logarithmic Loss__  

<p>$$L(\hat{y}, y) = \log{\left(1+e^{-y \hat{y}}\right)}$$</p>  

![img](/main_files/dl/concepts/loss_funcs/2.png){: width="30%" .center-image}  

__Properties:__{: style="color: red"}  
{: #lst-p}
* The logistic loss function does not assign zero penalty to any points. Instead, functions that correctly classify points with high confidence (i.e., with high values of $${\displaystyle \vert f({\vec {x}})\vert}$$) are penalized less. This structure leads the logistic loss function to be sensitive to outliers in the data.  
<br>


### **Cross-Entropy (Log Loss)**{: style="color: SteelBlue; font-size: 1.27em"}{: .bodyContents3 #bodyContents34}  

<p>$$L(\hat{y}, y) = -\sum_{i} y_i \log \left(\hat{y}_ {i}\right)$$</p>  

__Binary Cross-Entropy:__{: style="color: red"}  
<p>$$L(\hat{y}, y) = -\left[y \log \hat{y}+\left(1-y\right) \log \left(1-\hat{y}_ {n}\right)\right]$$</p>  

![img](/main_files/dl/concepts/loss_funcs/4.png){: width="30%" .center-image}  


__Cross-Entropy and Negative-Log-Probability:__{: style="color: red"}  
The __Cross-Entropy__ is equal to the __Negative-Log-Probability__ (of predicting the true class) in the case that the true distribution that we are trying to match is *__peaked at a single point__* and is *__identically zero everywhere else__*; this is usually the case in ML when we are using a _one-hot encoded vector_ with one class $$y = [0 \: 0 \: \ldots \: 0 \: 1 \: 0 \: \ldots \: 0]$$ peaked at the $$j$$-th position   
$$\implies$$  
<p>$$L(\hat{y}, y) = -\sum_{i} y_i \log \left(\hat{y}_ {i}\right) = - \log (\hat{y}_ {j})$$</p>  

__Cross-Entropy and Log-Loss:__{: style="color: red"}    
The __Cross-Entropy__ is equal to the __Log-Loss__ in the case of $$0, 1$$ classification.  

__Equivalence of *Binary Cross-Entropy* and *Logistic-Loss*:__  
Given $$p \in\{y, 1-y\}$$ and $$q \in\{\hat{y}, 1-\hat{y}\}$$:  
<p>$$H(p,q)=-\sum_{x }p(x)\,\log q(x) = -y \log \hat{y}-(1-y) \log (1-\hat{y}) = L(\hat{y}, y)$$</p>  

* Notice the following property of the __logistic function__ $$\sigma$$ (used in derivation below):   
    $$\sigma(-x) = 1-\sigma(x)$$  

<button>Derivation</button>{: .showText value="show"
 onclick="showText_withParent_PopHide(event);"}
_Given:_{: hidden=""}  
* $$\hat{y} = \sigma(yf(x))$$,[^5]  
* $$y \in \{-1, 1\}$$,   
* $$\hat{y}' = \sigma(f(x))$$,  
* $$y' = (1+y)/2 = \left\{\begin{array}{ll}{1} & {\text { for }} y' = 1 \\ {0} & {\text { for }} y = -1\end{array}\right. \in \{0, 1\}$$[^4]   
* We start with the modified binary cross-entropy  
    $$\begin{aligned} -y' \log \hat{y}'-(1-y') \log (1-\hat{y}') &= \left\{\begin{array}{ll}{-\log\hat{y}'} & {\text { for }} y' = 1 \\ {-\log(1-\hat{y}')} & {\text { for }} y' = 0\end{array}\right. \\ \\
    &= \left\{\begin{array}{ll}{-\log\sigma(f(x))} & {\text { for }} y' = 1 \\ {-\log(1-\sigma(f(x)))} & {\text { for }} y' = 0\end{array}\right. \\ \\
    &= \left\{\begin{array}{ll}{-\log\sigma(1\times f(x))} & {\text { for }} y' = 1 \\ {-\log(\sigma((-1)\times f(x)))} & {\text { for }} y' = 0\end{array}\right. \\ \\
    &= \left\{\begin{array}{ll}{-\log\sigma(yf(x))} & {\text { for }} y' = 1 \\ {-\log(\sigma(yf(x)))} & {\text { for }} y' = 0\end{array}\right. \\ \\
    &= \left\{\begin{array}{ll}{-\log\hat{y}} & {\text { for }} y' = 1 \\ {-\log\hat{y}} & {\text { for }} y' = 0\end{array}\right. \\ \\
    &= -\log\hat{y} \\ \\
    &= \log\left[\dfrac{1}{\hat{y}}\right] \\ \\
    &= \log\left[\hat{y}^{-1}\right] \\ \\
    &= \log\left[\sigma(yf(x))^{-1}\right] \\ \\
    &= \log\left[ \left(\dfrac{1}{1+e^{-yf(x)}}\right)^{-1}\right] \\ \\
    &= \log \left(1+e^{-yf(x)}\right)\end{aligned}$$  
{: hidden=""}

> [Reference (Understanding binary-cross-entropy-log-loss)](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)  


__Cross-Entropy as Negative-Log-Likelihood (w/ equal probability outcomes):__{: style="color: red"}  


__Cross-Entropy and KL-Div:__{: style="color: red"}  
When comparing a distribution $${\displaystyle q}$$ against a fixed reference distribution $${\displaystyle p}$$, cross entropy and KL divergence are identical up to an additive constant (since $${\displaystyle p}$$ is fixed): both take on their minimal values when $${\displaystyle p=q}$$, which is $${\displaystyle 0}$$ for KL divergence, and $${\displaystyle \mathrm {H} (p)}$$ for cross entropy.  
> Basically, minimizing either will result in the same solution.  


__Cross-Entropy VS MSE (& Classification Loss):__{: style="color: red"}  
Basically, CE > MSE because the gradient of MSE $$z(1-z)$$ leads to saturation when then output $$z$$ of a neuron is near $$0$$ or $$1$$ making the gradient very small and, thus, slowing down training.  
CE > Class-Loss because Class-Loss is binary and doesn't take into account _"how well"_ are we actually approximating the probabilities as opposed to just having the target class be slightly higher than the rest (e.g. $$[c_1=0.3, c_2=0.3, c_3=0.4]$$).  
* [Why You Should Use Cross-Entropy Error Instead Of Classification Error Or Mean Squared Error For Neural Network Classifier Training](https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/)  


<br>


### **Exponential Loss**{: style="color: SteelBlue; font-size: 1.27em"}{: .bodyContents3 #bodyContents35}  

<p>$$L(\hat{y}, y) = e^{-\beta y \hat{y}}$$</p>  
<br>

### **Perceptron Loss**{: style="color: SteelBlue; font-size: 1.27em"}{: .bodyContents3 #bodyContents36}  

<p>$${\displaystyle L(z, y_i) = {\begin{cases}0&{\text{if }}\ y_i\cdot z_i \geq 0\\-y_i z&{\text{otherwise}}\end{cases}}}$$</p>  
<br>


### **Notes**{: style="color: SteelBlue; font-size: 1.27em"}{: .bodyContents3 #bodyContents37}  
* __Logistic loss__ diverges faster than __hinge loss__ [(image)](#losses). So, in general, it will be more sensitive to outliers. [Reference. Bad info?](https://towardsdatascience.com/support-vector-machine-vs-logistic-regression-94cc2975433f)   


<br><br>

[^1]: [Stability](#bodyContents22stability)  
[^2]: Reason is that the errors are equally weighted; so, tilting the regression line (within a region) will decrease the distance to a particular point and will increase the distance to other points by the same amount.  
[^3]: [Reference](http://articles.adsabs.harvard.edu//full/1982AJ.....87..928B/0000936.000.html)  
[^4]: We have to redefine the indicator/target variable to establish the equality.  
[^5]: $$f(x) = w^Tx$$ in logistic regression  

***
***

TITLE: Practical Concepts in Machine Learning
LINK: research/dl/concepts/practical_concepts.md


## FIRST
{: #content1}

1. **Data Snooping:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  

    __The Principle:__  
    If a data set has __*affected*__ any step in the __learning process__, its __ability to *assess the outcome*__ has been compromised.  

    __Analysis:__  
    {: #lst-p}
    * Making decision by __examining the dataset__ makes *__you__* a part of the learning algorithm.  
        However, you didn't consider your contribution to the learning algorithm when making e.g. VC-Analysis for Generalization.  
    * Thus, you are __vulnerable__ to designing the model (or choices of learning) according to the *__idiosyncrasies__* of the __dataset__.  
    * The real problem is that you are not _"charging" for the decision you made by examining the dataset_.    

    __What's allowed?__  
    {: #lst-p}
    * You are allowed (even encouraged) to look at all other information related to the __target function__ and __input space__.  
        e.g. number/range/dimension/scale/etc. of the inputs, correlations, properties (monotonicity), etc.  
    * EXCEPT, for the __*specific* realization of the training dataset__.  


    __Manifestations of Data Snooping with Examples (one/manifestation):__{: style="color: red"}  
    {: #lst-p}
    * __Changing the Parameters of the model (Tricky)__:  
        * __Complexity__:  
            Decreasing the order of the fitting polynomial by observing geometric properties of the __training set__.  
    * __Using statistics of the Entire Dataset (Tricky)__:  
        * __Normalization__:  
            Normalizing the data with the mean and variance of the __entire dataset (training+testing)__.  
            * E.g. In Financial Forecasting; the average affects the outcome by exposing the trend.  
    * __Reuse of a Dataset__:  
        If you keep Trying one model after the other *on the* __same data set__, you will eventually 'succeed'.  
        _"If you torture the data long enough, it will confess"_.  
        This bad because the final model you selected, is the __*union* of all previous models__: since some of those models were *__rejected__* by __you__ (a *__learning algorithm__*).  
        * __Fixed (deterministic) training set for Model Selection__:  
            Selecting a model by trying many models on the __same *fixed (deterministic)* Training dataset__.  
    * __Bias via Snooping__:  
        By looking at the data in the future when you are not allowed to have the data (it wouldn't have been possible); you are creating __sampling bias__ caused by _"snooping"_.  
        * E.g. Testing a __Trading__ algorithm using the *__currently__* __traded companies__ (in S&P500).  
            You shouldn't have been able to know which companies are being *__currently__* traded (future).  



    __Remedies/Solutions to Data Snooping:__{: style="color: red"}  
    {: #lst-p}
    1. __Avoid__ Data Snooping:  
        A strict discipline (very hard).  
    2. __Account for__ Data Snooping:  
        By quantifying "How much __data contamination__".  

    <br>


2. **Mismatched Data:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  

3. **Mismatched Classes:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  

4. **Sampling Bias:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    __Sampling Bias__ occurs when: $$\exists$$ Region with zero-probability $$P=0$$ in training, but with positive-probability $$P>0$$ in testing.  

    __The Principle:__  
    If the data is sampled in a biased way, learning will produce a similarly biased outcome.  

    __Example: 1948 Presidential Elections__  
    {: #lst-p}
    * Newspaper conducted a *__Telephone__* poll between: __Jackson__ and __Truman__  
    * __Jackson__ won the poll __decisively__.  
    * The result was NOT __unlucky__:  
        No matter how many times the poll was re-conducted, and no matter how many times the sample sized is increased; the outcome will be fixed.  
    * The reason is the *__Telephone__*:  
        (1) Telephones were __expensive__ and only __rich people__ had Telephones.  
        (2) Rich people favored __Jackson__.  
        Thus, the result was __well reflective__ of the (mini) population being sampled.  

    __How to sample:__{: style="color: red"}  
    Sample in a way that <span>matches the __distributions__ of __train__ and __test__</span>{: style="color: purple"} samples.  

    The solution __Fails__ (doesn't work) if:  
    $$\exists$$ Region with zero-probability $$P=0$$ in training, but with positive-probability $$P>0$$ in testing.  
    > This is when sampling bias exists.  


    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * Medical sources sometimes refer to sampling bias as __ascertainment bias__.  
    * Sampling bias could be viewed as a subtype of __selection bias__.  
    <br>


5. **Model Uncertainty:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  

    __Interpreting Softmax Output Probabilities:__{: style="color: red"}  
    Softmax outputs only measure [__Aleatoric Uncertainty__](https://en.wikipedia.org/wiki/Uncertainty_quantification#Aleatoric_and_epistemic_uncertainty).  
    In the same way that in regression, a NN with two outputs, one representing mean and one variance, that parameterise a Gaussian, can capture aleatoric uncertainty, even though the model is deterministic.  
    Bayesian NNs (dropout included), aim to capture epistemic (aka model) uncertainty.  

    __Dropout for Measuring Model (epistemic) Uncertainty:__{: style="color: red"}  
    Dropout can give us principled uncertainty estimates.  
    Principled in the sense that the uncertainty estimates basically approximate those of our [Gaussian process](/work_files/research/dl/archits/nns#bodyContents13).  

    __Theoretical Motivation:__ dropout neural networks are identical to <span>variational inference in Gaussian processes</span>{: style="color: purple"}.  
    __Interpretations of Dropout:__  
    {: #lst-p}
    * Dropout is just a diagonal noise matrix with the diagonal elements set to either 0 or 1.  
    * [What My Deep Model Doesn't Know (Blog! - Yarin Gal)](http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html)  
    <br>

6. **Probability Calibration:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    Modern NN are __miscalibrated__: not well-calibrated. They tend to be very confident. We cannot interpret the softmax probabilities as reflecting the true probability distribution or as a measure of confidence.  

    __Miscalibration:__ is the discrepancy between model confidence and model accuracy.  
    You assume that if a model gives $$80\%$$ confidence for 100 images, then $$80$$ of them will be accurate and the other $$20$$ will be inaccurate.  
    <button>Miscalibration in Modern Neural Networks</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/boMaW8Wx2tXfUYTJpd-rhcVGWnrtpC4_2AGbXPxtocc.original.fullsize.png){: width="100%" hidden=""}  

    __Model Confidence:__ probability of correctness.  
    __Calibrated Confidence (softmax scores) $$\hat{p}$$:__ $$\hat{p}$$ represents a true probability.  

    * [On Calibration of Modern Neural Networks](https://arxiv.org/pdf/1706.04599.pdf)    
        Paper that defines the problem and gives multiple effective solution for calibrating Neural Networks. 
    * [Calibration of Convolutional Neural Networks (Thesis!)](file:///Users/ahmadbadary/Downloads/KaÃàngsepp_ComputerScience_2018.pdf)  
    <br>

7. **Debugging Strategies for Deep ML Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    <button>Strategies</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    1. Visualize the model in action:  
        Directly observing qualitative results of a model (e.g. located objects, generated speech) can help avoid __evaluation bugs__ or __mis-leading evaluation results__. It can also help guide the expected quantitative performance of the model.  
    2. Visualize the worst mistakes:  
        By viewing the training set examples that are the hardest to model correctly by using a confidence measure (e.g. softmax probabilities), one can often discover problems with the way the data have been __preprocessed__ or __labeled__.  
    3. Reason about software using training and test error:  
        It is hard to determine whether the underlying software is correctly implemented.  
        We can use the training/test errors to help guide us:  
        * If training error is low but test error is high, then:  
            * it is likely that that the training procedure works correctly,and the model is overfitting for fundamental algorithmic reasons.  
            * or that the test error is measured incorrectly because of a problem with saving the model after training then reloading it for test set evaluation, or because the test data was prepared differently from the training data.  
        * If both training and test errors are high, then:  
            it is difficult to determine whether there is a software defect or whether the model is underfitting due to fundamental algorithmic reasons.  
            This scenario requires further tests, described next.  
    3. Fit a tiny dataset:  
        If you have high error on the training set, determine whether it is due to genuine underfitting or due to a software defect.  
        Usually even small models can be guaranteed to be able fit a suÔ¨Éciently small dataset. For example, a classification dataset with only one example can be fit just by setting the biase sof the output layer correctly.  
        This test can be extended to a small dataset with few examples.  
    4. Monitor histograms of activations and gradients:  
        It is often useful to visualize statistics of neural network activations and gradients, collected over a large amount of training iterations (maybe one epoch).  
        The __preactivation value__ of __hidden units__ can tell us if the units <span>__saturate__</span>{: style="color: purple"}, or how often they do.  
        For example, for rectifiers,how often are they off? Are there units that are always off?  
        For tanh units,the average of the absolute value of the preactivations tells us how saturated the unit is.  
        In a deep network where the propagated gradients quickly grow or quickly vanish, optimization may be hampered.  
        Finally, it is useful to compare the magnitude of parameter gradients to the magnitude of the parameters themselves. As suggested by Bottou (2015), we would like the magnitude of parameter updates over a minibatch to represent something like 1 percent of the magnitude of the parameter, not 50 percent or 0.001 percent (which would make the parametersmove too slowly). It may be that some groups of parameters are moving at a good pace while others are stalled. When the data is sparse (like in natural language) some parameters may be very rarely updated, and this should be kept in mind when monitoring their evolution.  
    5. Finally, many deep learning algorithms provide some sort of guarantee about the results produced at each step.  
        For example, in part III, we will see some approximate inference algorithms that work by using algebraic solutions to optimization problems.  
        Typically these can be debugged by testing each of their guarantees.Some guarantees that some optimization algorithms offer include that the objective function will never increase after one step of the algorithm, that the gradient with respect to some subset of variables will be zero after each step of the algorithm,and that the gradient with respect to all variables will be zero at convergence.Usually due to rounding error, these conditions will not hold exactly in a digital computer, so the debugging test should include some tolerance parameter. 
    {: hidden=""}



***

## SECOND
{: #content2}



***
***

TITLE: Normalization Methods in Deep Learning
LINK: research/dl/concepts/normalization.md


[Deeper Understanding of Batch Normalization](https://medium.com/@SeoJaeDuk/deeper-understanding-of-batch-normalization-with-interactive-code-in-tensorflow-manual-back-1d50d6903d35)  
[Preprocessing for Deep Learning](https://hadrienj.github.io/posts/Preprocessing-for-deep-learning/)  
[Self Normalizing Neural Networks](https://arxiv.org/pdf/1706.02515.pdf)  
[An Overview of Normalization Methods in Deep Learning](http://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/)    

![img](/main_files/dl/concepts/normalization/1.png){: width="100%"}  

## Normalization
{: #content1}

1. **Normalization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __Normalization__, aka __Feature Scaling__, is a method used to normalize the range of independent variables or features of data. It is generally performed during the data preprocessing step.  

    __Motivation:__{: style="color: red"}  
    Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization (e.g. dot-product-based functions are scale sensitive).  
    Moreover, normalizing the inputs leads to __spherical contours__ of the objective which makes the optimization easier (for vanilla sgd) and speeds up the convergence.  

    * [**Why&How to Normalize Inputs (Ng)**](https://www.youtube.com/embed/FDCfw-YqWTE){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/FDCfw-YqWTE"></a>
        <div markdown="1"> </div>    

    <br>

2. **Input Normalization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    

    __Rescaling (min-max normalization):__{: style="color: red"}  
    is the simplest method and consists in rescaling the range of features to scale the range in $$[0, 1]$$ or $$[‚àí1, 1]$$. Selecting the target range depends on the nature of the data. To rescale to $$[0, 1]$$ range:  
    <p>$$x'=\dfrac{x-{\text{min}}(x)}{ {\text{max}}(x)-{\text{min}}(x)}$$</p>  

    To rescale a range between an arbitrary set of values $$[a, b]$$, the formula becomes:  
    <p>$${\displaystyle x'=a+{\frac {(x-{\text{min}}(x))(b-a)}{ {\text{max}}(x)-{\text{min}}(x)}}}$$</p>  
    where $$a,\: b$$ are the min-max values.  


    __(Zero-) Centering - Mean Normalization:__{: style="color: red"}  
    * Define the mean $$\mu_j$$ of each feature of the datapoints $$x^{(i)}$$:  
        <p>$$\mu_{j}=\frac{1}{m} \sum_{i=1}^{m} x_{j}^{(i)}$$</p>  
    * Replace each $$x_j^{(i)}$$ with $$x_j - \mu_j$$  


    __Standardization (Z-score Normalization):__{: style="color: red"}  
    Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. Subtract the mean from each feature, then, divide the values of each feature by its standard deviation.  
    <p>$$x' = \frac{x - \bar{x}}{\sigma}$$</p>  


    __(Scaling to) Unit Length Normalization:__{: style="color: red"}  
    Another option that is widely used in machine-learning is to scale the components of a feature vector such that the complete vector has length one. This usually means dividing each component by the Euclidean length of the vector:  
    <p>$${\displaystyle x'={\frac {x}{\left\|{x}\right\|}}}$$</p>  
    We can use $$L_1$$ norm or other metrics based on problem.  

    <br>

3. **Activation Normalization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    Extends the idea of __input normalization__ to deeper networks. Interprets activations in a layer as a featurized input (abstract representation of the input) and aims to normalize those layer outputs/activations.  

    <span id="affine_transform">The difference however, is in the target mean and variance we want to achieve. Unlike _inputs_, we might not want to force the activations to have mean$$=0$$ and variance$$=1$$. E.g. if we are using the sigmoid activation; mean$$=0$$ and variance$$=1$$ will utilize the *__linear__* part of sigmoid. So, changing the mean and variance will allow the network to take advantage of the non-linearity.</span>   
    ![img](/main_files/concepts/3.png){: width="40%" .center-image}  

    > In practice, it's much more common to normalize the outputs before applying the activation (e.g. in Batch-Norm)  

    <br>

4. **[Statistical Normalization](https://en.wikipedia.org/wiki/Normalization_(statistics)) - Transformations:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    Let $$X$$ be $$n \times d$$ design matrix of sample pts.  
    Each row $$i$$ of $$X$$ is a sample pt $$X_i^T$$.  

    __Centering Transform:__{: style="color: red"}  
    AKA __Mean Normalization__ is just removing the mean from each observation. It centers the data around $$0$$.  
    <p>$$X' = X - \mathbf{\mu}$$</p>   
    where $$\mathbf{\mu}$$ is the mean of all the rows of $$X$$.  

    __Decorrelation Transform:__{: style="color: red"}  
    removes only the correlations but leaves variances intact,  
    <p>$$Z = X'V$$</p>  
    where $$\text{Var}(X') = \Sigma = \dfrac{1}{n}X'^TX' = V\Lambda V^T$$, and $$\text{Var}(Z) = \Lambda$$ is the sample covariance matrix.  

    It transforms the sample points to the eigenvector coordinate system.  

    __Standardization Transform:__{: style="color: red"}  
    sets variances to $$1$$ but leaves correlations intact,  
    <p>$$X'_s = \dfrac{X - \mathbf{\mu}}{\mathbf{\sigma}_ X}$$</p>  
    where $$\mathbf{\sigma}_ X$$ is the standard deviation of all the rows of $$X$$.  

    __Sphering Transform:__{: style="color: red"}  
    Rescales the uncorrelated matrix $$Z$$ in order to obtain a covariance matrix corresponding to the identity matrix. To do that we scale our decorrelated data by dividing each dimension by the square-root of its corresponding eigenvalue.  
    <p>$$W = X'\Sigma^{-1/2} = X'(V \Lambda^{-1/2} V^T)$$</p>  
    this is __ZCA whitening__.  

    __Whitening Transform:__{: style="color: red"}  
    The [Whitening Transformation](https://en.wikipedia.org/wiki/Whitening_transformation) is a linear transformation that transforms a vector of random variables with a known covariance matrix into a set of new variables whose covariance is the identity matrix, meaning that they are uncorrelated and each have variance 1. The transformation is called "whitening" because it changes the input vector into a white noise vector.  
    = __Centering__ + __Sphering__  

    Then $$W$$ has covariance matrix $$I . \left[\text { If } X_{i} \sim \mathcal{N}(\mu, \Sigma), \text { then approximately, } W_{i} \sim \mathcal{N}(0, I)\right]$$.  



    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * __Decorrelation:__ intuitively, it means that we want to rotate the data until there is no correlation anymore.  
    * __Centering__ seems to make it easier for hidden units to get into a good operating region of the sigmoid or ReLU  
    * __Standardization (normalizing variance)__ makes the objective function better conditioned, so gradient descent converges faster  

    <br>
  

***

## Batch Normalization
{: #content2}

1. **Batch Normalization:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    __Batch Normalization__ is a normalization method that normalizes activations in a network across the mini-batch. For each feature, batch normalization computes the mean and variance of that feature in the mini-batch. It then subtracts the mean and divides the feature by its mini-batch standard deviation.  
    <span>This restricts the activations to have __$$0$$__ mean and __unit__ variance.</span>{: style="color: goldenrod"}  
    This is followed by an __affine transformation__ of the normalized activations to __rescale the mean and variance__, in a learnable way, to whatever the network sees fit. [This is done because restricting the mean and variance of the activations might hinder the network from taking advantage of the freedom of setting the distribution of the activations to something that might help the later layers learn faster](#affine_transform). _This means that the expressiveness of the network does not change._  

    <button>Show Paper</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
    <p hidden=""><img style="float: left" width="45%" src="https://cdn.mathpix.com/snip/images/iQfN-SDV0z9nzlyRhX8oUuX8ZUgxd9kuAjmpKhROwBk.original.fullsize.png" />
    <iframe src="https://docs.google.com/viewerng/viewer?url=https://arxiv.org/pdf/1502.03167.pdf&amp;embedded=true" frameborder="0" height="535" width="415" title="Batch Normalization" scrolling="auto"></iframe></p>
    <br>

2. **Effectiveness of Batch Normalization:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    There are many different proposed reasons that try to explain the wide success and effectiveness of the method. We summarize here some of those reasons and give an intuition to why those reasons apply.  

    __Summarizing the intuition of why BN works:__  
    The overall intuition is that batch normalization makes the loss surface ‚Äúeasier to navigate‚Äù, making optimization easier, enabling the use of higher learning rates, and improving model performance across multiple tasks.  
    Further, there is a _regularization_ effect when using BN induced by added noise to the estimate of the mean and variance of the data due to using mini-batches instead.  

    * [Busting the myth about batch normalization](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/)  

    <br>

22. **Internal Covariate Shift as an Intuitive but Wrong Motivation for the Success of BN:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents222}  
    The correlation between batch normalization and internal covariate shift is widely accepted but was not supported by experimental results. Scholars recently show with experiments that the hypothesized relationship is not an accurate one. Rather, the enhanced accuracy with the batch normalization layer seems to be independent of internal covariate shift.  

    __Two Problems with the ICS Explanation:__  
    1. Even if the mean and variance are constant, the distribution of activations can still change. Why are the mean and variance so important?  
    2. If we introduce $$\gamma$$  and $$\beta$$, the mean and variance will deviate from $$0$$ and $$1$$ anyway. What then, is the point of batch normalization?  
    <br>


3. **Suppressing Higher-Order Effects (Goodfellow):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  

    __Quick Intuition:__  
    {: #lst-p}
    * In a neural network, changing one weight affects subsequent layers, which then affect subsequent layers, and so on.
    * This means changing one weight can make the activations fly all over the place in complex ways.
    * This forces us to use lower learning rates to prevent the gradients from exploding, or ‚Äì if we use sigmoid activations ‚Äì to prevent the gradients from disappearing.
    * Batch normalization to the rescue! Batch normalization reparameterizes the network to make optimization easier.
    * With batch normalization, we can control the magnitude and mean of the activations __independent__ of all other layers.
    * This means weights don‚Äôt fly all over the place (as long as we have sensible initial means and magnitudes), and we can optimize much easier.  

    __Further Intuition:__  
    Deep Neural networks have higher-order interactions, which means changing weights of one layer might also effect the statistics of other layers in addition to the loss function. These cross layer interactions, when unaccounted lead to internal covariate shift. Every time we update the weights of a layer, there is a chance that it effects the statistics of a layer further in the neural network in an unfavorable way.  
    Convergence may require careful initializing, hyperparameter tuning and longer training durations in such cases. However, when we add the batch normalized layer between the layers, the statistics of a layer are only effected by the two hyperparameters  $$\gamma$$  and $$\beta$$. Now our optimization algorithm has to adjust only two hyperparameters to control the statistics of any layer, rather than the entire weights in the previous layer. This greatly speeds up convergence, and avoids the need for careful initialization and hyperparameter tuning. Therefore, Batch Norm acts more like a check pointing mechanism.  
    > Notice that the ability to arbitrarily set the mean and the standard deviation of a layer also means that we can recover the original distribution if that was sufficient for proper training.  


    * [**GoodFellow Lecture**](https://www.youtube.com/embed/Xogn6veSyxA?start=225){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/Xogn6veSyxA?start=225"></a>
        <div markdown="1"> </div>    
    * [Further Discussion of Higher-Order Effects](http://mlexplained.com/2018/01/10/an-intuitive-explanation-of-why-batch-normalization-really-works-normalization-in-deep-learning-part-1/)   

    <br>


4. **Induced Smoothness of the Optimization Landscape ([Santurkar et al.](https://arxiv.org/pdf/1805.11604.pdf)):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    In a new paper, the authors claim that BN works because it makes the loss surface __smoother__. Concretely, it improves the $$\beta$$-smoothness or the Lipschitzness of the function. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.  


    * [**How Does Batch Normalization Help Optimization? (Santurkar et al.)**](https://arxiv.org/pdf/1805.11604.pdf){: value="show" onclick="iframePopA(event)"}
    <a href="https://arxiv.org/pdf/1805.11604.pdf"></a>
        <div markdown="1"> </div>    

    <br>

5. **Batch Norm as a Regularizer:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    BN also acts a regularizer. The mean and the variance estimated for each batch is a noisier version of the true mean, and this injects randomness in our optima search. This helps in regularization.  
    <br>

6. **Length-Direction Decoupling:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    It is argued that the success of batch normalization could be at least partially credited to the __length-direction decoupling effect__ that the method provides.  

    By interpreting the batch normalization procedure as the reparametrization of weight space, it could be shown that the length and the direction of the weights are separated after the procedure, and they could thus be trained separately.  

    This property could then be used to __prove the faster convergence of problems with batch normalization__:  
    {: #lst-p}
    * [Linear Convergence of the __Least-Squares__ Problem with Batch Normalization](https://en.wikipedia.org/wiki/Batch_normalization#Linear_Convergence_of_the_Least-Square_Problem_with_Batch_Normalization)  
    * [Linear Convergence of the __Learning Halfspace__ Problem with Batch Normalization](https://en.wikipedia.org/wiki/Batch_normalization#Linear_Convergence_of_the_Learning_Halfspace_Problem_with_Batch_Normalization)  
    * [Linear Convergence of __Neural Networks__ with Batch Normalization](https://en.wikipedia.org/wiki/Batch_normalization#Linear_Convergence_of_Neural_Networks_with_Batch_Normalization)  
    <br>

7. **Limitations/Problems of BN:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  
    
    When doing normalization, we ideally want to the use the __global__ mean and variance to standardize our data. Computing this for each layer is far too expensive though, so we need to approximate using some other measures. BN approximates the mean and variance with those of the __mini batch__, which is a noisy estimate. Although, we motivated some of the effectiveness of BN to its regularizing effects due to this, same, noisy estimate, this estimate can be problematic in the following scenarios:  
    1. __Small Batch Sizes:__  
        If the batch size is $$1$$, the variance is $$0$$ so batch normalization cannot be applied. Slightly larger mini-batch sizes won‚Äôt have this problem, but small mini-batches make our estimates very noisy and can negatively impact training, meaning batch normalization imposes a certain lower bound on our batch size.  
    2. __Recurrent Connections in an RNN:__  
        In an RNN, _the recurrent activations of each time-step will have different statistics_. This means that we have to _fit a separate batch normalization layer for each time-step_. This makes the model _more complicated_ and ‚Äì more importantly ‚Äì it _forces us to store the statistics for each time-step during training_.  
    3. __Dependence of the loss between samples in a mini-batch:__  
        BN makes the loss value for each sample in a mini-batch dependent on other samples in the mini-batch. For instance, if a sample causes a certain layer‚Äôs activations to become much larger, this will make the mean and variance larger as well. This will change the activation for all other samples in the mini-batch as well. Furthermore, the mini-batch statistics will depend on the mini-batch size as well (a smaller mini-batch size will increase the random variation in the mean and variance statistics).
        The problem arises not when training a model on a single machine, but when we start to conduct distributed training, things can get ugly. As mentioned in [this paper](https://arxiv.org/pdf/1706.02677.pdf), we need to take extra care in choosing the batch size and learning rate in the presence of batch normalization when doing distributed training. If two different machines use different batch sizes, they will indirectly be optimizing different loss functions: this means that the value of $$\gamma$$ that worked for one machine is unlikely to work for another machine. This is why the authors stressed that the batch size for each worker must be kept constant across all machines.  
    4. __BN parameters in Fine-Tuning Applications:__  
        When Fine-Tuning a larger network by freezing all the layers except the last layer; it is unclear if one should use the mean and variance computed on the *__original dataset__* or use the mean and variance of the mini-batches. Though most frameworks use the mini-batch statistics, if we are using a different mini-batch size there will be a mismatch between the optimal batch normalization parameters and the parameters in the network.  
        If there is a mis-match in the mini-batch sizes it seems to be better to use the statistics of the _original dataset_ instead.  
        [Further Discussion](https://forums.fast.ai/t/freezing-batch-norm/8377/5)  


    <br>



***

## Weight Normalization
{: #content3}


<button>Paper</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
<iframe hidden="" src="https://arxiv.org/pdf/1602.07868.pdf" frameborder="0" height="780" width="600" title="Weight Normalization" scrolling="auto"></iframe>


1. **Weight Normalization:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    __Weight Normalization__ is a normalization method that, instead of normalizing the _mini-batch_, <span>normalizes the weights of the layer</span>{: style="color: goldenrod"}.  

    WN reparameterizes the weights $$\boldsymbol{w}$$ of any layer in the network in the following way:  
    <p>$$\boldsymbol{w}=\frac{g}{\|\boldsymbol{v}\|} \boldsymbol{v} \:\:\:\:\:\:\: $$</p>  
    where $$\boldsymbol{v}$$ is a $$k$$-dimensional vector, $$g$$ is a scalar, and $$\|\boldsymbol{v}\|$$ denotes the Euclidean norm of $$\boldsymbol{v}$$.  
    This reparameterization has the effect of fixing the Euclidean norm of the weight vector $$\boldsymbol{w}$$: we now have $$\|\boldsymbol{w}\| = g$$, independent of the parameters $$\boldsymbol{v}$$.  

    Weight Normalization <span>separates the norm of the weight vector from its direction without reducing expressiveness</span>{: style="color: goldenrod"}:  
    {: #lst-p}
    * For variance, this has a similar effect to _dividing the inputs by the standard deviation in batch normalization_.  
    * As for the mean, the authors of the paper proposed using a method called [__‚Äúmean-only batch normalization‚Äù__](#bodyContents33) together with weight normalization.  
    <br>

2. **Advantages:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    * Since WN separates the norm of the weight vector from its direction, and then optimizes both $$g$$ and $$\boldsymbol{v}$$ using gradient descent. This change in learning dynamics makes optimization easier.  
    * It makes the mean and variance __independent of the batch__; since now they are connected to the weights of the network.  
    * It is often __much faster__ than BN. In CNNs, the number of weights tends to be far smaller than the number of inputs, meaning weight normalization is computationally cheaper compared to batch normalization (CNNs share weights).   
    <br>

3. **Mean-Only Batch Normalization:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    Although weight normalization on its own can assist training, the authors of the paper proposed using a method called ‚Äúmean-only batch normalization‚Äù in conjunction with weight normalization.  

    __Mean-Only Batch Normalization__ is a method that subtracts out the mean of the mini-batch but does not divide the inputs by the standard deviation or rescales them.  

    Though this method counteracts some of the computational speed-up of weight normalization, it is cheaper than batch-normalization since it does not need to compute the standard deviations. The authors claim that this method provides the following benefits:  
    {: #lst-p}
    1. It makes the mean of the activations independent from $$\boldsymbol{v}$$:  
        Weight normalization independently cannot isolate the mean of the activations from the weights of the layer, causing high-level dependencies between the means of each layer. Mean-only batch normalization can resolve this problem.  
    2. It adds ‚Äúgentler noise‚Äù to the activations:  
        One of the side-effects of batch normalization is that it adds some stochastic noise to the activations as a result of using noisy estimates computed on the mini-batches. This has a regularization effect in some applications but can be potentially harmful in some noise-sensitive domains like reinforcement learning. The noise caused by the mean estimations, however, are ‚Äúgentler‚Äù since the law of large numbers ensures the mean of the activations is approximately normally distributed.  
        Thus, weight normalization <span>can still work in settings with a smaller mini-batch size</span>{: style="color: goldenrod"}.  
    <br>
 



***

## Layer Normalization
{: #content4}

<button>Paper</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
<iframe hidden="" src="https://arxiv.org/pdf/1607.06450.pdf" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>


1. **Layer Normalization:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    __Layer Normalization__ is a normalization method developed by Hinton that, instead of normalizing the inputs across batches like BN, <span>normalizes the inputs across the __features__</span>{: style="color: goldenrod"}:  
    <p>$$\begin{aligned} \mu_{i} &=\frac{1}{m} \sum_{j=1}^{m} x_{i j} \\ \sigma_{i}^{2} &=\frac{1}{m} \sum_{j=1}^{m}\left(x_{i j}-\mu_{i}\right)^{2} \\ \hat{x}_{i j} &=\frac{x_{i j}-\mu_{i}}{\sqrt{\sigma_{i}^{2}+\epsilon}} \end{aligned}$$</p>  

    This is deceptively similar to the batch norm equations:  
    <p>$$\begin{aligned} \mu_{j} &=\frac{1}{m} \sum_{i=1}^{m} x_{i j} \\ \sigma_{j}^{2} &=\frac{1}{m} \sum_{i=1}^{m}\left(x_{i j}-\mu_{j}\right)^{2} \\ \hat{x}_{i j} &=\frac{x_{i j}-\mu_{j}}{\sqrt{\sigma_{j}^{2}+\epsilon}} \end{aligned}$$</p>  
    where $$x_{i j}$$  is the $$i,j$$-th element of the input, the first dimension represents the batch and the second represents the feature (I have modified the notation from the original papers to make the contrast clearer).  

    <button>BN vs LN Illustration</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![LN vs BN](https://cdn.mathpix.com/snip/images/oLjDSgRfXNgvvaobfQKjDuNfUhcjce-K7TKcX8-DjWM.original.fullsize.png){: width="70%" hidden=""}  
    In batch normalization, the statistics are computed across the batch and are the same for each example in the batch. In contrast, in layer normalization, the statistics are computed across each feature and are __independent of other examples__.  

    This means that layer normalization is __not a simple reparameterization of the network__, unlike the case of weight normalization and batch normalization, which both have the same expressive power as an unnormalized neural network. The layer normalized model, thus, has __different invariance properties than the other methods__.   
    <br>

2. **Advantages:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}  
    * The independence between inputs means that each input has a different normalization operation, allowing arbitrary mini-batch sizes to be used.
    * The experimental results show that layer normalization performs well for recurrent neural networks.  
    <br>

3. **Analysis of the Invariance Properties of LN:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}  
    <br>

***

## Instance Normalization
{: #content5}

<button>Paper</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
<iframe hidden="" src="https://docs.google.com/viewer?url=https://arxiv.org/pdf/1607.08022.pdf&amp;embedded=true" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>





1. **Instance Normalization:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents51}  
    __Instance Normalization__ is similar to layer normalization but with an extra restriction: it computes the mean/standard deviation and normalize across __each channel in each training example__.  

    __Motivation:__{: style="color: red"}  
    In __Style Transfer__ problems, the network should be *agnostic to the __contrast__ of the original image*.   
    Therefore, it is specific to images and not trivially extendable to RNNs.  


    Experimental results show that instance normalization performs well on style transfer when replacing batch normalization. Recently, instance normalization has also been used as a replacement for batch normalization in __GANs__.   
    <br>




***

## Group Normalization
{: #content6}

<button>Paper</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
<iframe hidden="" src="https://docs.google.com/viewer?url=https://arxiv.org/pdf/1803.08494.pdf&amp;embedded=true" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>


1. **Group Normalization:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents61}  
    __Group Normalization__ computes the mean and standard deviation over __groups of channels__ for _each training example_.  
    You can think of GN as being half way between _layer normalization_ and _instance normalization_:  
    {: #lst-p}
    * When we put all the channels into a single group, it becomes __Layer Normalization__  
    * When we put each channel in a different group, it becomes __Instance Normalization__  
    <br>

2. **Motivation/Advantages:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents62}  
    _Layer Norm_ and _Instance Norm_ were significantly inferior to BN on image recognition tasks. Group Normalization was able to achieve much closer performance to BN with a batch-size of 32 on ImageNet and outperformed it on smaller batch sizes.  

    For tasks like _object detection_ and _segmentation_ that __use much higher resolution images__ (and therefore cannot increase their batch size due to memory constraints), Group Normalization was shown to be a very effective normalization method.  
    <br>

3. **Effectiveness of Group Normalization:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents63}  
    > __Why is group normalization so effective compared to layer normalization and instance normalization?__  

    __Layer Norm__ makes an implicit assumption that <span>all channels are ‚Äúequally important‚Äù when computing the mean</span>{: style="color: goldenrod"}. This assumption is often not true in __convolution layers__.  
    For instance, neurons near the edge of an image and neurons near the center of an image will have very different activation statistics.  This means that computing different statistics for different channels can give models much-needed flexibility.  

    __Instance Norm__, on the other hand, assumes that the <span>channels are completely independent from each other</span>{: style="color: goldenrod"}. Channels in an image are not completely independent though, so being able to leverage the statistics of nearby channels is an advantage group normalization has over instance normalization.  
    <br>


***

## Batch ReNormalization
{: #content7}

<button>Paper</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
<iframe hidden="" src="https://docs.google.com/viewer?url=https://arxiv.org/pdf/1702.03275.pdf&amp;embedded=true" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>


1. **Batch ReNormalization:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents71}  
    __Batch ReNormalization__ is an extension of BN for applying batch normalization to small batch sizes.  
    > In the Authors words: "an effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire mini-batch"  

    The authors propose to use a moving average while also taking the effect of previous layers on the statistics into account. Their method is ‚Äì at its core ‚Äì a simple reparameterization of normalization with the moving average. If we denote the moving average mean and standard deviation as $$\mu$$ and $$\sigma$$ and the mini-batch mean and standard deviation as $$\mu_B$$ and $$\sigma_B$$, the batch renormalization equation is:  
    <p>$$\frac{x_{i}-\mu}{\sigma}=\frac{x_{i}-\mu_{\mathcal{B}}}{\sigma_{\mathcal{B}}} \cdot r+d, \quad \text { where } r=\frac{\sigma_{\mathcal{B}}}{\sigma}, \quad d=\frac{\mu_{\mathcal{B}}-\mu}{\sigma}$$</p>  

    In other words, we multiply the batch normalized activations by $$r$$ and add $$d$$, where both $$r$$ and $$d$$ are computed from the mini-batch statistics and moving average statistics. The trick here is to __not backpropagate__ through $$r$$ and $$d$$. Though this means we ignore some of the effects of previous layers on previous mini batches, since the mini batch statistics and moving average statistics should be the same on average, the overall effect of this should cancel out on average as well.  
    <br>


2. **Motivation (derivation):**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents72}  
    The basic idea behind batch renormalization comes from the fact that we do not use the individual mini-batch statistics for batch normalization during inference. Instead, we use a __moving average__ of the mini-batch statistics. This is because a moving average provides a better estimate of the true mean and variance compared to individual mini-batches.  

    __Why don‚Äôt we use the moving average during training?__  
    The answer has to do with the fact that during training, we need to perform backpropagation. In essence, when we use some statistics to normalize the data, we need to __backpropagate through those statistics as well__. If we use the statistics of activations from previous mini-batches to normalize the data, we need to account for how the previous layer affected those statistics during backpropagation. If we ignore these interactions, we could potentially cause previous layers to keep on increasing the magnitude of their activations even though it has no effect on the loss. This means that if we use a moving average, we would need to store the data from all previous mini-batches during training, which is far too expensive.  
    <br>

3. **Performance:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents73}  
    Unfortunately, batch renormalization‚Äôs performance still degrades when the batch size decreases (though not as badly as batch normalization), meaning group normalization still has a slight advantage in the small batch size regime.
    <br>

***

## Batch-Instance Normalization
{: #content8}

<button>Paper</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
<iframe hidden="" src="https://docs.google.com/viewer?url=https://arxiv.org/pdf/1805.07925.pdf&amp;embedded=true" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>


1. **Batch-Instance Normalization:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents81}  
    __Batch-Instance Normalization__ is an extension of _instance normalization_ that attempts to <span>account for differences in contrast and style in images</span>{: style="color: goldenrod"}. It is basically, just, an _interpolation between batch normalization and instance normalization_.  

    Denoting the batch normalized outputs and the instance normalized outputs as $$\hat{x}^{(B)}$$ and $$\hat{x}^{(I)}$$ each, the batch-instance normalized output can be expressed as:  
    <p>$$\mathbf{y}=\left(\rho \cdot \hat{\mathbf{x}}^{(B)}+(1-\rho) \cdot \hat{\mathbf{x}}^{(I)}\right) \cdot \gamma+\beta$$</p>  
    The interesting aspect of batch-instance normalization is that the balancing parameter $$\rho$$ is __learned__ through gradient descent.  
    <br>

2. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents82}  
    The problem with instance normalization is that it __completely erases style information__. Though this is beneficial in certain settings like style transfer, it can be problematic in settings like weather classification where the style (e.g. the brightness of the image) can be a crucial feature. In other words, the degree of style information that should be removed is dependent on the task at hand. Batch-instance normalization attempts to deal with this by __learning__ how much style information should be used for each __task and feature map (channel)__.  
    Thus, B-IN extends instance normalization to account for differences in contrast and style in images.  
    <br>

3. **Performance:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents83}  
    Batch-instance normalization outperformed batch normalization on CIFAR-10/100, ImageNet, domain adaptation, and style transfer. In image classification tasks, the value of $$\rho$$  tended to be close to $$0$$ or $$1$$, meaning many layers used either instance or batch normalization almost exclusively. In addition, layers tended to use batch normalization more than instance normalization, which fits the intuition proposed by the authors that instance normalization serves more as a method to eliminate unnecessary style variation. On style transfer ‚Äì on the other hand ‚Äì the model tended to use instance normalization more, which makes sense given style is much less important in style transfer.  

    The authors also found that in practice, using a higher learning rate for $$\rho$$ improves performance.  

    One important contribution of batch-instance normalization is that it showed that __models could learn to adaptively use different normalization methods using gradient descent__. This raises the question: _could models learn to use an even wider variety of normalization methods?_  
    This nicely leads us to the next normalization method: __Switchable Normalization__.    

***

## Switchable Normalization
{: #content9}

<button>Paper</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
<iframe hidden="" src="https://docs.google.com/viewer?url=https://arxiv.org/pdf/1811.07727.pdf&amp;embedded=true" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>

<button>Arxiv</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
<iframe hidden="" src="https://arxiv.org/pdf/1811.07727.pdf" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>


1. **Switchable Normalization:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents91}  
    __Switchable Normalization__ is a method that uses a weighted average of different mean and variance statistics from batch normalization, instance normalization, and layer normalization. Similar to batch-instance normalization, the weights were learned through backpropagation.  
    <br>

2. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents92}  
    Given the different methods proposed for normalization, common questions arise, including:  
    Is batch normalization still the best normalization method out-of-the-box? What if we combine different normalization methods? What if the best normalization method actually differs depending on the depth of the layer?  
    Switchable Normalization aims to answer those questions.  
    <br>

3. **Performance:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents93}  
    The authors showed that switch normalization could potentially outperform batch normalization on tasks such as image classification and object detection.  

    Perhaps more interestingly, the paper showed that <span>the statistics of instance normalization were used more heavily in earlier layers</span>{: style="color: goldenrod"}, whereas <span>layer normalization was preferred in the later layers</span>{: style="color: goldenrod"}, and <span>batch normalization being used in the middle</span>{: style="color: goldenrod"}. Smaller batch sizes lead to a preference towards layer normalization and instance normalization, as is expected.  
    <br>

***

## Spectral Normalization
{: #content10}

<button>Paper</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
<iframe hidden="" src="https://docs.google.com/viewer?url=https://arxiv.org/pdf/1805.07925.pdf&amp;embedded=true" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>


1. **Spectral Normalization:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents101}  
    __Spectral Normalization__ is a method proposed to improve the training of __GANs__ by limiting the Lipschitz constant of the discriminator.  
    The authors restrict the Lipschitz constant by normalizing the weight matrices by their largest eigenvalue (or their spectral norm ‚Äì hence the name). The largest eigenvalue is computed using the _power method_ which makes the computational cost of this method very cheap. (Compared to weight normalization, spectral normalization does not reduce the rank of the weight matrix.)[^1]  
    <br>

    SN is not designed to be a replacement for batch normalization, but it gives us a very interesting look into normalization in deep learning in general.  
    <br>

2. **Performance:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents102}  
    Experimental results show that spectral normalization improves the training of GANs with minimal additional tuning.  
    <br>

***

## Further Exploration/Discussions
{: #content11}

Though recent papers have explored different normalization methods at different depths of the network, there are still many dimensions that can be explored. In [this paper](https://arxiv.org/pdf/1805.11604.pdf), the authors show that $$L_1$$ normalization performs better than batch normalization, suggesting that as we understand batch normalization better, we might be able to come up with more principled methods of normalization. This means that we might see new normalization methods use different statistics instead of changing what they compute the statistics over.  

[^1]: Most likely a wrong statement

***
***

TITLE: Data Processing
LINK: research/dl/concepts/data_proc.md



* [Non-Negative Matrix Factorization NMF Tutorial](http://mlexplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/)  


## Dimensionality Reduction
{: style="font-size: 1.60em"}
{: #content1}


### **Dimensionality Reduction**{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents1 #bodyContents11}  
__Dimensionality Reduction__ is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. It can be divided into __feature selection__{: style="color: goldenrod"} and __feature extraction__{: style="color: goldenrod"}.  
<br>

### **t-SNE \| T-distributed Stochastic Neighbor Embeddings**{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents1 #bodyContents12}  

<button>Paper</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
<iframe hidden="" src="https://docs.google.com/viewer?url=http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf&amp;embedded=true" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>

[Understanding t-SNE Part 1: SNE algorithm and its drawbacks](https://medium.com/@layog/i-dont-understand-t-sne-part-1-50f507acd4f9)  
[Understanding t-SNE Part 2: t-SNE improvements over SNE](https://medium.com/@layog/i-do-not-understand-t-sne-part-2-b2f997d177e3)  
[t-SNE (statwiki)](https://wiki.math.uwaterloo.ca/statwiki/index.php?title=visualizing_Data_using_t-SNE)  
[t-SNE tutorial (video)](https://www.youtube.com/watch?v=W-9L6v_rFIE)  
[series (deleteme)](https://www.youtube.com/watch?v=FQmCzpKWD48&list=PLupD_xFct8mHqCkuaXmeXhe0ajNDu0mhZ)  



__SNE - Stochastic Neighbor Embeddings:__{: style="color: red"}  
__SNE__ is a method that aims to _match_ __distributions of distances__ between points in high and low dimensional space via __conditional probabilities__.  
It Assumes distances in both high and low dimensional space are __Gaussian-distributed__.  
* [**Algorithm**](https://www.youtube.com/embed/ohQXphVSEQM?start=130){: value="show" onclick="iframePopA(event)"}
<a href="https://www.youtube.com/embed/ohQXphVSEQM?start=130"></a>
    <div markdown="1"> </div>    
    ![img](/main_files/dl/concepts/data_proc/2.png){: width="65%"}  
<br> 


__t-SNE:__{: style="color: red"}  
__t-SNE__ is a machine learning algorithm for visualization developed by Laurens van der Maaten and Geoffrey Hinton.  
It is a *__nonlinear__* *__dimensionality reduction__* technique well-suited for _embedding high-dimensional data for visualization in a low-dimensional space_ of _two or three dimensions_.  
Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that <span>similar objects are modeled by nearby points</span>{: style="color: goldenrod"} and <span>dissimilar objects are modeled by distant points</span>{: style="color: goldenrod"}  __with high probability__.  
> It tends to *preserve __local structure__*, while at the same time, *preserving the __global structure__* as much as possible.  

<br>

__Stages:__{: style="color: red"}  
{: #lst-p}
1. It Constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked while dissimilar points have an extremely small probability of being picked.  
2. It Defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the __Kullback‚ÄìLeibler divergence__ between the two distributions with respect to the locations of the points in the map.  
<br>

__Key Ideas:__{: style="color: red"}  
It solves two big problems that __SNE__ faces:  
{: #lst-p}
1. __The Crowding Problem:__  
    The "crowding problem" that are addressed in the paper is defined as: "the area of the two-dimensional map that is available to accommodate moderately distant datapoints will not be nearly large enough compared with the area available to accommodate nearby datepoints". This happens when the datapoints are distributed in a region on a high-dimensional manifold around i, and we try to model the pairwise distances from i to the datapoints in a two-dimensional map. For example, it is possible to have 11 datapoints that are mutually equidistant in a ten-dimensional manifold but it is not possible to model this faithfully in a two-dimensional map. Therefore, if the small distances can be modeled accurately in a map, most of the moderately distant datapoints will be too far away in the two-dimensional map. In SNE, this will result in very small attractive force from datapoint i to these too-distant map points. The very large number of such forces collapses together the points in the center of the map and prevents gaps from forming between the natural clusters. This phenomena, crowding problem, is not specific to SNE and can be observed in other local techniques such as Sammon mapping as well.  
    * __Solution - Student t-distribution for $$q$$__:  
        Student t-distribution is used to compute the similarities between data points in the low dimensional space $$q$$.  
2. __Optimization Difficulty of KL-div:__  
    The KL Divergence is used over the conditional probability to calculate the error in the low-dimensional representation. So, the algorithm will be trying to minimize this loss and will calculate its gradient:  
    <p>$$\frac{\delta C}{\delta y_{i}}=2 \sum_{j}\left(p_{j | i}-q_{j | i}+p_{i | j}-q_{i | j}\right)\left(y_{i}-y_{j}\right)$$</p>  
    This gradient involves all the probabilities for point $$i$$ and $$j$$. But, these probabilities were composed of the exponentials. The problem is that: We have all these exponentials in our gradient, which can explode (or display other unusual behavior) very quickly and hence the algorithm will take a long time to converge.  
    * __Solution - Symmetric SNE__:  
        The Cost Function is a __symmetrized__ version of that in SNE. i.e. $$p_{i\vert j} = p_{j\vert i}$$ and $$q_{i\vert j} = q_{j\vert i}$$.  
<br>

__Application:__{: style="color: red"}  
It is often used to visualize high-level representations learned by an __artificial neural network__.  
<br>

__Motivation:__{: style="color: red"}  
There are a lot of problems with traditional dimensionality reduction techniques that employ _feature projection_; e.g. __PCA__. These techniques attempt to *__preserve the global structure__*, and in that process they *__lose the local structure__*. Mainly, projecting the data on one axis or another, may (most likely) not preserve the _neighborhood structure_ of the data; e.g. the clusters in the data:  
![img](/main_files/dl/concepts/data_proc/1.png){: width="70%"}  
t-SNE finds a way to project data into a low dimensional space (1-d, in this case) such that the clustering ("local structure") in the high dimensional space is preserved.  
<br>


__t-SNE Clusters:__{: style="color: red"}  
While t-SNE plots often seem to display clusters, the visual clusters can be influenced strongly by the chosen parameterization and therefore a good understanding of the parameters for t-SNE is necessary. Such "clusters" can be shown to even appear in non-clustered data, and thus may be false findings.  
It has been demonstrated that t-SNE is often able to _recover well-separated clusters_, and with special parameter choices, [approximates a simple form of __spectral clustering__](https://arxiv.org/abs/1706.02582).  
<br>

__Properties:__{: style="color: red"}  
{: #lst-p}
* It preserves the _neighborhood structure_ of the data  
* Does NOT preserve _distances_ nor _density_  
* Only to some extent preserves _nearest-neighbors_?  
    [discussion](https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne/264647#264647)  
* It learns a __non-parametric mapping__, which means that it does NOT learn an _explicit function_ that maps data from the input space to the map  
<br>

__Algorithm:__{: style="color: red"}  
<button>Algorithm Details (wikipedia)</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
<iframe hidden="" src="https://www.wikiwand.com/en/T-distributed_stochastic_neighbor_embedding#/Details" frameborder="0" height="840" width="646" title="Layer Normalization"></iframe>

<br>

__Issues/Weaknesses/Drawbacks:__{: style="color: red"}  
{: #lst-p}
1. The paper only focuses on the date visualization using t-SNE, that is, embedding high-dimensional date into a two- or three-dimensional space. However, this behavior of t-SNE presented in the paper cannot readily be extrapolated to $$d>3$$ dimensions due to the heavy tails of the Student t-distribution.  
2. It might be less successful when applied to data sets with a high intrinsic dimensionality. This is a result of the *__local linearity assumption__ on the manifold* that t-SNE makes by employing Euclidean distance to present the similarity between the datapoints. 
3. The cost function is __not convex__. This leads to the problem that several optimization parameters (hyperparameters) need to be chosen (and tuned) and the constructed solutions depending on these parameters may be different each time t-SNE is run from an initial random configuration of the map points.  
4. It cannot work __"online"__. Since it learns a non-parametric mapping, which means that it does not learn an explicit function that maps data from the input space to the map. Therefore, it is not possible to embed test points in an existing map. You have to re-run t-SNE on the full dataset.  
    A potential approach to deal with this would be to train a multivariate regressor to predict the map location from the input data.  
    Alternatively, you could also [make such a regressor minimize the t-SNE loss directly (parametric t-SNE)](https://lvdmaaten.github.io/publications/papers/AISTATS_2009.pdf).  

<br>

__t-SNE Optimization:__{: style="color: red"}  
* [Accelerating t-SNE using Tree-Based Algorithms](https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf)  
* [Barnes-Hut-SNE Optimization](https://arxiv.org/pdf/1301.3342.pdf)  


<br>

__Discussion and Information:__{: style="color: red"}  
* __What is perplexity?__  
    Perplexity is a measure for information that is defined as 2 to the power of the Shannon entropy. The perplexity of a fair die with k sides is equal to k. In t-SNE, the perplexity may be viewed as a knob that sets the number of effective nearest neighbors. It is comparable with the number of nearest neighbors k that is employed in many manifold learners.  
* __Choosing the perplexity hp:__   
    The performance of t-SNE is fairly robust under different settings of the perplexity. The most appropriate value depends on the density of your data. Loosely speaking, one could say that a larger / denser dataset requires a larger perplexity. Typical values for the perplexity range between $$5$$ and $$50$$.  
* __Every time I run t-SNE, I get a (slightly) different result?__  
    In contrast to, e.g., PCA, t-SNE has a non-convex objective function. The objective function is minimized using a gradient descent optimization that is initiated randomly. As a result, it is possible that different runs give you different solutions. Notice that it is perfectly fine to run t-SNE a number of times (with the same data and parameters), and to select the visualization with the lowest value of the objective function as your final visualization.  
* __Assessing the "Quality of Embeddings/visualizations":__  
    Preferably, just look at them! Notice that t-SNE does not retain distances but probabilities, so measuring some error between the Euclidean distances in high-D and low-D is useless. However, if you use the same data and perplexity, you can compare the Kullback-Leibler divergences that t-SNE reports. It is perfectly fine to run t-SNE ten times, and select the solution with the lowest KL divergence.  
        



<br>



***
***

## Feature Selection
{: style="font-size: 1.60em"}
{: #content2}


### **Feature Selection**{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents2 #bodyContents21}  
__Feature Selection__ is the process of selecting a subset of relevant features (variables, predictors) for use in model construction.  

__Applications:__{: style="color: red"}  
{: #lst-p}
* Simplification of models to make them easier to interpret by researchers/users  
* Shorter training time  
* A way to handle _curse of dimensionality_  
* Reduction of Variance $$\rightarrow$$ Reduce Overfitting $$\rightarrow$$ Enhanced Generalization  

__Strategies/Approaches:__{: style="color: red"}  
{: #lst-p}
* __Wrapper Strategy__:  
    Wrapper methods use a predictive model to score feature subsets. Each new subset is used to train a model, which is tested on a hold-out set. Counting the number of mistakes made on that hold-out set (the error rate of the model) gives the score for that subset. As wrapper methods train a new model for each subset, they are very computationally intensive, but usually provide the best performing feature set for that particular type of model.  
    __e.g.__ __Search Guided by Accuracy__{: style="color: goldenrod"}, __Stepwise Selection__{: style="color: goldenrod"}   
* __Filter Strategy__:  
    Filter methods use a _proxy measure_ instead of the error rate _to score a feature subset_. This measure is chosen to be fast to compute, while still capturing the usefulness of the feature set.  
    Filter methods produce a feature set which is _not tuned to a specific model_, usually giving lower prediction performance than a wrapper, but are more general and more useful for exposing the relationships between features.  
    __e.g.__ __Information Gain__{: style="color: goldenrod"}, __pointwise-mutual/mutual information__{: style="color: goldenrod"}, __Pearson Correlation__{: style="color: goldenrod"}    
* __Embedded Strategy:__  
    Embedded methods are a catch-all group of techniques which perform feature selection as part of the model construction process.  
    __e.g.__ __LASSO__{: style="color: goldenrod"}  


<br>

### **Correlation Feature Selection**{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents2 #bodyContents22}  
The __Correlation Feature Selection (CFS)__ measure evaluates subsets of features on the basis of the following hypothesis:  
"__Good feature subsets contain features highly correlated with the classification, yet uncorrelated to each other__{: style="color: goldenrod"}".  

The following equation gives the __merit of a feature subset__ $$S$$ consisting of $$k$$ features:  
<p>$${\displaystyle \mathrm {Merit} _{S_{k}}={\frac {k{\overline {r_{cf}}}}{\sqrt {k+k(k-1){\overline {r_{ff}}}}}}.}$$</p>  
where, $${\displaystyle {\overline {r_{cf}}}}$$ is the average value of all feature-classification correlations, and $${\displaystyle {\overline {r_{ff}}}}$$ is the average value of all feature-feature correlations.  

The __CFS criterion__ is defined as follows:  
<p>$$\mathrm {CFS} =\max _{S_{k}}\left[{\frac {r_{cf_{1}}+r_{cf_{2}}+\cdots +r_{cf_{k}}}{\sqrt {k+2(r_{f_{1}f_{2}}+\cdots +r_{f_{i}f_{j}}+\cdots +r_{f_{k}f_{1}})}}}\right]$$</p>  

<br>

### **Feature Selection Embedded in Learning Algorithms**{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents2 #bodyContents23}  
* $$l_{1}$$-regularization techniques, such as sparse regression, LASSO, and $${\displaystyle l_{1}}$$-SVM
* Regularized trees, e.g. regularized random forest implemented in the RRF package
* Decision tree
* Memetic algorithm
* Random multinomial logit (RMNL)
* Auto-encoding networks with a bottleneck-layer
* Submodular feature selection

<br>

### **Information Theory Based Feature Selection Mechanisms**{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents2 #bodyContents24}  
There are different Feature Selection mechanisms around that __utilize mutual information for scoring the different features__.  
They all usually use the same algorithm:  
1. Calculate the mutual information as score for between all features ($${\displaystyle f_{i}\in F}$$) and the target class ($$c$$)
1. Select the feature with the largest score (e.g. $${\displaystyle argmax_{f_{i}\in F}(I(f_{i},c))}$$) and add it to the set of selected features ($$S$$)
1. Calculate the score which might be derived form the mutual information
1. Select the feature with the largest score and add it to the set of select features (e.g. $${\displaystyle {\arg \max }_{f_{i}\in F}(I_{derived}(f_{i},c))}$$)
5. Repeat 3. and 4. until a certain number of features is selected (e.g. $${\displaystyle \vert S\vert =l}$$)  



***
***

## Feature Extraction
{: style="font-size: 1.60em"}
{: #content3}

### **Feature Extraction**{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents3 #bodyContents31}  
__Feature Extraction__ starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations.  

In __dimensionality reduction__, feature extraction is also called __Feature Projection__, which is a method that transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist.  

__Methods/Algorithms:__{: style="color: red"}  
{: #lst-p}
* Independent component analysis  
* Isomap  
* Kernel PCA  
* Latent semantic analysis  
* Partial least squares  
* Principal component analysis  
* Autoencoder  
* Linear Discriminant Analysis (LDA)  
* Non-negative matrix factorization (NMF)


<br>


[Outliers](https://en.wikipedia.org/wiki/Outlier#Working_with_outliers)  
[Replacing Outliers](https://en.wikipedia.org/wiki/Robust_statistics#Replacing_outliers_and_missing_values)  
[Data Transformation - Outliers - Standardization](https://en.wikipedia.org/wiki/Data_transformation_(statistics))  
[PreProcessing in DL - Data Normalization](https://hadrienj.github.io/posts/Preprocessing-for-deep-learning/)  
[Imputation and Feature Scaling](https://towardsdatascience.com/the-complete-beginners-guide-to-data-cleaning-and-preprocessing-2070b7d4c6d)  
[Missing Data - Imputation](https://en.wikipedia.org/wiki/Missing_data#Techniques_of_dealing_with_missing_data)  
[Dim-Red - Random Projections](https://en.wikipedia.org/wiki/Random_projection)  
[F-Selection - Relief](https://en.wikipedia.org/wiki/Relief_(feature_selection))  
[Box-Cox Transf - outliers](https://www.statisticshowto.datasciencecentral.com/box-cox-transformation/)  
[ANCOVA](https://en.wikipedia.org/wiki/Analysis_of_covariance)  
[Feature Selection Methods](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/)  

***
***

TITLE: Activation Functions
LINK: research/dl/concepts/activation_funcs.md


[Comprehensive list of activation functions in neural networks with pros/cons
](https://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons)  



## Introduction
{: #content1}

1. **Activation Functions:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    In NNs, the __activation function__ of a node defines the output of that node given an input or set of inputs.  
    The activation function is an abstraction representing the rate of action potential firing in the cell.  
    <br>

2. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  

3. **Desirable Properties:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    * __Non-Linearity__:  
    When the activation function is non-linear, then a two-layer neural network can be proven to be a universal function approximator. The identity activation function does not satisfy this property. When multiple layers use the identity activation function, the entire network is equivalent to a single-layer model. 
    * __Range__:  
    When the range of the activation function is finite, gradient-based training methods tend to be more stable, because pattern presentations significantly affect only limited weights. When the range is infinite, training is generally more efficient because pattern presentations significantly affect most of the weights. In the latter case, smaller learning rates are typically necessary.  
    * __Continuously Differentiable__:  
    This property is desirable for enabling gradient-based optimization methods. The binary step activation function is not differentiable at 0, and it differentiates to 0 for all other values, so gradient-based methods can make no progress with it.
    * __Monotonicity__:  
    * When the activation function is monotonic, the error surface associated with a single-layer model is guaranteed to be convex.  
    * During the training phase, backpropagation informs each neuron how much it should influence each neuron in the next layer. If the activation function isn't monotonic then increasing the neuron's weight might cause it to have less influence, the opposite of what was intended.  
        > However, Monotonicity isn't required. Several papers use non monotonic trained activation functions.  
        > Gradient descent finds a local minimum even with non-monotonic activation functions. It might only take longer.  
    * From a biological perspective, an "activation" depends on the sum of inputs, and once the sum surpasses a threshold, "firing" occurs. This firing should happen even if the sum surpasses the threshold by a small or a large amount; making monotonicity a desirable property to not limit the range of the "sum".  
    * __Smoothness with Monotonic Derivatives__:  
    These have been shown to generalize better in some cases.  
    * __Approximating Identity near Origin__:  
    Equivalent to $${\displaystyle f(0)=0}$$ and $${\displaystyle f'(0)=1}$$, and $${\displaystyle f'}$$ is continuous at $$0$$.  
    When activation functions have this property, the neural network will learn efficiently when its weights are initialized with small random values. When the activation function does not approximate identity near the origin, special care must be used when initializing the weights.  
    * __Zero-Centered Range__:  
    Has effects of centering the data (zero mean) by centering the activations. Makes learning easier.   
    > [WHY NORMALIZING THE DATA/SIGNAL IS IMPORTANT](https://www.youtube.com/watch?v=FDCfw-YqWTE&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=10&t=0s)  

    <br>

4. **Undesirable Properties:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    * __Saturation__:  
    An activation functions output, with finite range, may saturate near its tail or head (e.g. $$\{0, 1\}$$ for sigmoid). This leads to a problem called __vanishing gradient__.  
    * __Vanishing Gradients__:  
    Happens when the gradient of an activation function is very small/zero. This usually happens when the activation function __saturates__ at either of its tails.  
    The chain-rule will *__multiply__* the local gradient (of activation function) with the whole objective. Thus, when gradient is small/zero, it will "kill" the gradient $$\rightarrow$$ no signal will flow through the neuron to its weights or to its data.  
    __Slows/Stops learning completely__.  
    * __Range Not Zero-Centered__:  
    This is undesirable since neurons in later layers of processing in a Neural Network would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. $$x>0$$ elementwise in $$f=w^Tx+b$$), then the gradient on the weights $$w$$ will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression $$f$$). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above.  
    __Makes optimization harder.__   
    <br>


***

## Activation Functions
{: #content2}

![img](/main_files/concepts/16.png){: max-width="180%" width="180%"}  

1. **Sigmoid:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    <p>$$S(z)=\frac{1}{1+e^{-z}} \\ S^{\prime}(z)=S(z) \cdot(1-S(z))$$</p>  
    ![img](/main_files/concepts/3.png){: width="68%" .center-image}  
    __Properties:__{: style="color: red"}  
    Never use as activation, use as an output unit for binary classification.  
    * __Pros__:  
        * Has a nice interpretation as the firing rate of a neuron  
    * __Cons__:  
        * They Saturate and kill gradients $$\rightarrow$$ Gives rise to __vanishing gradients__[^1] $$\rightarrow$$ Stop Learning  
        * Happens when initialization weights are too large  
        * or sloppy with data preprocessing  
        * Neurons Activation saturates at either tail of $$0$$ or $$1$$  
        * Output NOT __Zero-Centered__ $$\rightarrow$$ Gradient updates go too far in different directions $$\rightarrow$$ makes optimization harder   
        * The local gradient $$(z * (1-z))$$ achieves maximum at $$0.25$$, when $$z = 0.5$$. $$\rightarrow$$ very time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more) $$\rightarrow$$ with basic SGD, the lower layers of a network train much slower than the higher one  

2. **Tanh:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    <p>$$\tanh (z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \\ \tanh ^{\prime}(z)=1-\tanh (z)^{2}$$</p>  
    ![img](/main_files/concepts/4.png){: width="68%" .center-image}  

    __Properties:__{: style="color: red"}  
    Strictly superior to Sigmoid (scaled version of sigmoid \| stronger gradient). Good for activation.  
    * __Pros__:  
        * Zero Mean/Centered  
    * __Cons__:  
        * They Saturate and kill gradients $$\rightarrow$$ Gives rise to __vanishing gradients__[^1] $$\rightarrow$$ Stop Learning  
    <br>

3. **ReLU:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    <p>$$R(z)=\left\{\begin{array}{cc}{z} & {z>0} \\ {0} & {z<=0}\end{array}\right\} \\  R^{\prime}(z)=\left\{\begin{array}{ll}{1} & {z>0} \\ {0} & {z<0}\end{array}\right\}$$</p>  
    ![img](/main_files/concepts/5.png){: width="68%" .center-image}  

    __Properties:__{: style="color: red"}  
    The best for activation (Better gradients).  
    * __Pros__:  
        * Non-saturation of gradients which _accelerates convergence_ of SGD  
        * Sparsity effects and induced regularization. [discussion](https://stats.stackexchange.com/questions/176794/how-does-rectilinear-activation-function-solve-the-vanishing-gradient-problem-in/176905#176905)  
        * Not computationally expensive  
    * __Cons__:  
        * __ReLU not zero-centered problem__:  
        The problem that ReLU is not zero-centered can be solved/mitigated by using __batch normalization__, which normalizes the signal before activation:  
        > From paper: We add the BN transform immediately before the nonlinearity, by normalizing $$x =  Wu + b$$; normalizing it is likely to produce activations with a stable distribution.  
        > * [WHY NORMALIZING THE SIGNAL IS IMPORTANT](https://www.youtube.com/watch?v=FDCfw-YqWTE&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=10&t=0s)
        * __Dying ReLUs (Dead Neurons):__  
        If a neuron gets clamped to zero in the forward pass (it doesn‚Äôt "fire" / $$x<0$$), then its weights will get zero gradient. Thus, if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron‚Äôs weights ever get knocked off with a large update during training into this regime (usually as a symptom of aggressive learning rates), then this neuron will remain permanently dead.  
        * [**cs231n Explanation**](https://www.youtube.com/embed/gYpoJMlgyXA?start=1249){: value="show" onclick="iframePopA(event)"}
        <a href="https://www.youtube.com/embed/gYpoJMlgyXA?start=1249"></a>
            <div markdown="1"> </div>    
        * __Infinite Range__:  
        Can blow up the activation.  
    <br>

4. **Leaky-ReLU:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    <p>$$R(z)=\left\{\begin{array}{cc}{z} & {z>0} \\ {\alpha z} & {z<=0}\end{array}\right\} \\ 
        R^{\prime}(z)=\left\{\begin{array}{ll}{1} & {z>0} \\ {\alpha} & {z<0}\end{array}\right\}$$</p>  
    ![img](/main_files/concepts/6.png){: width="68%" .center-image}  

    __Properties:__{: style="color: red"}  
    Sometimes useful. Worth trying.  
    * __Pros__:  
        * Leaky ReLUs are one attempt to fix the ‚Äúdying ReLU‚Äù problem by having a small negative slope (of 0.01, or so).  
    * __Cons__:  
        The consistency of the benefit across tasks is presently unclear.  
    <br>

5. **ELU:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  

8. **Notes:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}  
    * It is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so.  
    * __Identity Mappings__:  
    When an activation function cannot achieve an identity mapping (e.g. ReLU map all negative inputs to zero); then adding extra depth actually decreases the best performance, in the case a shallower one would suffice (Deep Residual Net paper).  
    <br>

1. **Softmax:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    
    __Motivation:__{: style="color: red"}  
    {: #lst-p}
    * __Information Theory__ - from the perspective of information theory the softmax function can be seen as trying to minimize the cross-entropy between the predictions and the truth.  
    * __Probability Theory__ - from this perspective since $$\hat{y}_ i$$ represent log-probabilities we are in fact looking at the log-probabilities, thus when we perform exponentiation we end up with the raw probabilities. In this case the softmax equation find the MLE (Maximum Likelihood Estimate).  
        If a neuron's output is a log probability, then the summation of many neurons' outputs is a multiplication of their probabilities. That's more commonly useful than a sum of probabilities.  
    * It is a softened version of the __argmax__ function (limit as $$T \rightarrow 0$$)  

    * __Properties__{: style="color: red"}  
        * There is one nice attribute of Softmax as compared with standard normalisation:  
            It react to low stimulation (think blurry image) of your neural net with rather uniform distribution and to high stimulation (ie. large numbers, think crisp image) with probabilities close to 0 and 1.   
            While standard normalisation does not care as long as the proportion are the same.  
            Have a look what happens when soft max has 10 times larger input, ie your neural net got a crisp image and a lot of neurones got activated.  
            <button>Example SM</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
            > >>> softmax([1,2])              # blurry image of a ferret  
                [0.26894142,      0.73105858])  #     it is a cat perhaps !?  
                >>> softmax([10,20])            # crisp image of a cat  
                [0.0000453978687, 0.999954602]) #     it is definitely a CAT !   
            {: hidden=""}  
            And then compare it with standard normalisation:  
            <button>Example Normalization</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
            > >>> std_norm([1,2])                      # blurry image of a ferret  
                [0.3333333333333333, 0.6666666666666666] #     it is a cat perhaps !?  
                >>> std_norm([10,20])                    # crisp image of a cat  
                [0.3333333333333333, 0.6666666666666666] #     it is a cat perhaps !?  
            {: hidden=""}

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * Alternatives to Softmax:  
        * [AN EXPLORATION OF SOFTMAX ALTERNATIVES BELONGING TO THE SPHERICAL LOSS FAMILY (paper)](https://arxiv.org/pdf/1511.05042.pdf)  
        * [From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification (paper)](http://proceedings.mlr.press/v48/martins16.pdf)  
    <br>
    





***
***

TITLE: Gradient-Based Optimization
LINK: research/dl/concepts/grad_opt.md


[Optimizing Gradient Descent](http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms)  
[Blog: SGD, Momentum and Adaptive Learning Rate](https://deepnotes.io/sgd-momentum-adaptive)  
[EE227C Notes: Convex Optimization and Approximation](https://ee227c.github.io/notes/ee227c-notes.pdf)  
[Convex Functions](http://www.princeton.edu/~amirali/Public/Teaching/ORF523/S16/ORF523_S16_Lec7_gh.pdf)  
[Strong Convexity](http://xingyuzhou.org/blog/notes/strong-convexity)  
[Lipschitz continuous gradient (condition)](http://xingyuzhou.org/blog/notes/Lipschitz-gradient)  
[Conjugate Gradient](http://www.seas.ucla.edu/~vandenbe/236C/lectures/cg.pdf)  
[NOTES ON FIRST-ORDER METHODS FOR MINIMIZING SMOOTH FUNCTIONS](http://web.stanford.edu/class/msande318/notes/notes-first-order-smooth.pdf)  
[Gradient Descent (paperspace)](https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/)  
[An Intuitive Introduction to the Hessian for Deep Learning](http://mlexplained.com/2018/02/02/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1/)  
[NIPS Optimization Lecture!!](https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Large-Scale-Optimization-Beyond-Stochastic-Gradient-Descent-and-Convexity)  



## Gradient-Based Optimization
{: #content1}

1. **Gradient-Based Optimization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __Gradient Methods__ are algorithms to solve problems of the form:  
    <p>$$\min_{x \in \mathbb{R}^{n}} f(x)$$</p>  
    with the search directions defined by the __gradient__ of the function at the current point.  
    <br>

2. **Gradient-Based Algorithms:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    Examples, include:  
    * __Gradient Descent__: minimizes arbitrary differentiable functions. 
    * __Conjugate Gradient__: minimizes sparse linear systems w/ symmetric & positive-definite matrices.  
    * __Coordinate Descent__: minimizes functions of two variables.  
    <br>


***

## Gradient Descent
{: #content2}

1. **Gradient Descent:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    __Gradient Descent__ is a _first-order, iterative_ algorithm to minimize an objective function $$J(\theta)$$ parameterized by a model's parameters $$\theta \in \mathbb{R}^{d}$$ by updating the parameters in the opposite direction of the gradient of the objective function $$\nabla_{\theta} J(\theta)$$  w.r.t. to the parameters.  
    <br>

    
    __Intuition (for derivation):__{: style="color: red"}  
    1. __Local Search from a starting location on a hill__{: style="color: DarkMagenta"}
    1. __Feel around how a small movement/step around your location would change the height of the surrounding hill (is the ground higher or lower)__{: style="color: DarkGray"}
    1. __Make the movement/step consistent as a small fixed step along some direction__{: style="color: Olive"}
    1. __Measure the steepness of the hill at the new location in the chosen direction__{: style="color: MediumBlue"}
    1. __Do so by Approximating the steepness with some local information__{: style="color: Crimson"}
    1. __Find the direction that decreases the steepness the most__{: style="color: SpringGreen"}    

    $$\iff$$ _._{: hidden=""}  

    1. __Local Search from an initial point $$x_0$$ on a function__{: style="color: DarkMagenta"}
    1. __Explore the value of the function at different small nudges around $$x_0$$__{: style="color: DarkGray"}
    1. __Make the nudges consistent as a small fixed step $$\delta$$ along a normalized direction $$\hat{\boldsymbol{u}}$$__{: style="color: Olive"}
    1. __Evaluate the function at the new location $$x_0 + \delta \hat{\boldsymbol{u}}$$__{: style="color: MediumBlue"}
    1. __Do so by Approximating the function w/ first-order information (Taylor expansion)__{: style="color: Crimson"}
    1. __Find the direction $$\hat{\boldsymbol{u}}$$ that minimizes the function the most__{: style="color: SpringGreen"}  
    <br>

2. **Derivation:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    __A small change in $$\boldsymbol{x}$$:__{: style="color: red"}  
    We would like to know how would a small change in $$\boldsymbol{x}$$, namely $$\Delta \boldsymbol{x}$$ would affect the value of the function $$f(x)$$. This will allow us to evaluate the function:  
    <p>$$f(\mathbf{x}+\Delta \mathbf{x})$$</p>  
    to find the direction that makes $$f$$ decrease the fastest.  

    Let's set up $$\Delta \boldsymbol{x}$$, the change in $$\boldsymbol{x}$$, as a fixed step $$\delta$$ along some normalized direction $$\hat{\boldsymbol{u}}$$:  
    <p>$$\Delta \boldsymbol{x} = \delta \hat{\boldsymbol{u}}$$</p>  

    __The Gradient:__{: style="color: red"}  
    The gradient tells us how that _small change in $$f(\mathbf{x}+\Delta \mathbf{x})$$ affects $$f$$_ through the __first-order approximation__:  
    <p>$$f(\mathbf{x}+\Delta \mathbf{x}) \approx f(\mathbf{x})+\Delta \mathbf{x}^{T} \nabla_{\mathbf{x}} f(\mathbf{x})$$</p>  

    In the single variable case, $$f(x+\delta) \approx f(x)+\delta f'(x)$$, we know that $$f\left(x-\delta \operatorname{sign}\left(f^{\prime}(x)\right)\right)$$ is less than $$f(x)$$ for small enough $$\delta$$.  
    <span>We can thus reduce $$f(x)$$ by moving $$x$$ in small steps with the opposite sign of the derivative.</span>{: style="color: goldenrod"}   


    __The Change in $$f$$:__{: style="color: red"}  
    The change in the objective function is:  
    <p>$$\begin{aligned} \Delta f &= f(\boldsymbol{x}_ 0 + \Delta \boldsymbol{x}) - f(\boldsymbol{x}_ 0) \\
        &= f(\boldsymbol{x}_ 0 + \delta \hat{\boldsymbol{u}}) - f(\boldsymbol{x}_ 0)\\
        &= \delta \nabla_x f(\boldsymbol{x}_ 0)^T\hat{\boldsymbol{u}} + \mathcal{O}(\delta^2) \\
        &= \delta \nabla_x f(\boldsymbol{x}_ 0)^T\hat{\boldsymbol{u}} \\
        &\geq -\delta\|\nabla f(\boldsymbol{x}_ 0)\|_ 2
        \end{aligned}
    $$</p>   
    using the first order approximation above.  
    Notice:  
    <p>$$\nabla_x f(\boldsymbol{x}_ 0)^T\hat{\boldsymbol{u}} \in \left[-\|\nabla f(\boldsymbol{x}_ 0)\|_ 2, \|\nabla f(\boldsymbol{x}_ 0)\|_ 2\right]$$</p>  
    since $$\hat{\boldsymbol{u}}$$ is a unit vector; either aligned with $$\nabla_x f(\boldsymbol{x}_ 0)$$ or in the opposite direction; it contributes nothing to the magnitude of the dot product.  

    So, the $$\hat{\boldsymbol{u}}$$ that <span>changes the above inequality to equality, achieves the largest negative value</span>{: style="color: goldenrod"} (moves the most downhill). That vector $$\hat{\boldsymbol{u}}$$ is, then, the one in the negative direction of $$\nabla_x f(\boldsymbol{x}_ 0)$$; the opposite direction of the gradient.  


    __The Directional Derivative:__{: style="color: red"}  
    The directional derivative in direction $$\boldsymbol{u}$$ (a unit vector) is the slope of the function $$f$$ in direction $$\boldsymbol{u}$$. In other words, the directional derivative is the derivative of the function $$f(\boldsymbol{x}+\alpha \boldsymbol{u})$$ with respect to $$\delta$$, evaluated at $$\delta= 0$$.  
    Using the _chain rule_, we can see that $$\frac{\partial}{\partial \delta} f(\boldsymbol{x}+\delta \boldsymbol{u})$$ evaluates to $$\boldsymbol{u}^{\top} \nabla_{\boldsymbol{x}} f(\boldsymbol{x})$$ when $$\delta=0$$.  


    __Minimizing $$f$$:__{: style="color: red"}  
    To minimize $$f$$, we would like to find _the direction in which $$f$$ decreases the fastest_. We do so by using the __directional derivative__:  
    <p>$$\begin{aligned} & \min _{\boldsymbol{u}, \boldsymbol{u}^{\top} \boldsymbol{u}=1} \boldsymbol{u}^{\top} \nabla_{\boldsymbol{x}} f(\boldsymbol{x}) \\
    =& \min_{\boldsymbol{u}, \boldsymbol{u}^{\top} \boldsymbol{u}=1}\|\boldsymbol{u}\|_{2}\left\|\nabla_{\boldsymbol{x}} f(\boldsymbol{x})\right\|_{2} \cos \theta \\
    =& \min_{\boldsymbol{u}} \cos \theta \\ \implies& \boldsymbol{u} = -\nabla_x f(x)\end{aligned}$$</p>    
    by substituting $$\|\boldsymbol{u}\|_2 = 1$$ and ignoring factors that do not depend on $$\boldsymbol{u}$$, we get $$\min_{\boldsymbol{u}} \cos \theta$$; this is minimized when $$\boldsymbol{u}$$ points in the opposite direction as the gradient.  
    Or rather, because $$\hat{\boldsymbol{u}}$$ is a unit vector, we need:  
    <p>$$\hat{\boldsymbol{u}} = - \dfrac{\nabla_x f(x)}{\|\nabla_x f(x)\|_ 2}$$</p>  

    > In other words, the gradient points directly uphill, and the negative gradient points directly downhill.  We can decrease $$f$$ by moving in the direction of the negative gradient.  

    __The method of steepest/gradient descent:__{: style="color: red"}  
    Proposes a new point to decrease the value of $$f$$:  
    <p>$$\boldsymbol{x}^{\prime}=\boldsymbol{x}-\epsilon \nabla_{\boldsymbol{x}} f(\boldsymbol{x})$$</p>  
    where $$\epsilon$$ is the __learning rate__, defined as:  
    <p>$$\epsilon = \dfrac{\delta}{\left\|\nabla_{x} f(x)\right\|_ {2}}$$</p>    

    * [**Derivation Video**](https://www.youtube.com/embed/fpYC7KK5t7A){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/fpYC7KK5t7A"></a>
        <div markdown="1"> </div>    

    <br>

3. **The Learning Rate:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    The __learning rate__ is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient.  

    The learning rate comes from a modification of the step-size in the GD derivation.  
    We get the learning rate by employing a simple idea:  
    We have a __fixed step-size__ $$\delta$$ that dictated how much we should be moving in the direction of steepest descent. However, we would like to keep the step-size from being too small or overshooting. The idea is to *__make the step-size proportional to the magnitude of the gradient__* (i.e. some constant multiplied by the magnitude of the gradient):  
    <p>$$\delta = \epsilon \left\|\nabla_{x} f(x)\right\|_ {2}$$</p>   
    If we do so, we get a nice cancellation as follows:  
    <p>$$\begin{aligned}\Delta \boldsymbol{x} &= \delta \hat{\boldsymbol{u}}  \\
        &= -\delta \dfrac{\nabla_x f(x)}{\|\nabla_x f(x)\|_ 2} \\
        &= - \epsilon \left\|\nabla_{x} f(x)\right\|_ {2} \dfrac{\nabla_x f(x)}{\|\nabla_x f(x)\|_ 2} \\
        &= - \dfrac{\epsilon \left\|\nabla_{x} f(x)\right\|_ {2}}{\|\nabla_x f(x)\|_ 2} \nabla_x f(x) \\
        &= - \epsilon \nabla_x f(x)
    \end{aligned}$$</p>  
    where now we have a *__fixed learning rate__* instead of a _fixed step-size_.  

    __Choosing the Learning Rate:__{: style="color: red"}  
    {: #lst-p}
    * __Set it to a small constant__  
    * __Line Search__: evaluate $$f\left(\boldsymbol{x}-\epsilon \nabla_{\boldsymbol{x}} f(\boldsymbol{x})\right)$$ for several values of $$\epsilon$$ and choose the one that results in the smallest objective value.  
        Finds a local minimum along a search direction by solving an optimization problem in 1-D.  
        > e.g. for *__smooth $$f$$__*: __Secant Method__, __Newton-Raphson Method__ (may need Hessian, hard for large dims)  
            for *__non-smooth $$f$$__*: use __direct line search__ e.g. __golden section search__  
        > Note: usually NOT used in DL  
    * __Trust Region Method__  
    * __Grid Search__: is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. It is guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.  
    * [__Population-Based Training (PBT)__](https://deepmind.com/blog/article/population-based-training-neural-networks): is an elegant implementation of using a genetic algorithm for hyper-parameter choice.  
        In PBT, a population of models are created. They are all continuously trained in parallel. When any member of the population has had sufficiently long to train to show improvement, its validation accuracy is compared to the rest of the population. If its performance is in the lowest $$20\%$$, then it copies and mutates the hyper-parameters and variables of one of the top $$20\%$$ performers.  
        In this way, the most successful hyper-parameters spawn many slightly mutated variants of themselves and the best hyper-parameters are likely discovered.  
    * __Bayesian Optimization__: is a global optimization method for noisy black-box functions. Applied to hp optimization, it builds a probabilistic model of the function mapping from hp values to the objective evaluated on a validation set. By iteratively evaluating a promising hp configuration based on the current model, and then updating it, it, aims to gather observations revealing as much information as possible about this function and, in particular, the location of the optimum. It tries to balance exploration (hps for which the outcome is most uncertain) and exploitation (hps expected close to the optimum).  
        In practice, it has been shown to obtain better results in fewer evaluations compared to grid search and random search, due to the ability to reason about the quality of experiments before they are run.  
            

    __Line Search VS Trust Region:__  
    Trust-region methods are in some sense dual to line-search methods: trust-region methods first choose a step size (the size of the trust region) and then a step direction, while line-search methods first choose a step direction and then a step size.  


    __Learning Rate Schedule:__{: style="color: red"}  
    A learning rate schedule changes the learning rate during learning and is most often changed between epochs/iterations. This is mainly done with two parameters: __decay__ and __momentum__.  
    * __Decay__: serves to settle the learning in a nice place and avoid oscillations, a situation that may arise when a too high constant learning rate makes the learning jump back and forth over a minima, and is controlled by a hyperparameter.  
    * __Momentum__: is analogous to a ball rolling down a hill; we want the ball to settle at the lowest point of the hill (corresponding to the lowest error). Momentum both speeds up the learning (increasing the learning rate) when the error cost gradient is heading in the same direction for a long time and also avoids local minima by 'rolling over' small bumps.  
            

    __Types of learning rate schedules for Decay:__  
    {: #lst-p}
    * __Time-based__ learning schedules alter the learning rate depending on the learning rate of the previous time iteration. Factoring in the decay the mathematical formula for the learning rate is:  
        <p>$${\displaystyle \eta_{n+1}={\frac {\eta_{n}}{1+dn}}}$$</p>  
        where $$\eta$$ is the learning rate, $$d$$ is a decay parameter and $$n$$ is the iteration step.  
    * __Step-based__ learning schedules changes the learning rate according to some pre defined steps:  
        <p>$${\displaystyle \eta_{n}=\eta_{0}d^{floor({\frac {1+n}{r}})}}$$</p>   
        where $${\displaystyle \eta_{n}}$$ is the learning rate at iteration $$n$$, $$\eta_{0}$$ is the initial learning rate, $$d$$ is how much the learning rate should change at each drop (0.5 corresponds to a halving) and $$r$$ corresponds to the droprate, or how often the rate should be dropped ($$10$$ corresponds to a drop every $$10$$ iterations). The floor function here drops the value of its input to $$0$$ for all values smaller than $$1$$.  
    * __Exponential__ learning schedules are similar to step-based but instead of steps a decreasing exponential function is used. The mathematical formula for factoring in the decay is:  
        <p>$$ {\displaystyle \eta_{n}=\eta_{0}e^{-dn}}$$</p>  
        where $$d$$ is a decay parameter.  
    <br>

4. **Convergence:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    Gradient Descent converges when every element of the gradient is zero, or very close to zero within some threshold.  

    With certain assumptions on $$f$$ (convex, $$\nabla f$$ lipschitz) and particular choices of $$\epsilon$$ (chosen via line-search etc.), convergence to a local minimum can be guaranteed.  
    Moreover, if $$f$$ is convex, all local minima are global minimia, so convergence is to the global minimum.  
    <br>

5. **Choosing (tuning) the hyperparameters:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    We can set/tune most hyperparameters by reasoning about their effect on __model capacity__.  
    <button>Effect of HPs on model capacity</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/uEjsgHkxOZlxf4mXE6XnJYqegpjvoQfKG28_A4nG0Fk.original.fullsize.png){: width="100%" hidden=""}  

    __Important HPs:__  
    {: #lst-p}
    1. Learning Rate  
    1. \# Hidden Units  
    1. Mini-batch Size  
    1. Momentum Coefficient  

    
    __Hyperparameter search:__{: style="color: red"}  
    Sample at random in a grid (hypercube) of different parameters, then zoom in to a tighter range of "good" values.  
    Search (sample) on a logarithmic scale to get uniform sizes between values:  
    * Select value $$r \in [a, b]$$ (e.g. $$\in [-4, 0]$$, and set your hp as $$10^r$$ (e.g. $$\epsilon = 10^{r}$$). You'll be effectively sampling $$\in [10^{-4}, 10^0] \iff [0.0001, 1]$$.     
    <br>



8. **Notes:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}  
    * Gradient descent can be viewed as __applying Euler's method for solving ordinary differential equations $${\displaystyle x'(t)=-\nabla f(x(t))}$$ to a [gradient flow](https://en.wikipedia.org/wiki/Vector_field#Gradient_field_in_euclidean_spaces)__.  
    * Neural nets are unconstrained optimization problems with many, many local minima. They sometimes benefit from line search or second-order optimization algorithms, but when the input data set is very large, researchers often favor the dumb, blind, stochastic versions of gradient descent.  
    * Grid search suffers from the curse of dimensionality, but is often embarrassingly parallel because typically the hyperparameter settings it evaluates are independent of each other.  
    <br>

***

## Gradient Descent Variants
{: #content3}

There are three variants of gradient descent, which differ in the amount of data used to compute the gradient. The amount of data imposes a trade-off between the accuracy of the parameter updates and the time it takes to perform the update.  


1. **Batch Gradient Descent:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    __Batch Gradient Descent__ AKA __Vanilla Gradient Descent__, computes the gradient of the objective wrt. the parameters $$\theta$$ for the entire dataset:  
    <p>$$\theta=\theta-\epsilon \cdot \nabla_{\theta} J(\theta)$$</p>  

    Since we need to compute the gradient for the entire dataset for each update, this approach can be very slow and is intractable for datasets that can't fit in memory.  
    Moreover, batch-GD doesn't allow for an _online_ learning approach.  
    <br>

2. **Stochastic Gradient Descent:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    __SGD__ performs a parameter update for each data-point:  
    <p>$$\theta=\theta-\epsilon \cdot \nabla_{\theta} J\left(\theta ; x^{(i)} ; y^{(i)}\right)$$</p>  

    SGD exhibits a lot of fluctuation and has a lot of variance in the parameter updates. However, although, SGD can potentially move in the wrong direction due to limited information; in-practice, if we slowly decrease the learning-rate, it shows the same convergence behavior as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively.  
    Moreover, the fluctuations it exhibits enables it to jump to new and potentially better local minima.  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * __Why reduce the learning rate after every epoch?__  
        This is due to the fact that the random sampling of batches acts as a source of noise which might make SGD keep oscillating around the minima without actually reaching it.  
        It is necessary to guarantee convergence.  
    * __The following conditions guarantee convergence under convexity conditions for SGD__:  
        <p>$$\begin{array}{l}{\sum_{k=1}^{\infty} \epsilon_{k}=\infty, \quad \text { and }} \\ {\sum_{k=1}^{\infty} \epsilon_{k}^{2}<\infty}\end{array}$$</p>  
    * [Stochastic Gradient Descent Escapes Saddle Points Efficiently (M.J.)](https://arxiv.org/pdf/1902.04811.pdf)  
            
    <br>

3. **Mini-batch Gradient Descent:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}   
    A hybrid approach that perform updates for a, pre-specified, mini-batch of $$n$$ training examples:  
    <p>$$\theta=\theta-\epsilon \cdot \nabla_{\theta} J\left(\theta ; x^{(i : i+n)} ; y^{(i : i+n)}\right)$$</p>

    This allows it to:  
    1. Reduce the variance of the parameter updates $$\rightarrow$$ more stable convergence
    2. Makes use of matrix-vector highly optimized libraries  



***

## Gradient Descent "Optimization"
{: #content4}

1. **Challenges in vanilla approaches to gradient descent:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    All the variants described above, however, do not guarantee _"good" convergence_ due to some challenges:  
    * Choosing a proper learning rate is usually difficult:  
        A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.  
    *  Learning rate schedules[^1] try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset's characteristics[^2].  
    * The learning rate is _fixed_ for all parameter updates:  
        If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.  
    * Another key challenge of minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima. Dauphin et al.[^3] argue that the difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.  
    <br>

22. **Preliminaries - Important Concepts:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents422}  
    __Exponentially Weighted Averages:__{: style="color: red"}  
    [EWAs (NG)](https://www.youtube.com/watch?v=lAq96T8FkTw&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=17)  
    <p>$$v_{t}=\beta v_{t-1}+ (1-\beta) \theta_{t}$$</p>  
    You can think of $$v_t$$ as approximately averaging over $$\approx \dfrac{1}{1-\beta}$$ previous values $$\theta_i$$.  

    The _larger_ the $$\beta$$ the _slower_ $$v_t$$ adapts to changes in (new) $$\theta$$ and the _less_ __noisy__ the value of $$v_t$$.  

    __Intuition:__{: style="color: brown"}  
    It is a recursive equation. Thus, 
    <p>$$v_{100} = (1-\beta) \theta_{100} + (1-\beta)\beta \theta_{99} + (1-\beta)\beta^{2} \theta_{98} + (1-\beta)\beta^{3} \theta_{97} + (1-\beta)\beta^{4} \theta_{96} + \ldots + (1-\beta)\beta^{100} \theta_{1}$$</p>  
    * It is an element-wise product between the values of $$\theta_i$$ and an __exponentially decaying__ function $$v(i)$$.  
        For _$$ T=100, \beta=0.9$$_:  
        ![img](https://cdn.mathpix.com/snip/images/e4SvtXH_4R88TdhgIJmfbhSMgTqc4lt5QJOnpi-hBuw.original.fullsize.png){: width="34%"}  
    * The sum of the coefficients of $$\theta_i$$ is equal to $$\approx 1$$   
        > But not exactly $$1$$ which is why __bias correction__ is needed.  
    * It takes about $$(1-\beta)^{\dfrac{1}{\beta}}$$ time-steps for $$v$$ to decay to about a third of its peak value. So, after $$(1-\beta)^{\dfrac{1}{\beta}}$$ steps, the weight decays to about a third of the weight of the current time-step $$\theta$$ value.  
        In general:   
        <p>$$(1-\epsilon)^{\dfrac{1}{\epsilon}} \approx \dfrac{1}{e} \approx 0.35 \approx \dfrac{1}{3}$$</p>    
    [EWAs Intuition (NG)](https://www.youtube.com/watch?v=NxTFlzBjS-4&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=18)  
    <br>
    <br>

    __Exponentially Weighted Averages Bias Correction:__{: style="color: red"}  

    __The Problem:__{: style="color: brown"}  
    The estimate of the first value $$\theta_1$$ will not be a good estimate of because it will be multiplied by $$(1-\beta) << 1$$. This will be a much lower estimate especially during the initial phase of the estimate. It will produce the _purple_ curve instead of the _green_ curve:  
    ![img](https://cdn.mathpix.com/snip/images/_IAbXpX9_bnr1pvchJG3UxN-354OAsrmDoFpFYocI8g.original.fullsize.png){: width="50%"}  

    __Bias Correction:__{: style="color: brown"}    
    Replace $$v_t$$ with:  
    $$\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\: \dfrac{v_{t}}{1-\beta^{t}}$$   
    * __Small $$t$$:__ $$\implies \beta^t$$ is large $$\implies \dfrac{1}{1-\beta^t}$$ is large  
    * __Large $$t$$__: $$\implies \beta^t$$ is large $$\implies \dfrac{1}{1-\beta^t} \approx 1$$  
    <br> 


2. **Momentum:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}  
    __Motivation:__{: style="color: red"}  
    SGD has trouble navigating _ravines_ (i.e. areas where the surface curves much more steeply in one dimension than in another[^4]) which are common around local optima.  
    In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.  

    __Momentum:__{: style="color: red"}  
    __Momentum__[^5] is a method that helps accelerate SGD in the relevant direction and dampens oscillations (image^). It does this by adding a fraction $$\gamma$$ of the update vector of the past time step to the current update vector:  
    <p>$$\begin{aligned} v_{t} &=\gamma v_{t-1}+\eta \nabla_{\theta} J(\theta) \\ \theta &=\theta-v_{t} \end{aligned}$$</p>  
    > Note: Some implementations exchange the signs in the equations. The momentum term $$\gamma$$ is usually set to $$0.9$$ or a similar value, and $$v_0 = 0$$.  

    ![img](https://cdn.mathpix.com/snip/images/jnTKkZmZsRZXw_8BNtgUxb7tZEg68sgRCspxVOmTw0I.original.fullsize.png){: width="70%"}  

    Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e.  $$\gamma < 1$$). The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.  
    In this case we think of the equation as:  
    <p>$$v_{t} =\underbrace{\gamma}_{\text{friction }} \: \underbrace{v_{t-1}}_{\text{velocity}}+\eta \underbrace{\nabla_{\theta} J(\theta)}_ {\text{acceleration}}$$</p>  
    > Instead of using the gradient to change the position of the weight "particle," use it to change the velocity. - Hinton  


    * [**Momentum NG**](https://www.youtube.com/embed/k8fTYJPd3_I){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/k8fTYJPd3_I"></a>
        <div markdown="1"> </div>    
        __Momentum Calculation (EWAs):__  
        <p>$$\begin{align} 
            v_{dw} &= \beta\:v_{dw}+(1-\beta) dw \\ 
            v_{db} &=\beta\:v_{db}+(1-\beta) db \end{align}$$</p>  
        __Parameter Updates:__  
        <p>$$\begin{align} 
            w &= w - \epsilon\:v_{dw} \\ 
            b &= b - \epsilon\:v_{d_b} \end{align}$$</p>  
    * [**Why Momentum Really Works (distill)**](https://distill.pub/2017/momentum/){: value="show" onclick="iframePopA(event)"}
    <a href="https://distill.pub/2017/momentum/"></a>
        <div markdown="1"> </div>    

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * Bias Correction is NOT used in practice; only 10 iterations needed to catch up.  
    * The $$(1-\beta)$$ coefficient usually gets dropped in the literature. The effect is that that lr needs to be rescaled which is not a problem.  
    * [Learning representations by back-propagating errors (Rumelhart, Hinton)](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)  
    * [visualizing Momentum (video)](https://www.youtube.com/watch?v=7HZk7kGk5bU)  
    <br>


3. **Nesterov Accelerated Gradient:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}  
    __Motivation:__{: style="color: red"}  
    Momentum is good, however, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We'd like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.  

    __Nesterov Accelerated Gradient (NAG):__{: style="color: red"}  
    __NAG__[^6] is a way to five our momentum term this kind of prescience. Since we know that we will use the momentum term $$\gamma\:v_{t-1}$$ to move the parameters $$\theta$$, we can compute a rough approximation of the next position of the parameters with $$\theta - \gamma v_{t-1}$$ (w/o the gradient). This allows us to, effectively, look ahead by calculating the gradient not wrt. our current parameters $$\theta$$ but wrt. the approximate future position of our parameters:  
    <p>$$\begin{aligned} v_{t} &=\gamma v_{t-1}+\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right) \\ \theta &=\theta-v_{t} \end{aligned}$$</p>  
    > $$\gamma = 0.9$$,  

    While Momentum first computes the current gradient (small blue vector) and then takes a big jump in the direction of the updated accumulated gradient (big blue vector), NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (red vector), which results in the complete NAG update (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks[^7].  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * This really helps the optimization of __recurrent neural networks__[^8]  
    * Momentum allows us to __adapt our updates to the slope of our error function__ and speed up SGD  
    * [A Dynamical Systems Perspective on Nesterov Acceleration (M.Jordan)](https://arxiv.org/pdf/1905.07436.pdf)  
    <br> 

4. **Adagrad:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents44}  
    __Motivation:__{: style="color: red"}  
    Now that we are able to __adapt our updates to the slope of our error function__{: style="color: goldenrod"} and speed up SGD in turn, we would also like to __adapt our updates to each individual parameter__{: style="color: goldenrod"} to perform larger or smaller updates depending on their importance.  
    > The magnitude of the gradient can be very different for different weights and can change during learning: This makes it <span>hard to choose single global learning rate</span>{: style="color: goldenrod"}.  - Hinton


    __Adagrad:__{: style="color: red"}  
    __Adagrad__[^9] is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features.  

    __Adagrad per-parameter update:__  
    Adagrad uses a different learning rate for every parameter $$\theta_i$$ at every time step $$t$$, so, we first show Adagrad's per-parameter update.  
    
    The SGD update for every parameter $$\theta_i$$ at each time step $$t$$ is:  
    <p>$$\theta_{t+1, i}=\theta_{t, i}-\eta \cdot g_{t, i}$$</p>  
    where $$g_{t, i}=\nabla_{\theta} J\left(\theta_{t, i}\right)$$, is the partial derivative of the objective function w.r.t. to the parameter $$\theta_i$$ at time step $$t$$, and $$g_{t}$$ is the gradient at time-step $$t$$.  

    In its update rule, Adagrad modifies the general learning rate $$\eta$$ at each time step $$t$$ for every parameter $$\theta_i$$ based on the past gradients that have been computed for $$\theta_i$$:  
    <p>$$\theta_{t+1, i}=\theta_{t, i}-\frac{\eta}{\sqrt{G_{t, i i}+\epsilon}} \cdot g_{t, i}$$</p>  

    $$G_t \in \mathbb{R}^{d \times d}$$ here is a diagonal matrix where each diagonal element $$i, i$$ is the sum of the squares of the gradients wrt $$\theta_i$$ up to time step $$t$$[^12], while $$\epsilon$$ is a smoothing term that avoids division by zero ($$\approx 1e - 8$$).  
    > Without the sqrt, the algorithm performs __much worse__  

    As $$G_t$$ contains the sum of the squares of the past gradients w.r.t. to all parameters $$\theta$$ along its diagonal, we can now vectorize our implementation by performing a matrix-vector product $$\odot$$ between $$G_t$$ and  $$g_t$$:  
    <p>$$\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{G_{t}+\epsilon}} \odot g_{t}$$</p>  


    __Properties:__{: style="color: red"}  
    {: #lst-p}
    * Well-suited for dealing with __sparse data__ (because it adapts the lr of each parameter wrt __feature frequency__)  
        * Pennington et al.[^11] used Adagrad to train GloVe word embeddings, as infrequent words require much larger updates than frequent ones.  
    * __Eliminates need for manual tuning of lr__:  
        One of Adagrad's main benefits is that it eliminates the need to manually tune the learning rate. Most implementations use a default value of $$0.01$$ and leave it at that.  
    * __Weakness -> Accumulation of the squared gradients in the denominator__:  
        Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge.  
    * [Visualization - How adaptive gradient methods speedup convergence](https://www.youtube.com/watch?v=Cy2g9_hR-5Y)  
    <br>

5. **Adadelta:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents45}  
    __Motivation:__{: style="color: red"}  
    __Adagrad__ has a weakness where it suffers from __aggressive, monotonically decreasing lr__{: style="color: goldenrod"} by _accumulation of the squared gradients in the denominator_. The following algorithm aims to resolve this flow.    

    __Adadelta:__{: style="color: red"}  
    Adadelta[^13] is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size $$w$$.  

    Instead of inefficiently storing $$w$$ previous squared gradients, the sum of gradients is recursively defined as a decaying average of all past squared gradients. The running average $$E\left[g^{2}\right]_ {t}$$ at time step $$t$$ then depends (as a fraction $$\gamma$$ similarly to the Momentum term) only on the previous average and the current gradient:  
    <p>$$E\left[g^{2}\right]_{t}=\gamma E\left[g^{2}\right]_{t-1}+(1-\gamma) g_{t}^{2}$$</p>  

    We set $$\gamma$$ to a similar value as the momentum term, around $$0.9$$.  
    For clarity, we now rewrite our vanilla SGD update in terms of the parameter update vector $$\Delta \theta_{t}$$:
    <p>$$\begin{aligned} \Delta \theta_{t} &=-\eta \cdot g_{t, i} \\ \theta_{t+1} &=\theta_{t}+\Delta \theta_{t} \end{aligned}$$</p>  

    In the __parameter update vector__ of Adagrad, we replace the diagonal matrix $$G_t$$ with the _decaying average over pas squared gradients_ $$E[g^2]_ t$$:  
    <p>$$-\frac{\eta}{\sqrt{E[g^2]_ t+\epsilon}} \odot g_{t}$$</p>  
    Since the denominator is just the __root mean squared (RMS) error criterion__ _of the gradient_, we can replace it with the criterion short-hand:  
    <p>$$-\frac{\eta}{RMS[g]_{t}} \odot g_{t}$$</p>  

    This __*modified* parameter update vector__ does NOT have the same __hypothetical units__ as the parameter.  
    We accomplish this by first defining another __exponentially decaying average__, this time not of squared gradients but __of squared parameter updates__:  
    <p>$$E[\Delta \theta^2]_t = \gamma E[\Delta \theta^2]_{t-1} + (1 - \gamma) \Delta \theta^2_t$$</p>  
    The __RMS Error of parameter updates__ is thus:  
    <p>$$RMS[\Delta \theta]_ {t} = \sqrt{E[\Delta \theta^2]_ t + \epsilon}$$</p>  

    Since $$RMS[\Delta \theta]_ {t}$$ is __unknown__, we approximate it with the $$RMS$$ of parameter updates up to (until) the previous time step. Replacing the learning rate $$\eta$$ in the previous update rule with $$RMS[\Delta \theta]_ {t-1}$$ finally yields the Adadelta update rule:  
    <p>$$\begin{align}  \begin{split}  \Delta \theta_t &= - \dfrac{RMS[\Delta \theta]_{t-1}}{RMS[g]_{t}} g_{t} \\  \theta_{t+1} &= \theta_t + \Delta \theta_t  \end{split}  \end{align}$$</p>  
    <br>

    __Properties:__{: style="color: red"}  
    {: #lst-p}
    * __Eliminates need for lr completely__:  
        With Adadelta, we do not even need to set a default learning rate, as it has been eliminated from the update rule.  
    <br>


6. **RMSprop:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents46}  
    __Motivation:__   
    RMSprop and Adadelta have both been developed independently around the same time stemming from the need to __resolve Adagrad's radically diminishing learning rates__.  

    __RMSprop:__{: style="color: red"}  
    __RMSprop__ is an unpublished, adaptive learning rate method proposed by Geoff Hinton in [Lecture 6e of his Coursera Class](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf).  

    RMSprop in fact is identical to the first update vector of Adadelta that we derived above:  
    <p>$$\begin{align}  \begin{split}  E[g^2]_t &= 0.9 E[g^2]_{t-1} + 0.1 g^2_t \\  \theta_{t+1} &= \theta_{t} - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}  \end{split}  \end{align}$$</p>  

    RMSprop as well __divides the learning rate by an exponentially decaying average of squared gradients__{: style="color: goldenrod"}.  
    Hinton suggests $$\gamma$$ to be set to $$0.9$$, while a good default value for the learning rate $$\eta$$ is $$0.001$$.  


    __RMSprop as an extension of Rprop:__{: style="color: red"}  
    Hinton, actually, thought of RMSprop as a way of extending _Rprop_ to work with __mini-batches__.  
    <button>Why Rprop does not work with mini-batches?</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/KxK61epXdFZw6Rqb7mwqXSIDUouk-TWeNdLu3M_wBFU.original.fullsize.png){: width="100%" hidden=""}  
    __Rprop:__ is equivalent to using the gradient but also dividing by the magnitude of the gradient.  
    The problem with mini-batch rprop is that we divide by a different number for each mini-batch.  
    So why not __force the number we divide by to be very similar for adjacent mini-batches__?  
    That is the idea behind RMSprop.  
    <br>

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * It is of note that Hinton has tried to add _momentum_ to RMSprop and found that "it does not help as much as it normally does - needs more investigation".  
    * [Visualizing Rprop - How adaptive gradient methods speedup convergence](https://www.youtube.com/watch?v=Cy2g9_hR-5Y)  


7. **Adam:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents47}  
    __Motivation:__  
    Adding __momentum__ to Adadelta/RMSprop.  

    __Adam:__{: style="color: red"}  
    __Adaptive Moment Estimation (Adam)__[^14] is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients $$v_t$$ like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients $$m_t$$, similar to momentum.  

    __Adam VS Momentum:__  
    Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface[^15].  

    We compute the decaying averages of past and past squared gradients $$m_t$$ and $$v_t$$ respectively as follows:  
    <p>$$\begin{align}  \begin{split}  m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\  v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2  \end{split}  \end{align}$$</p>  

    $$m_t$$ and $$v_t$$ are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method.  
    As $$m_t$$ and $$v_t$$ are initialized as vectors of $$0$$'s, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. $$\beta_1$$ and $$\beta_2$$ are close to $$1$$).  

    They counteract these biases by computing bias-corrected first and second moment estimates:  
    <p>$$\begin{align}  \begin{split}  \hat{m}_ t &= \dfrac{m_t}{1 - \beta^t_1} \\  \hat{v}_ t &= \dfrac{v_t}{1 - \beta^t_2} \end{split}  \end{align}$$</p>  

    They then use these to update the parameters just as we have seen in Adadelta and RMSprop, which yields the Adam update rule:
    <p>$$\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_ t} + \epsilon} \hat{m}_ t$$</p>  

    The authors propose default values of $$0.9$$ for $$\beta_1$$, $$0.999$$ for $$\beta_2$$, and $$10^{ ‚àí8}$$ for $$\epsilon$$. They show empirically that Adam works well in practice and compares favorably to other adaptive learning-method algorithms.  
    <br>

8. **[AdaMax](http://ruder.io/optimizing-gradient-descent/index.html#adamax)**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents38}  

9. **[Nadam](http://ruder.io/optimizing-gradient-descent/index.html#nadam)**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents39}  

10. **[AMSGrad](http://ruder.io/optimizing-gradient-descent/index.html#amsgrad)**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents310}  

11. **Visualization of the Algorithms**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents311}  
    __SGD optimization on loss surface contours:__{: style="color: red"}  
    ![img](/main_files/dl/concepts/grad_opt/1.gif){: width="100%"}  

    We see their behavior on the contours of a loss surface (the Beale function) over time. Note that Adagrad, Adadelta, and RMSprop almost immediately head off in the right direction and converge similarly fast, while Momentum and NAG are led off-track, evoking the image of a ball rolling down the hill. NAG, however, is quickly able to correct its course due to its increased responsiveness by looking ahead and heads to the minimum.  

    <br>

    __SGD optimization on saddle point:__{: style="color: red"}  
    ![img](/main_files/dl/concepts/grad_opt/2.gif){: width="100%"}  

    Image shows the behavior of the algorithms at a saddle point, i.e. a point where one dimension has a positive slope, while the other dimension has a negative slope, which pose a difficulty for SGD as we mentioned before. Notice here that SGD, Momentum, and NAG find it difficulty to break symmetry, although the two latter eventually manage to escape the saddle point, while Adagrad, RMSprop, and Adadelta quickly head down the negative slope.  


    __Analysis:__{: style="color: red"}  
    As we can see, the adaptive learning-rate methods, i.e. Adagrad, Adadelta, RMSprop, and Adam are most suitable and provide the best convergence for these scenarios.  


    [__Tutorial for Visualization__](http://louistiao.me/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/)  


12. **Analysis - Choosing an Optimizer:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents312}  
    * __Sparse Input Data__:  
        If your input data is sparse, then you likely achieve the best results using one of the adaptive learning-rate methods. An additional benefit is that you won't need to tune the learning rate but likely achieve the best results with the default value.  

    In summary, RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the numerator update rule. Adam, finally, adds bias-correction and momentum to RMSprop. Insofar, RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances. Kingma et al.[^14] show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, Adam might be the best overall choice.  

    Interestingly, many recent papers use vanilla SGD without momentum and a simple learning rate annealing schedule. As has been shown, SGD usually achieves to find a minimum, but it might take significantly longer than with some of the optimizers, is much more reliant on a robust initialization and annealing schedule, and may get stuck in saddle points rather than local minima. Consequently, if you care about fast convergence and train a deep or complex neural network, you should choose one of the adaptive learning rate methods.

***

## [Parallelizing and distributing SGD](http://ruder.io/optimizing-gradient-descent/index.html#parallelizinganddistributingsgd)  
{: #content5}


***

## [Additional strategies for optimizing SGD](http://ruder.io/optimizing-gradient-descent/index.html#additionalstrategiesforoptimizingsgd)  
{: #content6}

<p class="message">For a great overview of some other common tricks, refer to[^24]</p>  



1. **[Shuffling and Curriculum Learning](http://ruder.io/optimizing-gradient-descent/index.html#shufflingandcurriculumlearning):**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents61}  
    Generally, we want to avoid providing the training examples in a meaningful order to our model as this may bias the optimization algorithm. Consequently, it is often a good idea to shuffle the training data after every epoch.  

    On the other hand, for some cases where we aim to solve progressively harder problems, supplying the training examples in a meaningful order may actually lead to improved performance and better convergence. The method for establishing this meaningful order is called Curriculum Learning[^25].  

    Zaremba and Sutskever[^26] were only able to train LSTMs to evaluate simple programs using Curriculum Learning and show that a combined or mixed strategy is better than the naive one, which sorts examples by increasing difficulty.  
    <br>

2. **[Batch Normalization](http://ruder.io/optimizing-gradient-descent/index.html#batchnormalization):**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents62}  
    To facilitate learning, we typically normalize the initial values of our parameters by initializing them with zero mean and unit variance. As training progresses and we update parameters to different extents, we lose this normalization, which slows down training and amplifies changes as the network becomes deeper.  

    Batch normalization[^27] reestablishes these normalizations for every mini-batch and changes are back-propagated through the operation as well. By making normalization part of the model architecture, we are able to use higher learning rates and pay less attention to the initialization parameters. Batch normalization additionally acts as a regularizer, reducing (and sometimes even eliminating) the need for Dropout.  

    [Goodfellow on BN](https://www.youtube.com/watch?v=Xogn6veSyxA&feature=youtu.be&t=326s)  
    [Excellent Blog on BN](https://rohanvarma.me/Batch-Norm/)  
    <br>

3. **[Early Stopping](http://ruder.io/optimizing-gradient-descent/index.html#earlystopping):**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents63}  
    According to Geoff Hinton: "Early stopping (is) beautiful free lunch" ([NIPS 2015 Tutorial slides](https://media.nips.cc/Conferences/2015/tutorialslides/DL-Tutorial-NIPS2015.pdf), slide 63). You should thus always monitor error on a validation set during training and stop (with some patience) if your validation error does not improve enough.  
    <br>

4. **[Gradient Noise](http://ruder.io/optimizing-gradient-descent/index.html#gradientnoise):**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents64}  
    Neelakantan et al.[^28] add noise that follows a Gaussian distribution $$\mathcal{N}(0, \sigma^2_t)$$ to each gradient update:  
    <p>$$g_{t, i} = g_{t, i} + \mathcal{N}(0, \sigma^2_t)$$</p>  

    They anneal the variance according to the following schedule:  
    <p>$$\sigma^2_t = \dfrac{\eta}{(1 + t)^\gamma}$$</p>  

    They show that adding this noise makes networks more robust to poor initialization and helps training particularly deep and complex networks. They suspect that the added noise gives the model more chances to escape and find new local minima, which are more frequent for deeper models.  


***

## [Further Advances in DL Optimization](http://ruder.io/deep-learning-optimization-2017/index.html)  
{: #content7}


[^1]: H. Robinds and S. Monro, ‚ÄúA stochastic approximation method,‚Äù Annals of Mathematical Statistics, vol. 22, pp. 400‚Äì407, 1951.
[^2]: Darken, C., Chang, J., &amp; Moody, J. (1992). Learning rate schedules for faster stochastic gradient search. Neural Networks for Signal Processing II Proceedings of the 1992 IEEE Workshop, (September), 1‚Äì11. <a href="http://doi.org/10.1109/NNSP.1992.253713">http://doi.org/10.1109/NNSP.1992.253713</a>
[^3]: Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., &amp; Bengio, Y. (2014). Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. arXiv, 1‚Äì14. Retrieved from <a href="http://arxiv.org/abs/1406.2572">http://arxiv.org/abs/1406.2572</a>
[^4]: Sutton, R. S. (1986). Two problems with backpropagation and other steepest-descent learning procedures for networks. Proc. 8th Annual Conf. Cognitive Science Society.
[^5]: Qian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural Networks : The Official Journal of the International Neural Network Society, 12(1), 145‚Äì151. <a href="http://doi.org/10.1016/S0893-6080(98)00116-6">http://doi.org/10.1016/S0893-6080(98)00116-6</a>
[^6]: Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady ANSSSR (translated as Soviet.Math.Docl.), vol. 269, pp. 543‚Äì 547.
[^7]: Bengio, Y., Boulanger-Lewandowski, N., &amp; Pascanu, R. (2012). Advances in Optimizing Recurrent Networks. Retrieved from <a href="http://arxiv.org/abs/1212.0901">http://arxiv.org/abs/1212.0901</a>
[^8]: Sutskever, I. (2013). Training Recurrent neural Networks. PhD Thesis.
[^9]: Duchi, J., Hazan, E., &amp; Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121‚Äì2159. Retrieved from <a href="http://jmlr.org/papers/v12/duchi11a.html">http://jmlr.org/papers/v12/duchi11a.html</a>
[^10]: Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V, ‚Ä¶ Ng, A. Y. (2012). Large Scale Distributed Deep Networks. NIPS 2012: Neural Information Processing Systems, 1‚Äì11. <a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf">http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf</a>
[^11]: Pennington, J., Socher, R., &amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532‚Äì1543. <a href="http://doi.org/10.3115/v1/D14-1162">http://doi.org/10.3115/v1/D14-1162</a>
[^12]: Duchi et al. [3] give this matrix as an alternative to the <em>full</em> matrix containing the outer products of all previous gradients, as the computation of the matrix square root is infeasible even for a moderate number of parameters $$d$$.  
[^13]: Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. Retrieved from <a href="http://arxiv.org/abs/1212.5701">http://arxiv.org/abs/1212.5701</a>
[^14]: Kingma, D. P., &amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 1‚Äì13.
[^24]: LeCun, Y., Bottou, L., Orr, G. B., &amp; Muller, K. R. (1998). Efficient BackProp. Neural Networks: Tricks of the Trade, 1524, 9‚Äì50. <a href="http://doi.org/10.1007/3-540-49430-8_2">http://doi.org/10.1007/3-540-49430-8_2</a>
[^25]: Bengio, Y., Louradour, J., Collobert, R., &amp; Weston, J. (2009). Curriculum learning. Proceedings of the 26th Annual International Conference on Machine Learning, 41‚Äì48. <a href="http://doi.org/10.1145/1553374.1553380">http://doi.org/10.1145/1553374.1553380</a>
[^26]: Zaremba, W., &amp; Sutskever, I. (2014). Learning to Execute, 1‚Äì25. Retrieved from <a href="http://arxiv.org/abs/1410.4615">http://arxiv.org/abs/1410.4615</a>
[^27]: Ioffe, S., &amp; Szegedy, C. (2015). Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv Preprint arXiv:1502.03167v3.
[^28]: Neelakantan, A., Vilnis, L., Le, Q. V., Sutskever, I., Kaiser, L., Kurach, K., &amp; Martens, J. (2015). Adding Gradient Noise Improves Learning for Very Deep Networks, 1‚Äì11. Retrieved from <a href="http://arxiv.org/abs/1511.06807">http://arxiv.org/abs/1511.06807</a>

***
***

TITLE: Sentence and Contextualized Word Representations <br /> (Multi-Task/Transfer Learning)
LINK: research/dl/nlp/sent_embeds.md


## Sentence Representations
{: #content1}

1. **What?:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}
    :   Sentence Representation/Embedding Learning is focused on producing one feature vector to represent a sentence in a latent (semantic) space, while preserving linear properties (distances, angles).  

2. **Tasks:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}
    * Sentence Classification
    * Paraphrase Identification
    * Semantic Similarity
    * Textual Entailment (i.e. Natural Language Inference)
    * Retrieval

3. **Methods:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}
    * __Multi-Task Learning__:  
        In particular, people do __*Pre-Training*__ on other tasks, and then use the pre-trained weights and fine-tune them on a new task.  

4. **End-To-End VS Pre-Training:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    We can always use End-To-End objectives, however, there are two problems that arise, and can be mitigated by pre-training:  
    * Paucity of Training Data  
    * Weak Feedback from end of sentence only for text classification (explain?)

***

## Training Sentence Representations
{: #content2}

1. **Language Model Transfer _(Dai and Le 2015)_:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    * __Model:__ LSTM
    * __Objective:__ Language modeling objective
    * __Data:__ Classification data itself, or Amazon reviews
    * __Downstream:__ On text classification, initialize weights and continue training

2. **Unidirectional Training + Transformer - OpenAI GPT _(Radford et al. 2018)_:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    * __Model:__  Masked self-attention
    * __Objective:__  Predict the next word left->right
    * __Data:__  BooksCorpus
    * __Downstream:__  Some task fine-tuning, other tasks additional multi-sentence training

3. **Auto-encoder Transfer _(Dai and Le 2015)_:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    * __Model:__ LSTM
    * __Objective:__ From single sentence vector, reconstruct the sentence
    * __Data:__ Classification data itself, or Amazon reviews
    * __Downstream:__ On text classification, initialize weights and continue training

4. **Context Prediction Transfer - SkipThought Vectors _(Kiros et al. 2015)_:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    * __Model:__  LSTM
    * __Objective:__  Predict the surrounding sentences
    * __Data:__  Books, important because of context
    * __Downstream:__  Train logistic regression on $$[\|u-v\|; u * v]$$ (component-wise)

5. **Paraphrase ID Transfer _(Wieting et al. 2015)_:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}
    * __Model:__ Try many different ones
    * __Objective:__ Predict whether two phrases are paraphrases or not from
    * __Data:__ Paraphrase database (http://paraphrase.org), created from bilingual data  
        * **Large Scale Paraphrase Data - ParaNMT-50MT _(Wieting and Gimpel 2018)_:**  
            * Automatic construction of large paraphrase DB:
                * Get large parallel corpus (English-Czech) 
                * Translate the Czech side using a SOTA NMT system 
                * Get automated score and annotate a sample  
            * Corpus is huge but includes noise, 50M sentences (about 30M are high quality) 
            * Trained representations work quite well and generalize  
    * __Downstream Usage:__ Sentence similarity, classification, etc.
    * __Result:__ Interestingly, LSTMs work well on in-domain data, but word averaging generalizes better


7. **Entailment Transfer - InferSent _(Conneau et al. 2017)_:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  
    * __Previous objectives__ use no human labels, but what if:
    * __Objective:__ supervised training for a task such as entailment learn generalizable embeddings?  
        * Task is more difficult and requires capturing nuance ‚Üí yes?, or data is much smaller ‚Üí no?  
    * __Model:__ Bi-LSTM + max pooling  
    * __Data:__ Stanford NLI, MultiNLI
    * __Results:__ Tends to be better than unsupervised objectives such as SkipThought  


***

## Contextualized Word Representations
{: #content3}









***

## FOURTH
{: #content4}









***

***
***

TITLE: 2.1 <br /> Basics and Definitions
LINK: research/dl/nlp/suki_pres.md


![img](gauss.png){: width="100%"}

***

![img](1.png){: width="100%"}

***

![img](2.png){: width="100%"}


## Linear Functions and Transformations, and Maps
{: #content7}

1. **Linear Functions:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents71}
    :    **Linear functions** are functions which preserve *scaling* and *addition of the input* argument.
    :   > **Formally**,
    :     A function $$f: \mathbf{R}^n \rightarrow \mathbf{R}$$ is linear if and only if $$f$$ preserves scaling and addition of its arguments:  
    :   * for every $$x \in \mathbf{R}^n$$, and $$\alpha \in \mathbf{R}, \ f(\alpha x) = \alpha f(x)$$; and
    :   * for every $$x_1, x_2 \in \mathbf{R}^n, f(x_1+x_2) = f(x_1)+f(x_2)$$.

2. **Affine Functions:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents72}
    :   **Affine functions** are linear functions plus constant functions.
    :   **Formally,**  
    :   A function f is affine if and only if the function $$\tilde{f}: \mathbf{R}^n \rightarrow \mathbf{R}$$ with values $$\tilde{f}(x) = f(x)-f(0)$$ is linear. $$\diamondsuit$$
    :   > **Equivalently**,
    :   A map $$f : \mathbf{R}^n \rightarrow \mathbf{R}^m$$ is affine if and only if the map $$g : \mathbf{R}^n \rightarrow \mathbf{R}^m$$ with values $$g(x) = f(x) - f(0)$$ is linear.

0. **Quadratic Function**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents10} 
    :   A function $$q : \mathbf{R}^n \rightarrow \mathbf{R}$$ is said to be a quadratic function if it can be expressed as
    :   $$
        q(x) = \sum_{i=1}^n \sum_{j=1}^n A_{ij} x_i x_j + 2 \sum_{i=1}^n b_i x_i + c, 
        $$  
    :   for numbers $$A_{ij}, b_i,$$ and $$c, i, j \in {1, \ldots, n}$$.
        > A quadratic function is thus an affine combination of the $$\ x_i$$'s and all the "cross-products" $$x_ix_j$$.  
    :   > We observe that the coefficient of $$x_ix_j$$ is $$(A_{ij} + A_{ji})$$.  
    :   > The function is said to be a quadratic form if there are no linear or constant terms in it: $$b_i = 0, c=0.$$
    :   > The _Hessian_ of a quadratic function is always constant.


3. **Equivalent Definitions of Linear Functions [Theorem]:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents73}
    :   A map $$f : \mathbf{R}^n \rightarrow \mathbf{R}^m$$ is linear if and only if either one of the following conditions hold:
    :   * $$f$$ preserves scaling and addition of its arguments:
            *  for every $$x \in \mathbf{R}^n$$, and $$\alpha \in \mathbf{R},  f(\alpha x) = \alpha f(x)$$; and
            * for every $$x_1, x_2 \in \mathbf{R}^n, f(x_1+x_2) =  f(x_1)+f(x_2).$$
    :   * $$f$$ vanishes at the origin:
            * $$f(0) = 0$$, and
            * It transforms any line segment $$\in \mathbf{R}^n$$ into another segment $$\in \mathbf{R}^m$$:
            $$\forall \: x, y \in \mathbf{R}^n, \; \forall \: \lambda \in [0,1] ~:~ f(\lambda x + (1-\lambda) y) = \lambda f(x) + (1-\lambda) f(y)$$.  
                * $$f$$ is differentiable, vanishes at the origin, and the matrix of its derivatives is constant.
                * There exist $$A \in \mathbf{R}^{m \times n}$$ such that, $$\ \forall  x \in \mathbf{R}^n ~:~ f(x) = Ax$$. 
    <button>Example</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/2/2.1/7.png){: hidden=""}

4. **Vector Form (and the scalar product):**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents74}  \\
    **Theorem**: *Representation of affine function via the scalar product.*  
    $$\ \ \ \ \ \ \ \ $$    A function $$f: \mathbf{R}^n \rightarrow \mathbf{R}$$ is affine if and only if it can be expressed via a scalar product:  
        $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ \ \ \ \ \ \ \ \ $$  $$f(x) = a^Tx + b$$ ,  
        $$\ \ \ \ \ \ \ \ $$ for some unique pair $$(a,b)$$, with $$a \in \mathbf{R}^{n}$$ and $$b \in \mathbf{R}$$, given by $$a_i = f(e_i)-f(0)$$, with $$e_i$$ $$\ \ \ \ \ \ \ \ \ $$the $$i-th$$ unit vector $$\in \mathbf{R}^n, i=1, \ldots, n,$$ and $$\ b = f(0)$$.  \\
    > The function is linear $$\iff b = 0$$.  

    > The theorem shows that a vector can be seen as a (linear) function from the "input" space $$\mathbf{R}^n$$ to the "output" space $$\mathbf{R}$$.  

    > Both points of view (matrices as simple collections of numbers, or as linear functions) are useful.

0. **Gradient of a Linear Function:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents70} \\
    ![img](/main_files/conv_opt/2/2.1/8.png){: width="60%"}
    

5. **Gradient of an Affine Function:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents75}
    :   The **gradient** of a function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$ at a point $$x$$, denoted $$\nabla f(x)$$, is the vector of first derivatives with respect to $$x_1, \ldots, x_n$$.
    :   > When $$n=1$$ (there is only one input variable), the gradient is simply the derivative.  
    :   An affine function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$, with values $$f(x) = a^Tx+b$$ has the gradient:
    :   $$\nabla f(x) = a$$.  
    :   > i.e. For all Affine Functions, the gradient is the constant vector $$a$$.

6. **Interpreting $$a$$ and $$b$$:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents76}
    :   * The $$b=f(0)$$ is the constant term. For this reason, it is sometimes referred to as the bias, or intercept.  
            > as it is the point where $$f$$ intercepts the vertical axis if we were to plot the graph of the function.
    :   * The terms $$a_j, j=1, \ldots, n,$$ which correspond to the gradient of $$f$$, give the coefficients of influence of $$x_j$$ on $$f$$. 
            > **For example**, if $$a_1 >> a_3$$, then the first component of $$x$$ has much greater influence on the value of $$f(x)$$ than the third.

7. **First-order approximation of non-linear functions:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents77}
    :   * **One-dimensional case**:  
        Consider a function of one variable $$f : \mathbf{R} \rightarrow \mathbf{R}$$, and assume it is differentiable everywhere.  
        Then we can approximate the values function at a point $$x$$ near a point $$x_0$$ as follows:  
    :   $$ f(x) \simeq l(x) := f(x_0) + f'(x_0) (x-x_0) , $$
    :   $$\ \ \ \ \  \ \ \ $$ where $$f'(x)$$ denotes the derivative of $$f$$ at $$x$$.
    :   * **Multi-dimensional:**  
        Let us approximate a differentiable function $$f : \mathbf{R}^n \rightarrow \mathbf{R}$$ by a linear function $$l$$, so that $$f$$ and $$l$$ coincide up and including to the first derivatives.  
        The approximate function l must be of the form:  
    :   $$l(x) = a^Tx + b, $$  
    :   $$\ \ \ \ \  \ \ \ $$ where $$a \in \mathbf{R}^n$$ and $$b \in \mathbf{R}$$.  
    :   > The corresponding approximation $$l$$ is called the first-order approximation to $$f$$ at $$x_0$$.  

    :   * Our condition that $$l$$ coincides with $$f$$ up and including to the first derivatives shows that we must have:  
    :   $$  \nabla l(x) = a = \nabla f(x_0), \;\; a^Tx_0 + b = f(x_0), $$  
    :   $$\ \ \ \ \  \ \ \ $$   where $$\nabla f(x_0)$$ is the gradient, of $$f$$ at $$x_0$$. 

8. **First-order Expansion of a function [Theorem]:**{: style="color: SteelBlue  "}{: .bodyContents7 #bodyContents78}
    :   The first-order approximation of a differentiable function $$f$$ at a point $$x_0$$ is of the form:  
    :   $$f(x) \approx l(x) = f(x_0) + \nabla f(x_0)^T (x-x_0)$$   
    :   where $$\nabla f(x_0) \in \mathbf{R}^n$$ is the gradient of $$f$$ at $$x_0$$.
    <button>Example: a linear approximation to a non-linear function.</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/2/2.1/9.png){: hidden="" width="70%"}

***

![img](4.png){: width="100%"}

***


## Matrices
{: #content8}

0. **Matrix Transpose:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents80}
    :   $$ A_{ij} =  A_{ji}^T, \; \forall i, j \in \mathbf{F}$$  
    * **Properties:**  
        * $$(AB)^T = B^TA^T.$$  

1. **Matrix-vector product:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents81}
    :    $$(Ax)_i = \sum_{j=1}^n A_{ij}x_j , \;\; i=1, \ldots, m. $$
    :    Where the Matrix is $$\in {\mathbf{R}}^{m \times n}$$ and the vector is $$ \in {\mathbf{R}}^m$$.
    :    **Interpretations:**  
    :       1. **A _linear combination_ of the _columns_ of $$A$$:**    
            $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \   Ax = \left( \begin{array}{c} a_1^Tx  \ldots  a_m^Tx \end{array} \right)^T$$ .   
            where the columns of $$A$$ are given by the vectors $$a_i, i=1, \ldots, n$$, so that $$A = (a_1 , \ldots, a_n)$$.

            2. **_Scalar Products_ of _Rows_ of $$A$$ with $$x$$:**    
            $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  Ax = \sum_{i=1}^n x_i a_i$$ .   
            where the rows of $$A$$ are given by the vectors $$a_i^T, i=1, \ldots, m$$:
            $$A = \left( \begin{array}{c} a_1^T  \ldots  a_m^T \end{array} \right)^T$$.

    <button>Example: Network Flows</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/2/2.1/10_11.png){: hidden=""}

2. **Left Product:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents82}
    :    If $$z \in \mathbf{R}^m$$, then the notation $$z^TA$$ is the row vector of size $$n$$ equal to the transpose of the column vector $$A^Tz \in \mathbf{R}^n$$:  
    :   $$ (z^TA)_j = \sum_{i=1}^m A_{ij}z_i , \;\; j=1, \ldots, n. $$
    <button>Example: Representing the constraint that the columns of a matrix sum to zero.</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/2/2.1/12.png){: hidden=""}


3. **Matrix-matrix product:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents83}
    :   $$  (AB)_{ij} = \sum_{k=1}^n A_{ik} B_{kj}$$.  
    :   where $$A \in \mathbf{R}^{m \times n}$$ and $$B \in \mathbf{R}^{n \times p}$$, and the notation $$AB$$ denotes the $$m \times p$$ matrix given above.
    :    **Interpretations:**  
    :       1. **_Transforming_ the _columns_ of $$B$$ into $$Ab_i$$:**    
            $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \    AB = A \left( \begin{array}{ccc} b_1 & \ldots & b_n \end{array} \right) =  \left( \begin{array}{ccc} Ab_1 & \ldots & Ab_n \end{array} \right)$$ .   
            where the columns of $$B$$ are given by the vectors $$b_i, i=1, \ldots, n$$, so that $$B = (b_1 , \ldots, b_n)$$.  
            2. **_Transforming_ the _Rows_ of $$A$$ into $$a_i^TB$$:**      
            $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  AB = \left(\begin{array}{c} a_1^T \\ \vdots \\ a_m^T \end{array}\right) B = \left(\begin{array}{c} a_1^TB \\ \vdots \\ a_m^TB \end{array}\right)$$.   
            where the rows of $$A$$ are given by the vectors $$a_i^T, i=1, \ldots, m$$:
            $$A = \left( \begin{array}{c} a_1^T  \ldots  a_m^T \end{array} \right)^T$$.

4. **Block Matrix Products:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents84}  \\
    ![img](/main_files/conv_opt/2/2.1/block.png){: width="100%"}

5. **Outer Products:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents85}
    ![img](/main_files/conv_opt/2/2.1/outer_products.png){: width="100%"}

6. **Trace:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents86}
    :   The trace of a square $$n \times n$$ matrix $$A$$, denoted by $$\mathbf{Tr} A$$, is the sum of its diagonal elements:  
    :   $$ \mathbf{Tr} A = \sum_{i=1}^n A_{ii}$$.  
    * **Properties:**  
        * $$\mathbf{Tr} A = \mathbf{Tr} A^T$$.  
        * $$\mathbf{Tr} (AB) = \mathbf{Tr} (BA)$$.
        * $$\mathbf{Tr}(XYZ) = \mathbf{Tr}(ZXY) = \mathbf{Tr}(YZX)$$.
        * $${\displaystyle \operatorname{tr} (A+B) = \operatorname{tr} (A)+\operatorname{tr} (B)}$$.
        * $${\displaystyle \operatorname{tr} (cA) = c\operatorname{tr} (A)}$$.
        * $${\displaystyle \operatorname{tr} \left(X^{\mathrm {T} }Y\right)=\operatorname{tr} \left(XY^{\mathrm {T} }\right)=\operatorname{tr} \left(Y^{\mathrm {T} }X\right)=\operatorname{tr} \left(YX^{\mathrm {T} }\right)=\sum _{i,j}X_{ij}Y_{ij}}$$.
        * $${\displaystyle \operatorname{tr} \left(X^{\mathrm {T} }Y\right)=\sum _{ij}(X\circ Y)_{ij}}\ \ \ \ $$ (The _Hadamard_ product).
        * Arbitrary permutations of the product of matrices is not allowed. Only, **cyclic permutations** are.
            > However, if products of three symmetric matrices are considered, any permutation is allowed.
        * The trace of an idempotent matrix $$A$$, is the dimension of A.
        * The trace of a nilpotent matrix is zero.
        * If $$f(x) = (x ‚àí \lambda_1)^{d_1} \cdots (x ‚àí \lambda_k)^{d_k}$$ is the characteristic polynomial of a matrix $$A$$, then $${\displaystyle \operatorname{tr} (A)=d_{1}\lambda_{1} + \cdots + d_{k} \lambda_{k}}$$.
        * When both $$A$$ and $$B$$ are $$n \times n$$, the trace of the (ring-theoretic) commutator of $$A$$ and $$B$$ vanishes: $$\mathbf{tr}([A, B]) = 0$$; one can state this as "the trace is a map of Lie algebras $${\displaystyle \mathbf{GL_{n}} \to k}$$ from operators to scalars", as the commutator of scalars is trivial (it is an abelian Lie algebra).
        * The trace of a projection matrix is the dimension of the target space.
            $${\displaystyle 
            P_{X} = X\left(X^{\mathrm {T} }X\right)^{-1}X^{\mathrm {T} } \\
            \Rightarrow \\
            \operatorname {tr} \left(P_{X}\right) = \operatorname {rank} \left(X\right)}$$



7. **Scalar Product:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents87}
    :   $$\langle A, B \rangle := \mathbf{Tr}(A^TB) = \displaystyle\sum_{i=1}^m\sum_{j=1}^n A_{ij}B_{ij}.$$  
    :   > The above definition is **Symmetric**:  
    :   $$\implies \langle A,B \rangle =  \mathbf{Tr} (A^TB) = \mathbf{Tr} (A^TB)^T =  \mathbf{Tr} (B^TA) = \langle B,A \rangle .$$  
    :   > We can **interpret** the matrix scalar product as the _vector scalar product between two long vectors_ of length $$mn$$ each, obtained by stacking all the columns of $$A, B$$ on top of each other.

8. **Special Matrices:**{: style="color: SteelBlue  "}{: .bodyContents8 #bodyContents88} \\
    * [**Diagonal matrices:**](/work_files/research/la/sym_mat) are square matrices $$A$$ with $$A_{ij} = 0$$ when $$i \ne j$$.  
    * **Symmetric matrices:** are square matrices that satisfy $$A_{ij} = A_{ji} $$for every pair $$(i,j)$$.
    * **Triangular matrices:** are square matrices that satisfy $$A_{ij} = A_{ji} $$for every pair $$(i,j)$$.    
$${F}}^{2}=\|AR\|_{\rm {F}}^{2}=\|RA\|_{\rm {F}}^{2}}$$ for any rotation matrix $$R$$.

        3. Invariant under a unitary transformation for complex matrices.

        4. $${\displaystyle \|A^{\rm {T}}A\|_{\rm {F}}=\|AA^{\rm {T}}\|_{\rm {F}}\leq \|A\|_{\rm {F}}^{2}}$$.

        5. $${\displaystyle \|A+B\|_{\rm {F}}^{2}=\|A\|_{\rm {F}}^{2}+\|B\|_{\rm {F}}^{2}+2\langle A,B\rangle _{\mathrm {F} }}$$.


4. **$$l_{\infty,\infty}$$ (Max Norm):**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents94}
    :   $$ \|A\|_{\max} = \max_{ij} |a_{ij}|.$$

    * **Properties:**  
        1. **NOT** Submultiplicative.

5. **The Spectral Norm:**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents95}
    :   $${\displaystyle \|A\|_{2}={\sqrt {\lambda _{\max }(A^{^{*}}A)}}=\sigma _{\max }(A)} = {\displaystyle \max_{\|x\|_2!=0}(\|Ax\|_2)/(\|x\|_2)}.$$  
    > The spectral norm of a matrix $${\displaystyle A} $$ is the largest singular value of $${\displaystyle A}$$. 
    > i.e. the square root of the largest eigenvalue of the positive-semidefinite matrix $${\displaystyle A^{*}A}.$$

    * **The Spectral Radius of $$A \ $$  [denoted $$\rho(A)$$]:**
    :   $$ \lim_{r\rightarrow\infty}\|A^r\|^{1/r}=\rho(A).$$

    * **Properties:**  
        1. Submultiplicative.

        2. Satisfies, $${\displaystyle \|A^{r}\|^{1/r}\geq \rho (A),}$$, where $$\rho(A)$$ is **the spectral radius** of $$A$$.

        3. It is an "_induced vector-norm_".



8. **Equivalence of Norms:**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents98} \\
    <button>CLICK TO VIEW</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/conv_opt/2/2.1/13.png){: hidden=""}

8. **Applications:**{: style="color: SteelBlue  "}{: .bodyContents9 #bodyContents98} \\
    1. **RMS Gain:** Frobenius Norm.

    2. **Peak Gain:** Spectral Norm.

    3. **Distance between Matrices:** Frobenius Norm.
        <button>Click to View</button>{: .showText value="show"
         onclick="showTextPopHide(event);"}
        ![img](/main_files/conv_opt/2/2.1/14.png){: hidden=""}

    4. **Direction of Maximal Variance:** Spectral Norm.
        <button>Click to View</button>{: .showText value="show"
         onclick="showTextPopHide(event);"}
        ![img](/main_files/conv_opt/2/2.1/15.png){: hidden=""}



***
***

TITLE: Contextual Word Representations and Pretraining
LINK: research/dl/nlp/ctxt_word_repr.md


[Paper on Contextual Word Representations](https://arxiv.org/pdf/1902.06006.pdf)  


## Word Representations and their progress
{: #content1}

1. **Word Representations (Accepted Methods):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    The current accepted methods provide one representation of words:  
    1. __Word2Vec__
    2. __GloVe__ 
    3. __FastText__  

    <button>Early Day Results</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](/main_files/dl/nlp/ctxt_word_repr/2.png){: width="80%" hidden=""}  

    __Problems:__  
    * __Word Senses__: Always the same representation for a __word type__ regardless of the context in which a __word token__ occurs  
        * We might want very fine-grained word sense disambiguation (e.g. not just 'holywood star' and 'astronomical star'; but also 'rock star', 'star student' etc.)  

    * We just have one representation for a word, but words have different __aspects__, including semantics, syntactic behavior, and register/connotations (e.g. when is it appropriate to use 'bathroom' vs 'shithole' etc.; 'can'-noun vs 'can'-verb have same vector)  

    __Possible Solution (that we always had?):__  
    * In a NLM, LSTM layers are trained to predict the next word, producing hidden/state vectors, that are basically __context-specific__ word representations, at each position  
    <br>  

2. **TagLM (Peters et al. 2017) ‚Äî Pre-Elmo:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    __Idea:__ 
    * Want meaning of word in context, but standardly learn task RNN only on small task-labeled data (e.g. NER).  
    * Do __semi-supervised__ approach where we train NLM on large unlabeled corpus, rather than just word vectors.  
    * Run a BiRNN-LM and concatenate the For and Back representations
    * Also, train a traditional word-embedding (w2v) on the word and concatenate with Bi-LM repr.
    * Also, train a Char-CNN/RNN to get character level embedding and concatenate all of them together

    __Details:__  
    * Language model is trained on 800 million training words of "Billion word benchmark"  
    * __Language model observations__:  
        * An LM trained on supervised data does not help 
        * Having a bidirectional LM helps over only forward, by about 0.2 
        * Having a huge LM design (ppl 30) helps over a smaller model (ppl 48) by about 0.3 
    * __Task-specific BiLSTM observations__:  
        * Using just the LM embeddings to predict isn't great: 88.17 F1  
            * Well below just using an BiLSTM tagger on labeled data      

    <button>Tag LM</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](/main_files/dl/nlp/ctxt_word_repr/3.png){: width="100%" hidden=""}
    <br>

3. **Cove - Pre-Elmo:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    * Also has idea of using a trained sequence model to provide context to other NLP models 
    * Idea: Machine translation is meant to preserve meaning, so maybe that's a good objective? 
    * Use a 2-layer bi-LSTM that is the encoder of seq2seq + attention NMT system as the context provider 
    * The resulting CoVe vectors do outperform GloVe vectors on various tasks 
    * But, the results aren't as strong as the simpler NLM training described in the rest of these slides so seems abandoned 
        * Maybe NMT is just harder than language modeling? 
        * Maybe someday this idea will return?

    <br>

4. **Elmo - Embeddings from Language Models (Peters et al. 2018):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    __Idea:__  
    * Train a bidirectional LM  
    * Aim at performant but not overly large LM:  
        * Use 2 biLSTM layers  
        * Use character CNN to build initial word representation (only)  
            * 2048 char n-gram filters and 2 highway layers, 512 dim projection  
        * Use 4096 dim hidden/cell LSTM states with 512 dim projections to next input  
        * Use a residual connection  
        * Tie parameters of token input and output (softmax) and tie these between forward and backward LMs  

    __Key Results:__  
    * ELMo learns task-specific combination of BiLM representations  
    * This is an innovation that improves on just using top layer of LSTM stack
    <p>$$\begin{aligned} R_{k} &=\left\{\mathbf{x}_{k}^{L M}, \overrightarrow{\mathbf{h}}_{k, j}^{L M}, \mathbf{h}_{k, j}^{L M} | j=1, \ldots, L\right\} \\ &=\left\{\mathbf{h}_{k, j}^{L M} | j=0, \ldots, L\right\} \end{aligned}$$</p>  
    <p>$$\mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}=E\left(R_{k} ; \Theta^{t a s k}\right)=\gamma^{task} \sum_{j=0}^{L} s_{j}^{task} \mathbf{h}_ {k, j}^{L M}$$</p>  
    * $$\gamma^{\text { task }}$$ scales overall usefulness of ELMo to task;  
    * $$s^{\text { task }}$$ are softmax-normalized mixture model weights  
    > Possibly this is a way of saying different semantic and syntactic meanings of a word are represented in different layers; and by doing a weighted average of those, in a task-specific manner, we can leverage the appropriate kind of information for each task.  

    __Using ELMo with a Task:__  
    * First run biLM to get representations for each word
    * Then let (whatever) end-task model use them
        * Freeze weights of ELMo for purposes of supervised model
        * Concatenate ELMo weights into task-specific model
            * Details depend on task
                * Concatenating into intermediate layer as for TagLM is typical
                * Can provide ELMo representations again when producing outputs, as in a question answering system  

    
    __Weighting of Layers:__  
    * The two Bi-LSTM NLP Layers have differentiated uses/meanings  
        * Lower layer is better for lower-level syntax, etc.  
            * POS-Tagging, Syntactic Dependencies, NER  
        * Higher layer is better for higher-level semantics  
            * Sentiment, Semantic role labeling, QA, SNLI  


    __Reason for Excitement:__  
    ELMo proved to be great for __all NLP tasks__ (even tho the core of the idea was in _TagLM_)   
    * <button>ELMo Results</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](/main_files/dl/nlp/ctxt_word_repr/5.png){: width="70%" hidden=""}  
    <br>        
    
5. **ULMfit - Universal Language Model Fine-Tuning (Howard and Ruder 2018):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    ULMfit - Universal Language Model Fine-Tuning for Text Classification:  
    ![img](/main_files/dl/nlp/ctxt_word_repr/4.png){: width="90%"}  


6. **BERT (Devlin et al. 2018):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    BERT - Bidirectional Encoder Representations from Transformers:  
    __Idea:__ Pre-training of Deep Bidirectional Transformers for Language Understanding.  

    __Model Architecture:__  
    * Transformer Encoder
    * Self-attention --> no locality bias
        * Long-distance context has "equal opportunity"  
    * Single multiplication per layer --> efficiency on GPU/TPU
    * __Architectures__:  
        * __BERT-Base__: 12 layer, 768-hidden, 12-head  
        * __BERT-Large__: 24 layer, 1024 hudden, 16 heads  


    __Model Training:__  
    * Train on Wikipedia + BookCorpus
    * Train 2 model sizes:  
        * __BERT-Base__
        * __BERT-Large__
    * Trained on $$4\times 4$$  or $$8\times 8$$ TPU slice for 4 days  


    __Model Fine-Tuning:__  
    * Simply learn a classifier built on top layer for each task that you fine-tune for.
                

    __Problem with Unidirectional and Bidirectional LMs:__  
    * __Uni:__ build representation incrementally; not enough context from the sentence  
    * __Bi:__  Cross-Talk; words can "see themselves"  

    __Solution:__  
    * Mask out $$k\%$$ of the input words, and then predict the masked words
        * They always use $$k=15%$$  
        > Ex: "The man went to the _[MASK]_ to buy a _[MASK]_ of milk."  

        * __Too little Masking__:  Too Expensive to train
        * __Too much Masking__:  Not enough context   
    * __Other Benefits__:  
        * In ELMo, bidirectional training is done independently for each direction and then concatenated. No joint-context in the model during the building of contextual-reprs.
        * In GPT, there is only unidirectional context.  
            
    __Another Objective - Next Sentence Prediction:__  
    To learn _relationships_ between sentences, predict whether sentence B is actual sentence that proceeeds sentence A, or a random sentence (for QA, NLU, etc.).  


    __Results:__  
    Beats every other architecture in every __GLUE__ task (NL-Inference).  

    * [BERT Word Embeddings Tutorial](http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)  

    


***

## The Transformer
{: #content2}


1. **Self-Attention:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    * __Computational Complexity Comparison__:  
        ![img](/main_files/dl/nlp/ctxt_word_repr/1.png){: width="70%"}  
    * __Self-Attention/Relative-Attention Interpretations__:  
        * Can achieve __Translational Equivariance__ (like convs) (by removing pos-encoding).
        * Can model similarity graphs. 
        * Connected to __message-passing NNs__. Can think of self-atten as _passing messages bet pairs of nodes in graph_; equiv. _imposing a complete bipartite graph_ and you're passing messages between nodes.  
            Mathematically, the difference is message-passing NNs impose condition that messages pass ONLY bet pairs of nodes; while self-atten uses softmax and thus passes messages between all nodes.  

    * __Self-Attention Summary__:  
        * Constant path-length between any two positions
        * Unbounded memory (ie no fixed size h_state) 
        * Trivial to parallelize (per layer)
        * Models self-similarity
        * Relative attention provides expressive timing, equivariance, and extends naturally to graphs  

    * __Current Issues__:  
        * __Slow Generation__:  
            Mainly due to __Auto-Regressive__ generation, which is necessary to break the multimodality of generation. Multimodality prohibits naive parallel generation.  
            Multimodality refers to the fact that there are multiple different sentences in german that are considered a correct translation of a sentence in english, and they all depend on the word that was generated first (ie no parallelization).   
        * __Active Area of Research__:  
            __Non Auto-Regressive Transformers__.  
            * Papers:  
                * _Non autoregressive transformer (Gu and Bradbury et al., 2018)_  
                * _Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement (Lee, Manismov, and Cho, 2018)_  
                * _Fast Decoding in Sequence Models Using Discrete Latent Variables (ICML 2018) Kaiser, Roy, Vaswani, Pamar, Bengio, Uszkoreit, Shazeer_  
                * _Towards a Better Understanding of Vector Quantized Autoencoders Roy, Vaswani, Parmar, Neelakantan, 2018_  
                * _Blockwise Parallel Decoding For Deep Autogressive Models (NeurIPS 2019) Stern, Shazeer, Uszkoreit_   



***
***

TITLE: Word Vector Representations <br /> word2vec
LINK: research/dl/nlp/wordvec.md


[W2V Detailed Tutorial - Skip Gram (Stanford)](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)  
[W2V Detailed Tutorial - Negative Sampling (Stanford)](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)  
[Commented word2vec C code](https://github.com/chrisjmccormick/word2vec_commented)  
[W2V Resources](http://mccormickml.com/2016/04/27/word2vec-resources/)  
[An overview of word embeddings and their connection to distributional semantic models
](http://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/)  
[On Word Embeddings (Ruder)](http://ruder.io/word-embeddings-1/)  




## Word Meaning
{: #content1}

1. **Representing the Meaning of a Word:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    Commonest linguistic way of thinking of meaning:  
    Signifier $$\iff$$ Signified (idea or thing) = denotation
    
2. **How do we have usable meaning in a computer:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    Commonly:  Use a taxonomy like WordNet that has hypernyms (is-a) relationships and synonym sets
    
3. **Problems with this discrete representation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    * __Great as a resource but missing nuances__:  
        * Synonyms:  
            adept, expert, good, practiced, proficient, skillful
    * __Missing New Words__
    * __Subjective__  
    * __Requires human labor to create and adapt__  
    * __Hard to compute accurate word similarity__:  
        * _One-Hot Encoding_: in vector space terms, this is a vector with one 1 (at the position of the word) and a lot of zeroes (elsewhere).  
            * It is a __localist__ representation   
            * There is __no__ natural __notion of similarity__ in a set of one-hot vectors   
    
4. **Distributed Representations of Words:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    A method where vectors encode the similarity between the words.  
    
    The meaning is represented with real-valued numbers and is "_smeared_" across the vector.  
    
    > Contrast with __one-hot encoding__.  
    

5. **Distributional Similarity:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    is an idea/hypothesis that one can describe the meaning of words by the context in which they appear in.   
    
    > Contrast with __Denotational Meaning__ of words.  
    
6. **The Big Idea:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    We will build a dense vector for each word type, chosen so that it is good at predicting other words appearing in its context.  
    
7. **Learning Neural Network Word Embeddings:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    We define a model that aims to predict between a center word $$w_t$$ and context words in terms of word vectors.    
    <p>$$p(\text{context} \vert  w_t) = \ldots$$</p>   

    __The Loss Function__:    
    <p>$$J = 1 - p(w_{-t} \vert  w_t)$$</p>    

    We look at many positions $$t$$ in a big language corpus  
    We keep adjusting the vector representations of words to minimize this loss  

    
8. **Relevant Papers:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    * Learning representations by back-propagating errors (Rumelhart et al., 1986) 
    * A neural probabilistic language model (Bengio et al., 2003) 
    * NLP (almost) from Scratch (Collobert & Weston, 2008) 
    * A recent, even simpler and faster model: word2vec (Mikolov et al. 2013) √† intro now
    
***

## Word Embeddings
{: #content2}

1. **Main Ideas:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    * Words are represented as vectors of real numbers
    * Words with similar vectors are _semantically_ similar 
    * Sometimes vectors are low-dimensional compared to the vocabulary size  
    
2. **The Clusterings:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    __Relationships (attributes) Captured__:    
    * __Synonyms:__ car, auto
    * __Antonyms:__ agree, disagree
    * __Values-on-a-scale:__ hot, warm, cold
    * __Hyponym-Hypernym:__ "Truck" is a type of "car", "dog" is a type of "pet"
    * __Co-Hyponyms:__ "cat"&"dog" is a type of "pet"
    * __Context:__ (Drink, Eat), (Talk, Listen)

3. **Word Embeddings Theory:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    Distributional Similarity Hypothesis

4. **History and Terminology:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    Word Embeddings = Distributional Semantic Model = Distributed Representation = Semantic Vector Space = Vector Space Model  

5. **Applications:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    * Word Similarity
    * Word Grouping
    * Features in Text-Classification
    * Document Clustering
    * NLP:  
        * POS-Tagging
        * Semantic Analysis
        * Syntactic Parsing

6. **Approaches:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    * __Count__: word count/context co-occurrences   
        * *__Distributional Semantics__*:    
            1. Summarize the occurrence statistics for each word in a large document set:   
                ![img](/main_files/dl/nlp/1/1.png){: width="40%"}  
            2. Apply some dimensionality reduction transformation (SVD) to the counts to obtain dense real-valued vectors:   
                ![img](/main_files/dl/nlp/1/2.png){: width="40%"}  
            3. Compute similarity between words as vector similarity:  
                ![img](/main_files/dl/nlp/1/3.png){: width="40%"}  
    * __Predict__: word based on context  
        * __word2vec__:  
            1. In one setup, the goal is to predict a word given its context.  
                ![img](/main_files/dl/nlp/1/4.png){: width="80%"}   
            2. Update word representations for each context in the data set  
            3. Similar words would be predicted by similar contexts

7. **Parameters:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}
    * Underlying Document Set   
    * Context Size
    * Context Type

8. **Software:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}  
    ![img](/main_files/dl/nlp/1/5.png){: width="80%"}  
    
***

## Word2Vec
{: #content3}

11. **Word2Vec:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents311}  
    __Word2Vec__ _(Mikolov et al. 2013)_ is a framework for learning word representations as vectors. It is based on the idea of _distributional similarity_.  
    <br>

1. **Main Idea:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    * Given a large corpus of text
    * Represent every word, in a fixed vocabulary, by a _vector_ 
    * Go through each position $$t$$ in the text, which has a __center word__ $$c$$ and __context words__ $$o$$ 
    * Use the *__similarity of the word vectors__* for $$c$$ and $$o$$ to *__calculate the probability__* of $$o$$ given $$c$$ (SG)  
    * *__Keep adjusting the word vectors__* to __maximize this probability__  

    ![img](/main_files/dl/nlp/1/6.png){: width="80%"}  
    
2. **Algorithms:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    1. __Skip-grams (SG)__:  
        Predict context words given target (position independent)
    2. __Continuous Bag of Words (CBOW)__:  
        Predict target word from bag-of-words context  
    <br>
    
3. **Training Methods:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    * __Basic__:    
        1. Naive Softmax  
    * __(Moderately) Efficient__:  
        1. Hierarchical Softmax
        2. Negative Sampling   
    <br>
    
4. **Skip-Gram Prediction Method:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    Skip-Gram Models aim to predict the _distribution (probability)_ of context words from a center word.  
    > CBOW does the opposite, and aims to predict a center word from the surrounding context in terms of word vectors.   

    __The Algorithm__:    
    1. We generate our one hot input vector $$x \in \mathbf{R}^{\vert V\vert }$$ of the center word.  
    2. We get our embedded word vector for the center word $$v_c = V_x \in \mathbf{R}^n$$  
    3. Generate a score vector $$z = \mathcal{U}_ {v_c}$$ 
    4. Turn the score vector into probabilities, $$\hat{y} = \text{softmax}(z)$$ 
        > Note that $$\hat{y}_{c‚àím}, \ldots, \hat{y}_{c‚àí1}, \hat{y}_{c+1}, \ldots, \hat{y}_{c+m}$$ are the probabilities of observing each context word.  
    5. We desire our probability vector generated to match the true probabilities, which is  
        $$ y^{(c‚àím)} , \ldots, y^{(c‚àí1)} , y^{(c+1)} , \ldots, y^{(c+m)}$$,  
        the one hot vectors of the actual output.  
    <br>
    
5. **Word2Vec Details:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}  
    * For each word (position) $$t = 1 \ldots T$$, predict surrounding (context) words in a window of _‚Äúradius‚Äù_ $$m$$ of every word.  

    __Calculating $$p(o \vert c)$$[^2] the probability of outside words given center word:__  
    * We use two vectors per word $$w$$:  
        * $$v_{w}$$: $$\:$$  when $$w$$ is a center word  
        * $$u_{w}$$: $$\:$$ when $$w$$ is a context word  
    * Now, for a center word $$c$$ and a context word $$o$$, we calculate the probability:  
        <p>$$\\{\displaystyle p(o \vert  c) = \dfrac{e^{u_o^Tv_c}}{\sum_{w\in V} e^{u_w^Tv_c}}} \:\:\:\:\:\:\:\:\:\:\:\:\\$$</p>  
        <button>Constructing the Probability Distribution (Prediction Function)</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
        * ![img](/main_files/dl/nlp/1/7.png){: width="80%"}   
        * The Probability Distribution $$p(o \vert c)$$ is an application of the __softmax__ function on the, __dot-product__, similarity function $$u_o^Tv_c$$  
        * The __Softmax__ function, allows us to construct a probability distribution by making the numerator positive, and normalizing the function (to $$1$$) with the denominator  
        * The __similarity function $$u_o^Tv_c$$__ allows us to model as follows: the more the _similarity_ $$\rightarrow$$ the larger the _dot-product_; the larger the _exponential_ in the softmax    
        {: hidden=""}
    <br>
    
6. **The Objective:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents36}  
    __Goal:__{: style="color: red"}   
    Maximize the probability of any context word given the current center word.  

    We start with the __Likelihood__ of being able to predict the context words given center words and the parameters $$\theta$$ (only the wordvectors).  
    __The Likelihood:__  
    <p>$$L(\theta)=\prod_{t=1}^{T} \prod_{-m \leq j \leq m \atop j \neq 0} P\left(w_{t+j} | w_{t} ; \theta\right)$$</p>  
    
    __The objective:__{: style="color: red"}    
    The Objective is just the (average) __negative log likelihood__:  
    <p>$$J(\theta) = -\frac{1}{T} \log L(\theta)= - \dfrac{1}{T} \sum_{t=1}^{t} \sum_{-m \leq j \leq m \\ \:\:\:\:j\neq 0} \log p(w_{t+j} \vert  w_t ; \theta))$$</p>  

    Notice: Minimizing objective function $$\iff$$ Maximizing predictive accuracy[^1]  
    <br>
    
7. **The Gradients:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents37}  
    We have a vector of parameters $$\theta$$ that we are trying to optimize over, and We need to calculate the gradient of the two sets of parameters in $$\theta$$; namely, $$\dfrac{\partial}{\partial v_c}$$ and $$\dfrac{\partial}{\partial u_o}$$.  

    __The gradient $$\dfrac{\partial}{\partial v_c}$$:__{: style="color: red"}  
    <p>$$\dfrac{\partial}{\partial v_c} \log p(o\vert c) = u_o - \sum_{w'\in V} p{(w' | c)} \cdot u_{w'}$$</p>  

    __Interpretation:__  
    We are getting the slope by: taking the __observed representation of the context word__ and subtracting away (_"what the model thinks the context should look like"_) the __weighted average of the representations of each word multiplied by its probability in the current model__  
    (i.e. the __Expectation of the context word vector__ i.e. __the expected context word according to our current model__)   
    > I.E.  
        __The difference between the expected context word and the actual context word__{: style="color: green"}  
    

    __Importance Sampling:__{: style="color: red"}  
    <p>$$\sum_{w_{i} \in V} \left[\frac{\exp \left(-\mathcal{E}\left(w_{i}\right)\right)}{\sum_{w_{i} \in V} \exp \left(-\mathcal{E}\left(w_{i}\right)\right)}\right] \nabla_{\theta} \mathcal{E}\left(w_{i}\right) \\ = \sum_{w_{i} \in V} P\left(w_{i}\right) \nabla_{\theta} \mathcal{E}\left(w_{i}\right)$$</p>  
    <br>

    <p>$$\mathbb{E}_{w_{i} \sim P}\left[\nabla_{\theta} \mathcal{E}\left(w_{i}\right)\right] =\sum_{w_{i} \in V} P\left(w_{i}\right) \nabla_{\theta} \mathcal{E}\left(w_{i}\right)$$</p>  

    * $$P\left(w_{i}\right) \approx \frac{r(w_i)}{R}$$,  

    <p>$$\mathbb{E}_{w_{i} \sim P}\left[\nabla_{\theta} \mathcal{E}\left(w_{i}\right)\right] \approx \sum_{w_{i} \in V} \frac{r(w_i)}{R} \nabla_{\theta} \mathcal{E}\left(w_{i}\right)$$</p>  

    <p>$$\mathbb{E}_{w_{i} \sim P}\left[\nabla_{\theta} \mathcal{E}\left(w_{i}\right)\right] \approx \frac{1}{R} \sum_{i=1}^{m} r\left(w_{i}\right) \nabla_{\theta} \mathcal{E}\left(w_{i}\right)$$</p>  
    
    where $$r(w)=\frac{\exp (-\mathcal{E}(w))}{Q(w)}$$, $$R=\sum_{j=1}^{m} r\left(w_{j}\right)$$, and $$Q$$ is the __unigram distribution__ of the training set.    

8. **Notes:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents38}  
    * __Mikolov on SkipGram vs CBOW__:  
        * Skip-gram: works well with small amount of the training data, represents well even rare words or phrases.  
        * CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words.  
    * __Further Readings__:  
        * [A Latent Variable Model Approach to PMI-based Word Embeddings](https://aclweb.org/anthology/Q16-1028)
        * [Linear Algebraic Structure of Word Senses, with Applications to Polysemy](https://transacl.org/ojs/index.php/tacl/article/viewFile/1346/320)
        * [On the Dimensionality of Word Embedding](https://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf)
        * [Improving Distributional Similarity with Lessons Learned from Word Embeddings](https://www.aclweb.org/anthology/Q15-1016)
            
    
[^1]: accuracy of predicting words in the context of another word  
[^2]: I.E. $$p(w_{t+j} \vert  w_t)$$  

***
***

TITLE: Language Modeling [(Oxford)](https://www.youtube.com/watch?v=nfyE8oF23yQ&list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm&index=6&t=0s)  <br /> Recurrent Neural Networks (RNNs)
LINK: research/dl/nlp/lm&rnns.md


## FIRST
{: #content1}









***

## SECOND
{: #content2}









***

## THIRD
{: #content3}











***
***

TITLE: ASR <br /> Research Papers
LINK: research/dl/nlp/speech_research.md


## Deep Speech 
{: #content1}

1. **Introduction:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   This paper takes a first attempt at an End-to-End system for ASR.  

2. **Structure:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}    
    :   * __Input__: vector of speech spectrograms  
            * An *__utterance__* $$x^{(i)}$$: is a time-series of length $$T^{(i)}$$ composed of time-slices where each is a vector of audio (spectrogram) features $$x_{t,p}^{(i)}, t=1,...,T^{(i)}$$, where $$p$$ denotes the power of the p'th frequency bin in the audio frame at time $$t$$.  
        * __Output__: English text transcript $$y$$  
    :   * __Goal__:  
            The goal of the RNN is to convert an input sequence $$x$$ into a sequence of character probabilities for the transcription $$y$$, with $$\tilde{y}_t = P(c_t\vert x)$$, where $$c_t \in \{\text{a, b, c, } \ldots \text{,  z, space,  apostrophe, blank}\}$$.
                
3. **Strategy:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   The goal is to replace the multi-part model with a single RNN network that captures as much of the information needed to do transcription in a single system.  

4. **Solves:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   * Previous models only used DNNs as a single component in a complex pipeline.  
            NNs are trained to classify __individual frames of acoustic data__, and then, their output distributions are reformulated as emission probabilities for a HMM.  
            In this case, the objective function used to train the networks is therefore substantially different from the true performance measure (sequence-level transcription accuracy.  
            This leads to problems where one system might have an improved accuracy rate but the overall transcription accuracy can still decrease.  
        *  An additional problem is that the frame-level training targets must be inferred from the alignments determined by the HMM. This leads to an awkward iterative procedure, where network retraining is alternated with HMM re-alignments to generate more accurate targets.  

5. **Key Insights:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    :   * As an __End-to-End__ model, this system avoids the problems of __multi-part__ systems that lead to inconsistent training criteria and difficulty of integration.   
            The network is trained directly on the text transcripts: no phonetic representation (and hence no pronunciation dictionary or state tying) is used.  
        * Using __CTC__ objective, the system is able to better approximate and solve the alignment problem avoiding HMM realignment training.  
            Since CTC integrates out over all possible input-output alignments, no forced alignment is required to provide training targets.  
        * The Dataset is augmented with newly synthesized data and modified to include all the variations and effects that face ASR problems.    
            This greatly increases the system performance on particularly noisy/affected speech.  

6. **Preparing Data (Pre-Processing):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    :   The paper uses __spectrograms__ of power normalized audio clips as features.  
    :   ![img](/main_files/dl/nlp/speech_research/2.png){: width="60%"}    

7. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    :   The system is composed of:  
        * An __RNN__:    
            * 5 layers of __hidden units__:  
                * 3 Layer of __Feed-forward Nets__:  
                    * For the __input layer__, the output depends on the spectrogram frame $$x_t$$ along with a context of $$C$$ frames on each side.  
                        > $$C \in \{5, 7, 9\}$$  
                    * The non-recurrent layers operate on independent data for each time step:  
                        $$h_t^{(l)} = g(W^{(l)} h_{(t)}^{(l-1)} + b^{(l)}),$$  
                        where $$g(z) = \min \{\max \{0, z\}, 20\}$$ is the *clipped RELU*.    
                * 2 layers of __Recurrent Nets__:  
                    * 1 layer of a __Bi-LSTM__:  
                        * Includes two sets of hidden units: 
                            A set with forward recurrence $$h^{(f)}$$  
                            A set with backward recurrence $$h^{(b)}$$:  
                            $$h_t^{(f)} = g(W^{(4)}h_t^{(3)} + W_r^{(b)} h_{t-1}^{(b)} + b ^{(4)}) \\ 
                            h_t^{(b)} = g(W^{(4)}h_t^{(3)} + W_r^{(b)} h_{t+1}^{(b)} + b ^{(4)})$$  
                            > Note that $$h^{(f)}$$ must be computed sequentially from $$t = 1$$ to $$t = T^{(i)}$$ for the i‚Äôth utterance, while the units $$h^{(b)}$$ must be computed sequentially in reverse from $$t = T^{(i)}$$ to $$t = 1$$.  
                    * 1 layer of __Feed-forward Nets__:   
                        * The fifth (non-recurrent) layer takes both the forward and backward units as inputs:  
                            $$h_t^{(5)} = g(W ^{(5)}h_t ^{(4)} + b ^{(5)}),$$  
                            where $$h_t^{(4)} = h_t^{(f)} + h_t^{(b)}$$ 
            * An __Output__ layer made of a standard __softmax function__ that yields the predicted character probabilities for each time-slice $$t$$ and character $$k$$ in the alphabet:   
                $$\displaystyle{h _{(t,k)} ^{(6)} = \hat{y} _{(t,k)} = P(c_t = k \vert x) = \dfrac{\exp (W_k ^{(6)} h_t ^{(5)} + b_k ^{(6)})}{\sum_j \exp (W_j ^{(6)}h_t ^{(5)} + b_j ^{(6)})}},$$  
                where $$W_k ^{(6)}$$ and $$b_k ^{(6)}$$ denote the k'th column of the weight matrix and k'th bias.  
        * A *__CTC__* __Loss Function__ $$\mathcal{L}(\hat{y}, y)$$  
        * An *__N-gram Language Model__* 
        * A __combined Objective Function__:  
    :   $$Q(c) = \log (P(x \vert x)) + \alpha \log (P_{\text{LM}}(c) + \beta \text{word_count}(c))$$   
    :   ![img](/main_files/dl/nlp/speech_research/1.png){: width="80%"}    

8. **Algorithm:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    :   * Given the output $$P(c \vert x)$$ of the RNN: perform a __search__ to find the sequence of characters $$c_1, c_2, ...$$ that is most probable according to both:  
            1. The RNN Output
            2. The Language Model  
        * We maximize the combined objective:  
            $$Q(c) = \log (P(x \vert x)) + \alpha \log (P_{\text{LM}}(c) + \beta \text{word_count}(c))$$  
            where the term $$P_{\text{lm}}$$ denotes the probability of the sequence $$c$$ according to the N-gram model.  
        * The objective is maximized using a highly optimized __beam search__ algorithm  
            > beam size: 1000-8000

9. **Training:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents19}  
    :   * The gradient of the CTC Loss $$\nabla_{\hat{y}} \mathcal{L}(\hat{y}, y)$$ with respect to the net outputs given the ground-truth character sequence $$y$$ is computed
    :   * Nesterov‚Äôs Accelerated gradient
        * Nesterov Momentum
        * Annealing the learning rate by a constant factor
        * Dropout  
        * Striding -- shortening the recurrent layers by taking strides of size $$2$$.  
            The unrolled RNN will have __half__ as many steps.  
            > similar to a convolutional network with a step-size of 2 in the first layer.  

10. **Parameters:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents110}  
    :   * __Momentum__: $$0.99$$ 
        * __Dropout__: $$5-10 \%$$ (FFN only)   
        * __Trade-Off Params__: use cross-validation for $$\alpha, \beta$$  

11. **Issues/The Bottleneck:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents111}  
    :   

12. **Results:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents112}  
    :   * __SwitchboardHub5‚Äô00__  (
    WER): 
            * Standard: $$16.0\%$$  
            * w/Lexicon of allowed words: $$21.9\%$$ 
            * Trigram LM: $$8.2\%$$ 
            * w/Baseline system: $$6.7\%$$

13. **Discussion:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents113}  
    :   * __Why avoid LSTMs__:  
            One disadvantage of LSTM cells is that they require computing and storing multiple gating neuron responses at each step.  
            Since the forward and backward recurrences are sequential, this small additional cost can become a computational bottleneck.  
    :   * __Why a homogeneous model__:  
             By using a homogeneous model we have made the computation of the recurrent activations as efficient as possible: computing the ReLu outputs involves only a few highly optimized BLAS operations on the GPU and a single point-wise nonlinearity.

14. **Further Development:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents114}  
    :   

***

## Towards End-to-End Speech Recognition with Recurrent Neural Networks
{: #content2}

1. **Introduction:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    :   This paper presents an ASR system that directly transcribes audio data with text, __without__ requiring an _intermediate phonetic representation_.

2. **Structure:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}    
    :   * __Input__: 
        * __Output__:  
                

3. **Strategy:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    :   The goal of this paper is a system where as much of the speech pipeline as possible is replaced by a single recurrent neural network (RNN) architecture.  
        The language model, however, will be lacking due to the limitation of the audio data to learn a strong LM. 

4. **Solves:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    :   * First attempts used __RNNs__ or standard __LSTMs__. These models lacked the complexity that was needed to capture all the models required for ASR. 

5. **Key Insights:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    :   * The model uses Bidirectional LSTMs to capture the nuances of the problem.  
        * The system uses a new __objective function__ that trains the network to directly optimize the __WER__.  

6. **Preparing the Data (Pre-Processing):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    :   The paper uses __spectrograms__ as a minimal preprocessing scheme.  

7. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  
    :   The system is composed of:  
        * A __Bi-LSTM__  
        * A __CTC output layer__  
        * A __combined objective function__:  
            The new objective function at allows an RNN to be trained to optimize the expected value of an arbitrary loss function defined over output transcriptions (such as __WER__).  
            Given input sequence $$x$$, the distribution $$P(y\vert x)$$ over transcriptions sequences $$y$$ defined by CTC, and a real-valued transcription loss function $$\mathcal{L}(x, y)$$, the expected transcription loss $$\mathcal{L}(x)$$ is defined:  
            <p>$$\begin{align}
                \mathcal{L}(x) &= \sum_y P(y \vert x)\mathcal{L}(x,y) \\ 
                &= \sum_y \sum_{a \in \mathcal{B}^{-1}(y)} P(a \vert x)\mathcal{L}(x,y) \\
                &= \sum_a P(a \vert x)\mathcal{L}(x,\mathcal{B}(a))
                \end{align}$$</p>  
        <button>Show Derivation</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![Approximation and Differentiation](/main_files/dl/nlp/speech_research/3.png){: hidden="" width="80%"}


8. **Algorithm:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}  
    :   

9. **Issues/The Bottleneck:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents29}  
    :   

10. **Results:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents210}  
    :   * __WSJC__ (
    WER): 
            * Standard: $$27.3\%$$  
            * w/Lexicon of allowed words: $$21.9\%$$ 
            * Trigram LM: $$8.2\%$$ 
            * w/Baseline system: $$6.7\%$$

***

## Attention-Based Models for Speech Recognition
{: #content3}

1. **Introduction:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   This paper introduces and extends the attention mechanism with features needed for ASR. It adds location-awareness to the attention mechanism to add robustness against different lengths of utterances.  

2. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}    
    :   Learning to recognize speech can be viewed as learning to generate a sequence (transcription) given another sequence (speech).  
        From this perspective it is similar to machine translation and handwriting synthesis tasks, for which attention-based methods have been found suitable. 
    :   __How ASR differs:__  
        Compared to _Machine Translation_, speech recognition differs by requesting much longer input sequences which introduces a challenge of distinguishing similar speech fragments in a single utterance.  
        > thousands of frames instead of dozens of words   
    :   It is different from _Handwriting Synthesis_, since the input sequence is much noisier and does not have a clear structure.  

2. **Structure:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}    
    :   * __Input__: $$x=(x_1, \ldots, x_{L'})$$ is a sequence of feature vectors  
            * Each feature vector is extracted from a small overlapping window of audio frames
        * __Output__: $$y$$ a sequence of __phonemes__   

3. **Strategy:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    :   The goal of this paper is a system, that uses attention-mechanism with location awareness, whose performance is comparable to that of the conventional approaches.   
    :   * For each generated phoneme, an attention mechanism selects or weighs the signals produced by a trained feature extraction mechanism at potentially all of the time steps in the input sequence (speech frames).  
        * The weighted feature vector then helps to condition the generation of the next element of the output sequence.  
        * Since the utterances in this dataset are rather short (mostly under 5 seconds), we measure the ability of the considered models in recognizing much longer utterances which were created by artificially concatenating the existing utterances.

4. **Solves:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    :   * __Problem__:  
            The [attention-based model proposed for NMT](https://arxiv.org/abs/1409.0473) demonstrates vulnerability to the issue of similar speech fragments with __longer, concatenated utterances__.  
            The paper argues that  this model adapted to track the absolute location in the input sequence of the content it is recognizing, a strategy feasible for short utterances from the original test set but inherently unscalable.  
        * __Solution__:  
            The attention-mechanism is modified to take into account the location of the focus from the previous step and the features of the input sequence by adding as inputs to the attention mechanism auxiliary *__Convolutional Features__* which are extracted by convolving the attention weights from the previous step with trainable filters.  

5. **Key Insights:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}  
    :   * Introduces attention-mechanism to ASR
        * The attention-mechanism is modified to take into account:  
            * location of the focus from the previous step  
            * features of the input sequence
        * Proposes a generic method of adding location awareness to the attention mechanism
        * Introduce a modification of the attention mechanism to avoid concentrating the attention on a single frame  

7. **Attention-based Recurrent Sequence Generator (ARSG):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents37}  
    :   is a recurrent neural network that stochastically generates an output sequence $$(y_1, \ldots, y_T)$$ from an input $$x$$.  
    In practice, $$x$$ is often processed by an __encoder__ which outputs a sequential input representation $$h = (h_1, \ldots, h_L)$$ more suitable for the attention mechanism to work with.  
    :   The __Encoder__: a deep bidirectional recurrent network.  
        It forms a sequential representation h of length $$L = L'$$.  
    :   __Structure:__{: style="color: red"}    
        * *__Input__*: $$x = (x_1, \ldots, x_{L'})$$ is a sequence of feature vectors   
            > Each feature vector is extracted from a small overlapping window of audio frames.  
        * *__Output__*: $$y$$ is a sequence of phonemes
    :   __Strategy:__{: style="color: red"}    
        At the $$i$$-th step an ARSG generates an output $$y_i$$ by focusing on the relevant elements of $$h$$:  
    :   $$\begin{align}
        \alpha_i &= \text{Attend}(s_{i-1}, \alpha _{i-1}), h) & (1) \\
        g_i &= \sum_{j=1}^L \alpha_{i,j} h_j & (2) //
        y_i &\sim \text{Generate}(s_{i-1}, g_i) & (3)  
        \end{align}$$
    :   where $$s_{i‚àí1}$$ is the $$(i ‚àí 1)$$-th state of the recurrent neural network to which we refer as the __generator__, $$\alpha_i \in \mathbb{R}^L$$ is a vector of the _attention weights_, also often called the __alignment__; and $$g_i$$ is the __glimpse__.  
        The step is completed by computing a *__new generator state__*:  
    :   $$s_i = \text{Recurrency}(s_{i-1}, g_i, y_i)$$  
    :   where the _Recurrency_ is an RNN.  
    :   ![img](/main_files/dl/nlp/speech_research/4.png){: width="100%"}  

12. **Attention-mechanism Types and Speech Recognition:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents312}  
    :   __Types of Attention:__{: style="color: red"}      
        * (Generic) Hybrid Attention: $$\alpha_i = \text{Attend}(s_{i-1}, \alpha_{i-1}, h)$$  
        * Content-based Attention: $$\alpha_i = \text{Attend}(s_{i-1}, h)$$   
            In this case, Attend is often implemented by scoring each element in h separately and normalizing the scores:  
            $$e_{i,j} = \text{Score}(s_{i-1}, h_j) \\$$ 
              $$\alpha_{i,j} = \dfrac{\text{exp} (e_{i,j}) }{\sum_{j=1}^L \text{exp}(e_{i,j})}$$  
            * __Limitations__:  
                The main limitation of such scheme is that identical or very similar elements of $$h$$ are scored equally regardless of their position in the sequence.  
                Often this issue is partially alleviated by an encoder such as e.g. a BiRNN or a deep convolutional network that encode contextual information into every element of h . However, capacity of h elements is always limited, and thus disambiguation by context is only possible to a limited extent.  
        * Location-based Attention: $$\alpha_i = \text{Attend}(s_{i-1}, \alpha_{i-1})$$   
            a location-based attention mechanism computes the alignment from the generator state and the previous alignment only.  
            * __Limitations__:  
                the model would have to predict the distance between consequent phonemes using $$s_{i‚àí1}$$ only, which we expect to be hard due to large variance of this quantity.  
    :   Thus, we conclude that the __*Hybrid Attention*__ mechanism is a suitable candidate.  
        Ideally, we need an attention model that uses the previous alignment $$\alpha_{i-1}$$ to select a short list of elements from $$h$$, from which the content-based attention, will select the relevant ones without confusion.  

6. **Preparing the Data (Pre-Processing):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents36}  
    :   The paper uses __spectrograms__ as a minimal preprocessing scheme.  

7. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents37}  
    :   Start with the __ARSG__-based model:  
        * __Encoder__: is a __Bi-RNN__  
        <p>$$e_{i,j} = w^T \tanh (Ws_{i-1} + Vh_j + b)$$</p>
        * __Attention__: Content-Based Attention extended for _location awareness_  
            <p>$$e_{i,j} = w^T \tanh (Ws_{i-1} + Vh_j + Uf_{i,j} + b)$$</p>
    :   __Extending the Attention Mechanism:__  
        Content-Based Attention extended for _location awareness_ by making it take into account the alignment produced at the previous step.  
        * First, we extract $$k$$ vectors $$f_{i,j} \in \mathbb{R}^k$$ for every position $$j$$ of the previous alignment $$\alpha_{i‚àí1}$$ by convolving it with a matrix $$F \in \mathbb{R}^{k\times r}$$:  
            <p>$$f_i = F * \alpha_{i-1}$$</p>
        * These additional vectors $$f_{i,j}$$ are then used by the scoring mechanism $$e_{i,j}$$:  
            <p>$$e_{i,j} = w^T \tanh (Ws_{i-1} + Vh_j + Uf_{i,j} + b)$$</p>  

                
            

8. **Algorithm:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents38}  
    :   

9. **Issues/The Bottleneck:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents39}  
    :   


***

## A Neural Transducer
{: #content4}

 

***

## Deep Speech 2
{: #content5}

1. **Introduction:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents51}  
    :   This paper improves on the previous attempt at an End-to-End system for ASR. It increases the complexity of the architecture and is able to achieve high accuracy on two different languages -- English and Chinese.   

2. **Structure:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents52}    
    :   * __Input__: vector of speech spectrograms  
            * An *__utterance__* $$x^{(i)}$$: is a time-series of length $$T^{(i)}$$ composed of time-slices where each is a vector of audio (spectrogram) features $$x_{t,p}^{(i)}, t=1,...,T^{(i)}$$, where $$p$$ denotes the power of the p'th frequency bin in the audio frame at time $$t$$.  
        * __Output__: English text transcript $$y$$  
    :   * __Goal__:  
            The goal of the RNN is to convert an input sequence $$x$$ into a sequence of character probabilities for the transcription $$y$$, with $$\tilde{y}_t = P(c_t\vert x)$$, where $$c_t \in \{\text{a, b, c, } \ldots \text{,  z, space,  apostrophe, blank}\}$$.
                
3. **Strategy:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents53}  
    :   The goal is to replace the multi-part model with a single RNN network that captures as much of the information needed to do transcription in a single system.  

4. **Solves:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents54}  
    :   * Previous models only used DNNs as a single component in a complex pipeline.  
            NNs are trained to classify __individual frames of acoustic data__, and then, their output distributions are reformulated as emission probabilities for a HMM.  
            In this case, the objective function used to train the networks is therefore substantially different from the true performance measure (sequence-level transcription accuracy.  
            This leads to problems where one system might have an improved accuracy rate but the overall transcription accuracy can still decrease.  
        *  An additional problem is that the frame-level training targets must be inferred from the alignments determined by the HMM. This leads to an awkward iterative procedure, where network retraining is alternated with HMM re-alignments to generate more accurate targets.  

5. **Key Insights:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents55}  
    :   * As an __End-to-End__ model, this system avoids the problems of __multi-part__ systems that lead to inconsistent training criteria and difficulty of integration.   
            The network is trained directly on the text transcripts: no phonetic representation (and hence no pronunciation dictionary or state tying) is used.  
        * Using __CTC__ objective, the system is able to better approximate and solve the alignment problem avoiding HMM realignment training.  
            Since CTC integrates out over all possible input-output alignments, no forced alignment is required to provide training targets.  
        * The Dataset is augmented with newly synthesized data and modified to include all the variations and effects that face ASR problems.    
            This greatly increases the system performance on particularly noisy/affected speech.  

6. **Preparing Data (Pre-Processing):**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents56}  
    :   The paper uses __spectrograms__ as a minimal preprocessing scheme.  
    :   ![img](/main_files/dl/nlp/speech_research/2.png){: width="60%"}    

7. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents57}  
    :   The system is composed of:  
        * An __RNN__:    
            * 5 layers of __hidden units__:  
                * 3 Layer of __Feed-forward Nets__:  
                    * For the __input layer__, the output depends on the spectrogram frame $$x_t$$ along with a context of $$C$$ frames on each side.  
                        > $$C \in \{5, 7, 9\}$$  
                    * The non-recurrent layers operate on independent data for each time step:  
                        $$h_t^{(l)} = g(W^{(l)} h_{(t)}^{(l-1)} + b^{(l)}),$$  
                        where $$g(z) = \min \{\max \{0, z\}, 20\}$$ is the *clipped RELU*.    
                * 2 layers of __Recurrent Nets__:  
                    * 1 layer of a __Bi-LSTM__:  
                        * Includes two sets of hidden units: 
                            A set with forward recurrence $$h^{(f)}$$  
                            A set with backward recurrence $$h^{(b)}$$:  
                            $$h_t^{(f)} = g(W^{(4)}h_t^{(3)} + W_r^{(b)} h_{t-1}^{(b)} + b ^{(4)}) \\ 
                            h_t^{(b)} = g(W^{(4)}h_t^{(3)} + W_r^{(b)} h_{t+1}^{(b)} + b ^{(4)})$$  
                            > Note that $$h^{(f)}$$ must be computed sequentially from $$t = 1$$ to $$t = T^{(i)}$$ for the i‚Äôth utterance, while the units $$h^{(b)}$$ must be computed sequentially in reverse from $$t = T^{(i)}$$ to $$t = 1$$.  
                    * 1 layer of __Feed-forward Nets__:   
                        * The fifth (non-recurrent) layer takes both the forward and backward units as inputs:  
                            $$h_t^{(5)} = g(W ^{(5)}h_t ^{(4)} + b ^{(5)}),$$  
                            where $$h_t^{(4)} = h_t^{(f)} + h_t^{(b)}$$ 
            * An __Output__ layer made of a standard __softmax function__ that yields the predicted character probabilities for each time-slice $$t$$ and character $$k$$ in the alphabet:   
                $$\displaystyle{h _{(t,k)} ^{(6)} = \hat{y} _{(t,k)} = P(c_t = k \vert x) = \dfrac{\exp (W_k ^{(6)} h_t ^{(5)} + b_k ^{(6)})}{\sum_j \exp (W_j ^{(6)}h_t ^{(5)} + b_j ^{(6)})}},$$  
                where $$W_k ^{(6)}$$ and $$b_k ^{(6)}$$ denote the k'th column of the weight matrix and k'th bias.  
        * A *__CTC__* __Loss Function__ $$\mathcal{L}(\hat{y}, y)$$  
        * An *__N-gram Language Model__* 
        * A __combined Objective Function__:  
    :   $$Q(c) = \log (P(x \vert x)) + \alpha \log (P_{\text{LM}}(c) + \beta \text{word_count}(c))$$   
    :   ![img](/main_files/dl/nlp/speech_research/1.png){: width="80%"}    
8. **Algorithm:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents58}  
    :   * Given the output $$P(c \vert x)$$ of the RNN: perform a __search__ to find the sequence of characters $$c_1, c_2, ...$$ that is most probable according to both:  
            1. The RNN Output
            2. The Language Model  
        * We maximize the combined objective:  
            $$Q(c) = \log (P(x \vert x)) + \alpha \log (P_{\text{LM}}(c) + \beta \text{word_count}(c))$$  
            where the term $$P_{\text{lm}}$$ denotes the probability of the sequence $$c$$ according to the N-gram model.  
        * The objective is maximized using a highly optimized __beam search__ algorithm  
            > beam size: 1000-8000

9. **Training:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents59}  
    :   * The gradient of the CTC Loss $$\nabla_{\hat{y}} \mathcal{L}(\hat{y}, y)$$ with respect to the net outputs given the ground-truth character sequence $$y$$ is computed
    :   * Nesterov‚Äôs Accelerated gradient
        * Nesterov Momentum
        * Annealing the learning rate by a constant factor
        * Dropout  
        * Striding -- shortening the recurrent layers by taking strides of size $$2$$.  
            The unrolled RNN will have __half__ as many steps.  
            > similar to a convolutional network with a step-size of 2 in the first layer.  

10. **Parameters:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents510}  
    :   * __Momentum__: $$0.99$$ 
        * __Dropout__: $$5-10 \%$$ (FFN only)   
        * __Trade-Off Params__: use cross-validation for $$\alpha, \beta$$  

11. **Issues/The Bottleneck:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents511}  
    :   

12. **Results:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents512}  
    :   * __SwitchboardHub5‚Äô00__  (
    WER): 
            * Standard: $$16.0\%$$  
            * w/Lexicon of allowed words: $$21.9\%$$ 
            * Trigram LM: $$8.2\%$$ 
            * w/Baseline system: $$6.7\%$$

13. **Discussion:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents513}  
    :   * __Why avoid LSTMs__:  
            One disadvantage of LSTM cells is that they require computing and storing multiple gating neuron responses at each step.  
            Since the forward and backward recurrences are sequential, this small additional cost can become a computational bottleneck.  
    :   * __Why a homogeneous model__:  
            By using a homogeneous model we have made the computation of the recurrent activations as efficient as possible: computing the ReLu outputs involves only a few highly optimized BLAS operations on the GPU and a single point-wise nonlinearity.

14. **Further Development:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents514}  
    :   

*** 

## Listen, Attend and Spell (LAS)
{: #content6}

1. **Introduction:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents61}  
    :   This paper presents a a neural network that learns to transcribe speech utterances to characters.  Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly.  
    :   The system has two components: a listener and a speller.  
    :   __LAS__ is based on the sequence to sequence learning framework with attention.  

2. **Structure:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents62}    
    :   * __Input__: $$\mathbb{x} = (x_1, \ldots, x_T)$$ a sequence of filter bank spectra (acoustic) features
            
        * __Output__: $$\mathbb{y} = (\text{<sos>}, y_1, \ldots, y_S, \text{<eos>}), y_i \in \{\text{a, b, c, ¬∑ ¬∑ ¬∑ , z, 0, ¬∑ ¬∑ ¬∑ , 9, <spacei,<comma>,<period>,<apostrophe>,<unk> }\}$$ the output sequence of characters  
    :   * __Goal__:  
            The goal of the RNN is to convert an input sequence $$x$$ into a sequence of character probabilities for the transcription $$y$$, with $$\tilde{y}_t = P(c_t\vert x)$$, where $$c_t \in \{\text{a, b, c, } \ldots \text{,  z, space,  apostrophe, blank}\}$$.
                
3. **Strategy:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents63}  
    :   __LAS__ is based on the sequence to sequence learning framework with attention.  
    :   * We want to model each character output $$y_i$$ as a conditional distribution over the previous characters $$y_{\leq i+1}$$ and the input signal $$\mathbb{x}$$ using the chain rule:  
        <p>$$P(\mathbb{y} \vert \mathbb{x}) = \prod_i P(y_i \vert \mathbb{x}, y_{\leq i+1}) \:\:\:\: (1)$$</p>
        * 

4. **Solves:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents64}  
    :   * __CTC__:  
            CTC assumes that the label outputs are conditionally independent of each other
        * __Seq2Seq__:  
            the sequence to sequence approach has only been applied to phoneme sequences, and not trained end-to-end for speech recognition. 
        * 

5. **Key Insights:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents65}  
    :   * Use a pyramidal RNN model for the listener, which reduces the number of time steps that the attention model has to extract relevant information from.  
        The pyramid structure also reduces the computational complexity.    
        * Character-based transcription allows the handling of rare and OOV words automatically  
        * Attention 

77. **The Model:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents677}  
:   __Listen__:  
    Uses a __Bi-directional LSTM__ with a pyramid structure.  
    > The pyramid structure is needed to reduce the length $$U$$ of $$\mathbf{h}$$, from $$T$$ , the length of the input $$\mathbb{h}$$ , because the input speech signals can be hundreds to thousands of frames long.  
    * __Pyramidal LSTM__:  
        The output at the i-th time step, from the j-th layer is changed from:  
        <p>$$h_i^j = \text{BLSTM}(h_{i-1}^j, h_{i}^{j-1})$$</p>  
        to, instead, we concatenate the outputs at consecutive steps of each layer before feeding it to the next layer:     
        <p>$$h_i^j = \text{pBLSTM}(h_{i-1}^j, \left[h_{2i}^{j-1}, h_{2i+1}^{j-1}\right])$$</p>  
        > In the model, we stack 3 pBLSTMs on top of the bottom BLSTM layer to reduce the time resolution $$2^3 = 8$$ times.  
:   __Attend and Spell:__  
    The function is computed using an __attention-based LSTM transducer__.  
    At every output step, the transducer produces a probability distribution over the next character conditioned on all the characters seen previously.  
    The __distribution__ for $$y_i$$ is a function of the decoder state $$s_i$$ and context $$c_i$$.  
    The __decoder state__ $$s_i$$ is a function of the previous state $$s_{i‚àí1}$$, the previously emitted character $$y_{i‚àí1}$$ and context $$c_{i‚àí1}$$.  
    The __context vector__ $$c_i$$ is produced by an attention mechanism:  
    <p>$$ c_i = \text{AttentionContext}(s_i, \mathbf{h}) \\
        s_i = \text{RNN}(s_{i-1}, y_{i-1}, c_{i-1}) \\
    P(y_i \vert \mathbf{x}, y_{\leq i+1}) = \text{CharacterDistribution}(s_i, c_i)$$  </p>  
    where __CharacterDistribution__ is an __MLP__ with softmax outputs over characters, and __RNN__ is a 2 layer LSTM.  
    The __Attention__ Mechanism:  
    At each step $$i$$, the attention mechanism, _AttentionContext_ generates a context vector $$c_i$$ encapsulating the information in the acoustic signal needed to generate the next character.  
    The attention model is __content based__ - the contents of the decoder state $$s_i$$ are matched to the contents of $$h_u$$ representing time step $$u$$ of $$\mathbf{h}$$ to generate an attention vector $$\alpha_i$$.  
    $$\alpha_i$$ is used to linearly blend vectors $$h_u$$ to create $$c_i$$.  
    Specifically, at each decoder timestep $$i$$ , the AttentionContext function computes the scalar energy
    $$e_{i,u}$$ for each time step $$u$$ , using vector $$h_u \in h$$ and $$s_i$$.  
    The scalar energy $$e_{i,u}$$ is converted into a
    probability distribution over times steps (or attention) $$\alpha_i$$  using a softmax function. This is used to create the context vector $$c_i$$  by linearly blending the listener features, $$h_u$$, at different time steps:  
    <p>  
    $$\begin{align}
        e_{i,u} &= <\phi(s_i), \psi(h_u)> \\
        \alpha_{i,u} &= \dfrac{\exp(e_{i,u})}{\sum_u \exp(e_{i,u})} \\
        c_i &= \sum_u \alpha_{i,u}h_u 
        \end{align}
        $$ 
    </p>  
    where $$\phi$$ and $$\psi$$ are __MLP__ Networks.  
    On convergence, the $$\alpha_i$$  distribution is typically very sharp, and focused on only a few frames of $$\mathbf{h}$$ ; $$c_i$$ can be seen as a continuous bag of weighted features of $$\mathbf{h}$$.  

6. **Preparing Data (Pre-Processing):**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents66}  

7. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents67}
    :   ![img](/main_files/dl/nlp/speech_research/5.png){: width="60%"}  
    :   * __Encoder (listener):__  
            An __acoustic model__ encoder, whose key operation is ```Listen```.  
            It converts low level speech signals into higher level features.  
            * __(pyramidal) RNN__:  
                * __Bi-Directional LSTM__:  
                            
                    * *__Structure__*:  
                        * *__Input:__* original signal $$ \mathbb{x}$$ 
                        * *__Output:__* a high level representation $$\mathbf{h} = (h_1, ]ldots, h_U)$$, with $$U \leq T$$   
                        
        * __Decoder (speller)__:  
            The speller is an _attention-based_ __character decoder__, whose key operation is ```AttendAndSpell```.  
            It converts the higher level features into output utterances by specifying a probability distribution over sequences of characters using the attention mechanism.  
            * __RNN__:  
                * *__Structure__*:  
                    * *__Input:__* features $$ \mathbf{h}$$ 
                    * *__Output:__* a probability distribution over character sequences:   $$\mathbf{h} = (h_1, ]ldots, h_U)$$, with $$U \leq T$$
    
8. **Algorithm:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents68}  
    :   

9. **Training:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents69}  
    :   * The gradient of the CTC Loss $$\nabla_{\hat{y}} \mathcal{L}(\hat{y}, y)$$ with respect to the net outputs given the ground-truth character sequence $$y$$ is computed
    :   * Nesterov‚Äôs Accelerated gradient
        * Nesterov Momentum
        * Annealing the learning rate by a constant factor
        * Dropout  
        * Striding -- shortening the recurrent layers by taking strides of size $$2$$.  
            The unrolled RNN will have __half__ as many steps.  
            > similar to a convolutional network with a step-size of 2 in the first layer.  

99. **Inference (Decoding and Rescoring):**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents699}  


10. **Parameters:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents610}  
    :   * __Momentum__: $$0.99$$ 
        * __Dropout__: $$5-10 \%$$ (FFN only)   
        * __Trade-Off Params__: use cross-validation for $$\alpha, \beta$$  


11. **Issues/The Bottleneck:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents611}  
    :   

12. **Results:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents612}  
    :   * __SwitchboardHub5‚Äô00__  (
    WER): 
            * Standard: $$16.0\%$$  
            * w/Lexicon of allowed words: $$21.9\%$$ 
            * Trigram LM: $$8.2\%$$ 
            * w/Baseline system: $$6.7\%$$

13. **Discussion:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents613}  
    :   * Without the attention mechanism, the model overfits the training data significantly, in spite of our large training set of three million utterances - it memorizes the training transcripts without paying attention to the acoustics.  
        * Without the pyramid structure in the encoder side, our model converges too slowly - even after a month of training, the error rates were significantly higher than the errors reported
        * To reduce the overfitting of the speller to the training transcripts, use a sampling trick during training

14. **Further Development:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents614}  
    :   

***

***
***

TITLE: Neural Machine Translation <br /> 
LINK: research/dl/nlp/nmt.md


## Machine Translation
{: #content1}

1. **Methods:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   * Methods are _statistical_  
        * Uses _parallel corpora_

3. **Traditional MT:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13} 
    :   Traditional MT was very complex and included multiple disciplines coming in together.  
        The systems included many independent parts and required a lot of human engineering and experts.  
        The systems also scaled very poorly as the search problem was essentially exponential.

2. **Statistical Machine Translation Systems:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12} 
    :   * __Input__:  
            * __Source Language__: $$f$$  
            * __Target Language__: $$e$$  
        * __The Probabilistic Formulation__:  
    :   $$ \hat{e} = \mathrm{arg\,min}_e \: p(e \vert f) = \mathrm{arg\,min}_e \: p(f \vert e) p(e)$$
    :   * __The Translation Model $$p(f \vert e)$$__: trained on parallel corpus
        * __The Language Model $$p(e)$$__: trained on English only corpus  
        > Abundant and free!
    :   ![img](/main_files/dl/nlp/9/1.png){: width="100%"}     


4. **Deep Learning Naive Approach:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   One way we can learn to translate is to learn to _translate directly with an RNN_. 
    :   ![img](/main_files/dl/nlp/9/2.png){: width="80%"}  
    :   * __The Model__:  
            * *__Encoder__* :  
                $$h_t = \phi(h_{t-1}, x_t) = f(W^{(hh)}h_{t-1} + W^{(hx)}x_t)$$
            * *__Decoder__* :  
                $$\begin{align}
                    h_t & = \phi(h_{t-1}) = f(W^{(hh)}h_{t-1}) \\
                    y_t & = \text{softmax}(W^{(S)}h_t)
                    \end{align}$$  
            * *__Error__* :  
                $$\displaystyle{\max_\theta \frac{1}{N} \sum_{n=1}^N \log p_\theta(y^{(n)}\vert x^{(n)})}$$  
                > Cross Entropy Error.  
            * *__Goal__* :  
                Minimize the __Cross Entropy Error__ for all target words conditioned on source words
    :   * __Drawbacks__:  
            The problem of this approach lies in the fact that the last hidden layer needs to capture the entire sentence to be translated.  
            However, since we know that the _RNN Gradient_ basically __vanishes__ as the length of the sequence increases, the last hidden layer is usually only capable of capturing upto ~5 words.
    :   ![img](/main_files/dl/nlp/9/3.png){: width="100%"}     

5. **Naive RNN Translation Model Extension:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15} 
    :   1. Train Different RNN weights for encoding and decoding
            ![img](/main_files/dl/nlp/9/4.png){: width="100%"}   
        2. Compute every hidden state in the decoder from the following concatenated vectors :  
            * Previous hidden state (standard)
            * Last hidden vector of encoder $$c=h_T$$  
            * Previous predicted output word $$y_{t-1}$$.  
            $$\implies h_{D, t} = \phi_D(h_{t-1}, c, y_{t-1})$$  
            > NOTE: Each input of $$\phi$$ has its own linear transformation matrix.  
        3. Train stacked/deep RNNs with multiple layers. 
        4. Potentially train Bidirectional Encoder
        5. Train input sequence in reverser order for simpler optimization problem:  
            Instead of $$A\:B\:C \rightarrow X\:Y$$ train with $$ C\:B\:A \rightarrow X\:Y$$  
        6. Better Units (Main Improvement):  
            * Use more complex hidden unit computation in recurrence
            * Use GRUs _(Cho et al. 2014)_  
            * _Main Ideas_:
                * Keep around memories to capture long distance dependencies
                * Allow error messages to flow at different strengths depending on the inputs

***

## GRUs
{: #content2}

1. **Gated Recurrent Units:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21} 
    :   __Gated Recurrent Units (GRUs)__ are a class of modified (_**Gated**_) RNNs that allow them to combat the _vanishing gradient problem_ by allowing them to capture more information/long range connections about the past (_memory_) and decide how strong each signal is.  

2. **Main Idea:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22} 
    :   Unlike _standard RNNs_ which compute the hidden layer at the next time step directly first, __GRUs__ computes two additional layers (__gates__):  
        > Each with different weights
    :   * *__Update Gate__*:  
    :   $$z_t = \sigma(W^{(z)}x_t + U^{(z)}h_{t-1})$$  
    :   * *__Reset Gate__*:  
    :   $$r_t = \sigma(W^{(r)}x_t + U^{(r)}h_{t-1})$$  
    :   The __Update Gate__ and __Reset Gate__ computed, allow us to more directly influence/manipulate what information do we care about (and want to store/keep) and what content we can ignore.  
        We can view the actions of these gates from their respecting equations as:  
    :   * *__New Memory Content__*:  
            at each hidden layer at a given time step, we compute some new memory content,  
            if the reset gate $$ = ~0$$, then this ignores previous memory, and only stores the new word information.  
    :   $$ \tilde{h}_t = \tanh(Wx_t + r_t \odot Uh_{t-1})$$
    :   * *__Final Memory__*:  
            the actual memory at a time step $$t$$, combines the _Current_ and _Previous time steps_,  
            if the _update gate_ $$ = ~0$$, then this, again, ignores the _newly computed memory content_, and keeps the old memory it possessed.  
    :   $$h_t = z_t \odot h_{t-1} + (1-z_t) \odot \tilde{h}_t$$  

***

## Long Short-Term Memory
{: #content3}

1. **LSTM:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31} 
    :   The __Long Short-Term Memory__ (LSTM) Network is a special case of the Recurrent Neural Network (RNN) that uses special gated units (a.k.a LSTM units) as building blocks for the layers of the RNN.  

2. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32} 
    :   The LSTM, usually, has four gates:  
    :   * __Input Gate__: 
            The input gate determines how much does the _current input vector (current cell)_ matters      
    :   $$i_t = \sigma(W^{(i)}x_t + U^{(i)}h_{t-1})$$ 
    :   * __Forget Gate__: 
            Determines how much of the _past memory_, that we have kept, is still needed   
    :   $$i_t = \sigma(W^{(i)}x_t + U^{(i)}h_{t-1})$$ 
    :   * __Output Gate__: 
            Determines how much of the _current cell_ matters for our _current prediction (i.e. passed to the sigmoid)_
    :   $$i_t = \sigma(W^{(i)}x_t + U^{(i)}h_{t-1})$$  
    :   * __Memory Cell__: 
            The memory cell is the cell that contains the _short-term memory_ collected from each input
    :   $$\begin{align}
            \tilde{c}_t & = \tanh(W^{(c)}x_t + U^{(c)}h_{t-1}) & \text{New Memory} \\
            c_t & = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t & \text{Final Memory}
        \end{align}$$
    :   The __Final Hidden State__ is calculated as follows:  
    :   $$h_t = o_t \odot \sigma(c_t)$$
     

3. **Properties:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33} 
    :   * __Syntactic Invariance__:  
            When one projects down the vectors from the _last time step hidden layer_ (with PCA), one can observe the spatial localization of _syntacticly-similar sentences_  
            ![img](/main_files/dl/nlp/9/5.png){: width="100%"}  

***

## Neural Machine Translation (NMT)
{: #content4}

1. **NMT:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41} 
    :   __NMT__ is an approach to machine translation that uses a large artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.

2. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42} 
    :   The approach uses an __Encoder-Decoder__ architecture.
    :   ![img](/main_files/dl/nlp/9/6.png){: width="70%"}   
    :   NMT models can be seen as a special case of _language models_.   
        In other words, they can be seen as __Conditional Recurrent Language Model__; a language model that has been conditioned on the calculated _encoded_ vector representation of the sentence.

3. **Modern Sequence Models for NMT:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43} 
    :   

4. **Issues of NMT:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents44} 
    :   * __Predicting Unseen Words__:  
            The NMT model is very vulnerable when presented with a word that it has never seen during training (e.g. a new name).  
            In-fact, the model might not even be able to place the (translated) unseen word correctly in the (translated) sentence.
        * __Solution__:  
            * A possible solution is to apply _character-based_ translation, instead of word-based, however, this approach makes for very long sequences and the computation becomes infeasible.  
            * The (current) proposed approach is to use a __Mixture Model of Softmax and Pointers__  
                > _Pointer-sentinel Model_

5. **The Big Wins of NMT:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents45} 
    :   1. __End-to-End Training__: All parameters are simultaneously optimized to minimize a loss function on the networks output 
        2. __Distributed Representations Share Strength__: Better exploitation of word and phrase similarities 
        3. __Better Exploitation of Context__: NMT can use a much bigger context - both source and partial target text - to translate more accurately
        4. __More Fluent Text Generation__: Deep Learning text generation is much higher quality

8. **(GNMT) Google's Multilingual Neural Machine Translation System - Zero shot Translation:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents48} 
    :   * __Multilingual NMT Approaches__:  
    :   ![img](/main_files/dl/nlp/9/7.png){: width="100%"}   
    :   * __Google's Approach__:  
            Add an __*Artificial Token*__ at the beginning of the input sentence to indicate the target language.  


***
***

TITLE: Introduction to NLP <br /> Natural Language Processing
LINK: research/dl/nlp/intro.md


## Introduction
{: #content1}

0. **NLP:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents10}  
    :   __Natural Language Processing__ is a field at the intersection of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language data.

1. **Problems in NLP:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11} 
    :   * Question Answering (QA) 
        * Information Extraction (IE)    
        * Sentiment Analysis  
        * Machine Translation (MT)  
        * Spam Detection  
        * Parts-of-Speech (POS) Tagging  
        * Named Entity Recognition (NER)
        * Conference Resolution  
        * Word Sense Disambiguation (WSD)  
        * Parsing  
        * Paraphrasing  
        * Summarization  
        * Dialog  
    :   * __Fully understanding and representing the meaning of language__ (or even defining it) is a difficult goal.
            * Perfect language understanding is __AI-complete__  

2. **(mostly) Solved Problems in NLP:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12} 
    :   * Spam Detection  
        * Parts-of-Speech (POS) Tagging  
        * Named Entity Recognition (NER)  


3. **Within-Reach Problems:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13} 
    :   * Sentiment Analysis  
        * Conference Resolution    
        * Word Sense Disambiguation (WSD)  
        * Parsing  
        * Machine Translation (MT)  
        * Information Extraction (IE)    


4. **Open Problems in NLP:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14} 
    :   * Question Answering (QA)   
        * Paraphrasing  
        * Summarization  
        * Dialog  

5. **Issues in NLP (why nlp is hard?):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15} 
    :   * __Non-Standard English__: "Great Job @ahmed_badary! I luv u 2!! were SOO PROUD of dis."  
        * __Segmentation Issues__: "New York-New Haven" vs "New-York New-Haven"  
        * __Idioms__: "dark horse", "getting cold feet", "losing face"  
        * __Neologisms__: "unfriend", "retweet", "google", "bromance"  
        * __World Knowledge__: "Ahmed and Zach are brothers", "Ahmed and Zach are fathers"    
        * __Tricky Entity Names__: "Where is _Life of Pie_ playing tonight?", "_Let it be_ was a hit song!"  

6. **Tools we need for NLP:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16} 
    :   * Knowledge about Language.  
        * Knowledge about the World.   
        * A way to combine knowledge sources.  

7. **Methods:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17} 
    :   In general we need to construct __Probabilistic Models__ built from _language data_.    
    :   We do so by using _rough text features_.  
        > All the names models, methods, and tools mentioned above will be introduced later as you progress in the text.  

8. **NLP in the Industry:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    :   * Search
        * Online ad Matching
        * Automated/Assisted Translation
        * Sentiment analysis for marketing   or finance/trading
        * Speech recognition
        * Chatbots/Dialog Agents
            * Automatic Customer Support
            * Controlling Devices
            * Ordering Goods

***

## NLP and Deep Learning
{: #content2}

1. **What is Special about Human Language:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    :   * Human Language is a system specifically constructed to convey the speaker/writer's meaning.
        > It is a deliberate communication, not just an environmental signal.  
        * Human Language is a __discrete/symbolic/categorical signaling system__  
        * The categorical symbols of a language can be encoded as a signal for communication in several ways:  
            * Sound
            * Gesture 
            * Images (Writing)  
            Yet, __the symbol is invariant__ across different encodings!  
            ![img](/main_files/dl/nlp/1/1.png){: width="80%"}

        * However, a brain encoding appears to be a __continuous pattern of activation__, and the symbols are transmitted via __continuous signals__ of sound/vision.  

2. **Issues of NLP in Machine Learning:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    :   * According to the above passage; we see that although human language is largely symbolic; it is still interpreted by the brain as a continuous signal.  
        This means that we cannot encode this information in a discrete manner; and rather must learn in a sequential, continuous way.  
        * The large vocabulary, symbolic encoding of words creates a problem for machine learning ‚Äì __sparsity__!

3. **Machine Learning vs Deep Learning:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    :   * Most Machine Learning methods work well because of __human-designed representations__ and __input features__.  
            Thus, the _learning_ here is done, mostly, by the people/scientists/engineers who are designing the features and __not__ by the machines.  
            This, rendered Machine Learning to become just a __numerical optimization method__ for __optimizing weights__ to best make a final prediction.   
    :   * How does that differ with Deep Learning (DL)?  
            * __Representation learning__ attempts to automatically learn good features or representations
            * __Deep learning__ algorithms attempt to learn (multiple levels of) representation and an output  
            * __Raw Inputs__: DL can deal directly with _raw_ inputs (e.g. sound, characters, words)  

4. **Why Deep-Learning?**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    :   * Manually designed features are often over-specified, incomplete and take a long time to design and validate
        * Manually designed features are often over-specified, incomplete and take a long time to design and validate
        * Deep learning provides a very flexible, (almost?) universal, learnable framework for representing world, visual and linguistic information.
        * Deep learning can learn unsupervised (from raw text) and supervised (with specific labels like positive/negative)
        * In ~2010 deep learning techniques started outperforming other machine learning techniques.


5. **Why is NLP Hard (revisited):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    :   * Complexity in representing, learning and using linguistic/situational/world/visual knowledge
        * Human languages are ambiguous (unlike programming and other formal languages)
        * Human language interpretation depends on real world, common sense, and contextual knowledge

6. **Improvements in _Recent Years_ in NLP:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    :   spanning different:  
        * __Levels__: speech, words, syntax, semantics  
        * __Tools__: POS, entities, parsing  
        * __Applications__: MT, sentiment analysis, dialogue agents, QA      

***

## Representations of NLP Levels
{: #content3}

1. **Morphology:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   * __Traditionally__: Words are made of morphemes.  
            * uninterested -> un (prefix) + interest (stem) + ed (suffix)
        * __DL__:
            * Every morpheme is a vectors
            * A Neural Network combines two vectors into one vector
            * Luong et al. 2013  
            ![img](/main_files/dl/nlp/1/2.png){: width="33%"}  

2. **Semantics:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   * __Traditionally__: Lambda Calculus  
            * Carefully engineered functions
            * Take as inputs specific other functions
            * No notion of similarity or fuzziness of language  
        * __DL__:
            * Every word and every phrase and every logical expression is a vector 
            * A Neural Network combines two vectors into one vector  
            * Bowman et al. 2014  
            ![img](/main_files/dl/nlp/1/3.png){: width="45%"}  

    :   

    :   

    :   

    :   

    :   

    :   

***

## NLP Tools
{: #content4}

1. **Parsing for Sentence Structure:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    :   Neural networks can accurately determine the structure of sentences, supporting interpretation.  
    ![img](/main_files/dl/nlp/1/4.png){: width="70%"}

    :   

    :   

    :   

    :   

    :   

    :   

***

## NLP Applications
{: #content5}

1. **Sentiment Analysis:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents51}  
    :   * __Traditional__: Curated sentiment dictionaries combined with either bag-of-words representations (ignoring word order) or hand-designed negation features (ain‚Äôt gonna capture everything)
        * __DL__: Same deep learning model that was used for morphology, syntax and logical semantics can be used;  
        __RecursiveNN__.        

2. **Question Answering:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents52}  
    :   * __Traditional__: A lot of feature engineering to capture world and other knowledge, e.g., regular expressions, Berant et al. (2014)
        * __DL__: Facts are stored in vectors.  FILL-IN  
        __FILL-IN__.      


3. **Machine Translation:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents53}  
    :   * __Traditional__: Complex approaches with very high error rates.  
        * __DL__: _Neural Machine Translation_.  
            Source sentence is mapped to _vector_, then output sentence generated.  
            [Sutskever et al. 2014, Bahdanau et al. 2014, Luong and Manning 2016]  
        __FILL-IN__.   
        ![img](/main_files/dl/nlp/1/5.png){: width="70%"}

    :   

    :   

    :   

***
***

TITLE: ASR <br /> Automatic Speech Recognition
LINK: research/dl/nlp/speech.md


## Introduction to Speech
{: #content8}

1. **Probabilistic Speech Recognition:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents81}  
    :   Statistical ASR has been introduced/framed by __Frederick Jelinek__ in his famous paper [Continuous Speech Recognition by Statistical Methods](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1454428) who framed the problem as an _information theory_ problem.  
    :   We can view the problem of __ASR__ as a __*sequence labeling*__ problem, and, so, use statistical models (such as HMMs) to model the conditional probabilities between the states/words by viewing speech signal as a piecewise stationary signal or a short-time stationary signal. 
    :   * __Representation__: we _represent_ the _speech signal_ as an *__observation sequence__* $$o = \{o_t\}$$  
        * __Goal__: find the most likely _word sequence_ $$\hat{w}$$   
        * __Set-Up__:  
            * The system has a set of discrete states
            * The transitions from state to state are markovian and are according to the transition probabilities  
                > __Markovian__: Memoryless  
            * The _Acoustic Observations_ when making a transition are conditioned on _the state alone_ $$P(o_t \vert c_t)$$
            * The _goal_ is to _recover the state sequence_ and, consequently, the _word sequence_  

2. **Speech Problems and Considerations:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents82}  
    :   * __ASR__:  
            * _Spontaneous_ vs _Read_ speech
            * _Large_ vs _Small_ Vocabulary
            * _Continuous_ vs _Isolated_ Speech  
                > Continuous Speech is harder due to __*Co-Articulation*__   
            * _Noisy_ vs _Clear_ input
            * _Low_ vs _High_ Resources 
            * _Near-field_ vs _Far-field_ input
            * _Accent_-independence 
            * _Speaker-Adaptive_ vs _Stand-Alone_ (speaker-independent) 
            * The cocktail party problem 
        * __TTS__:  
            * Low Resource
            * Realistic prosody
        * __Speaker Identification__
        * __Speech Enhancement__
        * __Speech Separation__       

3. **Acoustic Representation:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents83}  
    :   __What is speech?__{: style="color: red"}  
        * Waves of changing air pressure - Longitudinal Waves (consisting of compressions and rarefactions)
        * Realized through excitation from the vocal cords
        * Modulated by the vocal tract and the articulators (tongue, teeth, lips) 
        * Vowels are produced with an open vocal tract (stationary)
            > parametrized by position of tongue
        * Consonants are constrictions of vocal tract
        * They get __converted__ to _Voltage_ with a microphone
        * They are __sampled__ (and quantized) with an _Analogue to Digital Converter_ 
    :   __Speech as waves:__{: style="color: red"}  
        * Human hearing range is: $$~50 HZ-20 kHZ$$
        * Human speech range is: $$~85 HZ-8 kHZ$$
        * Telephone speech sampling is $$8 kHz$$ and a bandwidth range of $$300 Hz-4 kHz$$ 
        * 1 bit per sample is intelligible
        * Contemporary Speech Processing mostly around 16 khz 16 bits/sample  
            > A lot of data to handle
    :   __Speech as digits (vectors):__{: style="color: red"}  
        * We seek a *__low-dimensional__* representation to ease the computation  
        * The low-dimensional representation needs to be __invariant to__:  
            * Speaker
            * Background noise
            * Rate of Speaking
            * etc.
        * We apply __Fourier Analysis__ to see the energy in different frequency bands, which allows analysis and processing
            * Specifically, we apply _windowed short-term_ *__Fast Fourier Transform (FFT)__*  
                > e.g. FFT on overlapping $$25ms$$ windows (400 samples) taken every $$10ms$$  
        ![img](/main_files/dl/nlp/12/16.png){: width="70%"}  
        > Each frame is around 25ms of speech  
        * FFT is still too high-dimensional  
            * We __Downsample__ by local weighted averages on _mel scale_ non-linear spacing, an d take a log:  
                $$ m = 1127 \ln(1+\dfrac{f}{700})$$  
            * This results in *__log-mel features__*, $$40+$$ dimensional features per frame    
                > Default for NN speech modelling  
    :   __Speech dimensionality for different models:__{: style="color: red"}  
        * __Gaussian Mixture Models (GMMs)__: 13 *__MFCCs__*  
            * *__MFCCs - Mel Frequency Cepstral Coefficients__*: are the discrete cosine transformation (DCT) of the mel filterbank energies \| Whitened and low-dimensional.  
                They are similar to _Principle Components_ of log spectra.  
            __GMMs__ used local differences (deltas) and second-order differences (delta-deltas) to capture the dynamics of the speech $$(13 \times 3 \text{ dim})$$
        * __FC-DNN__: 26 stacked frames of *__PLP__*  
            * *__PLP - Perceptual Linear Prediction__*: a common alternative representation using _Linear Discriminant Analysis (LDA)_  
                > Class aware __PCA__    
        * __LSTM/RNN/CNN__: 8 stacked frames of *__PLP__*  
    :   __Speech as Communication:__{: style="color: red"}      
        * Speech Consists of sentences (in ASR we usually talk about "utterances")  
        * Sentences are composed of words 
        * Minimal unit is a "phoneme" Minimal unit that distinguishes one word from another.
            * Set of 40-60 distinct sounds.
            * Vary per language 
            * Universal representations: 
                * *__IPA__* : international phonetic alphabet
                * *__X-SAMPA__* : (ASCII) 
        * *__Homophones__* : distinct words with the same pronunciation. (e.g. "there" vs "their") 
        * *__Prosody__* : How something is said can convey meaning. (e.g. "Yeah!" vs "Yeah?")  

9. **Microphones and Speakers:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents89}  
    :   * __Microphones__:  
            * Their is a _Diaphragm_ in the Mic
            * The Diaphragm vibrates with air pressure
            * The diaphragm is connected to a magnet in a coil
            * The magnet vibrates with the diaphragm
            * The coil has an electric current induced by the magnet based on the vibrations of the magnet
    :   * __Speakers__:  
            * The electric current flows from the sound-player through a wire into a coil
            * The coil has a metal inside it
            * The metal becomes magnetic and vibrates inside the coil based on the intensity of the current 
            * The magnetized metal is attached to a cone that produces the sound


4. **(Approximate) History of ASR:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents84}  
    * 1960s Dynamic Time Warping 
    * 1970s Hidden Markov Models 
    * Multi-layer perceptron 1986 
    * Speech recognition with neural networks 1987-1995 
    * Superseded by GMMs 1995-2009 
    * Neural network features 2002‚Äî 
    * Deep networks 2006‚Äî (Hinton, 2002) 
    * Deep networks for speech recognition:
        * Good results on TIMIT (Mohamed et al., 2009) 
        * Results on large vocabulary systems 2010 (Dahl et al., 2011) * Google launches DNN ASR product 2011
        * Dominant paradigm for ASR 2012 (Hinton et al., 2012) 
    * Recurrent networks for speech recognition 1990, 2012 - New models (CTC attention, LAS, neural transducer) 

5. **Datasets:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents85}  
    * __TIMIT__: 
        * Hand-marked phone boundaries are given 
        * 630 speakers $$\times$$ 10 utterances 
    * __Wall Street Journal (WSJ)__ 1986 Read speech. WSJO 1991, 30k vocab 
    * __Broadcast News (BN)__ 1996 104 hours 
    * __Switchboard (SWB)__ 1992. 2000 hours spontaneous telephone speech -  500 speakers 
    * __Google voice search__ - anonymized live traffic 3M utterances 2000 hours hand-transcribed 4M vocabulary. Constantly refreshed, synthetic reverberation + additive noise 
    * __DeepSpeech__ 5000h read (Lombard) speech + SWB with additive noise. 
    * __YouTube__ 125,000 hours aligned captions (Soltau et al., 2016) 

5. **Development:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents85}  
    ![img](/main_files/dl/nlp/12/17.png){: width="75%"}    

*** 

## The Methods and Models of Speech Recognition
{: #content9}

1. **Probabilistic Speech Recognition:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents91}  
    :   Statistical ASR has been introduced/framed by __Frederick Jelinek__ in his famous paper [Continuous Speech Recognition by Statistical Methods](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1454428) who framed the problem as an _information theory_ problem.  
    :   We can view the problem of __ASR__ as a _sequence labeling_ problem, and, so, use statistical models (such as HMMs) to model the conditional probabilities between the states/words by viewing speech signal as a piecewise stationary signal or a short-time stationary signal. 
    :   * __Representation__: we _represent_ the _speech signal_ as an *__observation sequence__* $$o = \{o_t\}$$  
        * __Goal__: find the most likely _word sequence_ $$\hat{w}$$   
        * __Set-Up__:  
            * The system has a set of discrete states
            * The transitions from state to state are markovian and are according to the transition probabilities  
                > __Markovian__: Memoryless  
            * The _Acoustic Observations_ when making a transition are conditioned on _the state alone_ $$P(o_t \vert c_t)$$
            * The _goal_ is to _recover the state sequence_ and, consequently, the _word sequence_  
                
2. **Fundamental Equation of Speech Recognition:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents92}  
    :   We set the __decoders output__ as the *__most likely sequence__* $$\hat{w}$$ from all the possible sequences, $$\mathcal{S}$$, for an observation sequence $$o$$:  
    :   $$\begin{align}
            \hat{w} & = \mathrm{arg } \max_{w \in \mathcal{S}} P(w \vert o) & (1) \\
            & = \mathrm{arg } \max_{w \in \mathcal{S}} P(o \vert w) P(w) & (2)
            \end{align}
        $$  
    :   The __Conditional Probability of a sequence of observations given a sequence of (predicted) word__ is a _product (or sum of logs)_ of an __Acoustic Model__ ($$p(o \vert w)$$)  and a __Language Model__ ($$p(w)$$)  scores.
    :   The __Acoustic Model__ can be written as the following product:    
    :   $$P(o \vert w) = \sum_{d,c,p} P(o \vert c) P(c \vert p) P(p \vert w)$$ 
    :   where $$p$$ is the __phone sequence__ and $$c$$ is the __state sequence__.  

3. **Speech Recognition as Transduction:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents93}  
    :   The problem of speech recognition can be seen as a transduction problem - mapping different forms of energy to other forms (representations).  
        Basically, we are going from __Signal__ to __Language__.  
        ![img](/main_files/dl/nlp/12/6.png){: width="60%"}    

4. **Gaussian Mixture Models:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents94}  
    :   * Dominant paradigm for ASR from 1990 to 2010 
        * Model the probability distribution of the acoustic features for each state.  
            $$P(o_t \vert c_i) = \sum_j w_{ij} N(o_t; \mu_{ij}, \sigma_{ij})$$   
        * Often use diagonal covariance Gaussians to keep number of parameters under control. 
        * Train by the E-M (Expectation Maximization) algorithm (Dempster et al., 1977) alternating:  
            * __M__: forced alignment computing the maximum-likelihood state sequence for each utterance 
            * __E__: parameter $$(\mu , \sigma)$$ estimation  
        * Complex training procedures to incrementally fit increasing numbers of components per mixture:  
            * More components, better fit - 79 parameters component. 
        * Given an alignment mapping audio frames to states, this is parallelizable by state.   
        * Hard to share parameters/data across states.  
    :   __Forced Alignment:__  
        * Forced alignment uses a model to compute the maximum likelihood alignment between speech features and phonetic states. 
        * For each training utterance, construct the set of phonetic states for the ground truth transcription. 
        * Use Viterbi algorithm to find ML monotonic state sequence 
        * Under constraints such as at least one frame per state. 
        * Results in a phonetic label for each frame. 
        * Can give hard or soft segmentation.  
        ![img](/main_files/dl/nlp/12/7.png){: width="60%"}  
    * <button>Algorithm/Training</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![formula](/main_files/dl/nlp/12/8.png){: width="70%" hidden=""}   
    * __Decoding:__   
        ![img](/main_files/dl/nlp/12/9.png){: width="20%"}  
        * Speech recognition Unfolds in much the same way.
        *  Now we have a graph instead of a straight-through path.
        *  Optional silences between words Alternative pronunciation paths.
        *  Typically use max probability, and work in the log domain.
        *  Hypothesis space is huge, so we only keep a "beam" of the best paths, and can lose what would end up being the true best path.   

5. **Neural Networks in ASR:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents95}  
    :   * __Two Paradigms of Neural Networks for Speech__:  
            * Use neural networks to compute nonlinear feature representations:      
                * "Bottleneck" or "tandem" features (Hermansky et al., 2000)
                * Low-dimensional representation is modelled conventionally with GMMs.
                * Allows all the GMM machinery and tricks to be exploited. 
                * _Bottleneck features_ outperform _Posterior features_ (Grezl et al. 2017)
                * Generally, __DNN features + GMMs__ reach the same performance as hybrid __DNN-HMM__ systems but are much more _complex_
            * Use neural networks to estimate phonetic unit probabilities  

6. **Hybrid Networks:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents96}  
    :   * Train the network as a classifier with a softmax across the __phonetic units__  
        * Train with __cross-entropy__
        * Softmax:   
    :   $$y(i) = \dfrac{e^{\psi(i, \theta)}}{\sum_{j=1}^N e^{\psi(j, \theta)}}$$ 
    :   * We _converge to/learn_ the __posterior probability across phonetic states__:  
    :   $$P(c_i \vert o_t)$$   
    :   * We, then, model $$P(o \vert c)$$ with a __Neural-Net__ instead of a __GMM__:   
            > We can ignore $$P(o_t)$$ since it is the same for all decoding paths   
    :   $$\begin{align}
            P(o \vert c) & = \prod_t P(o_t \vert c_t) & (3) \\
            P(o_t \vert c_t) & = \dfrac{P(c_t \vert o_t) P(o_t)}{P(c_t)} & (4) \\
            & \propto \dfrac{P(c_t \vert o_t)}{P(c_t)} & (5) \\
            \end{align}
        $$  
    :   * The __log scaled posterior__  from the last term:  
    :   $$\log P(o_t \vert c_t) = \log P(c_t \vert o_t) - \alpha \log P(c_t)$$ 
    :   * Empirically, a *__prior smoothing__* on $$\alpha$$ $$(\alpha \approx 0.8)$$ works better 
    :   * __Input Features__:  
            * NN can handle high-dimensional, correlated, features
            * Use (26) stacked filterbank inputs (40-dim mel-spaced filterbanks)
    :   * __NN Architectures for ASR__:  
            * *__Fully-Connected DNN__*  
            * *__CNNs__*: 
                * Time delay neural networks: 
                    * Waibel et al. (1989) 
                    * Dilated convolutions (Peddinti et al., 2015)  
                        > Pooling in time results in a loss of information.  
                        > Pooling in frequency domain is more tolerable  
                * CNNs in time or frequency domain:
                    * Abdel-Hamid et al. (2014)
                    * Sainath et al. (2013) 
                * Wavenet (van den Oord et al., 2016) 
            * *__RNNs__* :  
                * RNN (Robinson and Fallside, 1991) 
                * LSTM Graves et al. (2013)
                * Deep LSTM-P Sak et al. (2014b)
                * CLDNN (Sainath et al , 2015a)
                * GRU. DeepSpeech 1/2 (Amodei et al., 2015)

                * __Tips__ :
                    * Bidirectional (Schuster and Paliwal, 1997) helps, but introduces latency. 
                    * Dependencies not long at speech frame rates (100Hz).
                    * Frame stacking and down-sampling help. 

7. **Sequence Discriminative Training:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents97}  
    ![img](/main_files/dl/nlp/12/11.png){: width="80%"}  
    * Conventional training uses Cross-Entropy loss ‚Äî Tries to maximize probability of the true state sequence given the data. 
    * We care about Word Error Rate of the complete system. 
    * Design a loss that's differentiable and closer to what we care about. 
    * Applied to neural networks (Kingsbury, 2009) 
    * Posterior scaling gets learnt by the network. 
    * Improves conventional training and CTC by $$\approx 15%$$ relative. 
    * bMMI, sMBR(Povey et al., 2008)  
    ![img](/main_files/dl/nlp/12/10.png){: width="70%"}  


    :   

***

## Transitioning into Deep Learning
{: #content1}  

1. **Classical Approach:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   Classically, _Speech Recognition_ was developed as a big machine incorporating different models from different fields.  
        The models were _statistical_ and they started from _text sequences_ to _audio features_.  
        Typically, a _generative language model_ is trained on the sentences for the intended language, then, to make the features, _pronunciation models_, _acoustic models_, and _speech processing models_ had to be developed. Those required a lot of feature engineering and a lot of human intervention and expertise and were very fragile.
    :   ![img](/main_files/dl/nlp/12/1.png){: width="100%"}  
    :   __Recognition__ was done through __*Inference*__: Given audio features $$\mathbf{X}=x_1x_2...x_t$$ infer the most likely tedxt sequence $$\mathbf{Y}^\ast=y_1y_2...y_k$$ that caused the audio features.
    :   $$\displaystyle{\mathbf{Y}^\ast =\mathrm{arg\,min}_{\mathbf{Y}} p(\mathbf{X} \vert \mathbf{Y}) p(\mathbf{Y})}$$

2. **The Neural Network Age:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   Researchers realized that each of the (independent) components/models that make up the ASR can be improved if it were replaced by a _Neural Network Based Model_.  
    :   ![img](/main_files/dl/nlp/12/2.png){: width="100%"}  

3. **The Problem with the component-based System:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   * Each component/model is trained _independently_, with a different _objective_  
        * Errors in one component may not behave well with errors in another component

4. **Solution to the Component-Based System:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   We aim to train models that encompass all of these components together, i.e. __End-to-End Model__:  
        * __Connectionist Temporal Classification (CTC)__
        * __Sequence-to-Sequence Listen Attend and Spell (LAS)__
                    
5. **End-to-End Speech Recognition:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    :   We treat __End-to-End Speech Recognition__ as a _modeling task_.
    :   Given __Audio__ $$\mathbf{X}=x_1x_2...x_t$$ (audio/processed spectogram) and corresponding output text $$\mathbf{Y}=y_1y_2...y_k$$  (transcript), we want to learn a *__Probabilistic Model__* $$p(\mathbf{Y} \vert \mathbf{X})$$ 

6. **Deep Learning - What's new?**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    :    * __Algorithms__:  
            * Direct modeling of context-dependent (tied triphone states) through the DNN  
            * Unsupervised Pre-training
            * Deeper Networks
            * Better Architectures  
        * __Data__:  
            * Larger Data
        * __Computation__:  
                * GPUs
                * TPUs
        * __Training Criterion__:  
            * Cross-Entropy -> MMI Sequence -level
        * __Features__:  
            * Mel-Frequency Cepstral Coefficients (MFCC) -> FilterBanks
        * __Training and Regularization__:  
            * Batch Norm
            * Distributed SGD
            * Dropout
        * __Acoustic Modelling__:  
            * CNN
            * CTC
            * CLDNN
        * __Language Modelling__:  
            * RNNs
            * LSTM
        * __DATA__:  
            * More diverse - Noisy, Accents, etc.  

***

## Connectionist Temporal Classification
{: #content2}

1. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    :   * RNNs require a _target output_ at each time step 
        * Thus, to train an RNN, we need to __segment__ the training output (i.e. tell the network which label should be output at which time-step) 
        * This problem usually arises when the timing of the input is variable/inconsistent (e.g. people speaking at different rates/speeds)

2. **Connectionist Temporal Classification (CTC):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    :   __CTC__ is a type of _neural network output_ and _associated scoring function_, for training recurrent neural networks (RNNs) such as LSTM networks to tackle sequence problems where the _timing is variable_.  
    :   Due to time variability, we don't know the __alignment__ of the __input__ with the __output__.  
        Thus, CTC considers __all possible alignments__.  
        Then, it gets a __closed formula__ for the __probability__ of __all these possible alignments__ and __maximizes__ it.

8. **Structure:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}  
    * __Input__:  
        A sequence of _observations_
    * __Output__:  
        A sequence of _labels_


3. **Algorithm:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
   ![img](/main_files/dl/nlp/12/3.png){: width="80%"}  
    1. Extract the (*__LOG MEL__*) _Spectrogram_ from the input  
        > Use raw audio iff there are multiple microphones
    2. Feed the _Spectogram_ into a _(bi-directional) RNN_
    3. At each frame, we apply a _softmax_ over the entire vocabulary that we are interested in (plus a _blank token_), producing a prediction _log probability_ (called the __score__) for a _different token class_ at that time step.   
        * Repeated Tokens are duplicated
        * Any original transcript is mapped to by all the possible paths in the duplicated space
        * The __Score (log probability)__ of any path is the sum of the scores of individual categories at the different time steps
        * The probability of any transcript is the sum of probabilities of all paths that correspond to that transcript
        * __Dynamic Programming__ allopws is to compute the log probability $$p(\mathbf{Y} \vert \mathbf{X})$$ and its gradient exactly.  
    ![img](/main_files/dl/nlp/12/4.png){: width="80%"}  

10. **The Math:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents210}  
    :   Given a length $$T$$ input sequence $$x$$, the output vectors $$y_t$$ are normalized with the __Softmax__ function, then interpreted as the probability of emitting the label (or blank) with index $$k$$ at time $$t$$: 
    :   $$P(k, t \vert x) = \dfrac{e^{(y_t^k)}}{\sum_{k'} e^{(y_t^{k'})}}$$ 
    :   where $$y_t^k$$ is element $$k$$ of $$y_t$$.  
    :   A __CTC alignment__ $$a$$ is a length $$T$$ sequence of blank and label indices.  
        The probability $$P(a \vert x)$$ of 
        $$a$$ is the product of the emission probabilities at every time-step:  
    :   $$P(a \vert x) = \prod_{t=1}^T P(a_t, t \vert x)$$ 
    :   Denoting by $$\mathcal{B}$$ an operator that removes first the repeated labels, then the blanks from alignments, and observing that the total probability of an output transcription $$y$$ is equal to the sum of the probabilities of the alignments corresponding to it, we can write:  
    :   $$P(y \vert x) = \sum_{a \in \mathcal{B}^{-1}(y)} P(a \vert x)\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:(*)$$ 
    :   Given a target transcription $$y^\ast$$, the network can then be trained to minimise the __CTC objective function__:  
    :   $$\text{CTC}(x) = - \log P(y^\ast \vert x)$$ 


11. **Intuition:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents211}  
    :   The above 'integrating out' over possible alignments eq. $$(*)$$ is what allows the network to be trained with unsegmented data.   
        The intuition is that, because we don‚Äôt know where the labels within a particular transcription will occur, we sum over all the places where they could occur can be efficiently evaluated and differentiated using a dynamic programming algorithm.


5. **Analysis:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    :   The _ASR_ model consists of an __RNN__ plus a __CTC__ layer.    
        Jointly, the model learns the __pronunciation__ and __acoustic__ model _together_.  
        However, a __language model__ is __not__ learned, because the RNN-CTC model makes __strong conditional independence__ assumptions (similar to __HMMs__).  
        Thus, the RNN-CTC model is capable of mapping _speech acoustics_ to _English characters_ but it makes many _spelling_ and _grammatical_ mistakes.  
        Thus, the bottleneck in the model is the assumption that the _network outputs_ at _different times_ are __conditionally independent__, given the _internal state_ of the network. 

4. **Improvements:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    :   * Add a _language model_ to CTC during training time for _rescoring_.
           This allows the model to correct spelling and grammar.
        * Use _word targets_ of a certain vocabulary instead of characters 

7. **Applications:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  
    :   * on-line Handwriting Recognition
        * Recognizing phonemes in speech audio  
        * ASR

9. **Tips:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents29}  
    :   * Continuous realignment - no need for a bootstrap model
        * Always use soft targets
        * Don't scale by the posterior
        * Produces similar results to conventional training
        * Simple to implement in the __FST__ framework 
        * CTC could learn to __delay__ output on its own in order to improve accuracy:  
            * In-practice, tends to align transcription closely
            * This is especially problematic for English letters (spelling)
            * __Sol__:  
                bake limited context into model structure; s.t. the model at time-step $$T$$ can see only some future frames. 
                * Caveat: may need to compute upper layers quickly after sufficient context arrives.  
                Can be easier if context is near top.   

***

## LAS - Seq2Seq with Attention
{: #content3}

1. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   The __CTC__ model can only make predictions based on the data; once it has made a prediction for a given frame, it __cannot re-adjust__ the prediction.  
    :   Moreover, the _strong independence assumptions_ that the CTC model makes doesn't allow it to learn a _language model_.   

2. **Listen, Attend and Spell (LAS):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   __LAS__ is a neural network that learns to transcribe speech utterances to characters.  
        In particular, it learns all the components of a speech recognizer jointly.
    :   ![img](/main_files/dl/nlp/12/5.png){: width="80%"}  
    :   The model is a __seq2seq__ model; it learns a _conditional probability_ of the next _label/character_ given the _input_ and _previous predictions_ $$p(y_{i+1} \vert y_{1..i}, x)$$.  
    :   The approach that __LAS__ takes is similar to that of __NMT__.     
        Where, in translation, the input would be the _source sentence_ but in __ASR__, the input is _the audio sequence_.  
    :   __Attention__ is needed because in speech recognition tasks, the length of the input sequence is very large; for a 10 seconds sample, there will be ~10000 frames to go through.      

3. **Structure:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    :   The model has two components:  
        * __A listener__: a _pyramidal RNN **encoder**_ that accepts _filter bank spectra_ as inputs
        * __A Speller__: an _attention_-based _RNN **decoder**_ that emits _characters_ as outputs 
    :   * __Input__:  
            
        * __Output__: 

6. **Limitations:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents36}  
    :   * Not an online model - input must all be received before transcripts can be produced
        * Attention is a computational bottleneck since every output token pays attention to every input time step
        * Length of input has a big impact on accuracy

***

## Online Seq2Seq Models
{: #content4}

1. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    :   * __Overcome limitations of seq2seq__:  
            * No need to wait for the entire input sequence to arrive
            * Avoids the computational bottleneck of Attention over the entire sequence
        * __Produce outputs as inputs arrive__:  
            * Solves this problem: When has enough information arrived that the model is confident enough to output symbols 

2. **A Neural Transducer:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}  
    :    Neural Transducer is a more general class of seq2seq learning models. It avoids the problems of offline seq2seq models by operating on local chunks of data instead of the whole input at once. It is able to make predictions _conditioned on partially observed data and partially made predictions_.    


*** 

## Real-World Applications
{: #content6}

1. **Siri:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents61}  
    :   * __Siri Architecture__:  
            ![img](/main_files/dl/nlp/12/12.png){: width="80%"}  
            * Start with a __Wave Form__
            * Pass the wave form through an ASR system
            * Then use a Natural Language Model to re-adjust the labels
            * Output Words
            * Based on the output, do some action or save the output, etc.
    :   * __"Hey Siri" DNN__:  
            ![img](/main_files/dl/nlp/12/13.png){: width="80%"}  
            * Much smaller DNN than for the full Vocab. ASR
            * Does _Binary Classification_ - Did the speaker say "hey Siri" or not?  
            * Consists of 5 Layers
            * The layers have few parameters
            * It has a __Threshold__ at the end  
            * So fast 
            * Capable of running on the __Apple Watch!__
    :   * __Two-Pass Detection__:  
            * *__Problem__*:  
                    A big problem that arises in the _always-on voice_, is that it needs to run 24/7. 
            * *__Solution__*:  
                ![img](/main_files/dl/nlp/12/14.png){: width="90%"}    
                We use a __Two-Pass Detection__ system:  
                * There are two processors implemented in the phone:  
                    * __Low-Compute Processor:__{: style="color: red"}  
                        * Always __ON__
                        * Given a threshold value of confidence over binary probabilities the Processor makes the following decision: "Should I wake up the Main Processor"  
                        * Low power consumption
                    * __Main Processor:__{: style="color: red"}  
                        * Only ON if woken up by the _low-compute_ processor 
                        * Runs a much larger DNN   
                        * High power consumption
    :   * __Computation for DL__:   
            ![img](/main_files/dl/nlp/12/15.png){: width="90%"}  

***

## Building ASR Systems
{: #content11}

1. **Pre-Processing:**{: style="color: SteelBlue"}{: .bodyContents11 #bodyContents111}  
    :   A __Spectrogram__ is a visual representation of the spectrum of frequencies of sound or other signal as they vary with time.
        * Take a small window (~20 ms) of waveform  
        * Compute __FFT__ and take magnitude (i.e. prower)  
            > Describes Frequency content in local window  
    :   ![img](/main_files/dl/nlp/12/18.png){: width="80%"}  
    :   * Concatenate frames from adjacent windows to form the "spectrogram"  
        ![img](/main_files/dl/nlp/12/19.png){: width="80%"}  

2. **Acoustic Model:**{: style="color: SteelBlue"}{: .bodyContents11 #bodyContents112}  
    :   An __Acoustic Model__ is used in automatic speech recognition to represent the relationship between an audio signal and the phonemes or other linguistic units that make up speech.  
    :   __Goal__: create a neural network (DNN/RNN) from which we can extract transcriptions, $$y$$ - by training on labeled pairs $$(x, y^\ast)$$.  

3. **Network (example) Architecture:**{: style="color: SteelBlue"}{: .bodyContents11 #bodyContents113}  
    :   __RNN to predict graphemes (26 chars + space + blank)__:  
        * Spectrograms as inputs
        * 1 Layer of Convolutional Filters
        * 3 Layers of Gated Recurrent Units
            * 1000 Neurons per Layer
        * 1 Fully-Connected Layer to predict $$c$$
        * Batch Normalization
        * *__CTC__* __Loss Function__  (Warp-CTC) 
        * SGD+Nesterov Momentum Optimization/Training
        ![img](/main_files/dl/nlp/12/20.png){: width="50%"}  

4. **Incorporating a Language Model:**{: style="color: SteelBlue"}{: .bodyContents11 #bodyContents114}  
    :   Incorporating a Language Model helps the model learn:  
        * Spelling 
        * Grammar
        * Expand Vocabulary
    :   __Two Ways__:  
        1. Fuse the __Acoustic Model__ with the language model $$p(y)$$  
        2. Incorporate linguistic data: 
            * Predict Phonemes + Pronunciation Lexicon + LM  

5. **Decoding with Language Models:**{: style="color: SteelBlue"}{: .bodyContents11 #bodyContents115}  
    :   * Given a word-based LM of form $$p(w_{t+1} \vert w_{1:t})$$  
        * Use __Beam Search__ to maximize _(Hannun et al. 2014)_:  
    :   $$\mathrm{arg } \max_{w} p(w \vert x)\: p(w)^\alpha \: [\text{length}(w)]^\beta$$  
    :   > * $$p(w \vert x) = p(y \vert x)$$ for characters that make up $$w$$.  
        > * We tend to penalize long transcriptions due to the multiplicative nature of the objective, so we trade off (re-weight) with $$\alpha , \beta$$  
    :   * Start with a set of candidate transcript prefixes, $$A = {}$$  
        * __For $$t = 1 \ldots T$$__:   
            * __For Each Candidate in $$A$$, consider__:  
                1. Add blank; don't change prefix; update probability using the AM. 
                2. Add space to prefix; update probability using LM. 
                3. Add a character to prefix; update probability using AM. Add new candidates with updated probabilities to $$A_{\text{new}}$$   
            * $$A := K$$ most probable prefixes in $$A_{\text{new}}$$  
    :   <button>Algorithm Description</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
        ![formula](/main_files/dl/nlp/12/21.png){: width="100%" hidden=""}  

6. **Rescoring with Neural LM:**{: style="color: SteelBlue"}{: .bodyContents11 #bodyContents116}  
    :   The output from the RNN described above consists of a __big list__ of the __top $$k$$ transcriptions__ in terms of probability.  
        We want to re-score these probabilities based on a strong LM.   
        * It is Cheap to evaluate $$p(w_k \vert w_{k-1}, w_{k-2}, \ldots, w_1)$$ NLM on many sentences  
        * In-practice, often combine with N-gram trained from big corpora  

7. **Scaling Up:**{: style="color: SteelBlue"}{: .bodyContents11 #bodyContents117}  
    :   * __Data__:  
            * Transcribing speech data isn't cheap, but not prohibitive  
                * Roughly 50¬¢ to $1 per minute
            * Typical speech benchmarks offer 100s to few 1000s of hours:  
                * LibriSpeech (audiobooks)  
                * LDC corpora (Fisher, Switchboard, WSJ) (\$\$)  
                * VoxForge  
            * Data is very Application/Problem dependent and should be chosen with respect to the problem to be solved
            * Data can be collected as "read"-data for <$10 -- Make sure the data to be read is scripts/plays to get a conversationalist response
            * Noise is __additive__ and can be incorporated  
    :   * __Computation__:  
            * How big is 1 experiment?{: style="color: red"}  
                $$\geq (\# \text{Connections}) \cdot (\# \text{Frames}) \cdot (\# \text{Utterances}) \cdot (\# \text{Epochs}) \cdot 3 \cdot 2 \:\text{ FLOPs}$$   
                E.g. for DS2 with 10k hours of data:  
                $$100*10^6 * 100 * 10^6*20 * 3 * 2 = 1.2*10^{19} \:\text{ FLOPs}$$  
                ~30 days (with well-optimized code on Titan X)  
            * Work-arounds and solutions:{: style="color: red"}  
                * More GPUs with data parallelism:  
                    * Minibatches up to 1024 
                    * Aim for $$\geq 64$$ utterances per GPU 
                ~$$< 1$$-wk training time (~8 Titans)
            * How to use more GPUs?{: style="color: red"}  

                * Synch. SGD
                * Synch SGD w/ backup workers
            * __Tips and Tricks__:  
                * Make sure the code is _optimized_ single-GPU.  
                    A lot of off-the-shelf code has inefficiencies.  
                    E.g. Watch for bad GEMM sizes.
                * Keep similar-length utterances together:  
                    The input must be block-sized and will be padded; thus, keeping similar lengths together reduces unnecessary padding.
    :   * __Throughput__:  
            * Large DNN/RNN models run well on GPUs, ONLY, if the batch size is high enough.  
                Processing 1 audio stream at a time is inefficient.  
                *__Performance for K1200 GPU__*:  
                | __Batch Size__ | __FLOPs__ | __Throughput__ |
                | 1 | 0.065 TFLOPs | 1x | 
                | 10 | 0.31 TFLOPs | 5x | 
                | 32 | 0.92 TFLOPs | 14x |  
            * Batch packets together as data comes in:  
                * Each packet (Arrow) of speech data ~ 100ms  
                    ![img](/main_files/dl/nlp/12/21.png){: width="80%"}  
                * Process packets that arrive at similar times in parallel (from    multiple users)  
                    ![img](/main_files/dl/nlp/12/22.png){: width="80%"}  

***
***

TITLE: Deep Learning <br /> Research Papers
LINK: research/dl/nlp/nlp_research.md


## Sequence to Sequence Learning with Neural Network
{: #content1}

1. **Introduction:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    This paper presents a general end-to-end approach to sequence learning that makes minimal assumptions (Domain-Independent) on the sequence structure.  
    It introduces __Seq2Seq__. 

2. **Structure:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}    
    * __Input__: sequence of input vectors  
    * __Output__: sequence of output labels
                
3. **Strategy:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    The idea is to use one LSTM to read the input sequence, one time step at a time, to obtain large fixed dimensional vector representation, and then to use another LSTM to extract the output sequence from that vector.  
    The second LSTM is essentially a recurrent neural network language model except that it is __conditioned__ on the __input sequence__.

4. **Solves:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    * Despite their flexibility and power, DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality. It is a significant limitation, since many important problems are best expressed with sequences whose lengths are not known a-priori.  
        The RNN can easily map sequences to sequences whenever the alignment between the inputs the outputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose input and the output sequences have different lengths with complicated and non-monotonic relationship.  


5. **Key Insights:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    * Uses LSTMs to capture the information present in a sequence of inputs into one vector of features that can then be used to decode a sequence of output features  
    * Uses two different LSTM, for the encoder and the decoder respectively  
    * Reverses the words in the source sentence to make use of short-term dependencies (in translation) that led to better training and convergence 

6. **Preparing Data (Pre-Processing):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    :   
                    

7. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    :   * __Encoder__:  
            * *__LSTM:__* 
                * 4 Layers:    
                    * 1000 Dimensions per layer
                    * 1000-dimensional word embeddings
        * __Decoder__:  
            * *__LSTM:__* 
                * 4 Layers:    
                    * 1000 Dimensions per layer
                    * 1000-dimensional word embeddings
        * An __Output__ layer made of a standard __softmax function__  
            > over 80,000 words  
        * __Objective Function__:  
            <p>$$\dfrac{1}{\vert \mathbb{S} \vert} \sum_{(T,S) \in \mathbb{S}} \log p(T \vert S)
            $$</p>  
            where $$\mathbb{S}$$ is the training set.  
                
8. **Algorithm:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
:   * Train a large deep LSTM 
    * Train by maximizing the log probability of a correct translation $$T$$  given the source sentence $$S$$  
    * Produce translations by finding the most likely translation according to the LSTM:   
        <p>$$\hat{T} = \mathrm{arg } \max_{T} p(T \vert S)$$</p>
    * Search for the most likely translation using a simple left-to-right beam search decoder which maintains a small number B of partial hypotheses  
        > A __partial hypothesis__ is a prefix of some translation  
    * At each time-step we extend each partial hypothesis in the beam with every possible word in the vocabulary  
        > This greatly increases the number of the hypotheses so we discard all but the $$B$$  most likely hypotheses according to the model‚Äôs log probability  
    * As soon as the ‚Äú<EOS>‚Äù symbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete hypotheses  
    *

9. **Training:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents19}  
    :   * SGD
        * Momentum 
        * Half the learning rate every half epoch after the 5th epoch
        * Gradient Clipping  
            > enforce a hard constraint on the norm of the gradient
        * Sorting input

10. **Parameters:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents110}  
    :   * __Initialization__ of all the LSTM params with __uniform distribution__ $$\in [-0.08, 0.08]$$  
        * __Learning Rate__: $$0.7$$ 
        * __Batches__: $$28$$ sequences
        * __Clipping__: 
    :   $$g = 5g/\|g\|_2 \text{ if } \|g\|_2 > 5 \text{ else } g$$ 
                  

11. **Issues/The Bottleneck:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents111}  
    :   * The decoder is __approximate__  
        * The system puts too much pressure on the last encoded vector to capture all the (long-term) dependencies

12. **Results:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents112}  
    :   

13. **Discussion:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents113}  
    :   * Sequence to sequence learning is a framework that attempts to address the problem of learning variable-length input and output sequences. It uses an encoder RNN to map the sequential variable-length input into a fixed-length vector. A decoder RNN then uses this vector to produce the variable-length output sequence, one token at a time. During training, the model feeds the groundtruth labels as inputs to the decoder. During inference, the model performs a beam search to generate suitable candidates for next step predictions.

14. **Further Development:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents114}  
    :   

***

## Towards End-to-End Speech Recognition with Recurrent Neural Networks
{: #content2}

1. **Introduction:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    :   This paper presents an ASR system that directly transcribes audio data with text, __without__ requiring an _intermediate phonetic representation_.

2. **Structure:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}    
    :   * __Input__: 
        * __Output__:  
                

3. **Strategy:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    :   The goal of this paper is a system where as much of the speech pipeline as possible is replaced by a single recurrent neural network (RNN) architecture.  
        The language model, however, will be lacking due to the limitation of the audio data to learn a strong LM. 

4. **Solves:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    :   * First attempts used __RNNs__ or standard __LSTMs__. These models lacked the complexity that was needed to capture all the models required for ASR. 

5. **Key Insights:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    :   * The model uses Bidirectional LSTMs to capture the nuances of the problem.  
        * The system uses a new __objective function__ that trains the network to directly optimize the __WER__.  

6. **Preparing the Data (Pre-Processing):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    :   The paper uses __spectrograms__ as a minimal preprocessing scheme.  

7. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  
    :   The system is composed of:  
        * A __Bi-LSTM__  
        * A __CTC output layer__  
        * A __combined objective function__:  
            The new objective function at allows an RNN to be trained to optimize the expected value of an arbitrary loss function defined over output transcriptions (such as __WER__).  
            Given input sequence $$x$$, the distribution $$P(y\vert x)$$ over transcriptions sequences $$y$$ defined by CTC, and a real-valued transcription loss function $$\mathcal{L}(x, y)$$, the expected transcription loss $$\mathcal{L}(x)$$ is defined:  
            <p>$$\begin{align}
                \mathcal{L}(x) &= \sum_y P(y \vert x)\mathcal{L}(x,y) \\ 
                &= \sum_y \sum_{a \in \mathcal{B}^{-1}(y)} P(a \vert x)\mathcal{L}(x,y) \\
                &= \sum_a P(a \vert x)\mathcal{L}(x,\mathcal{B}(a))
                \end{align}$$</p>  
        <button>Show Derivation</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![Approximation and Differentiation](/main_files/dl/nlp/speech_research/3.png){: hidden="" width="80%"}


8. **Algorithm:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}  
    :   

9. **Issues/The Bottleneck:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents29}  
    :   

10. **Results:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents210}  
    :   * __WSJC__ (
    WER): 
            * Standard: $$27.3\%$$  
            * w/Lexicon of allowed words: $$21.9\%$$ 
            * Trigram LM: $$8.2\%$$ 
            * w/Baseline system: $$6.7\%$$

***

## Attention-Based Models for Speech Recognition
{: #content3}

1. **Introduction:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   This paper introduces and extends the attention mechanism with features needed for ASR. It adds location-awareness to the attention mechanism to add robustness against different lengths of utterances.  

2. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}    
    :   Learning to recognize speech can be viewed as learning to generate a sequence (transcription) given another sequence (speech).  
        From this perspective it is similar to machine translation and handwriting synthesis tasks, for which attention-based methods have been found suitable. 
    :   __How ASR differs:__  
        Compared to _Machine Translation_, speech recognition differs by requesting much longer input sequences which introduces a challenge of distinguishing similar speech fragments in a single utterance.  
        > thousands of frames instead of dozens of words   
    :   It is different from _Handwriting Synthesis_, since the input sequence is much noisier and does not have a clear structure.  

2. **Structure:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}    
    :   * __Input__: $$x=(x_1, \ldots, x_{L'})$$ is a sequence of feature vectors  
            * Each feature vector is extracted from a small overlapping window of audio frames
        * __Output__: $$y$$ a sequence of __phonemes__   

3. **Strategy:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    :   The goal of this paper is a system, that uses attention-mechanism with location awareness, whose performance is comparable to that of the conventional approaches.   
    :   * For each generated phoneme, an attention mechanism selects or weighs the signals produced by a trained feature extraction mechanism at potentially all of the time steps in the input sequence (speech frames).  
        * The weighted feature vector then helps to condition the generation of the next element of the output sequence.  
        * Since the utterances in this dataset are rather short (mostly under 5 seconds), we measure the ability of the considered models in recognizing much longer utterances which were created by artificially concatenating the existing utterances.

4. **Solves:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    :   * __Problem__:  
            The [attention-based model proposed for NMT](https://arxiv.org/abs/1409.0473) demonstrates vulnerability to the issue of similar speech fragments with __longer, concatenated utterances__.  
            The paper argues that  this model adapted to track the absolute location in the input sequence of the content it is recognizing, a strategy feasible for short utterances from the original test set but inherently unscalable.  
        * __Solution__:  
            The attention-mechanism is modified to take into account the location of the focus from the previous step and the features of the input sequence by adding as inputs to the attention mechanism auxiliary *__Convolutional Features__* which are extracted by convolving the attention weights from the previous step with trainable filters.  

5. **Key Insights:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}  
    :   * Introduces attention-mechanism to ASR
        * The attention-mechanism is modified to take into account:  
            * location of the focus from the previous step  
            * features of the input sequence
        * Proposes a generic method of adding location awareness to the attention mechanism
        * Introduce a modification of the attention mechanism to avoid concentrating the attention on a single frame  

7. **Attention-based Recurrent Sequence Generator (ARSG):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents37}  
    :   is a recurrent neural network that stochastically generates an output sequence $$(y_1, \ldots, y_T)$$ from an input $$x$$.  
    In practice, $$x$$ is often processed by an __encoder__ which outputs a sequential input representation $$h = (h_1, \ldots, h_L)$$ more suitable for the attention mechanism to work with.  
    :   The __Encoder__: a deep bidirectional recurrent network.  
        It forms a sequential representation h of length $$L = L'$$.  
    :   __Structure:__{: style="color: red"}    
        * *__Input__*: $$x = (x_1, \ldots, x_{L'})$$ is a sequence of feature vectors   
            > Each feature vector is extracted from a small overlapping window of audio frames.  
        * *__Output__*: $$y$$ is a sequence of phonemes
    :   __Strategy:__{: style="color: red"}    
        At the $$i$$-th step an ARSG generates an output $$y_i$$ by focusing on the relevant elements of $$h$$:  
    :   $$\begin{align}
        \alpha_i &= \text{Attend}(s_{i-1}, \alpha _{i-1}), h) & (1) \\
        g_i &= \sum_{j=1}^L \alpha_{i,j} h_j & (2) //
        y_i &\sim \text{Generate}(s_{i-1}, g_i) & (3)  
        \end{align}$$
    :   where $$s_{i‚àí1}$$ is the $$(i ‚àí 1)$$-th state of the recurrent neural network to which we refer as the __generator__, $$\alpha_i \in \mathbb{R}^L$$ is a vector of the _attention weights_, also often called the __alignment__; and $$g_i$$ is the __glimpse__.  
        The step is completed by computing a *__new generator state__*:  
    :   $$s_i = \text{Recurrency}(s_{i-1}, g_i, y_i)$$  
    :   where the _Recurrency_ is an RNN.  
    :   ![img](/main_files/dl/nlp/speech_research/4.png){: width="100%"}  

12. **Attention-mechanism Types and Speech Recognition:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents312}  
    :   __Types of Attention:__{: style="color: red"}      
        * (Generic) Hybrid Attention: $$\alpha_i = \text{Attend}(s_{i-1}, \alpha_{i-1}, h)$$  
        * Content-based Attention: $$\alpha_i = \text{Attend}(s_{i-1}, h)$$   
            In this case, Attend is often implemented by scoring each element in h separately and normalizing the scores:  
            $$e_{i,j} = \text{Score}(s_{i-1}, h_j) \\$$ 
              $$\alpha_{i,j} = \dfrac{\text{exp} (e_{i,j}) }{\sum_{j=1}^L \text{exp}(e_{i,j})}$$  
            * __Limitations__:  
                The main limitation of such scheme is that identical or very similar elements of $$h$$ are scored equally regardless of their position in the sequence.  
                Often this issue is partially alleviated by an encoder such as e.g. a BiRNN or a deep convolutional network that encode contextual information into every element of h . However, capacity of h elements is always limited, and thus disambiguation by context is only possible to a limited extent.  
        * Location-based Attention: $$\alpha_i = \text{Attend}(s_{i-1}, \alpha_{i-1})$$   
            a location-based attention mechanism computes the alignment from the generator state and the previous alignment only.  
            * __Limitations__:  
                the model would have to predict the distance between consequent phonemes using $$s_{i‚àí1}$$ only, which we expect to be hard due to large variance of this quantity.  
    :   Thus, we conclude that the __*Hybrid Attention*__ mechanism is a suitable candidate.  
        Ideally, we need an attention model that uses the previous alignment $$\alpha_{i-1}$$ to select a short list of elements from $$h$$, from which the content-based attention, will select the relevant ones without confusion.  

6. **Preparing the Data (Pre-Processing):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents36}  
    :   The paper uses __spectrograms__ as a minimal preprocessing scheme.  

7. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents37}  
    :   Start with the __ARSG__-based model:  
        * __Encoder__: is a __Bi-RNN__  
        <p>$$e_{i,j} = w^T \tanh (Ws_{i-1} + Vh_j + b)$$</p>
        * __Attention__: Content-Based Attention extended for _location awareness_  
            <p>$$e_{i,j} = w^T \tanh (Ws_{i-1} + Vh_j + Uf_{i,j} + b)$$</p>
    :   __Extending the Attention Mechanism:__  
        Content-Based Attention extended for _location awareness_ by making it take into account the alignment produced at the previous step.  
        * First, we extract $$k$$ vectors $$f_{i,j} \in \mathbb{R}^k$$ for every position $$j$$ of the previous alignment $$\alpha_{i‚àí1}$$ by convolving it with a matrix $$F \in \mathbb{R}^{k\times r}$$:  
            <p>$$f_i = F * \alpha_{i-1}$$</p>
        * These additional vectors $$f_{i,j}$$ are then used by the scoring mechanism $$e_{i,j}$$:  
            <p>$$e_{i,j} = w^T \tanh (Ws_{i-1} + Vh_j + Uf_{i,j} + b)$$</p>  

                
            

8. **Algorithm:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents38}  
    :   

9. **Issues/The Bottleneck:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents39}  
    :   

    :   

***

## Attention Is All You Need
{: #content4}

1. **Introduction:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    This paper introduces the __Transformer__ network architecture.  
    The model relies completely on __Attention__ and disregards _recurrence/convolutions_ completely.

2. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}  
    Motivation for dropping:
    * __Recurrent Connections__:  
        Complex, tricky to train and regularize, capturing long-term dependencies is limited, and hard to parallelize. Sequence-aligned states in RNN are wasteful. Hard to model hierarchical-like domains such as languages.  
    * __Convolutional Connections__:  
        Convolutional approaches are sometimes effective (more on this), but they tend to be memory-intensive. Path length between positions can be logarithmic when using dilated convolutions, left-padding for text. (autoregressive CNNs WaveNet, ByteNET)

    Motivation for __Transformer__:
    * It gives us the shortest possible path through the network between any two input-output locations.  

    Motivation in __NLP__:  
    * The following quote:  
        > ‚ÄúYou can‚Äôt cram the meaning of a whole %&!$# sentence into a single $&!#* vector!‚Äù - ACL 2014

3. **From Attention to Self-Attention:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}  
    __The Encoder-Decoder Architecture:__{: style="color: red"}  
    For a fixed target output, $$t_j$$, all hidden state source inputs are taken into account to compute the cosine similarity with the source inputs $$s_i$$, to generate the $$\theta_i$$‚Äôs (attention weights) for every source input $$s_i$$.  

4. **Strategy:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents44}  
    The idea here is to learn a context vector (say $$U$$), which gives us a global level information on all the inputs and tells us about the most important information.  
    E.g. This could be done by taking a cosine similarity of this context vector $$U$$  w.r.t the input hidden states from the fully connected layer. We do this for each input $$x_i$$ and thus obtain a $$\theta_i$$ (attention weights).  

    __The Goal(s):__{: style="color: red"}  
    * __Parallelization of Seq2Seq:__ RNN/CNN handle sequences word-by-word sequentially which is an obstacle to parallelize. Transformer achieves parallelization by replacing recurrence with attention and encoding the symbol position in the sequence. This, in turn, leads to a significantly shorter training time.  
    * __Reduce sequential computation__: Constant $$\mathcal{O}(1)$$ number of operations to learn dependency between two symbols independently of their position distance in sequence.  

    The Transformer reduces the number of sequential operations to relate two symbols from input/output sequences to a constant $$\mathcal{O}(1)$$ number of operations. Transformer achieves this with the multi-head attention mechanism that allows to model dependencies regardless of their distance in input or output sentence (by counteracting reduced effective resolution due to averaging the attention-weighted positions).  

6. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents46}  
    ![img](/main_files/research/1.png){: width="48%"}  \\
    The Transformer follows a __Encoder-Decoder__ architecture using __stacked self-attention__ and __point-wise, fully connected layers__ for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively:  

    __Encoder:__  
    The encoder is composed of a stack of $$N = 6$$ identical layers. Each layer has two sub-layers. The first is a __multi-head self-attention mechanism__, and the second is a simple, __positionwise fully connected feed-forward network__. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is $$\text{LayerNorm}(x + \text{Sublayer}(x))$$, where $$\text{Sublayer}(x)$$ is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $$d_{\text{model}} = 512$$.  
    
    __Decoder:__  
    The decoder is also composed of a stack of $$N = 6$$ identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs __multi-head attention over the output of the encoder stack__. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $$i$$ can depend only on the known outputs at positions less than $$i$$.  
    \\
    The __Encoder__ maps an input sequence of symbol representations $$(x_1, \ldots, x_n)$$ to a sequence of continuous representations $$z = (z_1, ..., z_n)$$.  
    Given $$z$$, the decoder then generates an output sequence $$(y_1, ..., y_m)$$ of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.  

7. **The Model - Attention:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents47}  
    __Formulation:__{: style="color: red"}
    Standard attention with _queries_ and _key_, _value_ pairs.  
    * __Scaled Dot-Product Attention__:  
        ![img](/main_files/research/3.png){: width="28%"}  \\
        Given: (1) Queries $$\vec{q} \in \mathbb{R}^{d_k}$$  (2) Keys $$\vec{k} \in \mathbb{R}^{d_k}$$  (3) Values $$\vec{v} \in \mathbb{R}^{d_v}$$  
        Computes the dot products of the queries with all keys; scales each by $$\sqrt{d_k}$$; and normalizes with a _softmax_ to obtain the weights $$\theta_i$$s on the values.  
        For a given query vector $$\vec{q} = \vec{q}_j$$ for some $$j$$:  
        <p>$${\displaystyle \vec{o} = \sum_{i=0}^{d_k} \text{softmax} (\dfrac{\vec{q}^T \: \vec{k}_i}{\sqrt{d_k}}) \vec{v}_i
        = \sum_{i=0}^{d_k} \theta_i \vec{v}_i}$$</p>  
        In practice, we compute the attention function on a set of queries simultaneously, in matrix form (stacked row-wise):  
        <p>$${\displaystyle \text{Attention}(Q, K, V) = O = \text{softmax} (\dfrac{QK^T}{\sqrt{d_k}}) V } \tag{1}$$</p>  
        __Motivation__: We suspect that for large values of $$d_k$$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by $$\sqrt{\dfrac{1}{d_k}}$$.  
    * __Multi-Head Attention__:  
        ![img](/main_files/research/2.png){: width="28%"}  \\
        Instead of performing a single attention function with $$d_{\text{model}}$$-dimensional keys, values and queries; linearly project the queries, keys and values h times with different, learned linear projections to $$d_k, d_k$$ and $$d_v$$ dimensions, respectively. Then, attend (apply $$\text{Attention}$$ function) on each of the projected versions, _in parallel_, yielding $$d_v$$-dimensional output values. The final values are obtained by _concatenating_ and _projecting_ the $$d_v$$-dimensional output values from each of the attention-heads.  
        <p>$$\begin{aligned} \text { MultiHead }(Q, K, V) &=\text { Concat }\left(\text { head}_{1}, \ldots, \text { head}_{h}\right) W^{O} \\ \text { where head}_{i} &=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}$$</p>  
        Where the projections are parameter matrices $$ W_{i}^{Q} \in \mathbb{R}^{d_{\text {model }} \times d_{k}}, W_{i}^{K} \in $$ $$\mathbb{R}^{d_{\text {model }} \times d_{k}},$$ $$W_{i}^{V} \in \mathbb{R}^{d_{\text {model }} \times d_{v}} $$ and $$W^O \in \mathbb{R}^{hd_v \times d_{\text {model }}}$$.  
        This paper choses $$h = 8$$ parallel attention layers/_heads_. For each, they use $$d_k=d_v=d_{\text{model}}/h = 64$$. The reduced dimensionality of each head, allows the total computation cost to be similar to that of a single head w/ full dimensionality.   

        __Motivation:__ Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.  
    

    __Applications of Attention in the Model:__{: style="color: red"}  
    The Transformer uses multi-head attention in three different ways:  
    * __Encode-Decoder Attention Layer__ (standard layer):  
        * The *__queries__* come from: the _previous decoder layer_
        * The memory *__keys__* and *__values__* come from: the _output of the encoder_   

        This allows every position in the decoder to attend over all positions in the input sequence.  
    * __Encoder Self-Attention__:  
        The encoder contains self-attention layers.  
        * Both, The *__queries__*, and *__keys__* and *__values__*, come from: the _encoders output of previous layer_  

        Each position in the encoder can attend to all positions in the previous layer of the encoder.
    * __Decoder Self-Attention__:  
        The decoder, also, contains self-attention layers.  
        * Both, The *__queries__*, and *__keys__* and *__values__*, come from: the _decoders output of previous layer_  

        However, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder _up to_, and including, that _position_. Since, we need to prevent *__leftward information flow__* in the decoder to preserve the *__auto-regressive__* property.  
        This is implemented inside of scaled dot-product attention by masking out (setting to $$-\infty$$ ) all values in the input of the softmax which correspond to illegal connections.  


8. **The Model - Position-wise Feed-Forward Network (FFN):**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents48}  
    In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.  
    It consists of *__two linear transformations__* with a *__ReLU__* activation in between:  
    <p>$$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2 \tag{2}$$</p>  
    While the __linear transformations__ are the _same_ across different positions, they use _different parameters_ from layer to layer.  
    > Equivalently, we can describe this as, __two convolutions__ with __kernel-size__ $$= 1$$  

    __Dimensional Analysis__:  
    * Input/Output: $$\in \mathbb{R}^{d_\text{model} = 512} $$  
    * Inner-Layer: $$\in \mathbb{R}^{d_{ff} = 2048} $$  


9. **The Model - Embeddings and Softmax:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents49}  
    Use _learned embeddings_ to convert the __input tokens__ and __output tokens__ to __vectors__ $$\in \mathbb{R}^d_{\text{model}}$$.  
    Use the usual _learned linear transformation_ and _softmax_ to convert __decoder output__ to __predicted next-token probabilities__.  

    The model *__shares__* the same __weight matrix__ between the two embedding layers and the pre-softmax linear transformation.  
    In the embedding layers, multiply those weights by $$\sqrt{d_{\text{model}}}$$.  

10. **The Model - Positional Encoding:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents410}  
    __Motivation:__ Since the model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.  

    __Positional Encoding:__  
    A way to add positional information to an embedding.  
    There are many choices of positional encodings, learned and fixed. _[Gehring et al. 2017]_  
    The positional encodings have the same dimension $$d_{\text{model}}$$ as the embeddings, so that the two can be summed.  
    __Approach:__  
    Add "positional encodings" to the input embeddings at the bottoms of the encoder and decoder stacks.  
    Use __sine__ and __cosine__ functions of different frequencies:  
    <p>$$ \begin{aligned} P E_{(p o s, 2 i)} &=\sin \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right) \\ P E_{(p o s, 2 i+1)} &=\cos \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right) \end{aligned} $$</p>  
    where $$p o s$$ is the position and $$i$$ is the dimension.  
    That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $$2\pi$$ to $$10000 \cdot 2\pi$$.  

    __Motivation__:  
    We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $$k$$, $$PE_{pos + k}$$ can be represented as a linear function of $$P E_{pos}$$. 

    __Sinusoidal VS Learned:__ We chose the sinusoidal version (instead of _learned positional embeddings_, with similar results) because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.  


11. **Training Tips & Tricks:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents411}  
    * __Layer Normalization:__ Help ensure that layers remain in reasonable range  
    * __Specialized Training Schedule:__ Adjust default learning rate of the Adam optimizer  
    * __Label Smoothing:__ Insert some uncertainty in the training process  
    * __Masking (for decoder attention):__ for Efficient Training using matrix-operations  


12. **Why Self-Attention? (as opposed to Conv/Recur. layers):**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents412}  
    ![img](/main_files/dl/nlp/speech_research/6.png){: width="80%"}  \\
    __Total Computational Complexity per Layer:__{: style="color: red"}    
    * Self-Attention layers are faster than recurrent layers when the sequence length $$n$$ is smaller than the representation dimensionality $$d$$.  
        > Which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece and byte-pair representations.  

    * To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size $$r$$ in the input sequence centered around the respective output position.  
        This would increase the maximum path length to $$\mathcal{O}(n/r)$$.  

    __Parallelizable Computations:__{: style="color: red"} (measured by the minimum number of sequential ops required)  
    * Self-Attention layers connect all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires $$\mathcal{O}(n)$$ sequential operations.  


    __Path Length between Positions:__{: style="color: red"} (Long-Range Dependencies)  
    * __Convolutional Layers:__ A single convolutional layer with kernel width $$k < n$$ does not connect all pairs of input and output positions.  
        Doing so requires:  
        * __Contiguous Kernels (valid)__: a stack of $$\mathcal{O}(n/k)$$ convolutional layers
        * __Dilated Kernels__: $$\mathcal{O}(\log_k(n))$$  
            increasing the length of the longest paths between any two positions in the network.  
        * __Separable Kernels__: decrease the complexity considerably, to $$\mathcal{O}\left(k \cdot n \cdot d+n \cdot d^{2}\right)$$  
        
        > Convolutional layers are generally more expensive than recurrent layers, by a factor of $$k$$.  

    * __Self-Attention__:  
        Even with $$k = n$$, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach taken in this model.  

    * __Interpretability__:  
        As side benefit, self-attention could yield more interpretable models.  
        Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.              

19. **Results:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents419}  
    * __Attention Types__:  
        For small values of $$d_k$$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $$d_k$$.  
    * __Positional Encodings__:  
        We also experimented with using learned positional embeddings instead, and found that the two versions produced nearly identical results.  
            


***
 

*** 




***
***

TITLE: Language Modeling  <br /> Recurrent Neural Networks (RNNs)
LINK: research/dl/nlp/rnns.md


[Language Modeling and RNNS I (Oxford)](https://www.youtube.com/watch?v=nfyE8oF23yQ&list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm&index=6&t=0s)  
> Note: 25:00 (important problem not captured w/ newer models about smoothing and language distribution as Heaps law)  

[LMs Stanford Notes](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf)  




## Introduction to and History of Language Models 
{: #content1}

1. **Language Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    A __Language Model__ is a statistical model that computes a _probability distribution_ over sequences of words.  

    It is a __time-series prediction__ problem in which we must be _very careful_ to *__train on the past__* and *__test on the future__*.   


2. **Applications:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   * __Machine Translation (MT)__:   
            * Word Ordering:  
                p("the cat is small") > p("small the cat is")  
            * Word Choice:  
                p("walking home after school") > p("walking house after school")
        * __Speech Recognition__:     
            * Word Disambiguation:  
                p("The listeners _recognize speech_") > p("The listeners _wreck a nice beach_")  
        * __Information Retrieval__: 
            * Used in _query likelihood model_
            
3. **Traditional Language Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    * Most language models employ the chain rule to decompose the _joint probability_ into a _sequence of conditional probabilities_:  
        <p>$$\begin{array}{c}{P\left(w_{1}, w_{2}, w_{3}, \ldots, w_{N}\right)=} \\ {P\left(w_{1}\right) P\left(w_{2} | w_{1}\right) P\left(w_{3} | w_{1}, w_{2}\right) \times \ldots \times P\left(w_{N} | w_{1}, w_{2}, \ldots w_{N-1}\right)}\end{array}$$</p>  
        Note that this decomposition is exact and allows us to model complex joint distributions by learning conditional distributions over the next word $$(w_n)$$ given the history of words observed $$\left(w_{1}, \dots, w_{n-1}\right)$$.   
        Thus, the __Goal__ of the __LM-Task__ is to find *__good conditional distributions__* that we can _multiply_ to get the *__Joint Distribution__*.  
        > Allows you to predict the first word, then the second word _given the first word_, then the third given the first two, etc..  

    * The Probability is usually conditioned on window of $$n$$ previous words  
        * An incorrect but necessary Markovian assumption:  
            <p>$$P(w_1, \ldots, w_m) = \prod_{i=1}^m P(w_i | w_1, \ldots, w_{i-1}) \approx \prod_{i=1}^m P(w_i | w_{i-(n-1)}, \ldots, w_{i-1})$$</p>  
            * Only previous history matters
            * __Limited Memory__: only last $$n-1$$ words are included in history  
        > E.g. $$2-$$gram LM (only looks at the *__previous word__*):  
            <p>$$\begin{aligned} p\left(w_{1}, w_{2}, w_{3},\right.& \ldots &, w_{n} ) \\ &=p\left(w_{1}\right) p\left(w_{2} | w_{1}\right) p\left(w_{3} | w_{1}, w_{2}\right) \times \ldots \\ & \times p\left(w_{n} | w_{1}, w_{2}, \ldots w_{n-1}\right) \\ & \approx p\left(w_{1}\right) p\left(w_{2} | w_{1}\right) p\left(w_{3} | w_{2}\right) \times \ldots \times p\left(w_{n} | w_{n-1}\right) \end{aligned}$$</p>   
        The conditioning context, $$w_{i-1}$$, is called the __history__.  
    * The __MLE__ estimate for probabilities, compute for  
        * Bi-grams:  
            <p>$$P(w_2 \| w_1) = \dfrac{\text{count}(w_1, w_2)}{\text{count}(w_1)}$$</p>  
        * Tri-grams:  
            <p>$$P(w_3 \| w_1, w_2) = \dfrac{\text{count}(w_1, w_2, w_3)}{\text{count}(w_1, w_2)}$$</p>  

4. **Issues with the Traditional Approaches:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    To improve performance we need to:  
    * Keep higher n-gram counts  
    * Use Smoothing  
    * Use Backoff (trying n-gram, (n-1)-gram, (n-2)-grams, ect.)  
        When? If you never saw a 3-gram b4, try 2-gram, 1-gram etc.  
    However, 
    * There are __A LOT__ of n-grams
        * $$\implies$$ Gigantic RAM requirements  

5. **NLP Tasks as LM Tasks:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    Much of Natural Language Processing can be structured as *__(conditional) Language Modeling__*:  
    * __Translation__:  
        <p>$$p_{\mathrm{LM}}(\text { Les chiens aiment les os }\| \| \text { Dogs love bones) }$$</p>  
    * __QA__:  
        <p>$$p_{\mathrm{LM}}(\text { What do dogs love? }\| \| \text { bones } . | \beta)$$</p>
    * __Dialog__:  
        <p>$$p_{\mathrm{LM}}(\text { How are you? }\| \| \text { Fine thanks. And you? } | \beta)$$</p>    
    > where $$\| \|$$ means "concatenation", and $$\beta$$ is an observed data (e.g. news article) to be conditioned on.  

            
6. **Analyzing the LM Tasks:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    The simple objective of _modeling the next word given observed history_ contains much of the complexity of __natural language understanding (NLU)__ (e.g. reasoning, intelligence, etc.).  

    Consider predicting the extension of the utterance:  
    <p>$$p(\cdot | \text { There she built a) }$$</p>  
    > The distribution of what word to predict right now is quite flat; you dont know where _"there"_ is, you dont know who _"she"_ is, you dont know what she would want to _"build"_.    

    However, With more context we are able to use our knowledge of both language and the world to heavily constrain the distribution over the next word.  
    <p>$$p(\cdot | \color{red} {\text { Alice }} \text {went to the} \color{blue} {\text { beach. } } \color{blue} {\text {There}} \color{red} {\text { she}} \text { built a})$$</p>  
    > At this point your distributions getting _very peaked_ about what could come next and the reason is because you understand language you understand that in the second utterance "she" is "Alice" and "There" is "Beach" so you've resolved those Co references and you can do that because you understand the syntactic structure of the first utterance; you understand we have a subject and object, where the verb phrase is, all of these things you do automatically and then, using the semantics that "at a beach you build things like sandcastles or boats" and so you can __constrict your distribution__.  

    If we can get a automatically trained machine to do that then we've come a long way to solving AI.  
    > "The diversity of tasks the model is able to perform in a zero-shot setting suggests that high-capacity models trained to _maximize the likelihood of a sufficiently varied text corpus_ begin to learn how to perform a surprising amount of tasks without the need for explicit supervision" - GPT 2 


7. **Evaluating a Language Model \| The Loss:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    For a probabilistic model, it makes sense to evaluate how well the "learned" distribution matches the real distribution of the data (of real utterances). A good model assigns real utterances $$w_{1}^{N}$$  from a language a high probability. This can be measured with __Cross-Entropy__:  
    <p>$$H\left(w_{1}^{N}\right)=-\frac{1}{N} \log _{2} p\left(w_{1}^{N}\right)$$</p>  
    __Why Cross-Entropy:__ It is a measure of _how many bits are need to encode text with our model_ (bits you would need to represent the distribution).[^1]  
    > Commonly used for __character-level__.  

    Alternatively, people tend to use __Perplexity__:  
    <p>$$\text { perplexity }\left(w_{1}^{N}\right)=2^{H\left(w_{1}^{N}\right)}$$</p>  
    __Why Perplexity:__  It is a measure of how _surprised our model is on seeing each word_.     
    > If __no surprise__, the perplexity $$ = 1$$.    
    > Commonly used for __word-level__.  
    <br>

8. **Language Modeling Data:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    Language modelling is a time series prediction problem in which we must be careful to train on the past and test on the future.  
    If the corpus is composed of articles, it is best to ensure the test data is drawn from a disjoint set of articles to the training data.  

    Two popular data sets for language modeling evaluation are a preprocessed version of the Penn Treebank,1 and the Billion Word Corpus.2 Both are __flawed__:     
    * The PTB is very small and has been heavily processed. As such it is not representative of natural language.  
    * The Billion Word corpus was extracted by first randomly permuting sentences in news articles and then splitting into training and test sets. As such train and test sentences come from the same articles and overlap in time
    
    The recently introduced __WikiText datasets__ are a better option.  

9. **Three Approaches to Parameterizing Language Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents19}  
    1. __Count-Based N-gram models__: we approximate the history of observed words with just the previous $$n$$ words.  
        They capture __Multinomial distributions__.   
    2. __Neural N-gram models__: embed the same fixed n-gram history in a _continuous space_ and thus better capture _correlations between histories_.  
        Replace the _Multinomial distributions_ with an __FFN__.  
    3. __RNNs__: drop the fixed n-gram history and _compress the entire history in a fixed length vector_, enabling _long range correlations_ to be captured.   
        Replace the __finite__ history, captured by the conditioning context $$w_{i-1}$$, with an __infinite__ history, captured by the (previous) hidden state $$h_{n-1}$$ (but also $$w_{n=1})$$.  
    <br>

10. **Bias vs Variance in LM Approximations:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents110}  
    The main issue in language modeling is compressing the history (a string). This is useful beyond language modeling in classification and representation tasks.  
    * With n-gram models we approximate the history with only the last n words  
    * With recurrent models (RNNs, next) we compress the unbounded history into a fixed sized vector  

    We can view this progression as the classic __Bias vs. Variance tradeoff__ in ML:  
    * __N-gram models__: are biased but low variance.  
        No matter how much data (infinite) they will always be wrong/biased.  
    * __RNNs:__ decrease the bias considerably, hopefully at a small cost to variance.  

    Consider predicting the probability of a sentence by how many times you have seen it before. This is an _unbiased estimator with (extremely) high variance_.  
    * In the limit of infinite data, gives true distribution.  
    <br>

11. **Scaling Language Models (Large Vocabularies):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents111}  
    __Bottleneck:__  
    Much of the computational cost of a Neural LM is a function of the __size of the vocabulary__ and is dominated by calculating the softmax:  
    <p>$$\hat{p}_{n}=\operatorname{softmax}\left(W h_{n}+b\right)$$</p>  

    __Solutions:__  
    * __Short-Lists__: use the neural LM for the most frequent words, and a traditional _n-gram_ LM for the rest.  
        While easy to implement, this nullifies the Neural LMs main advantage, i.e. generalization to rare events.  
    * __Batch local short-lists__: approximate the full partition function for data instances from a segment for the data with a subset of vocabulary chosen for that segment.  
    * __Approximate the gradient/change the objective__:  if we did not have to sum over the vocabulary to normalize during training, it would be much faster. It is tempting to consider maximizing likelihood by making the log partition function an independent parameter $$c$$, but this leads to an ill defined objective:  
        <p>$$\hat{p}_{n} \equiv \exp \left(W h_{n}+b\right) \times \exp (c)$$</p>  
        > What does the Softmax layer do?  
        > The idea of the Softmax is to say: at each time step look at the word we want to predict and the whole vocab; where we try to __maximize the probability of the word we want to predict__ and __minimize the probability of ALL THE OTHER WORDS__.  

        So, The better solution is to try to approximate what the softmax does using: 
        * __Noise Contrastive Estimation (NCE)__: this amounts to learning a binary classifier to distinguish data samples from $$(k)$$ samples from a noise distribution (a unigram is a good choice):  
        <p>$$p\left(\text { Data }=1 | \hat{p}_{n}\right)=\frac{\hat{p}_{n}}{\hat{p}_{n}+k p_{\text { noise }}\left(w_{n}\right)}$$</p>   
        Now parametrizing the log partition function as $$c$$ does not degenerate. This is very effective for _speeding up training_ but has no effect on _testing_.   
        * __Importance Sampling (IS)__: similar to NCE but defines a multiclass classification problem between the true word and noise samples, with a Softmax and cross entropy loss.   
        * [**(more on) Approximating the Softmax**](http://ruder.io/word-embeddings-softmax/index.html){: value="show" onclick="iframePopA(event)"}
            <a href="http://ruder.io/word-embeddings-softmax/index.html"></a>
            <div markdown="1"> </div>    
    * __Factorize the output vocabulary__: the idea is to decompose the (one big) softmax into a series of softmaxes (2 in this case). We map words to a set of classes, then we, first, predict which class the word is in, and then we predict the right word from the words in that class.  
        One level factorization works well (Brown clustering is a good choice, frequency binning is not):  
        <p>$$p\left(w_{n} | \hat{p}_{n}^{\text { class }}, \hat{p}_{n}^{\text { word }}\right)=p\left(\operatorname{class}\left(w_{n}\right) | \hat{p}_{n}^{\text { class }}\right) \times p\left(w_{n} | \operatorname{class}\left(w_{n}\right), \hat{p}_ {n}^{\text { word }}\right)$$</p>  
        where the function $$\text{ class}(\cdot)$$ maps each word to one class. Assuming balanced classes, this gives a quadratic, $$\root{V}$$ speedup.  
        * [**Binary Tree Factorization for $$\log{V} Speedup**](https://www.youtube.com/embed/eDUaRvMDs-s?start=2818){: value="show" onclick="iframePopA(event)"}
        <a href="https://www.youtube.com/embed/eDUaRvMDs-s?start=2818"></a>
            <div markdown="1"> </div>   

    __Complexity Comparison of the different solutions:__  
    ![img](/main_files/dl/nlp/rnn/8.png){: width="70%"}  
    <br>
        
12. **Sub-Word Level Language Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents112}  
    Could be viewed as an alternative to changing the softmax by changing the input granularity and model text at the __morpheme__ or __character__ level.  
    This results in a much smaller softmax and no unknown words, but the downsides are longer sequences and longer dependencies; moreover, a lot of the structure in a language is in the words and we want to learn correlations amongst the words but since the model doesn't get the words as a unit, it will have to _learn what/where a is_ before it can learn its correlation with other sequences; which effecitely means that we made the learning problem harder and more non-linear     
    This, also, allows the model to capture subword structure and morphology: e.g. "disunited" <-> "disinherited" <-> "disinterested".  
    Character LMs __lag__ behind word-based models in perplexity, but are clearly the future of language modeling.  

13. **Conditional Language Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents113}  
    A __Conditional LM__ assigns probabilities to sequences of words given some conditioning context $$x$$. It models "What is the probability of the next word, given the history of previously generated words AND conditioning context $$x$$?".  
    The probability, decomposed w/ chain rule:  
    <p>$$p(\boldsymbol{w} | \boldsymbol{x})=\prod_{t=1}^{\ell} p\left(w_{t} | \boldsymbol{x}, w_{1}, w_{2}, \ldots, w_{t-1}\right)$$</p>  
    * <button>Applications</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](/main_files/dl/nlp/rnn/9.png){: width="80%" hidden=""}   


[^1]: the problem of assigning a probability to a string and text compression is exactly the same problem so if you have a good language model you also have a good text compression algorithm and both we think of it in terms of the number of bits we can compress our sequence into.  


***

## Recurrent Neural Networks
{: #content2}

1. **Recurrent Neural Networks:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyCoxqntents21}  
    :   An __RNN__ is a class of artificial neural network where connections between units form a directed cycle, allowing it to exhibit dynamic temporal behavior.
    :   The standard RNN is a nonlinear dynamical system that maps sequences to sequences.  

2. **The Structure of an RNN:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    :   The RNN is parameterized with three weight matrices and three bias vectors:  
    :   $$ \theta = [W_{hv}, W_{hh}, W_{oh}, b_h, b_o, h_0] $$
    :   These parameter completely describe the RNN.  

3. **The Algorithm:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    :   Given an _input sequence_ $$\hat{x} = [x_1, \ldots, x_T]$$, the RNN computes a sequence of hidden states $$h_1^T$$ and a sequence of outputs $$y_1^T$$ in the following way:  
        __for__ $$t$$ __in__ $$[1, ..., T]$$ __do__  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$u_t \leftarrow W_{hv}x_t + W_{hh}h_{t-1} + b_h$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$h_t \leftarrow g_h(u_t)$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$o_t \leftarrow W_{oh}h_{t} + b_o$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$y_t \leftarrow g_y(o_t)$$   

4. **The Loss:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    :   The loss of an RNN is commonly a sum of per-time losses:  
    :   $$L(y, z) = \sum_{t=1}^TL(y_t, z_t)$$
    :   * __Language Modelling__: 
            We use the *__Cross Entropy__* Loss function but predicting _words_ instead of classes
    :   $$ J^{(t)}(\theta) = - \sum_{j=1}^{\vert V \vert} y_{t, j} \log \hat{y_{t, j}}$$
    :   $$\implies$$
    :   $$L(y,z) = J = -\dfrac{1}{T} \sum_{t=1}^{T} \sum_{j=1}^{\vert V \vert} y_{t, j} \log \hat{y_{t, j}}$$
    :   To __Evaluate__ the model, we use *__Preplexity__* : 
    :   $$ 2^J$$
    :   > Lower Preplexity is _better_

5. **Analyzing the Gradient:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    :   Assuming the following formulation of an __RNN__:  
    :   $$h_t = Wf(h_{t-1}) + W^{(hx)}x_{[t]} \\
        \hat{y_t} = W^{(S)}f(h_t)$$
    :   * The __Total Error__ is the sum of each error at each time step $$t$$:  
    :   $$ \dfrac{\partial E}{\partial W} = \sum_{t=1}^{T} \dfrac{\partial E_t}{\partial W}$$
    :   * The __local Error__ at a time step $$t$$:  
    :   $$\dfrac{\partial E_t}{\partial W} = \sum_{k=1}^{t} \dfrac{\partial E_t}{\partial y_t} \dfrac{\partial y_t}{\partial h_t} \dfrac{\partial h_t}{\partial h_k} \dfrac{\partial h_k}{\partial W}$$
    :   * To compute the _local derivative_ we need to compute:  
    :   $$\dfrac{\partial h_t}{\partial h_k}$$
    :   
    :   $$\begin{align}
        \dfrac{\partial h_t}{\partial h_k} &= \prod_{j=k+1}^t \dfrac{\partial h_j}{\partial h_{j-1}} \\
        &= \prod_{j=k+1}^t J_{j, j-1}
        \end{align}$$
    :   $$\:\:\:\:\:\:\:\:$$ where each $$J_{j, j-1}$$ is the __jacobina matrix__ of the partial derivatives of each respective  
        $$\:\:\:\:\:\:\:\:$$ hidden layer.

9. **The Vanishing Gradient Problem:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents29}  
    :   * __Analyzing the Norms of the Jacobians__ of each partial:  
    :   $$\| \dfrac{\partial h_j}{\partial h_{j-1}} \| \leq \| W^T \| \cdot \| \text{ diag}[f'(h_{j-1})] \| \leq \beta_W \beta_h$$
    :   $$\:\:\:\:\:\:\:$$ where we defined the $$\beta$$s as _upper bounds_ of the _norms_.        
    :   * __The Gradient is the product of these Jacobian Matrices__ (each associated with a step in the forward computation):  
    :   $$ \| \dfrac{\partial h_t}{\partial h_k} \| = \| \prod_{j=k+1}^t \dfrac{\partial h_j}{\partial h_{j-1}} \| \leq (\beta_W \beta_h)^{t-k}$$
    :   * *__Conclusion__*:  
            Now, as the exponential $$(t-k) \rightarrow \infty$$:  
            * __If $$(\beta_W \beta_h) < 1$$__:   
                $$(\beta_W \beta_h)^{t-k} \rightarrow 0$$.  
                known as __Vanishing Gradient__. 
            * __If $$(\beta_W \beta_h) > 1$$__:  
                $$(\beta_W \beta_h)^{t-k} \rightarrow \infty$$.  
                known as __Exploding Gradient__. 
    :   As the bound can become __very small__ or __very large__ quickly, the _locality assumption of gradient descent_ breaks down.

                    
            

                


6. **BPTT:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    :   __for__ $$t$$ __from__ $$T$$ __to__ $$1$$ __do__  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$do_t \leftarrow g_y'(o_t) ¬∑ dL(y_t ; z_t)/dy_t$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$db_o \leftarrow db_o + do_t$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$dW_{oh} \leftarrow dW_{oh} + do_th_t^T$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$dh_t \leftarrow dh_t + W_{oh}^T do_t$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$dy_t \leftarrow g_h'(y_t) ¬∑ dh_t$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$dW_{hv} \leftarrow dW_{hv} + dy_tx_t^T$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$db_h \leftarrow db_h + dy_t$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$dW_{hh} \leftarrow dW_{hh} + dy_th_{t-1}^T$$  
            $$\ \ \ \ \ \ \ \ \ \ $$ $$dh_{t-1} \leftarrow W_{hh}^T dy_t$$  
        __Return__ $$\:\:\:\: d\theta = [dW_{hv}, dW_{hh}, dW_{oh}, db_h, db_o, dh_0]$$


7. **Backpropagation Through Time:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  
    * We can think of the recurrent net as a layered, feed-forward net with shared weights and then train the feed-forward net with (linear) weight constraints.
    * We can also think of this training algorithm in the time domain:
        * The forward pass builds up a stack of the activities of all the units at each time step
        * The backward pass peels activities off the stack to compute the error derivatives at each time step
        * After the backward pass we add together the derivatives at all the different times for each weight.  

    __Complexity:__  
    *__Linear__* in the length of the longest sequence.  
    _Minibatching_ can be inefficient as the sequences in a batch may have different lengths.  
    > Can be alleviated w/ __padding__.  

8. **Truncated BPTT:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}  
    Same as BPTT, but tries to avoid the problem of long sequences as inputs. It does that by _"breaking"_ the gradient flow every nth time-step (if input is article, then n could be average length of a sentence), thus, avoiding problems of __(1) Memory__ __(2) Exploding Gradient__.  

    __Downsides:__  
    If there are _dependencies_ between the segments where BPTT was truncated they will __not be learned__ because the _gradient doesn't flow back to teach the hidden representation about what information was useful_.  

    __Complexity:__  
    *__Constant__* in the truncation length $$T$$.  
    _Minibatching_ is efficient as all sequences have length $$T$$.  

    __Notes:__  
    * In TBPTT, we __Forward Propagate__ through-the-break/between-segments normally (through the entire comp-graph). Only the back-propagation is truncated.  
    * __Mini-batching__ on a GPU is an effective way of speeding up big matrix vector products [^2]. RNNLMs have two such products that dominate their computation: the _recurrent matrix_ $$V$$ and the _softmax matrix_ $$W$$.  



[^2]: By making them Matrix-Matrix products instead.  


__LSTMS:__  
* The core of the history/memory is captured in the _cell-state $$c_{n}$$_ instead of the hidden state $$h_{n}$$.  
* (&) __Key Idea:__ The update to the cell-state $$c_{n}=c_{n-1}+\operatorname{stanh}\left(V\left[w_{n-1} ; h_{n-1}\right]+b_{c}\right)$$  here are __additive__. (differentiating a sum gives the identity) Making the gradient flow nicely through the sum. As opposed to the multiplicative updates to $$h_n$$ in vanilla RNNs.  
    > There is non-linear funcs applied to the history/context cell-state. It is composed of linear functions. Thus, avoids gradient shrinking.  

* In the recurrency of the LSTM the activation function is the identity function with a derivative of 1.0. So, the backpropagated gradient neither vanishes or explodes when passing through, but remains constant.
* By the selective read, write and forget mechanism (using the gating architecture) of LSTM, there exist at least one path, through which gradient can flow effectively from $$L$$  to $$\theta$$. Hence no vanishing gradient.   
* However, one must remember that, this is not the case for exploding gradient. It can be proved that, there __can exist__ at-least one path, thorough which gradient can explode.  
* LSTM decouples cell state (typically denoted by c) and hidden layer/output (typically denoted by h), and only do additive updates to c, which makes memories in c more stable. Thus the gradient flows through c is kept and hard to vanish (therefore the overall gradient is hard to vanish). However, other paths may cause gradient explosion.  
* The Vanishing gradient solution for LSTM is known as _Constant Error Carousel_.  
* [**Why can RNNs with LSTM units also suffer from ‚Äúexploding gradients‚Äù?**](https://stats.stackexchange.com/questions/320919/why-can-rnns-with-lstm-units-also-suffer-from-exploding-gradients/339129#339129){: value="show" onclick="iframePopA(event)"}
<a href="https://stats.stackexchange.com/questions/320919/why-can-rnns-with-lstm-units-also-suffer-from-exploding-gradients/339129#339129"></a>
    <div markdown="1"> </div>    
* [Lecture on gradient flow paths through gates](https://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Teaching/pdf/Lecture15.pdf)  

* [**LSTMs (Lec Oxford)**](https://www.youtube.com/embed/eDUaRvMDs-s?start=775){: value="show" onclick="iframePopA(event)"}
<a href="https://www.youtube.com/embed/eDUaRvMDs-s?start=776"></a>
    <div markdown="1"> </div>    


__Important Links:__  
[The unreasonable effectiveness of Character-level Language Models](https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139)  
[character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy](https://gist.github.com/karpathy/d4dee566867f8291f086)  
[Visualizing and Understanding Recurrent Networks - Karpathy Lec](https://skillsmatter.com/skillscasts/6611-visualizing-and-understanding-recurrent-networks)  
[Cool LSTM Diagrams - blog](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714)  
[Illustrated Guide to Recurrent Neural Networks: Understanding the Intuition](https://www.youtube.com/watch?v=LHXXI4-IEns)  
[Code LSTM in Python](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)  
[Mikolov Thesis: STATISTICAL LANGUAGE MODELS BASED ON NEURAL NETWORKS](http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf)  


***

## RNNs Extra!
{: #content3}


5. **Vanishing/Exploding Gradients:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}  
    :   * __Exploding Gradients__:  
            * Truncated BPTT 
            * Clip gradients at threshold 
            * RMSprop to adjust learning rate 
        * __Vanishing Gradient__:   
            * Harder to detect 
            * Weight initialization 
            * ReLu activation functions 
            * RMSprop 
            * LSTM, GRUs 

1. **Applications:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   * __NER__  
        * __Entity Level Sentiment in context__  
        * __Opinionated Expressions__

2. **Bidirectional RNNs:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   * __Motivation__:  
            For _classification_ we need to incorporate information from words both preceding and following the word being processed
    :   ![img](/main_files/dl/nlp/rnn/1.png){: width="100%"}  
    :   $$\:\:\:\:$$ Here $$h = [\overrightarrow{h};\overleftarrow{h}]$$ represents (summarizes) the _past_ and the _future_ around a single token.
    :   * __Deep Bidirectional RNNs__:  
    :   ![img](/main_files/dl/nlp/rnn/2.png){: width="100%"}  
    :   $$\:\:\:\:\:$$ Each memory layer passes an _intermediate sequential representation_ to the next.

3. **Math to Code:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    :   The Parameters: $$\{W_{hx}, W_{hh}, W_{oh} ; b_h, b_o, h_o\}$$   
    :   $$\begin{align}
        h_t &= \phi(W_{hx}x_t + W_{hh}h_{t-1} + b_h) \\
        h_t &= \phi(\begin{bmatrix}
    W_{hx} & ; & W_{hh}
\end{bmatrix}   
        \begin{bmatrix} x_t  \\ ;   \\ h_{t-1} \end{bmatrix} + b_h)
        \end{align}
        $$ 
    :   $$y_t = \phi'(W_{oh}h_t + b_o)$$ 

4. **Initial States:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    :   ![img](/main_files/dl/nlp/rnn/3.png){: width="76%"}  

6. **Specifying the Initial States:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents36}  
    :   ![img](/main_files/dl/nlp/rnn/4.png){: width="76%"}  

7. **Teaching Signals:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents37}  
    :   ![img](/main_files/dl/nlp/rnn/5.png){: width="76%"}  


5. **Vanishing/Exploding Gradients:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}  
    :   * __Exploding Gradients__:  
            * Truncated BPTT 
            * Clip gradients at threshold 
            * RMSprop to adjust learning rate 
        * __Vanishing Gradient__:   
            * Harder to detect 
            * Weight initialization 
            * ReLu activation functions 
            * RMSprop 
            * LSTM, GRUs 

9. **Rectifying the Vanishing/Exploding Gradient Problem:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents39}  
    :   ![img](/main_files/dl/nlp/rnn/7.png){: width="76%"}  

8. **Linearity of BackProp:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents38}  
    :   ![img](/main_files/dl/nlp/rnn/6.png){: width="76%"}  
    :   The derivative update are also __Correlated__ which is bad for SGD.  

***
***

TITLE: Attention Mechanism for DNNs 
LINK: research/dl/nlp/attention.md


[Important Blog on Attention](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)   
[Attention and Augmented RNNs (Distill)](https://distill.pub/2016/augmented-rnns/)  
[Transformer Implementation TF G-Colab](https://www.tensorflow.org/alpha/tutorials/text/transformer)  
[A Guide to Attention Mechanisms and Memory Networks (skymind)](https://skymind.ai/wiki/attention-mechanism-memory-network)  
[Soft and Hard Attention](https://jhui.github.io/2017/03/15/Soft-and-hard-attention/)  
[Attention (WildML)](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)  
* [A Critical Review of Neural Attention Models in Natural Language Processing](https://arxiv.org/pdf/1902.02181.pdf)  
* [Attention Mechanism (d2l)](https://www.d2l.ai/chapter_attention-mechanism/index.html)  
* [Intuitive Understanding of Attention Mechanism in Deep Learning (medium + code)](https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f)  




## Introduction
{: #content1}

1. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    In __Vanilla Seq2Seq models__, the only representation of the input is the _fixed-dimensional vector representation $$(y)$$_, that we need to carry through the entire decoding process.   

    This presents a __bottleneck__ in condensing all of the information of the _entire input sequence_ into just one _fixed-length_ vector representation.  

2. **Attention:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    Attention is a mechanism that allows DNNs to focus on (view) certain local or global features of the input sequence as a whole or in part.     

    Attention involves focus on _certain parts_ of the input, while having a _low-resolution_ view of the rest of the input -- similar to human attention in vision/audio.  

    An __Attention Unit__ considers all sub regions and contexts as its input and it outputs the weighted arithmetic mean of these regions.  
    > The __arithmetic mean__ is the inner product of actual values and their probabilities.  
    <p>$$m_i = \tanh (x_iW_{x_i} + CW_C)$$</p>   
    These __probabilities__ are calculated using the *__context__*.  
    The __Context__ $$C$$ represents everything the RNN has outputted until now.  

    The difference between using the _hyperbolic tanh_ and a _dot product_ is the __granularity__ of the output regions of interest - __tanh__ is more fine-grained with less choppy and smoother sub-regions chosen.  

    The probabilities are interpreted as corresponding to the relevance of the sub-region $$x_i$$ given context $$C$$.  
    <br>

4. **Types of Attention:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    * __Soft Attention__: we consider different parts of _different subregions_   
        * Soft Attention is __deterministic__ 
    * __Hard Attention__: we consider only _one subregion_  
        * Hard Attention is a __stochastic__ process 
    <br>

5. **Strategy:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    * Encode each word in the sentence into a vector (representation)
    * When decoding, perform a linear combination of these vectors, weighted by _attention weights_ 
    * Use this combination in picking the next word (subregion)  
    <br>

6. **Calculating Attention:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.  
    * Use __query__ vector (decoder state) and __key__ vectors (all encoder states)
    * For each query-key pair, calculate weight 
    * Normalize to add to one using softmax 
    * Combine together value vectors (usually encoder states, like key vectors) by taking the weighted sum
    * Use this in any part of the model  
    <br>

7. **Attention Score Functions:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    $$q$$ is the query, $$k$$ is the key:  
    * __Multi-Layer Perceptron__ _(Bahdanau et al. 2015)_:  
        * Flexible, often very good with large data   
        <p>$$a(q,k) = w_2^T \tanh (W_1[q;k])$$</p>   
    * __Bilinear__ _(luong et al. 2015)_:  
        * Not used widely in Seq2Seq models
        * Results are inconsistent
        <p>$$a(q,k) = q^TWk$$</p>  
    * __Dot Product__ _(luong et al. 2015)_:  
        * No parameters
        * Requires the sizes to be the same
        <p>$$a(q,k) = q^Tk$$</p>  
    * __Scaled Dot Product__ _(Vaswani et al. 2017)_:  
        * Solves the scale problem of the dot-product: the scale of the dot product increases as dimensions get larger
        <p>$$a(q,k) = \dfrac{q^Tk}{\sqrt{\vert k \vert}}$$</p>  

    <button>Complete (List)</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/uapdC_biDofPElWdzYkFsWVdtGAOnTM720Bo8I5Q9Vg.original.fullsize.png){: width="100%" hidden=""}  

    <button>Aggregation Functions</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/eCbLZeQ926y2I8BGSaDW9NV5_MIQyYy2XKuhT3IwFBY.original.fullsize.png){: width="100%" hidden=""}  
    <br> 
            
                
8. **What to Attend to?**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    * __Input Sentence__:  
        * A previous word for translation - [Neural Machine Translation](/)  
        * Copying Mechanism - [Gu et al. 2016](/)  
        * Lexicon bias [Arthur et al. 2016](/)  
    * __Previously Generated Things__:  
        * In *__language modeling__*: attend to the previous words - [Merity et al. 2016](/)   
            > Attend to the previous words that you generated and decide whether to use them again (copy)  
        * In __*translation*__: attend to either input or previous output - [Vaswani et al. 2017](/)  
    <br>

9. **Modalities:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents19}
    * __Images__ (Xu et al. 2015)    
    * __Speech__ (Chan et al. 2015)  
    * __Hierarchical Structures__ (Yang et al. 2016):  
        * Encode with attention over each sentence then attention over each sentence in the document  
    * __Multiple Sources__:    
        * Attend to multiple sentences in different languages to be translated to one target language (Zoph et al. 2015)  
        * Attend to a sentence and an image (Huang et al. 2016)  
    <br>
                
10. **Intra-Attention/Self-Attention:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents110}      
    Each element in the sentence attends to other elements -- context sensitive encodings.  

    It behaves similar to a __Bi-LSTM__ in that it tries to encode information about the context (words around the current input) into the representation of the word.  
    It differs however:  
    1. Intra-Attention is much more direct, as it takes the context directly without being influenced by many steps inside the RNN 
    2. It is much faster as it is only a dot/matrix product  
    <br>

11. **Improvement to Attention:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents111}  
    * __The Coverage Problem:__{: style="color: red"}  Neural models tend to drop or repeat content when tested on data not very similar to the training set  
    * __Solution:__{: style="color: red"} Model how many times words have been covered  
        * __Impose a penalty__ if attention is not $$\approx 1$$ for each word (Cohn et al. 2015)   
            It forces the system to translate each word at least once.  
        * __Add embeddings indicating coverage__ (Mi.. et al. 2016)  
        * Incorporating Markov Properties (Cohn et al. 2015)  
            * Intuition: attention from last time tends to be correlated with attention this time
            * Strategy: Add information about the last attention when making the next decision
        * __Bidirectional Training__ (Cohn et al. 2015): 
            * Intuition: Our attention should be roughly similar in forward and backward directions
            * Method: Train so that we get a bonus based on the trace of the matrix product for training in both directions  
                $$\mathrm{Tr} (A_{X \rightarrow Y}A^T_{Y \rightarrow X})$$  
        * __Supervised Training__ (Mi et al. 2016):   
            * Sometimes we can get "gold standard" alignments a-priori:  
                * Manual alignments
                * Pre-trained with strong alignment model
            * Train the model to match these strong alignments (bias the model)  
    <br>


12. **Attention is not Alignment _(Koehn and Knowles 2017)_:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents121}  
    * Attention is often blurred
    * Attention is often off by one:  
        Since the DNN has already seen parts of the information required to generate previous outputs, it might not need all of the information from the word that is actually matched with its current output.  

    Thus, even if _Supervised training_ is used to increase alignment accuracy, the overall error rate of the task might not actually decrease. 

***

## Specialized Attention Varieties
{: #content2}

1. **Hard Attention _(Xu et al. 2015)_:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    * Instead of a _soft interpolation_, make a __Zero-One decision__ about where to attend (Xu et al. 2015)
        * Harder to train - requires reinforcement learning methods
    * It helps interpretability (Lei et al. 2016)   
    <br>

2. **Monotonic Attention _(Yu et al. 2016)_:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    * In some cases, we might know the output will be the same order as the input:  
        * Speech Recognition
        * Incremental Translation
        * Morphological Inflection - sometimes
        * Summarization - sometimes
    * Hard decisions about whether to read more
    <br>

3. **Convolutional Attention _(Allamanis et al. 2016)_:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    * __Intuition__: we might want to be able to attend to "the word after 'Mr.'"  
    <br>

4. **Multi-headed Attention:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    * __Idea__: multiple _attention heads_ focus on different parts of the sentence
    * Different heads for "copy" vs regular (Allamanis et al. 2016)   
    * Multiple independently learned heads (Vaswani et al. 2017)
    <br>

5. **Tips:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    * Don't use attention with very long sequences - especially those you want to summarize and process efficiently 
    * __Fertility__: we impose the following heuristic "It is bad to pay attention to the same subregion many times" 
    <br>

6. **Notes:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    * Attention is a mean field approximation of sampling from a categorical distribution over source word embeddings (or the rnn state aligned with a source word, etc)  
    * __Additive VS Multiplicative Attention__:  
        Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.  
        Multiplicative attention uses the dot-product.  
        While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.  


***

## Representing Sentences: Solving the Vector Problem
{: #content3}

1. **The Problem: Conditioning with Vectors:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}    
    The Problem: We are compressing a lot of information into a __finite-sized__ vector.  
    Moreover, gradients flow a very long time/distance; making, even, LSTMs forget.  

    Sentences are of different sizes but vectors are of the same size; making the compression inherently, very lossy.  
    <br>

2. **The Solution: Representing Sentences as Matrices:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    We represent a __source sentence__ as a matrix, and generate the __target sentence__ from a matrix:  
    * Fixed number of rows, but number of columns depends on the number of words.  

    This will:  
    * Solve the __capacity problem__  
    * Solve the __gradient flow problem__ 
    <br>

3. **How to build the Matrices?**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}   
    1. __Concatenation:__  
        * Each word type is represented by an n-dimensional vector.  
        * Take all the vectors for the sentence and concatenate them into a matrix
        * This is the simplest possible model: that there are no published results on it...
    2. __Convolutional Networks:__  
        * Apply CNNs to transform the naive concatenated matrix to obtain a context-dependent matrix  
        * Remove the pooling layer at the end to ensure variable sized output  
    3. __BiRNNs:__  
        * Most widely used in NMT _(Bahdanau et al 2015)_  
        * One column per word
        * Each column (word) has two halves concatenated together:  
            * A "forward representation" (word and its LEFT context)  
            * A "reverse representation" (word and its RIGHT context)  


***
***

TITLE: CNNs in NLP 
LINK: research/dl/nlp/cnnsNnlp.md


## FIRST
{: #content1}

1. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    Combination (consecutively) of words are hard to capture/model/detect.  

2. **Padding:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
__Padding:__  
* After convolution, the rows and columns of the output tensor are either:  
    * Equal to rows/columns of input tensor ("same" convolution)  
        > Keeps the output dimensionality intact.  

    * Equal to rows/columns of input tensor minus the size of the filter plus one ("valid" or "narrow')  
    * Equal to rows/columns of input tensor plus filter minus one ("wide")  

__Striding:__  
Skip some of the outputs to reduce length of extracted feature vector  \\
![img](/main_files/dl/nlp/misc/1.png){: width="68%"}  \\

__Pooling:__  
Pooling is like convolution, but calculates some reduction function feature-wise.  
* __Types__:  
    * __Max Pooling__: "Did you see the feature anywhere in the range?"  
    * __Average pooling:__ "How prevalent is this feature over the entire range?"  
    * __k-Max pooling:__ "Did you see this feature up to k times?"  
    * __Dynamic pooling:__ "Did you see this feature in the beginning? In the middle? In the end?"  

__Stacking - Stacked Convolution:__  
* Feeding in convolution from previous layer results in larger are of focus for each feature  
* The increase in the number of _words_ that are covered by stacked convolution (e.g. n-grams) is *__exponential__* in the number of layers  \\
![img](/main_files/dl/nlp/misc/2.png){: width="68%"}  \\

__Dilation - Dilated Convolution:__  
Gradually increase _stride_, every time step (no reduction in length).  
![img](/main_files/dl/nlp/misc/3.png){: width="68%"}  \\
One can use the final output vector, for next target output prediction. Very useful if the problem we are modeling requires a fixed size output (e.g. auto-regressive models).  
* Why (Dilated) Convolution for Modeling Sentences?  
    * In contrast to recurrent neural networks:
        * + Fewer steps from each word to the final representation: RNN $$O(N)$$, Dilated CNN $$0(\log{N})$$ 
        * + Easier to parallelize on GPU 
        * - Slightly less natural for arbitrary-length dependencies 
        * - A bit slower on CPU?  
* Interesting Work:  
    _"Iterated Dilated Convolution [Strubell 2017]"_:  
    * A method for __sequence labeling__:  
        Multiple Iterations of the same stack of dilated convolutions (with different widths) to calculate context  
    * __Results:__
        * Wider context 
        * Shared parameters (i.e. more parameter efficient)  

__Structured Convolution:__  
* __Why?__  
    Language has structure, would like it to localize features.  
    > e.g. noun-verb pairs very informative, but not captured by normal CNNs   

* __Examples:__   
    * Tree-Structured Convolution _[Ma et al. 2015]_  
    * Graph Convolution _[Marcheggiani et al. 2017]_ 
        







***

## SECOND
{: #content2}









***

## THIRD
{: #content3}










***
***

TITLE: TensorFlow 
LINK: research/dl/nlp/tensorflow.md



## Tips and Tricks
{: #content11}


1. **Saving the model:**{: style="color: SteelBlue"}{: .bodyContents11 #bodyContents111}  
:   * After the model is run, it uses the most recent checkpoint
    * To run a different model with different architecture, use a different branch  

    :   

    :   

    :   

    :   

    :   

    :   

    :   


***
***

TITLE: Text Classification
LINK: research/dl/nlp/txt_cls.md


[Convolutional Neural Networks for Language (CMU)](https://www.youtube.com/watch?v=HBcr5jCBynI&t=7s)  
[Text Classification (Oxford)](https://www.youtube.com/watch?v=0qG7gjTNhwM&list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm&index=8)  




## Introduction
{: #content1}

1. **Text Classification Breakdown:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    We can think of text classification as being broken down into a two stage process:  
    1. __Representation:__ Process text into some (fixed) representation -> How to learn $$\mathbf{x}'$$.  
    2. __Classification:__ Classify document given that representation $$\mathbf{x}'$$ -> How to learn $$p(c\vert x')$$.  


2. **Representation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    __Bag of Words (BOW):__{: style="color: red"}  
    * __Pros:__  
        * Easy, no effort
    * __Cons__:  
        * Variable size, ignores sentential structure, sparse representations  

    __Continuous BOW:__{: style="color: red"}  
    * __Pros:__  
        * Continuous Repr.
    * __Cons__:  
        * Ignores word ordering  

    __Deep CBOW:__{: style="color: red"}  
    * __Pros:__  
        * Can learn feature combinations (e.g. "not" AND "hate")  
    * __Cons__:  
        * Cannot learn word-ordering (positional info) directly (e.g. "not hate")  

    __Bag of n-grams:__{: style="color: red"}  
    * __Pros:__  
        * Captures (some) combination features and word-ordering (e.g. "not hate"), works well  
    * __Cons__:  
        * Parameter Explosion, no sharing between similar words/n-grams


3. **CNNs for Text:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    Two main paradigms:  
    1. __Context-window modeling:__ for *__tagging__* etc. get the surrounding context before tagging.  
    2. __Sentence modeling:__ do convolution to extract n-grams, pooling to combine over whole sentence.  



***
***

TITLE: Articulated Body Pose Estimation <br /> (Human Pose Estimation)
LINK: research/dl/cv/hpe.md


## Introduction
{: #content1}
 
***

## DeepPose 
{: #content2}

[Further Reading](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42237.pdf)

1. **Main Idea:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    :   Pose Estimation is formulated as a __DNN-based regression problem__ towards __body joints__.  
        The __DNN regressors__ are presented as a cascade for higher precision in pose estimates.    

2. **Structure:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    :   * __Input__: 
            * Full Image
            * 7-layered generic Convolutional DNN    
        > Each Joint Regressor uses the full image as a signal.   


3. **Key Insights:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    :   * Replace the __explicitly designed feature representations and detectors for the parts, the model topology, and the interactions between joints__ by a *__learned representation through a ConvNet__*  
        * The (DNN-based) Pose Predictors are presented as a __cascade__ to increase the precision of _joint localization_  
        * Although the regression loss does not model explicit interactions between joints, such are implicitly captured by all of the 7 hidden layers ‚Äì all the internal features are shared by all joint regressors

4. **Method:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    :   * Start with an initial pose estimation (based on the full image)
        * Learn DNN-based regressors which refine the joint predictions by using higher resolution sub-images


5. **Notation:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    :   * __Pose Vector__ = $$\mathbf{y} = \left(\ldots, \mathbf{y}_i^T, \ldots\right)^T, \: i \in \{1, \ldots, k\}$$  
        * __Joint Co-ordinates__ = $$\mathbf{y}_i^T = (x_i, y_i)$$ of the $$i$$-th joint  
        * __Labeled Image__ = $$(x, \mathbf{y})$$  
            * $$x = $$ Image Data  
            * $$\mathbf{y} = $$ Ground-Truth Pose Vector
        * __Bounding Box__ = $$b$$: a box bounding the human body or parts of it   
        * __Normalization Function__ $$= N(\mathbf{y}_i; b)$$: normalizes the *joint coordinates* w.r.t a bounding box $$b$$  
            > Since the joint coordinates are in absolute image coordinates, and poses vary in size from image to image   

            ![img](/main_files/cv/pose_est/3.png){: width="60%"}  
            * _Translate_ by _box center_
            * _Scale_ by _box size_  
        * __Normalized pose vector__ = $$N(\mathbf{y}; b) = \left(\ldots, N(\mathbf{y}_i; b)^T, \ldots\right)^T$$  
        * __A crop of image $$x$$ by bounding box $$b$$__ = $$N(x; b)$$
        * *__Learned Function__* = $$\psi(x;\theta) \in \mathbb{R}^2k$$ is a functions that regresses to normalized pose vector, given an image:  
            * __Input__: image $$x$$
            * __Output__: Normalized pose vector $$N(\mathbf{y})$$                 
        * *__Pose Prediction__*:   
        :   $$y^\ast = N^{-1}(\psi(N(x);\theta))$$   

7. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  
    :   * __Problem__: Regression Problem
        * __Goal__: Learn a function $$\psi(x;\theta)$$ that is trained and used to regress to a pose vector.   
        * __Estimation__: $$\psi$$ is based on (learned through) Deep Neural Net
        * __Deep Neural Net__: is a Convolutional Neural Network; namely, __AlexNet__  
            * *__Input__*: image with pre-defined size $$ = \:$$ #-pixels $$\times 3$$-color channels  
                > $$(220 \times 220)$$ with a stride of $$4$$  
            * *__Output__*: target value of the regression$$ = 2k$$ joint coordinates  
    :   > Denote by $$\mathbf{C}$$ a convolutional layer, by $$\mathbf{LRN}$$ a local response normalization layer, $$\mathbf{P}$$ a pooling layer and by $$\mathbf{F}$$ a fully connected layer  
    :   > For $$\mathbf{C}$$ layers, the size is defined as width $$\times$$ height $$\times$$ depth, where the first two dimensions have a spatial meaning while the depth defines the number of filters.  
    :   * __Alex-Net__: 
            * *__Architecture__*:     $$\mathbf{C}(55 \times 55 \times 96) ‚àí \mathbf{LRN} ‚àí \mathbf{P} ‚àí \mathbf{C}(27 \times 27 \times 256) ‚àí \mathbf{LRN} ‚àí \mathbf{P} ‚àí \\\mathbf{C}(13 \times 13 \times 384) ‚àí \mathbf{C}(13 \times 13 \times 384) ‚àí \mathbf{C}(13 \times 13 \times 256) ‚àí \mathbf{P} ‚àí \mathbf{F}(4096) ‚àí \mathbf{F}(4096)$$   
            * *__Filters__*:  
                * $$\mathbf{C}_{1} = 11 \times 11$$,  
                * $$\mathbf{C}_{2} = 5 \times 5$$,  
                * $$\mathbf{C}_{3-5} = 3 \times 3$$.
            * *__Total Number of Parameters__* $$ = 40$$M   
            * *__Training Dataset__*:  
                Denote by $$D$$ the training set and $$D_N$$ the normalized training set:   
                $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  $$D_N = \{(N(x),N(\mathbf{y}))\vert (x,\mathbf{y}) \in D\}$$   
            * *__Loss__*: the Loss is modified; instead of a _classification loss_, we train a linear regression on top of the last network layer to predict a pose vector by minimizing $$L_2$$ distance between the prediction and the true pose vector,  
    :   $$\arg \min_\theta \sum_{(x,y) \in D_N} \sum_{i=1}^k \|\mathbf{y}_i - \psi_i(x;\theta)\|_2^2$$  
    :   * __Optimization__:  
            * *__BackPropagation__* in a distributed online implementation
            * *__Adaptive Gradient Updates__*
            * *__Learning Rate__* $$ = 0.0005 = 5\times 10^{-4}$$
            * *__Data Augmentation__*: randomly translated image crops, left/right flips
            * *__DropOut Regularization__* for the $$\mathbf{F}$$ layers $$ = 0.6$$

9. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents29}  
    :   * __Motivation__:   
            Although, the pose formulation of the DNN has the advantage that the joint estimation is based on the full image and thus relies on context, due its fixed input size of $$220 \times 220$$, the network has _limited capacity to look at detail_ - it _learns filters capturing pose properties at coarse scale_.  
            The _pose properties_ are necessary to _estimate rough pose_ but __insufficient__ to always _precisely localize the body joints_.  
            Increasing the input size is infeasible since it will increase the already large number of parameters.  
            Thus, a _cascade of pose regressors_ is used to achieve better precision.  
        * __Structure and Training__:   
            At the first stage: 
            * The cascade starts off by estimating an initial pose as outlined in the previous section.  
            At subsequent stages:  
            * Additional DNN regressors are trained to predict a displacement of the joint locations from previous stage to the true location.  
                > Thus, each subsequent stage can be thought of as a refinement of the currently predicted pose.   
            * Each subsequent stage uses the predicted joint locations to focus on the relevant parts of the image ‚Äì subimages are cropped around the predicted joint location from previous stage and the pose displacement regressor for this joint is applied on this sub-image.  
                > Thus, subsequent pose regressors see higher resolution images and thus learn features for finer scales which ultimately leads to higher precision  
        * __Method and Architecture__:  
            * The same network architecture is used for all stages of the cascade but learn different parameters.   
            * Start with a bounding box $$b^0$$: which either encloses the full image or is obtained by a person detector
            * Obtain an initial pose:  
                Stage 1: $$\mathbf{y}^1 \leftarrow N^{-1}(\psi(N(x;b^0);\theta_1);b^0)$$  
            * At stages $$s \geq 2$$, for all joints:
                * Regress first towards a refinement displacement $$\mathbf{y}_i^s - \mathbf{y}_i^{(s-1)}$$ by applying a regressor on the sub image defined by $$b_i^{(s-1)}$$ 
                * Estimate new joint boxes $$b_i^s$$:  
                Stage $$s$$: $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{y}_i^s \leftarrow \mathbf{y}_i^{(2-1)} + N^{-1}(\psi(N(x;b^0);\theta_s);b)  \:\: (6)  \\
 \ \ \ \ \ \ \ \ \ \ \ \ \ \                  \:\:\:\: \text{for } b = b_i^(s-1) \\
  \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
                b_i^s \leftarrow (\mathbf{y}_i^s, \sigma diam(\mathbf{y}^s), \sigma diam(\mathbf{y}^s))) \:\: (7)$$  
                where we considered a joint bounding box $$b_i$$ capturing the sub-image around $$\mathbf{y}_i: b_i(\mathbf{y}; \sigma) = (\mathbf{y}_i, \sigma diam(\mathbf{y}), \sigma diam(\mathbf{y}))$$ having as center the i-th joint and as dimension the pose diameter scaled by $$\sigma$$, to refine a given joint location $$\mathbf{y}_i$$.    
            * Apply the cascade for a fixed number of stages $$ = S$$  
        * __Loss__: (at each stage $$s$$)   
    :  $$\theta_s = \arg \min_\theta \sum_{(x,\mathbf{y}_i) \in D_A^s} \|\mathbf{y}_i - \psi_i(x;\theta)\|_2^2 \:\:\:\:\: (8)$$


6. **Advantages:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    :   * The DNN is capable of capturing the full context of each body joint  
        * The approach is simpler to formulate than graphical-models methods - no need to explicitly design feature representations and detectors for parts or to explicitly design a model topology and interactions between joints.   
            > Instead a generic ConvNet learns these representations

8. **Notes:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}  
    :   * The use of a generic DNN architecture is motivated by its outstanding results on both classification and localization problems and translates well to pose estimation  
        * Such a model is a truly holistic one ‚Äî the final joint location estimate is based on a complex nonlinear transformation of the full image  
        * The use of a DNN obviates the need to design a domain specific pose model
        * Although the regression loss does not model explicit interactions between joints, such are implicitly captured by all of the 7 hidden layers ‚Äì all the internal features are shared by all joint regressors  

***

## 
{: #content3}

    :   


    :   


    :   


    :   


    :   


    :   


    :   


    :   


***

## FOURTH
{: #content4}

    :   


    :   


    :   


    :   


    :   


    :   


***
***

TITLE: Generative Models <br /> Unsupervised Learning
LINK: research/dl/cv/generative_models_13.md


[Learning Deep Generative Models (pdf)](https://www.cs.cmu.edu/~rsalakhu/papers/annrev.pdf)  
[AutoRegressive Models (CS236 pdf)](https://deepgenerativemodels.github.io/notes/autoregressive/)  
[Deep Generative Models (CS236 pdf)](https://deepgenerativemodels.github.io/notes/index.html)  
[Deep Generative Models (Lecture)](https://www.youtube.com/watch?v=JrO5fSskISY)  
[CS294 Berkeley - Deep Unsupervised Learning](https://sites.google.com/view/berkeley-cs294-158-sp19/home)  



## Unsupervised Learning
{: #content1}

1. **Unsupervised Learning:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __Data:__ $$x$$ Just data, no labels!   
    __Goal:__ Learn some underlying hidden _structure_ of the data  
    __Examples:__ Clustering, dimensionality reduction, feature learning, density estimation, etc.  
    <br>

***

## Generative Models
{: #content2}

Given some data $$\{(d,c)\}$$ of paired observations $$d$$ and hidden classes $$c$$:  

1. **Generative (Joint) Models:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    __Generative Models__ are __Joint Models__.  
    __Joint Models__ place probabilities $$\left(P(c,d)\right)$$ over both the observed data and the "target" (hidden) variables that can only be computed from those observed.  
    
    Generative models are typically probabilistic, specifying a joint probability distribution ($$P(d,c)$$) over observation and target (label) values, and tries to __Maximize__ this __joint Likelihood__.  
    > Choosing weights turn out to be trivial: chosen as the __relative frequencies__.  

    They address the problem of __density estimation__, a core problem in unsupervised learning.  

    __Examples:__  
    {: #lst-p}
    * Gaussian Mixture Model
    * Naive Bayes Classifiers  
    * Hidden Markov Models (HMMs)
    * Restricted Boltzmann Machines (RBMs)
    * AutoEncoders
    * Generative Adversarial Networks (GANs)


2. **Discriminative (Conditional) Models:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    :   __Discriminative Models__ are __Conditional Models__.  
    :   __Conditional Models__ provide a model only for the "target" (hidden) variabless.  
        They take the data as given, and put a probability $$\left(P(c \vert d)\right)$$ over the "target" (hidden) structures given the data.  
    :   Conditional Models seek to __Maximize__ the __Conditional Likelihood__.  
        > This (maximization) task is usually harder to do.  
    :   __Examples:__  
        * Logistic Regression
        * Conditional LogLinear/Maximum Entropy Models  
        * Condtional Random Fields  
        * SVMs  
        * Perceptrons  
        * Neural Networks

3. **Generative VS Discriminative Models:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    Basically, _Discriminative Models_ infer outputs based on inputs,  
    while _Generative Models_ generate, both, inputs and outputs (typically given some hidden paramters).  
    
    However, notice that the two models are usually viewed as complementary procedures.  
    One does __not__ necessarily outperform the other, in either classificaiton or regression tasks.   

4. **Example Uses of Generative Models:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    :   * Clustering
        * Dimensionality Reduction
        * Feature Learning
        * Density Estimation

5. **Density Estimation:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    :   __Generative Models__, given training data, will generate new samples from the same distribution.   
    :   They address the __Density Estimation__ problem, a core problem in unsupervised learning.  
    :   * __Types__ of Density Estimation:  
            * *__Explicit__*: Explicitly define and solve for $$p_\text{model}(x)$$  
            * *__Implicit__*: Learn model that can sample from $$p_\text{model}(x)$$ without explicitly defining it     

6. **Applications of Generative Models:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    :   * Realistic samples for artwork
        * Super-Resolution
        * Colorization
        * Generative models of time-series data can be used for simulation and planning  
            > reinforcement learning applications  
        * Inference of __Latent Representations__ that can be useful as general feature descriptors 

7. **Taxonomy of Generative Models:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  
    :   ![img](/main_files/cs231n/13/1.png){: width="100%"}  

***

## AutoRegressive Models - PixelRNN and PixelCNN
{: #content3}

[AutoRegressive Models (pdf)](https://deepgenerativemodels.github.io/notes/autoregressive/)  

1. **Fully Visible (Deep) Belief Networks:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    __Deep Belief Network (DBNs)__ are generative graphical models, or alternatively a class of deep neural networks, composed of multiple layers of latent variables ("hidden units"), with connections between the layers but not between units within each layer.  
    
    DBNs undergo unsupervised training to _learn to probabilistically reconstruct the inputs_.  

    They generate an __Explicit Density Model__.  

    They use the __chain rule__ to _decompose the _likelihood of an image_ $$x$$ into products of 1-d distributions:  
    ![img](/main_files/cs231n/13/2.png){: width="70%"}    
    then, they __Maximize__ the __Likelihood__ of the training data.  

    The __conditional distributions over pixels__ are very _complex_.  
    We model them using a __neural network__. 

2. **PixelRNN:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   is a proposed architecture (part of the class of __Auto-Regressive__ models) to model an explicit distribution of natural images in an _expressive, tractable,_ and _scalable_ way.  
    :   It sequentially predicts the pixels in an image along two spatial dimensions.  
    :   The Method __models__ the __discrete probability of the raw pixel values__ and __encodes the complete set of dependencies__ in an image.  
    :   The approach is to use probabilistic density models (like Gaussian or Normal distribution) to quantify the pixels of an image as a product of conditional distributions.  
        This approach turns the modeling problem into a sequence problem where the next pixel value is determined by all the previously generated pixel values.  
    :   * __Key Insights__:  
            * Generate image pixels starting from corner  
            * Dependency on previous pixels is modeled using an _LSTM_  
    :   ![img](/main_files/cs231n/13/3.png){: width="100%"}  
    :   * __The Model__:  
            * Scan the image, one row at a time and one pixel at a time (within each row)
            * Given the scanned content, predict the distribution over the possible values for the next pixel
            * Joint distribution over the pixel values is factorized into a product of conditional distributions thus causing the problem as a sequence problem
            * Parameters used in prediction are shared across all the pixel positions
            * Since each pixel is jointly determined by 3 values (3 colour channels), each channel may be conditioned on other channels as well
    :   * __Drawbacks__:   
            * Sequential training is __slow__
            * Sequential generation is __slow__   
    :   [Further Reading](https://gist.github.com/shagunsodhani/e741ebd5ba0e0fc0f49d7836e30891a7)

3. **PixelCNN:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    :   Similar to the __PixelRNN__ model, the __PixelCNN__ models  the *__Pixel Distribution__* $$p(\vec{x})$$, where $$\vec{x} = (x_0, \ldots, x_n)$$ is the vector of pixel values of a given image.  
    :   Similarly, we use the chain rule for join distribution: $$p(x) = p(x_0) \prod_1^n p(x_i | x_{i<})$$.  
        > such that, the first pixel is independent, the second depends on the first, and the third depends on, both, the first and second, etc.  
    :   ![img](/main_files/cs231n/13/4.png){: width="40%"}  
    :   * __Key Insights__:  
            * Still generate image pixels starting from corner
            * Dependency on previous pixels now modeled using a CNN over context region
            * Training: maximize likelihood of training images  
    :   * __Upsides__:  
            * Training is faster than __PixelRNN__: since we can parallelize the convolutions because the context region values are known from the training images.  
    :   * __Issues__:  
            * Generation is still sequential, thus, slow.
    :   [Further Reading](http://sergeiturukin.com/2017/02/22/pixelcnn.html)


4. **Improving PixelCNN Performance:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    * Gated Convolutional Layers 
    * Short-cut connections
    * Discretized logistic loss
    * Multi-scale
    * Training tricks

    __Further Reading__:  
    {: #lst-p}
    * *__PixelCNN++__* \| _Salimans et al. 2017_    
    * _Van der Oord et al. NIPS 2016_
    * __Pixel-Snail__  

5. **Pros and Cons of Auto-Regressive Models:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}  
    * __Pros__:   
        * Can explicitly compute likelihood $$p(x)$$
        * Explicit likelihood of training data gives good evaluation metric
        * Good Samples
    * __Cons__:  
        * Sequential Generation is __Slow__  

***

## Variational Auto-Encoders
{: #content4}

[__Auto-Encoders__](http://ahmedbadary.ml/work_files/research/dl/aencdrs) (_click to read more_) are unsupervised learning methods that aim to learn a representation (encoding) for a set of data in a smaller dimension.  
Auto-Encoders generate __Features__ that capture _factors of variation_ in the training data.

0. **Auto-Regressive Models VS Variational Auto-Encoders:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents40}  
    :   __Auto-Regressive Models__ defined a *__tractable__* (discrete) density function and, then, optimized the likelihood of training data:   
    :   $$p_\theta(x) = p(x_0) \prod_1^n p(x_i | x_{i<})$$  
    :   On the other hand, __VAEs__ defines an *__intractable__* (continuous) density function with latent variable $$z$$:  
    :   $$p_\theta(x) = \int p_\theta(z) p_\theta(x|z) dz$$
    :   but cannot optimize directly; instead, derive and optimiz a lower bound on likelihood instead.  

1. **Variational Auto-Encoders (VAEs):**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    :   __Variational Autoencoder__ models inherit the autoencoder architecture, but make strong assumptions concerning the distribution of latent variables.  
    :   They use variational approach for latent representation learning, which results in an additional loss component and specific training algorithm called Stochastic Gradient Variational Bayes (SGVB).  

2. **Assumptions:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}  
    :   VAEs assume that: 
        * The data is generated by a directed __graphical model__ $$p(x\vert z)$$ 
        * The encoder is learning an approximation $$q_\phi(z|x)$$ to the posterior distribution $$p_\theta(z|x)$$  
            where $${\displaystyle \mathbf {\phi } }$$ and $${\displaystyle \mathbf {\theta } }$$ denote the parameters of the encoder (recognition model) and decoder (generative model) respectively.  
        * The training data $$\left\{x^{(i)}\right\}_{i=1}^N$$ is generated from underlying unobserved (latent) representation $$\mathbf{z}$$

3. **The Objective Function:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}    
    :   $${\displaystyle {\mathcal {L}}(\mathbf {\phi } ,\mathbf {\theta } ,\mathbf {x} )=D_{KL}(q_{\phi }(\mathbf {z} |\mathbf {x} )||p_{\theta }(\mathbf {z} ))-\mathbb {E} _{q_{\phi }(\mathbf {z} |\mathbf {x} )}{\big (}\log p_{\theta }(\mathbf {x} |\mathbf {z} ){\big )}}$$
    :   where $${\displaystyle D_{KL}}$$ is the __Kullback‚ÄìLeibler divergence__ (KL-Div).  

4. **The Generation Process:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents44}  
    :   
    :   ![img](/main_files/cs231n/13/5.png){: width="40%"} 

5. **The Goal:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents45}  
    :   The goal is to estimate the true parameters $$\theta^\ast$$ of this generative model.

6. **Representing the Model:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents46}  
    :   * To represent the prior $$p(z)$$, we choose it to be simple, usually __Gaussian__  
        * To represent the conditional (which is very complex), we use a neural-network  

7. **Intractability:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents47}  
    :   The __Data Likelihood__:  
    :   $$p_\theta(x) = \int p_\theta(z) p_\theta(x|z) dz$$
    :   is intractable to compute for every $$z$$.  
    :   Thus, the __Posterior Density__:  
    :   $$p_\theta(z|x) = \dfrac{p_\theta(x|z) p_\theta(z)}{p_\theta(x)} = \dfrac{p_\theta(x|z) p_\theta(z)}{\int p_\theta(z) p_\theta(x|z) dz}$$ 
    :   is, also, intractable

8. **Dealing with Intractability:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents48}  
    :   In addition to decoder network modeling $$p_\theta(x\vert z)$$, define additional encoder network $$q_\phi(z\vert x)$$ that approximates $$p_\theta(z\vert x)$$
    :   This allows us to derive a __lower bound__ on the data likelihood that is tractable, which we can optimize.  

9. **The Model:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents49}  
    :   * The __Encoder__ (recognition/inference) and __Decoder__ (generation) networks are probabilistic and output means and variances of each the conditionals respectively:  
            ![img](/main_files/cs231n/13/6.png){: width="70%"}   
        * The generation (forward-pass) is done via sampling as follows:  
            ![img](/main_files/cs231n/13/7.png){: width="72%"}   

10. **The Log-Likelihood of Data:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents410}  
    :   * Deriving the Log-Likelihood:  
    :   ![img](/main_files/cs231n/13/8.png){: width="100%"}   

11. **Training the Model:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents411}  
    :   

12. **Pros, Cons and Research:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents412}  
    :   * __Pros__: 
            * Principled approach to generative models
            * Allows inference of $$q(z\vert x)$$, can be useful feature representation for other tasks  
    :   * __Cons__: 
            * Maximizing the lower bound of likelihood is okay, but not as good for evaluation as Auto-regressive models
            * Samples blurrier and lower quality compared to state-of-the-art (GANs)
    :   * __Active areas of research__:   
            * More flexible approximations, e.g. richer approximate posterior instead of diagonal Gaussian
            * Incorporating structure in latent variables


***

## Generative Adversarial Networks (GANs)
{: #content5}

0. **Auto-Regressive Models VS Variational Auto-Encoders VS GANs:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents40}  
    :   __Auto-Regressive Models__ defined a *__tractable__* (discrete) density function and, then, optimized the likelihood of training data:   
    :   $$p_\theta(x) = p(x_0) \prod_1^n p(x_i | x_{i<})$$  
    :   While __VAEs__ defined an *__intractable__* (continuous) density function with latent variable $$z$$:  
    :   $$p_\theta(x) = \int p_\theta(z) p_\theta(x|z) dz$$
    :   but cannot optimize directly; instead, derive and optimize a lower bound on likelihood instead.  
    :   On the other hand, __GANs__ rejects explicitly defining a probability density function, in favor of only being able to sample.     

1. **Generative Adversarial Networks:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents51}  
    :   are a class of AI algorithms used in unsupervised machine learning, implemented by a system of two neural networks contesting with each other in a zero-sum game framework.

2. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents52}  
    :   * __Problem__: we want to sample from complex, high-dimensional training distribution; there is no direct way of doing this.  
        * __Solution__: we sample from a simple distribution (e.g. random noise) and learn a transformation that maps to the training distribution, by using a __neural network__.  
    :   * __Generative VS Discriminative__: discriminative models had much more success because deep generative models suffered due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies, and due to difficulty of leveraging the benefits of piecewise linear units in the generative context.  
        GANs propose a new framework for generative model estimation that sidesteps these difficulties.      

3. **Structure:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents53}  
    :   * __Goal__: estimating generative models that capture the training data distribution  
        * __Framework__: an adversarial process in which two models are simultaneously trained a generative model $$G$$ that captures the data distribution, and a discriminative model $$D$$ that estimates the probability that a sample came from the training data rather than $$G$$.  
        * __Training__:  
            * $$G$$ maximizes the probability of $$D$$ making a mistake       


***
***

TITLE: CNNs <br /> Convolutional Neural Networks
LINK: research/dl/cv/cnns_5.md


## Introduction
{: #content1}

1. **CNNs:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11} 
    :   In machine learning, a convolutional neural network (CNN, or ConvNet) is a class of deep, feed-forward artificial neural networks that has successfully been applied to analyzing visual imagery.

2. **The Big Idea:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12} 
    :   CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing.

3. **Inspiration Model:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13} 
    :   Convolutional networks were inspired by biological processes in which the connectivity pattern between neurons is inspired by the organization of the animal visual cortex.  
    Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.

4. **Design:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   A CNN consists of an input and an output layer, as well as multiple hidden layers.  
        The hidden layers of a CNN typically consist of convolutional layers, pooling layers, fully connected layers and normalization layers.

***

## Architecture and Design
{: #content2}


1. **Volumes of Neurons:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21} 
    :   Unlike neurons in traditional Feed-Forward networks, the layers of a ConvNet have neurons arranged in 3-dimensions: **width, height, depth**.  
    > Note: __Depth__ here refers to the third dimension of an activation volume, not to the depth of a full Neural Network, which can refer to the total number of layers in a network.  

2. **Connectivity:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22} 
    :   The neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner.

3. **Functionality:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23} 
    :   A ConvNet is made up of Layers. 
    Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.  

    
    ![img](/main_files/dl/cnn/1.png){: width="100%"}
    
4. **Layers:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24} 
    :   We use three main types of layers to build ConvNet architectures: 
    :   * Convolutional Layer  
        * Pooling Layer  
        * Fully-Connected Layer

41. **Process:**{: style="color: SteelBlue"}{: .bodyContents2  #bodyContents241} 
    :   ConvNets transform the original image layer by layer from the original pixel values to the final class scores. 

5. **Example Architecture (CIFAR-10):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25} 
    :   Model: [INPUT - CONV - RELU - POOL - FC]
    :   * **INPUT:** [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.   
        * **CONV-Layer** will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume.    
        This may result in volume such as [$$32\times32\times12$$] if we decided to use 12 filters.  
        * **RELU-Layer:**  will apply an element-wise activation function, thresholding at zero. This leaves the size of the volume unchanged ([$$32\times32\times12$$]).  
        * **POOL-Layer:** will perform a down-sampling operation along the spatial dimensions (width, height), resulting in volume such as [$$16\times16\times12$$].  
        * **Fully-Connected:** will compute the class scores, resulting in volume of size [$$1\times1\times10$$], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10.  
        As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.

6. **Fixed Functions VS Hyper-Parameters:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26} 
    :   Some layers contain parameters and other don‚Äôt.
    :   * **CONV/FC layers** perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons).
    :   * **RELU/POOL** layers will implement a fixed function. 
    :   > The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image.  

7. **[Summary](http://cs231n.github.io/convolutional-networks/):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27} 
    * A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores)  
    * There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)  
    * Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function  
    * Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don‚Äôt)  
    * Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn‚Äôt)  
> [Click this for Credits](http://cs231n.github.io/convolutional-networks/)  

    ![img](/main_files/dl/cnn/2.png){: width="100%"}


***

## Convolutional Layers
{: #content3}

1. **Convolutions:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   A Convolution is a mathematical operation on two functions (f and g) to produce a third function, that is typically viewed as a modified version of one of the original functions, giving the integral of the point-wise multiplication of the two functions as a function of the amount that one of the original functions is translated.
    :   The convolution of the __continous__ functions f and g:  
    :   $${\displaystyle {\begin{aligned}(f*g)(t)&\,{\stackrel {\mathrm {def} }{=}}\ \int _{-\infty }^{\infty }f(\tau )g(t-\tau )\,d\tau \\&=\int _{-\infty }^{\infty }f(t-\tau )g(\tau )\,d\tau .\end{aligned}}}$$
    :   The convolution of the __discreet__ functions f and g: 
    :   $${\displaystyle {\begin{aligned}(f*g)[n]&=\sum _{m=-\infty }^{\infty }f[m]g[n-m]\\&=\sum _{m=-\infty }^{\infty }f[n-m]g[m].\end{aligned}}} (commutativity)$$

2. **Cross-Correlation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   Cross-Correlation is a measure of similarity of two series as a function of the displacement of one relative to the other.
    :   The __continuous__ cross-correlation on continuous functions f and g:  
    :   $$(f\star g)(\tau )\ {\stackrel {\mathrm {def} }{=}}\int _{-\infty }^{\infty }f^{*}(t)\ g(t+\tau )\,dt,$$
    :   The __discrete__ cross-correlation on discreet functions f and g:  
    :   $$(f\star g)[n]\ {\stackrel {\mathrm {def} }{=}}\sum _{m=-\infty }^{\infty }f^{*}[m]\ g[m+n].$$

3. **Convolutions and Cross-Correlation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    :   * Convolution is similar to cross-correlation.  
        * _For discrete real valued signals_, they differ only in a time reversal in one of the signals.  
        * _For continuous signals_, the cross-correlation operator is the **adjoint operator** of the convolution operator.

4. **CNNs, Convolutions, and Cross-Correlation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    :   The term Convolution in the name "Convolution Neural Network" is unfortunately a __misnomer__.  
        CNNs actually __use Cross-Correlation__ instead as their similarity operator.  
        The term 'convolution' has stuck in the name by convention.

5. **The Mathematics:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35} 
    :   * The CONV layer‚Äôs __parameters__ consist of __a set of learnable filters__.  
            * Every filter is small spatially (along width and height), but extends through the full depth of the input volume.  
            >  For example, a typical filter on a first layer of a ConvNet might have size 5x5x3 (i.e. 5 pixels width and height, and 3 because images have depth 3, the color channels).  
        * In the __forward pass__, we slide (convolve) each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position.  
            * As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position.  
            > Intuitively, the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the first layer, or eventually entire honeycomb or wheel-like patterns on higher layers of the network. 
            * Now, we will have an entire set of filters in each CONV layer (e.g. 12 filters), and each of them will produce a separate 2-dimensional activation map.   
        * We will __stack__ these activation maps along the depth dimension and produce the output volume.  
    <p style="color: red">As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input. </p>    
    
6. **The Brain Perspective:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26} 
    :   Every entry in the 3D output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially.  

7. **Local Connectivity:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27} 
    :   * Convolutional networks exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers: 
            * Each neuron is connected to only a small region of the input volume.
        * The __Receptive Field__ of the neuron defines the extent of this connectivity as a hyperparameter.  
        >  For example, suppose the input volume has size $$[32x32x3]$$ and the receptive field (or the filter size) is $$5x5$$, then each neuron in the Conv Layer will have weights to a $$[5x5x3]$$ region in the input volume, for a total of $$5*5*3 = 75$$ weights (and $$+1$$ bias parameter).  
    <p style="color: red">Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.</p>

8. **Spatial Arrangement:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28} 
    :   There are __three__ hyperparameters control the size of the output volume:  
    :       1. __The Depth__ of the output volume is a hyperparameter that corresponds to the number of filters we would like to use (each learning to look for something different in the input).  
            2. __The Stride__ controls how depth columns around the spatial dimensions (width and height) are allocated.  
                > e.g. When the stride is 1 then we move the filters one pixel at a time.  

                > The __Smaller__ the stride, the __more overlapping regions__ exist and the __bigger the volume__.  
                > The __bigger__ the stride, the __less overlapping regions__ exist and the __smaller the volume__.  
            3. The __Padding__ is a hyperparameter whereby we pad the input the input volume with zeros around the border.  
                > This allows to _control the spatial size_ of _the output_ volumes.  


9. **The Spatial Size of the Output Volume:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents29} 
    :   We compute the spatial size of the output volume as a function of:  
    :       * **$$W$$**: The input volume size.  
            * **$$F$$**: $$\:\:$$The receptive field size of the Conv Layer neurons.  
            * **$$S$$**: The stride with which they are applied.  
            * **$$P$$**: The amount of zero padding used on the border.  
    :   Thus, the __Total Size of the Output__:  
    :   $$\dfrac{W‚àíF+2P}{S} + 1$$  
    :   * __Potential Issue__: If this number is not an integer, then the strides are set incorrectly and the neurons cannot be tiled to fit across the input volume in a symmetric way.  
    :   * __Fix__: In general, setting zero padding to be $${\displaystyle P = \dfrac{K-1}{2}}$$ when the stride is $${\displaystyle S = 1}$$ ensures that the input volume and output volume will have the same size spatially.  


0. **The Convolution Layer:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents30} 
    :   

    ![img](/main_files/dl/cnn/3.png){: width="100%"}

***

## Layers
{: #content3}

1. **Convolution Layer:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   One image becomes a stack of filtered images.

***

## Distinguishing features
{: #contentx}


2. **Image Features:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents92}
    :   are certain quantities that are calculated from the image to _better describe the information in the image_, and to _reduce the size of the input vectors_. 
    :   * Examples:  
            * __Color Histogram__: Compute a (bucket-based) vector of colors with their respective amounts in the image.  
            * __Histogram of Oriented Gradients (HOG)__: we count the occurrences of gradient orientation in localized portions of the image.   
            * __Bag of Words__: a _bag of visual words_ is a vector of occurrence counts of a vocabulary of local image features.  
                > The __visual words__ can be extracted using a clustering algorithm; K-Means.  


***
***

TITLE: Deep-Dream <br /> Visualizing Features in ConvNets
LINK: research/dl/cv/feature_vis_12.md


## Deep-Dream: Deep Feature Visualization
{: #content1}

0. **DeepDream:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents10}  
    :   __DeepDream__ is a computer vision program created by Google which uses a ConvNet to find and enhance patterns in images via _algorithmic pareidolia_, thus creating a dream-like hallucinogenic appearance in the image.  

1. **The General Idea:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   Rather than synthesizing an image to maximize a specific neuron, instead try to __amplify__ the neuron activations at some layer in the network

2. **The Process:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   1. Start with an input image
        2. Run it through a ConvNet up to some layer
        3. Forward: compute activations at chosen layer
        4. Set gradient of chosen layer equal to its activation
        5. Backward: Compute gradient on image
        6. Update Image 
        7. Repeat

***

## Feature Inversion
{: #content2}

    :   

    :   

    :   

    :   

    :   

    :   

    :   

    :   

***

## THIRD
{: #content3}

    :   

    :   

    :   

    :   

    :   

    :   

    :   

    :   

***

## FOURTH
{: #content4}

    :   

    :   






***
***

TITLE: Recurrent Neural Networks <br /> Applications in Computer Vision
LINK: research/dl/cv/rnns_in_cv_10.md


## RNNs
{: #content1}

### [Refer to this section on RNNs](ahmedbadary.ml/work_files/research/dl/nlp/rnns)

1. **Process Sequences:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   * __One-to-One__  
        * __One-to-Many__: 
            Image Captioning: image -> seq of words
        * __Many-to-One__: 
            Sentiment Classification: seq of words -> Sentiment
        * __Many-to-Many__:   
            Machine Translation: seq of words -> seq of words
        * __(Discrete) Many-to-Many__:  
            Frame-Level Video Classification: seq. of frames -> seq of classes per frame  
    :   ![img](/main_files/cs231n/10/1.png){: width="80%"}  


2. **RNN Structure:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   We can process a sequence of vectors $$\vec{x}$$ by applying a recurrence formula__ at every time step:  
    :   $$h_t = f_W(h_{t-1}, x_t)$$
    :   where $$h_t$$ is the new state, $$f_W$$ is some function with weights $$W$$, $$h_{t-1}$$ is the old state, and $$x_t$$ is the input vector at some time step $$t$$.   
        > The __same__ _function_ and _set of parameters (weights)_ are used at every time step.  

3. **A Vanilla Architecture of an RNN:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   $$
        \begin{align}
        h_t &= f_W(h_{t-1}, x_t)
        h_t &= tanh(W_{hh}h_t{t-1} + W_{xh}x_t)  
        y_t &= W_{hy}h_t
        \end{align}
        $$

4. **The RNN Computational Graph:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   * __Many-to-Many__:       
    :   ![img](/main_files/cs231n/10/2.png){: width="80%"}
    :   * __One-to-Many__:  
    :  ![img](/main_files/cs231n/10/3.png){: width="80%"} 
    :   * __Seq-to-Seq__:   
    :   ![img](/main_files/cs231n/10/4.png){: width="80%"}

5. **Example Architecture: Character-Level Language Model:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    :   ![img](/main_files/cs231n/10/5.png){: width="80%"}
    :   ![img](/main_files/cs231n/10/6.png){: width="80%"}

6. **The Functional Form of a Vanilla RNN (Gradient Flow):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    :   ![img](/main_files/cs231n/10/7.png){: width="80%"}

***

## Applications in CV
{: #content2}

### Coming Soon!

***

## Implementations and Training (LSTMs and GRUs)
{: #content3}

### Coming Soon!

***
***

TITLE: Image Segmentation <br /> with Deep Learning
LINK: research/dl/cv/img_seg_11.md


## Semantic Segmentation
{: #content1}

1. **Semantic Segmentation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   __Semantic Segmentation__ is the task of understanding an image at the pixel level. It seeks to assign an object class to each pixel in the image.  
    :   ![img](/main_files/cs231n/11/1.png){: width="20%"}


2. **The Structure:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   * __Input__: Image  
        * __Output__: A class for each pixel in the image.  

3. **Properties:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   In Semantic Segmentation, we don't differentiate among the instances, instead, we only care about the pixels.
    :   ![img](/main_files/cs231n/11/2.png){: width="40%"}  

***

## Approaches (The Pre-DeepLearning Era)
{: #content2}

1. **Semantic Texton Forests:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    :   This approach consists of ensembles of decision trees that act directly on image pixels.  
    :   Semantic Texton Forests (STFs) 
are randomized decision forests that use only simple pixel comparisons on local image patches, performing both an
implicit hierarchical clustering into semantic textons and an explicit local classification of the patch category.  
    :   STFs allow us to build powerful texton codebooks without computing expensive filter-banks or descriptors, and without performing costly k-means clustering and nearest-neighbor assignment.
    :   _Semantic Texton Forests for Image Categorization and Segmentation, Shawton et al. (2008)_

2. **Random Forest-based Classifiers:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    :   Random Forests have also been used to perform semantic segmentation for a variety of tasks.

3. **Conditional Random Fields:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    :   CRFs provide a probabilistic framework for labeling and segmenting structured data.  
    :   They try to model the relationship between pixels, e.g.:
        1. nearby pixels more likely to have same label
        2. pixels with similar color more likely to have same label
        3. the pixels above the pixels "chair" more likely to be "person" instead of "plane"
        4. refine results by iterations
    :   _W. Wu, A. Y. C. Chen, L. Zhao and J. J. Corso (2014): "Brain Tumor detection and segmentation in a CRF framework with pixel-pairwise affinity and super pixel-level features"_
    :   _Plath et al. (2009): "Multi-class image segmentation using conditional random fields and global classification"_

4. **SuperPixel Segmentation:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    :   The concept of superpixels was first introduced by Xiaofeng Ren and Jitendra Malik in 2003.  
    :   __Superpixel__ is a group of connected pixels with similar colors or gray levels.  
        They produce an image patch which is better aligned with intensity edges than a rectangular patch.  
    :   __Superpixel segmentation__ is the idea of dividing an image into hundreds of non-overlapping superpixels.  
        Then, these can be fed into a segmentation algorithm, such as __Conditional Random Fields__ or __Graph Cuts__, for the purpose of segmentation.  
    :   _Efficient graph-based image segmentation, Felzenszwalb, P.F. and Huttenlocher, D.P. International Journal of Computer Vision, 2004_
    :   _Quick shift and kernel methods for mode seeking, Vedaldi, A. and Soatto, S. European Conference on Computer Vision, 2008_
    :   _Peer Neubert & Peter Protzel (2014). Compact Watershed and Preemptive_  

***

## Approaches (The Deep Learning Era)
{: #content3}

1. **The Sliding Window Approach:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   We utilize _classification_ for _segmentation_ purposes.  
    :   * __Algorithm__:    
            * We break up the input image into tiny "crops" of the input image.  
            * Use Classification to find the class of the center pixel of the crop.  
                > Using the same machinery for classification.
    :   Basically, we do classification on each crop of the image.
    :   * __DrawBacks:__  
            * Very Inefficient and Expensive:  
                To label every pixel in the image, we need a separate "crop" for each pixel in the image, which would be quite a huge number.  
            * Disregarding Localized Information:  
                This approach does __not__ make use of the shared features between overlapping patches in the image.  
                Further, it does not make use of the spatial information between the pixels.  
    :   _Farabet et al, ‚ÄúLearning Hierarchical Features for Scene Labeling,‚Äù TPAMI 2013_
    :   _Pinheiro and Collobert, ‚ÄúRecurrent Convolutional Neural Networks for Scene Labeling‚Äù, ICML 2014_

2. **Fully Convolutional Networks:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   We make use of convolutional networks by themselves, trained end-to-end, pixels-to-pixels.  
    :   * __Structure__:  
            * _Input_: Image vector  
            * _Output_: A Tensor $$(C \times H \times W)$$, where $$C$$ is the number of classes.  
    :   The key observation is that one can view __Fully Connected Layers__ as __Convolutions__ over the entire image.  
        Thus, the structure of the ConvNet is just a stacked number of convolutional layers that __preserve the size of the image__.  
        * __Issue with the Architecture:__   
            The proposed approach of preserving the size of the input image leads to an exploding number of hyperparamters.  
            This makes training the network very tedious and it almost never converges.  
        * __Solution__:  
            We allow the network to perform an encoding of the image by   
            first __Downsampling__ the image,  
            then, __Upsampling__ the image back, inside the network.  
            The __Upsampling__ is __not__ done via _bicubic interpolation_, instead, we use __Deconvolutional__ layers (Unpooling) for learning the upsampling.   
            However, (even learnable)upsampling produces coarse segmentation maps because of loss of information during pooling. Therefore, shortcut/skip connections are introduced from higher resolution feature maps. 
    :   _Long et. al (2014)_  
    :   ![img](/main_files/cs231n/11/3.png){: width="80%"}

***

## Methods, Approaches and Algorithms in Training DL Models
{: #content4}

1. **Upsampling:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    :   Also, known as __"Unpooling"__.  
    :   * __Nearest Neighbor__: fill each region with the corresponding pixel value in the original image.  
            ![img](/main_files/cs231n/11/4.png){: width="40%"}  
    :   * __Bed of Nails__: put each corresponding pixel value in the original image into the upper-left corner in each new sub-region, and fill the rest with zeros.   
            ![img](/main_files/cs231n/11/5.png){: width="40%"}  
    :   * __Max-Unpooling__: The same idea as _Bed of Nails_, however, we re-place the pixel values from the original image into their original values that they were extracted from in the _Max-Pooling_ step.  
            ![img](/main_files/cs231n/11/6.png){: width="80%"}

2. **Learnable Upsampling: Deconvolutional Layers (Transpose Convolution):**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}  
    :   * __Transpose Convolution__: is a convolution performed on a an _input_ of a small size, each element in the input acts a _scalar_ that gets multiplied by the filter, and then gets placed on a, larger, _output_ matrix, where the regions of overlap get summed.   
        ![img](/main_files/cs231n/11/7.png){: width="80%"}
    :   Also known as:  
        * Deconvolution
        * UpConvolution
        * Fractionally Strided Convolution  
            > Reason: if you think of the stride as the ratio in step between the input and the output; this is equivalent to a stride one-half convolution, because of the ratio of 1-to-2 between the input and the output.  
        * Backward Strided Convolution
            > Reason: The forward pass of a Transpose Convolution is the same mathematical operation as the backward pass of a normal convolution.   
    :   * __1-D Example:__  
            ![img](/main_files/cs231n/11/8.png){: width="60%"}
    :   * __Convolution as Tensor Multiplication__: All Convolutions (with stride and padding) can be framed as a __Tensor Product__ by placing the filters intelligently in a tensor.  
            The name __Transpose Convolution__ comes from the fact that the __Deconvolution__ operation, viewed as a __Tensor Product__, is just the __Transpose__ of the Convolution operation.  
            * 1-D Example:   
                ![img](/main_files/cs231n/11/9.png){: width="90%"}  
            * In-fact, the name __Deconvolution__ is a mis-nomer exactly because of this interpretation:  
                The __Transpose__ matrix of the Convolution operation is a convolution __iff__ the __stride__ is equal to 1.  
                If the stride>1, then the transpose matrix no longer represents a convolution.  
                    ![img](/main_files/cs231n/11/10.png){: width="90%"}
    :   * __Issues with Transpose Convolution:__    
            * Since we sum the values that overlap in the region of the upsampled image, the magnitudes in the output will __vary depending on the number of receptive fields in the output__.  
                This leads to some _checkerboard artifacts_.  
        * __Solution:__ 
            * Avoid (3x3) stride two deconvolutions.  
            * Use (4x4) stride two, or (2x2) stride two deconvolutions. 

***
***

TITLE: Articulated Body Pose Estimation <br /> (Human Pose Estimation)
LINK: research/dl/cv/pose_est.md


## Introduction
{: #content1}

1. **Human (Articulated) Body Pose Estimation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   __Human Pose Estimation__ is the process of estimating the configuration of the body (pose) from a single, typically monocular, image. 
    :   In computer vision, __Body Pose Estimation__ is the study of algorithms and systems that recover the pose of an articulated body, which consists of joints and rigid parts using image-based observations.

2. **Difficulties in Pose Estimation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   Pose estimation is hard due to many reasons including:  
        * *__High Degree of Freedom (DOF)__*: 244 DOF  
        * *__Variability of human visual appearance__*
        * *__Variability in lighting conditions__* 
        * *__Variability in human physique__*
        * *__(partial) Occlusions__*
        * *__Complexity of the human physical structure__*
        * *__high dimensionality of the pose__* 
        * *__loss of 3d information that results from observing the pose from 2d planar image projections__* 
        * *__(variability in) Clothes__*  

3. **Theory:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   Human pose estimation is usually formulated __Probabilistically__ to account for the variability and ambiguities that exist in the inference.  
    :   In __Probabilistic__ approaches, we are interested in estimating the *__posterior distribution__* $$p(\mathbf{x}\vert \mathbf{z})$$, where $$\mathbf{x}$$ is the pose of the body and and $$\mathbf{z}$$ is a feature set derived from the image.  
    :   * __The Key Modeling choices__ that affect the inference are:   
            * The representation of the pose ‚Äì $$\mathbf{x}$$
            * The nature and encoding of image features ‚Äì $$\mathbf{z}$$
            * The inference framework required to estimate the posterior ‚Äì $$p(\mathbf{x}\vert \mathbf{z})$$
    :   [Further Reading](https://cs.brown.edu/~ls/Publications/SigalEncyclopediaCVdraft.pdf)   

4. **Model-based Approaches:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   The typical body pose estimation system involves a __model-based approach__, in which the pose estimation is achieved by _maximizing/minimizing_ a _similarity/dissimilarity_ between an _observation_ (input) and a _template model_.   
    :   Different kinds of sensors have been explored for use in making the observation.  
        * __Sensors__:   
            * Visible wavelength imagery
            * Long-wave thermal infrared imagery
            * Time-of-flight imagery
            * Laser range scanner imagery
    :   These sensors produce intermediate representations that are directly used by the model.
        * __Representations__: 
            * Image appearance
            * Voxel (volume element) reconstruction
            * 3D point clouds, and sum of Gaussian kernels
            * 3D surface meshes.

5. **The Representation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    :   A __Representation__ is a model to depict the configuration of the human body.  
        The _configuration of the human body_ can be represented in a variety of ways.  
    :   There are two common representations used for the human body:  
        * __Kinematic Skeleton Tree__  
        * __Part Models__  

6. **Kinematic Skeleton Tree with Quaternions:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    :   The most direct and common representation is obtained by parameterizing the body as a kinematic tree, $$\vec{x} = \{\tau, \theta_\tau, \theta_1, \theta_2, \ldots, \theta_N\}$$, where the pose is encoded using position of the root segment (the __pelvis__ is typically used as root to minimize the height of the kinematic tree), $$\tau$$, orientation of the root segment in the world, $$\theta_\tau$$, and a set of relative joint angels, $$\{\theta_i\}_{i=1}^N$$, that represent the orientation of the body parts with respect to their parents along the tree.  
        > e.g., the orientation of the thigh with respect to the pelvis, shin with respect to the thigh, etc.  
    :   ![img](/main_files/cv/pose_est/1.png){: width="60%"}
    :   The kinematic skeleton is constructed by a tree-structured chain where each rigid body segment has its local coordinate system that can be transformed to the world coordinate system via a 4√ó4 transformation matrix $${\displaystyle T_{l}}$$, 
    :   $${\displaystyle T_{l}=T_{\operatorname {par} (l)}R_{l},}$$
    :   where $${\displaystyle R_{l}}$$ denotes the local transformation from body segment $${\displaystyle S_{l}}$$ to its parent $${\displaystyle \operatorname {par} (S_{l})}$$.  
    :   Kinematic tree representation can be obtained for 2d, 2.5d, and 3d body models.  
        * __2-D__:   
            * $$\tau \in \mathcal{R}^2$$, 
            * $$\theta_\tau \in \mathcal{R}^1$$,
            * $$\theta_i \in \mathcal{R}^1$$:   corresponds to pose of the cardboard person in the image plane  
        * __3-D__:   
            * $$\tau \in \mathcal{R}^3$$, 
            * $$\theta_\tau \in SO(3)$$,
            * $$\theta_i \in SO(3)$$: for spherical joints, e.g. neck  
              $$\theta_i \in \mathcal{R}^2$$: for saddle joints, e.g. wrist  
              $$\theta_i \in \mathbb{R}^1$$: for hinge joints, e.g. knee    
        * __2.5-D__: are extensions of the __2-D__ representations where the pose, $$\mathbf{x}$$, is augmented with (discrete)  variables encoding the relative depth (layering) of body parts with respect to one another in the 2-d _cardboard_ model.  
            This representation is not very common.  
    :   Each joint in the body has 3 degrees of freedom (DoF) rotation. Given a transformation matrix $${\displaystyle T_{l}}$$, the joint position at the T-pose can be transferred to its corresponding position in the world coordination.  
        The __3-D joint rotation__ is, usually, expressed as a *__normalized quaternion__* $$[x, y, z, w]$$ due to its continuity that can facilitate gradient-based optimization in the parameters estimation.  
    :   In all (dimensionality) cases, kinematic tree representation results in a __high-dimensional pose vector__, $$\mathbf{x}$$, in $$\mathbb{R}^{30} - \mathbb{R}^{70}$$, depending on the fidelity and exact parameterization of the skeleton and joints.  
    :   Another parameterization uses the (2d or 3d) locations of the major joints in the world.   
        However, this parametrization is __not invariant to the morphology__ (body segment lengths) of a given individual.   

7. **Part-based Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    :   The body is modeled as a __set of parts__, $$\mathbf{x} = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_M\}$$, each with its own position and orientation in space, $$\mathbf{x}_i = \{\tau_i, \theta_i\}$$, that are connected by a set of statistical or physical constraints that enforce skeletal (and sometimes image) consistency.  
    :   The part model is motivated by the human skeleton, since any object having the property of articulation can be broken down into smaller parts wherein each part can take different orientations, resulting in different articulations of the same object.   
    Different scales and orientations of the main object can be articulated to scales and orientations of the corresponding parts.
    :   Mathematically, the parts are connected by springs; the model is, also, known as a __spring model__.  
        The degree of closeness between each part is accounted for by the compression and expansion of the springs. There is geometric constraint on the orientation of springs. For example, limbs of legs cannot move 360 degrees. Hence parts cannot have that extreme orientation.  This reduces the possible permutations.  
    :   The model can be formulated in 2-D or in 3-D.  
        The 2-D parameterizations are much more common.  
        In 2-D, each part‚Äôs representation is often augmented with an additional variable, $$s_i$$, that accounts for uniform scaling of the body part in the image, i.e., $$\mathbf{x}_i = \{\tau_i, \theta_i, s_i\}$$ with $$\tau_i \in \mathbb{R}^2, \theta_i \in \mathbb{R}^1$$ and $$s_i \in \mathbb{R}^1$$.  
    :   The model results in very high dimensional vectors, even higher than that of _kinematic trees_.  

8. **Applications:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    :   * Markerless motion capture for human-computer interfaces,
        * Physiotherapy 
        * 3D animation 
        * Ergonomics studies 
        * Robot control  and
        * Visual surveillance
        * Human-robot interaction
        * Gaming
        * Sports performance analysis


9. **Image Features:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents19}  
    :   In many of the classical approaches image features that represent the salient parts of the image with respect to the human pose play a huge rule in the performance of any pose estimation approach.   
    :   ![img](/main_files/cv/pose_est/2.png){: width="70%"}
    :   * __The most common features__: 
            * *__Silhouettes__*: for effectively separating the person from background in static scenes  
            * *__Color__*: for modeling un-occluded skin or clothing
            * *__Edges__*: for modeling external and internal contours of the body    
            * *__Gradients__*: for modeling the texture over the body parts  
        Other, less common features, include, __Shading__ and __Focus__.  
    :   To __reduce dimensionality__ and __increase robustness to noise__, these raw features are often encapsulated in _image descriptors_, such as __shape context__, __SIFT__, and __histogram of oriented gradients (HoG)__.  
        Alternatively, _hierarchical multi-level image encodings_ can be used, such as __HMAX__, __spatial pyramids__, and __vocabulary trees__.   
***

## DeepPose 
{: #content2}

[Further Reading](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42237.pdf)

1. **Main Idea:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    :   Pose Estimation is formulated as a __DNN-based regression problem__ towards __body joints__.  
        The __DNN regressors__ are presented as a cascade for higher precision in pose estimates.    

2. **Structure:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    :   * __Input__: 
            * Full Image
            * 7-layered generic Convolutional DNN    
        > Each Joint Regressor uses the full image as a signal.   


3. **Key Insights:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    :   * Replace the __explicitly designed feature representations and detectors for the parts, the model topology, and the interactions between joints__ by a *__learned representation through a ConvNet__*  
        * The (DNN-based) Pose Predictors are presented as a __cascade__ to increase the precision of _joint localization_  
        * Although the regression loss does not model explicit interactions between joints, such are implicitly captured by all of the 7 hidden layers ‚Äì all the internal features are shared by all joint regressors

4. **Method:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    :   * Start with an initial pose estimation (based on the full image)
        * Learn DNN-based regressors which refine the joint predictions by using higher resolution sub-images


5. **Notation:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    :   * __Pose Vector__ = $$\mathbf{y} = \left(\ldots, \mathbf{y}_i^T, \ldots\right)^T, \: i \in \{1, \ldots, k\}$$  
        * __Joint Co-ordinates__ = $$\mathbf{y}_i^T = (x_i, y_i)$$ of the $$i$$-th joint  
        * __Labeled Image__ = $$(x, \mathbf{y})$$  
            * $$x = $$ Image Data  
            * $$\mathbf{y} = $$ Ground-Truth Pose Vector
        * __Bounding Box__ = $$b$$: a box bounding the human body or parts of it   
        * __Normalization Function__ $$= N(\mathbf{y}_i; b)$$: normalizes the *joint coordinates* w.r.t a bounding box $$b$$  
            > Since the joint coordinates are in absolute image coordinates, and poses vary in size from image to image   

            ![img](/main_files/cv/pose_est/3.png){: width="60%"}  
            * _Translate_ by _box center_
            * _Scale_ by _box size_  
        * __Normalized pose vector__ = $$N(\mathbf{y}; b) = \left(\ldots, N(\mathbf{y}_i; b)^T, \ldots\right)^T$$  
        * __A crop of image $$x$$ by bounding box $$b$$__ = $$N(x; b)$$
        * *__Learned Function__* = $$\psi(x;\theta) \in \mathbb{R}^2k$$ is a functions that regresses to normalized pose vector, given an image:  
            * __Input__: image $$x$$
            * __Output__: Normalized pose vector $$N(\mathbf{y})$$                 
        * *__Pose Prediction__*:   
        :   $$y^\ast = N^{-1}(\psi(N(x);\theta))$$   

7. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  
    :   * __Problem__: Regression Problem
        * __Goal__: Learn a function $$\psi(x;\theta)$$ that is trained and used to regress to a pose vector.   
        * __Estimation__: $$\psi$$ is based on (learned through) Deep Neural Net
        * __Deep Neural Net__: is a Convolutional Neural Network; namely, __AlexNet__  
            * *__Input__*: image with pre-defined size $$ = \:$$ #-pixels $$\times 3$$-color channels  
                > $$(220 \times 220)$$ with a stride of $$4$$  
            * *__Output__*: target value of the regression$$ = 2k$$ joint coordinates  
    :   > Denote by $$\mathbf{C}$$ a convolutional layer, by $$\mathbf{LRN}$$ a local response normalization layer, $$\mathbf{P}$$ a pooling layer and by $$\mathbf{F}$$ a fully connected layer  
    :   > For $$\mathbf{C}$$ layers, the size is defined as width $$\times$$ height $$\times$$ depth, where the first two dimensions have a spatial meaning while the depth defines the number of filters.  
    :   * __Alex-Net__: 
            * *__Architecture__*:     $$\mathbf{C}(55 \times 55 \times 96) ‚àí \mathbf{LRN} ‚àí \mathbf{P} ‚àí \mathbf{C}(27 \times 27 \times 256) ‚àí \mathbf{LRN} ‚àí \mathbf{P} ‚àí \\\mathbf{C}(13 \times 13 \times 384) ‚àí \mathbf{C}(13 \times 13 \times 384) ‚àí \mathbf{C}(13 \times 13 \times 256) ‚àí \mathbf{P} ‚àí \mathbf{F}(4096) ‚àí \mathbf{F}(4096)$$   
            * *__Filters__*:  
                * $$\mathbf{C}_{1} = 11 \times 11$$,  
                * $$\mathbf{C}_{2} = 5 \times 5$$,  
                * $$\mathbf{C}_{3-5} = 3 \times 3$$.
            * *__Total Number of Parameters__* $$ = 40$$M   
            * *__Training Dataset__*:  
                Denote by $$D$$ the training set and $$D_N$$ the normalized training set:   
                $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  $$D_N = \{(N(x),N(\mathbf{y}))\vert (x,\mathbf{y}) \in D\}$$   
            * *__Loss__*: the Loss is modified; instead of a _classification loss_, we train a linear regression on top of the last network layer to predict a pose vector by minimizing $$L_2$$ distance between the prediction and the true pose vector,  
    :   $$\arg \min_\theta \sum_{(x,y) \in D_N} \sum_{i=1}^k \|\mathbf{y}_i - \psi_i(x;\theta)\|_2^2$$  
    :   * __Optimization__:  
            * *__BackPropagation__* in a distributed online implementation
            * *__Adaptive Gradient Updates__*
            * *__Learning Rate__* $$ = 0.0005 = 5\times 10^{-4}$$
            * *__Data Augmentation__*: randomly translated image crops, left/right flips
            * *__DropOut Regularization__* for the $$\mathbf{F}$$ layers $$ = 0.6$$

9. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents29}  
    :   * __Motivation__:   
            Although, the pose formulation of the DNN has the advantage that the joint estimation is based on the full image and thus relies on context, due its fixed input size of $$220 \times 220$$, the network has _limited capacity to look at detail_ - it _learns filters capturing pose properties at coarse scale_.  
            The _pose properties_ are necessary to _estimate rough pose_ but __insufficient__ to always _precisely localize the body joints_.  
            Increasing the input size is infeasible since it will increase the already large number of parameters.  
            Thus, a _cascade of pose regressors_ is used to achieve better precision.  
        * __Structure and Training__:   
            At the first stage: 
            * The cascade starts off by estimating an initial pose as outlined in the previous section.  
            At subsequent stages:  
            * Additional DNN regressors are trained to predict a displacement of the joint locations from previous stage to the true location.  
                > Thus, each subsequent stage can be thought of as a refinement of the currently predicted pose.   
            * Each subsequent stage uses the predicted joint locations to focus on the relevant parts of the image ‚Äì subimages are cropped around the predicted joint location from previous stage and the pose displacement regressor for this joint is applied on this sub-image.  
                > Thus, subsequent pose regressors see higher resolution images and thus learn features for finer scales which ultimately leads to higher precision  
        * __Method and Architecture__:  
            * The same network architecture is used for all stages of the cascade but learn different parameters.   
            * Start with a bounding box $$b^0$$: which either encloses the full image or is obtained by a person detector
            * Obtain an initial pose:  
                Stage 1: $$\mathbf{y}^1 \leftarrow N^{-1}(\psi(N(x;b^0);\theta_1);b^0)$$  
            * At stages $$s \geq 2$$, for all joints:
                * Regress first towards a refinement displacement $$\mathbf{y}_i^s - \mathbf{y}_i^{(s-1)}$$ by applying a regressor on the sub image defined by $$b_i^{(s-1)}$$ 
                * Estimate new joint boxes $$b_i^s$$:  
                Stage $$s$$: $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{y}_i^s \leftarrow \mathbf{y}_i^{(2-1)} + N^{-1}(\psi(N(x;b^0);\theta_s);b)  \:\: (6)  \\
 \ \ \ \ \ \ \ \ \ \ \ \ \ \                  \:\:\:\: \text{for } b = b_i^(s-1) \\
  \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
                b_i^s \leftarrow (\mathbf{y}_i^s, \sigma diam(\mathbf{y}^s), \sigma diam(\mathbf{y}^s))) \:\: (7)$$  
                where we considered a joint bounding box $$b_i$$ capturing the sub-image around $$\mathbf{y}_i: b_i(\mathbf{y}; \sigma) = (\mathbf{y}_i, \sigma diam(\mathbf{y}), \sigma diam(\mathbf{y}))$$ having as center the i-th joint and as dimension the pose diameter scaled by $$\sigma$$, to refine a given joint location $$\mathbf{y}_i$$.    
            * Apply the cascade for a fixed number of stages $$ = S$$  
        * __Loss__: (at each stage $$s$$)   
    :  $$\theta_s = \arg \min_\theta \sum_{(x,\mathbf{y}_i) \in D_A^s} \|\mathbf{y}_i - \psi_i(x;\theta)\|_2^2 \:\:\:\:\: (8)$$


6. **Advantages:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    :   * The DNN is capable of capturing the full context of each body joint  
        * The approach is simpler to formulate than graphical-models methods - no need to explicitly design feature representations and detectors for parts or to explicitly design a model topology and interactions between joints.   
            > Instead a generic ConvNet learns these representations

8. **Notes:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}  
    :   * The use of a generic DNN architecture is motivated by its outstanding results on both classification and localization problems and translates well to pose estimation  
        * Such a model is a truly holistic one ‚Äî the final joint location estimate is based on a complex nonlinear transformation of the full image  
        * The use of a DNN obviates the need to design a domain specific pose model
        * Although the regression loss does not model explicit interactions between joints, such are implicitly captured by all of the 7 hidden layers ‚Äì all the internal features are shared by all joint regressors  


***
***

TITLE: Generative Adversarial Networks
LINK: research/dl/cv/gans.md


[Generative Adversarial Networks (CS236 pdf)](https://deepgenerativemodels.github.io/notes/gan/)  
[GANs (Goodfellow)](https://www.youtube.com/watch?v=AJVyzd0rqdc)  
[GANs (tutorial lectures)](https://sites.google.com/view/cvpr2018tutorialongans/)  


## Generative Adversarial Networks (GANs)
{: #content1}

0. **Auto-Regressive Models VS Variational Auto-Encoders VS GANs:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents10}  
    __Auto-Regressive Models__ defined a *__tractable__* (discrete) density function and, then, optimized the likelihood of training data:   
    <p>$$p_\theta(x) = p(x_0) \prod_1^n p(x_i | x_{i<})$$  </p>  
    
    While __VAEs__ defined an *__intractable__* (continuous) density function with latent variable $$z$$:  
    <p>$$p_\theta(x) = \int p_\theta(z) p_\theta(x|z) dz$$</p>  
    but cannot optimize directly; instead, derive and optimize a lower bound on likelihood instead.  
    
    On the other hand, __GANs__ rejects explicitly defining a probability density function, in favor of only being able to sample.     
    <br>

1. **Generative Adversarial Networks:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __GANs__ are a class of AI algorithms used in unsupervised machine learning, implemented by a system of two neural networks contesting with each other in a zero-sum game framework.  
    <br>

2. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    * __Problem__: we want to sample from complex, high-dimensional training distribution; there is no direct way of doing this.  
    * __Solution__: we sample from a simple distribution (e.g. random noise) and learn a transformation that maps to the training distribution, by using a __neural network__.  

    ![img](https://cdn.mathpix.com/snip/images/28l-qxaMy4eTm9gBce5VKfd4K98boqFs0eNOTCkMnog.original.fullsize.png){: width="40%"}  


    * __Generative VS Discriminative__: discriminative models had much more success because deep generative models suffered due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies, and due to difficulty of leveraging the benefits of piecewise linear units in the generative context.  
        GANs propose a new framework for generative model estimation that sidesteps these difficulties.      
    <br>

3. **Structure:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    * __Goal__:  
        estimating generative models that capture the training data distribution  
    * __Framework__:  
        an adversarial process in which two models are simultaneously trained a generative model $$G$$ that captures the data distribution, and a discriminative model $$D$$ that estimates the probability that a sample came from the training data rather than $$G$$.  
    * __Training__:  
        $$G$$ maximizes the probability of $$D$$ making a mistake       

4. **Training:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    __Generator network:__ try to fool the discriminator by generating real-looking images  
    __Discriminator network:__ try to distinguish between real and fake images  
    ![img](https://cdn.mathpix.com/snip/images/12HQ0mKEIlrX4e2EanOSjUhjgdTpW6nV2gyn8G7-s0A.original.fullsize.png){: width="67%"}  

    * Train __jointly__ in __minimax game__.  
        * Minimax objective function:  
            <p>$$\min _{\theta_{g}} \max _{\theta_{d}}\left[\mathbb{E}_{x \sim p_{\text {data }}} \log D_{\theta_{d}}(x)+\mathbb{E}_{z \sim p(z)} \log \left(1-D_{\theta_{d}}\left(G_{\theta_{g}}(z)\right)\right)\right]$$</p>  
            \- Discriminator outputs likelihood in $$(0,1)$$ of real image  
            \- $$D_{\theta_{d}}(x)$$: Discriminator output for real data $$\boldsymbol{x}$$  
            \- $$D_{\theta_{d}}\left(G_{\theta_{g}}(z)\right)$$: Discriminator output for generated fake data $$G(z)$$  
            \- Discriminator $$\left(\theta_{d}\right)$$ wants to maximize objective such that $$\mathrm{D}(\mathrm{x})$$ is close to $$1$$ (real) and $$\mathrm{D}(\mathrm{G}(\mathrm{z}))$$ is close to $$0$$ (fake)  
            \- Generator $$\left(\mathrm{f}_ {\mathrm{g}}\right)$$ "wants to minimize objective such that $$\mathrm{D}(\mathrm{G}(\mathrm{z}))$$ is close to $$1$$ (discriminator is fooled into thinking generated $$\mathrm{G}(\mathrm{z})$$ is real)  
    * Alternate between\*:  
        1. __Gradient Ascent__ on Discriminator:  
            <p>$$\max _{\theta_{d}}\left[\mathbb{E}_{x \sim p_{\text {data}}} \log D_{\theta_{d}}(x)+\mathbb{E}_{z \sim p(z)} \log \left(1-D_{\theta_{d}}\left(G_{\theta_{g}}(z)\right)\right)\right]$$</p>  
        2. __Gradient Ascent__ on Generator (different objective):  
            <p>$$\max _{\theta_{g}} \mathbb{E}_{z \sim p(z)} \log \left(D_{\theta_{d}}\left(G_{\theta_{g}}(z)\right)\right)$$</p>  

    __GAN Training Algorithm:__{: style="color: red"}  
    <button>Algorithm</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/vJzBJcFh5ASuKowEV6B5OMklS-PPNIgqvnnDclMMu3g.original.fullsize.png){: width="100%" hidden=""}  
    \- __\# of Training steps $$\mathrm{k}$$:__ some find $$\mathrm{k}=1$$ more stable, others use $$\mathrm{k}>1$$ no best rule.  
    {: #lst-p}
    * Recent work (e.g. Wasserstein GAN) alleviates this problem, better stability!  


    ![img](https://cdn.mathpix.com/snip/images/nFIqQ5afG4h-GIcPQSbOwUxro5zj55HY9rAOf_f3C_Q.original.fullsize.png){: width="67%"}  


    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * \* Instead of minimizing likelihood of discriminator being correct, now maximize likelihood of discriminator being wrong. Same objective of fooling discriminator, but now higher gradient signal for bad samples => works much better! Standard in practice.  
        * Previously we used to do gradient __descent__ on generator:  
            <p>$$\min _{\theta_{g}} \mathbb{E}_{z \sim p(z)} \log \left(1-D_{\theta_{d}}\left(G_{\theta_{g}}(z)\right)\right)$$</p>  
            In practice, optimizing this generator objective does not work well.  
            ![img](/main_files/cs231n/gans/2.png){: width="40%"}  
        * Now we are doing gradient *__ascent__* on the generator:  
            <p>$$\max _{\theta_{g}} \mathbb{E}_{z \sim p(z)} \log \left(D_{\theta_{d}}\left(G_{\theta_{g}}(z)\right)\right)$$</p>  
            ![img](/main_files/cs231n/gans/1.png){: width="40%"}  
    * Jointly training two networks is challenging, can be unstable. Choosing objectives with better loss landscapes helps training, is an active area of research.  
    * __The representations have nice structure:__  
        * Average $$\boldsymbol{z}$$ vectors, do arithmetic:  
            ![img](https://cdn.mathpix.com/snip/images/iNB2WF_B-fA6AUeNPkqjaPCQRzCi1iEp9O7AmNsNGX0.original.fullsize.png){: width="40%"}  
            <button>Glasses</button>{: .showText value="show" onclick="showTextPopHide(event);"}
            ![img](https://cdn.mathpix.com/snip/images/It81yGTPcd7AtoxXUL6zmFRv475nN_ryCP8EL2SsPUs.original.fullsize.png){: width="60%" hidden=""}  
        * Interpolating between random points in latent space is possible  
    <br>

5. **Generative Adversarial Nets: Convolutional Architectures:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    \- __Discriminator__ is a standard __convolutional network__.  
    \- __Generator__ is an __upsampling network__ with __fractionally-strided convolutions__.  
    ![img](https://cdn.mathpix.com/snip/images/sPjqzCuh41DM_xCX9RYshgKPi1TDZsFpZxELmngXn9g.original.fullsize.png){: width="40%"}  

    __Architecture guidelines for stable Deep Convolutional GANs:__{: style="color: red"}  
    {: #lst-p}
    * Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).
    * Use batchnorm in both the generator and the discriminator.
    * Remove fully connected hidden layers for deeper architectures.
    * Use ReLU activation in generator for all layers except for the output, which uses Tanh.
    * Use LeakyReLU activation in the discriminator for all layers.  
    <br>

6. **Pros, Cons and Research:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents46}  
    * __Pros__:  
        * Beautiful, state-of-the-art samples!  
    * __Cons__:  
        * Trickier / more unstable to train
        * Can‚Äôt solve inference queries such as $$p(x), p(z\vert x)$$  
    * __Active areas of research:__  
        * Better loss functions, more stable training (Wasserstein GAN, LSGAN, many others)
        * Conditional GANs, GANs for all kinds of applications


__Notes:__{: style="color: red"}  
{: #lst-p}
* **Generative Adversarial Network (GAN).** This has been one of the most popular models (research-wise) in recent years ([Goodfellow et al., 2014](https://arxiv.org/abs/1406.2661)). This OpenAI [blogpost](https://blog.openai.com/generative-models/) gives a good overview of a few architectures based on the GAN (although only from OpenAI). Other interesting models include pix2pix ([Isola et al., 2017](https://arxiv.org/abs/1611.07004)), CycleGAN ([Zhu et al., 2017](https://arxiv.org/abs/1703.10593)) and WGAN ([Arjovsky et al., 2017](https://arxiv.org/abs/1701.07875)). The first two deal with image-to-image translation (eg. photograph to Monet/Van Gogh or summer photo to winter photo), while the last work focuses on using Wasserstein distance as a metric for stabilizing the GAN (since GANs are known to be unstable and difficult to train).  
<br>

***
***

TITLE: CNNs <br /> Convolutional Neural Networks
LINK: research/dl/cv/3.md


## Introduction
{: #content1}

1. **CNNs:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11} 
    :   In machine learning, a convolutional neural network (CNN, or ConvNet) is a class of deep, feed-forward artificial neural networks that has successfully been applied to analyzing visual imagery.

2. **The Big Idea:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12} 
    :   CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing.

3. **Inspiration Model:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13} 
    :   Convolutional networks were inspired by biological processes in which the connectivity pattern between neurons is inspired by the organization of the animal visual cortex.  
    Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.

4. **Design:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   A CNN consists of an input and an output layer, as well as multiple hidden layers.  
        The hidden layers of a CNN typically consist of convolutional layers, pooling layers, fully connected layers and normalization layers.

***

## Architecture and Design
{: #content2}


1. **Volumes of Neurons:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21} 
    :   Unlike neurons in traditional Feed-Forward networks, the layers of a ConvNet have neurons arranged in 3-dimensions: **width, height, depth**.  
    > Note: __Depth__ here refers to the third dimension of an activation volume, not to the depth of a full Neural Network, which can refer to the total number of layers in a network.  

2. **Connectivity:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22} 
    :   The neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner.

3. **Functionality:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23} 
    :   A ConvNet is made up of Layers. 
    Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.

    
    ![img](/main_files/dl/cnn/1.png){: width="100%"}
    
4. **Layers:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24} 
    :   We use three main types of layers to build ConvNet architectures: 
    :   * Convolutional Layer  
        * Pooling Layer  
        * Fully-Connected Layer

41. **Process:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents241} 
    :   ConvNets transform the original image layer by layer from the original pixel values to the final class scores. 

5. **Example Architecture (CIFAR-10):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25} 
    :   Model: [INPUT - CONV - RELU - POOL - FC]
    :   * **INPUT:** [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.   
        * **CONV-Layer** will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume.    
        This may result in volume such as [32x32x12] if we decided to use 12 filters.  
        * **RELU-Layer:**  will apply an elementwise activation function, thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).  
        * **POOL-Layer:** will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].  
        * **Fully-Connected:** will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10.  
        As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.

6. **Fixed Functions VS Hyper-Parameters:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26} 
    :   Some layers contain parameters and other don‚Äôt.
    :   * **CONV/FC layers** perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons).
    :   * **RELU/POOL** layers will implement a fixed function. 
    :   > The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image.  

7. **[Summary](http://cs231n.github.io/convolutional-networks/):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27} 
    * A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores)  
    * There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)  
    * Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function  
    * Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don‚Äôt)  
    * Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn‚Äôt)  
> [Click this for Credits](http://cs231n.github.io/convolutional-networks/)  

    ![img](/main_files/dl/cnn/2.png){: width="100%"}


***

## Convolutional Layers
{: #content3}

1. **Convolutions:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   A Convolution is a mathematical operation on two functions (f and g) to produce a third function, that is typically viewed as a modified version of one of the original functions, giving the integral of the pointwise multiplication of the two functions as a function of the amount that one of the original functions is translated.
    :   The convolution of the __continous__ functions f and g:  
    :   $${\displaystyle {\begin{aligned}(f*g)(t)&\,{\stackrel {\mathrm {def} }{=}}\ \int _{-\infty }^{\infty }f(\tau )g(t-\tau )\,d\tau \\&=\int _{-\infty }^{\infty }f(t-\tau )g(\tau )\,d\tau .\end{aligned}}}$$
    :   The convolution of the __discreet__ functions f and g: 
    :   $${\displaystyle {\begin{aligned}(f*g)[n]&=\sum _{m=-\infty }^{\infty }f[m]g[n-m]\\&=\sum _{m=-\infty }^{\infty }f[n-m]g[m].\end{aligned}}} (commutativity)$$

2. **Cross-Correlation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   Cross-Correlation is a measure of similarity of two series as a function of the displacement of one relative to the other.
    :   The __continuous__ cross-correlation on continous functions f and g:  
    :   $$(f\star g)(\tau )\ {\stackrel {\mathrm {def} }{=}}\int _{-\infty }^{\infty }f^{*}(t)\ g(t+\tau )\,dt,$$
    :   The __discrete__ cross-correlation on discreet functions f and g:  
    :   $$(f\star g)[n]\ {\stackrel {\mathrm {def} }{=}}\sum _{m=-\infty }^{\infty }f^{*}[m]\ g[m+n].$$

3. **Convolutions and Cross-Correlation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    :   * Convolution is similar to cross-correlation.  
        * _For discrete real valued signals_, they differ only in a time reversal in one of the signals.  
        * _For continuous signals_, the cross-correlation operator is the **adjoint operator** of the convolution operator.

4. **CNNs, Convolutions, and Cross-Correlation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    :   The term Convolution in the name "Convolution Neural Network" is unfortunately a __misnomer__.  
        CNNs actually __use Cross-Correlation__ instead as their similarity operator.  
        The term 'convolution' has stuck in the name by convention.

5. **The Mathematics:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35} 
    :   * The CONV layer‚Äôs __parameters__ consist of __a set of learnable filters__.  
            * Every filter is small spatially (along width and height), but extends through the full depth of the input volume.  
            >  For example, a typical filter on a first layer of a ConvNet might have size 5x5x3 (i.e. 5 pixels width and height, and 3 because images have depth 3, the color channels).  
        * In the __forward pass__, we slide (convolve) each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position.  
            * As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position.  
            > Intuitively, the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the first layer, or eventually entire honeycomb or wheel-like patterns on higher layers of the network. 
            * Now, we will have an entire set of filters in each CONV layer (e.g. 12 filters), and each of them will produce a separate 2-dimensional activation map.   
        * We will __stack__ these activation maps along the depth dimension and produce the output volume.  
    <p style="color: red">As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input. </p>    
    
6. **The Brain Perspective:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26} 
    :   Every entry in the 3D output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially.  

7. **Local Connectivity:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27} 
    :   * Convolutional networks exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers: 
            * Each neuron is connected to only a small region of the input volume.
        * The __Receptive Field__ of the neuron defines the extent of this connectivity as a hyperparameter.  
        >  For example, suppose the input volume has size $$[32x32x3]$$ and the receptive field (or the filter size) is $$5x5$$, then each neuron in the Conv Layer will have weights to a $$[5x5x3]$$ region in the input volume, for a total of $$5*5*3 = 75$$ weights (and $$+1$$ bias parameter).  
    <p style="color: red">Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.</p>

8. **Spatial Arrangement:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28} 
    :   There are __three__ hyperparameters control the size of the output volume:  
    :       1. __The Depth__ of the output volume is a hyperparameter that corresponds to the number of filters we would like to use (each learning to look for something different in the input).  
            2. __The Stride__ controls how depth columns around the spatial dimensions (width and height) are allocated.  
                > e.g. When the stride is 1 then we move the filters one pixel at a time.  

                > The __Smaller__ the stride, the __more overlapping regions__ exist and the __bigger the volume__.  
                > The __bigger__ the stride, the __less overlapping regions__ exist and the __smaller the volume__.  
            3. The __Padding__ is a hyperparameter whereby we pad the input the input volume with zeros around the border.  
                > This allows to _control the spatial size_ of _the output_ volumes.  


9. **The Spatial Size of the Output Volume:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents29} 
    :   We compute the spatial size of the output volume as a function of:  
    :       * **$$W$$**: The input volume size.  
            * **$$F$$**: $$\:\:$$The receptive field size of the Conv Layer neurons.  
            * **$$S$$**: The stride with which they are applied.  
            * **$$P$$**: The amount of zero padding used on the border.  
    :   Thus, the __Total Size of the Output__:  
    :   $$\dfrac{W‚àíF+2P}{S} + 1$$  
    :   * __Potential Issue__: If this number is not an integer, then the strides are set incorrectly and the neurons cannot be tiled to fit across the input volume in a symmetric way.  
    :   * __Fix__: In general, setting zero padding to be $${\displaystyle P = \dfrac{K-1}{2}}$$ when the stride is $${\displaystyle S = 1}$$ ensures that the input volume and output volume will have the same size spatially.  


0. **The Convolution Layer:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents30} 
    :   

    ![img](/main_files/dl/cnn/3.png){: width="100%"}

***

## Layers
{: #content3}

1. **Convolution Layer:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   One image becomes a stack of filtered images.








## Distinguishing features
{: #content4}


2. **Image Features:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents92}
    :   are certain quantities that are calculated from the image to _better describe the information in the image_, and to _reduce the size of the input vectors_. 
    :   * Examples:  
            * __Color Histogram__: Compute a (bucked-based) vector of colors with their respective amounts in the image.  
            * __Histogram of Oriented Gradients (HOG)__: we count the occurrences of gradient orientation in localized portions of the image.   
            * __Bag of Words__: a _bag of visual words_ is a vector of occurrence counts of a vocabulary of local image features.  
                > The __visual words__ can be extracted using a clustering algorithm; K-Means.  








***
***

TITLE: Object Detection <br /> with Deep Learning
LINK: research/dl/cv/obj_detec_11_3.md


## Object Detection
{: #content1}

1. **Object Detection:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   __Object Detection__ is the process of finding multiple instances of real-world objects such as faces, vehicles, and animals in images.  
    :   ![img](/main_files/cs231n/11_3/1.png){: width="28%"}  


2. **The Structure:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   * __Input__: Image  
        * __Output__: A pair of (box-co-ords, class) of all the objects in a fixed number of classes that appear in the image.  

3. **Properties:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   In the problem of object detection, we normally do not know the number of objects that we need to detect.  
        This leads to a problem when trying to model the problem as a __regression__ problem due to the undefined number of coordinates of boxes.  
    :   Thus, this problem is mainly modeled as a classification problem.  

4. **Applications:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   * Image Retrieval
        * Surveillance 
        * Face Detection
        * Face Recognition
        * Pedestrian Detection
        * Self-Driving Cars

***

## Approaches (The Pre-DeepLearning Era)
{: #content2}

1. **Semantic Texton Forests:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    :   This approach consists of ensembles of decision trees that act directly on image pixels.  
    :   Semantic Texton Forests (STFs) 
are randomized decision forests that use only simple pixel comparisons on local image patches, performing both an
implicit hierarchical clustering into semantic textons and an explicit local classification of the patch category.  
    :   STFs allow us to build powerful texton codebooks without computing expensive filter-banks or descriptors, and without performing costly k-means clustering and nearest-neighbor assignment.
    :   _Semantic Texton Forests for Image Categorization and Segmentation, Shawton et al. (2008)_

2. **Region Proposals:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    :   
    :   * __Algorithm__:  
            * Find "blobby" image regions that are likely to contain objects  
    :   These are relatively fast algorithms.   
    :   _Alexe et al, ‚ÄúMeasuring the objectness of image windows‚Äù, TPAMI 2012_  
        _Uijlings et al, ‚ÄúSelective Search for Object Recognition‚Äù, IJCV 2013_  
        _Cheng et al, ‚ÄúBING: Binarized normed gradients for objectness estimation at 300fps‚Äù, CVPR 2014_  
        _Zitnick and Dollar, ‚ÄúEdge boxes: Locating object proposals from edges‚Äù, ECCV 2014_

3. **Conditional Random Fields:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    :   CRFs provide a probabilistic framework for labeling and segmenting structured data.  
    :   They try to model the relationship between pixels, e.g.:
        1. nearby pixels more likely to have same label
        2. pixels with similar color more likely to have same label
        3. the pixels above the pixels "chair" more likely to be "person" instead of "plane"
        4. refine results by iterations
    :   _W. Wu, A. Y. C. Chen, L. Zhao and J. J. Corso (2014): "Brain Tumor detection and segmentation in a CRF framework with pixel-pairwise affinity and super pixel-level features"_
    :   _Plath et al. (2009): "Multi-class image segmentation using conditional random fields and global classification"_

4. **SuperPixel Segmentation:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    :   The concept of superpixels was first introduced by Xiaofeng Ren and Jitendra Malik in 2003.  
    :   __Superpixel__ is a group of connected pixels with similar colors or gray levels.  
        They produce an image patch which is better aligned with intensity edges than a rectangular patch.  
    :   __Superpixel segmentation__ is the idea of dividing an image into hundreds of non-overlapping superpixels.  
        Then, these can be fed into a segmentation algorithm, such as __Conditional Random Fields__ or __Graph Cuts__, for the purpose of segmentation.  
    :   _Efficient graph-based image segmentation, Felzenszwalb, P.F. and Huttenlocher, D.P. International Journal of Computer Vision, 2004_
    :   _Quick shift and kernel methods for mode seeking, Vedaldi, A. and Soatto, S. European Conference on Computer Vision, 2008_
    :   _Peer Neubert & Peter Protzel (2014). Compact Watershed and Preemptive_  

***

## Approaches (The Deep Learning Era)
{: #content3}

1. **The Sliding Window Approach:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   We utilize _classification_ for _detection_ purposes.  
    :   * __Algorithm__:    
            * We break up the input image into tiny "crops" of the input image.  
            * Use Classification+Localization to find the class of the center pixel of the crop, or classify it as background.  
                > Using the same machinery for classification+Localization.  
            * Slide the window and look at more "crops"
    :   Basically, we do classification+Localization on each crop of the image.
    :   * __DrawBacks:__  
            * Very Inefficient and Expensive:  
                We need to apply a CNN to a huge number of locations and scales.   
    :   _Sermant et. al 2013: "OverFeat"_

2. **Region Proposal Networks (R-CNNs):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   A framework for object detection, that utilizes Region Proposals (Regions of Interest (ROIs)), consisting of three separate architectures.   
    :   ![img](/main_files/cs231n/11_3/3.png){: width="82%"}
    :   * __Structure__:  
            * _Input_: Image vector  
            * _Output_: A vector of bounding boxes coordinates and a class prediction for each box     
    :   * __Strategy__:  
            Propose a number of "bounding boxes", then check if any of them, actually, corresponds to an object.  
            > The bounding boxes are created using __Selective Search__.
    :   * __Selective Search__: A method that looks at the image through windows of different sizes, and for each size tries to group together adjacent pixels by texture, color, or intensity to identify objects.    
            ![img](/main_files/cs231n/11_3/2.png){: width="100%"}
    :   * __Key Insights__:  
            1. One can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects  
            2. When labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost.        
    :   * __Algorithm__:   
            * Create _Region Proposals (Regions of Interest (ROIs))_ of bounding boxes  
            * Warp the regions to a standard square size to fit the "cnn classification models", due to the FCNs    
            * Pass the warped images to a modified version of _AlexNet_ to _extract image features_  
            * Pass the _image features_ to an SVM to _classify the image regions_ into a _class_ or _background_
            * Run the bounding box coordinates in a __Linear Regression__ model to "tighten" the bounding boxes
                * *__Linear Regression__*: 
                    * __Structure__:   
                        * _Input_: sub-regions of the image corresponding to objects  
                        * _Output_: New bounding box coordinates for the object in the sub-region.
    :   * __Issues__:   
            * Ad hoc training objectives:  
                * Fine-tune network with softmax classifier (log loss)
                * Train post-hoc linear SVMs (hinge loss)
                * Train post-hoc bounding-box regressions (least squares)
            * Training is slow (84h), takes a lot of disk space
            * Inference (detection) is slow
                * 47s / image with VGG16 [Simonyan & Zisserman. ICLR15]
                * Fixed by SPP-net [He et al. ECCV14]
    :   R-CNN is slow because it performs a ConvNet forward pass for each region proposal, without sharing computation.  
    :   _R. Girshick, J. Donahue, T. Darrell, J. Malik. (2014): "Rich feature hierarchies for accurate object detection and semantic segmentation"_  

3. **Fast R-CNNs:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    :   A single, end-to-end, architecture for object detection based on R-CNNs, that vastly improves on its speed and accuracy by utilizing shared computations of features.     
    :   ![img](/main_files/cs231n/11_3/4.png){: width="82%"}
    :   * __Structure__:  
            * _Input_: Image vector  
            * _Output_: A vector of bounding boxes coordinates and a class prediction for each box     
    :   * __Key Insights__:  
            1. Instead of running the ConvNet on __each region proposal separately__, we run the ConvNet on the __entire image__.  
            2. Instead of taking __crops of the original image__, we __project the regions of interest__ onto the __ConvNet Feature Map__, corresponding to each RoI, and then use the __projected regions in the feature map__ _for classification_.  
                This allows us to __reuse__ a lot of the expensive computation of the features.  
            3. Jointly train the CNN, classifier, and bounding box regressor in a single model. Where earlier we had different models to extract image features (CNN), classify (SVM), and tighten bounding boxes (regressor), Fast R-CNN instead used a single network to compute all three.
    :   * __Algorithm__:   
            * Create _Region Proposals (Regions of Interest (ROIs))_ of bounding boxes       
            * Pass the entire image to a modified version of _AlexNet_ to _extract image features_ by creating an _image feature map_ for the __entire image__.  
            * Project each _RoI_ to the _feature map_ and crop each respective projected region
            * Apply __RoI Pooling__ to the _regions extracted from the feature map_ to a standard square size to fit the "cnn classification models", due to the FCNs
            * Pass the _image features_ to an SVM to _classify the image regions_ into a _class_ or _background_
            * Run the bounding box coordinates in a __Linear Regression__ model to "tighten" the bounding boxes
                * *__Linear Regression__*: 
                    * __Structure__:   
                        * _Input_: sub-regions of the image corresponding to objects  
                        * _Output_: New bounding box coordinates for the object in the sub-region.  
    :   * __RoI Pooling__: is a pooling technique aimed to perform max pooling on inputs of nonuniform sizes to obtain fixed-size feature maps (e.g. 7√ó7).  
            * *__Structure__*:   
                * _Input_: A fixed-size feature map obtained from a deep convolutional network with several convolutions and max pooling layers.  
                * _Output_: An N x 5 matrix of representing a list of regions of interest, where N is a number of RoIs. The first column represents the image index and the remaining four are the coordinates of the top left and bottom right corners of the region.  
            For every region of interest from the input list, it takes a section of the input feature map that corresponds to it and scales it to some pre-defined size.  
            * *__Scaling__*:    
                1. Divide the RoI into equal-sized sections (the number of which is the same as the dimension of the output)  
                2. Find the largest value in each section  
                3. Copy these max values to the output buffer  
            The __dimension of the output__ is determined solely by _the number of sections we divide the proposal_ into.   
            ![img](/main_files/cs231n/11_3/5.png){: width="90%"}
    :   * __The Bottleneck__:   
            It appears that Fast R-CNNs are capable of object detection at test time in:  
            * _Including RoIs_: 2.3s
            * _Excluding RoIs_: 0.3s  
            Thus, __the bottleneck__ for the speed seems to be the method of creating the RoIs, _Selective Search_
    :   _[1] Girshick, Ross (2015). "Fast R-CNN"_  

4. **Faster R-CNNs:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    :   A single, end-to-end, architecture for object detection based on Fast R-CNNs, that tackles the bottleneck in speed (i.e. computing RoIs) by introducing __Region Proposal Networks (RPNs)__ to make a CNN predict proposals from features.     
        Region Proposal Networks share full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals.   
        The network is jointly trained with 4 losses:
            1. RPN classify object / not object
            2. RPN regress box coordinates
            3. Final classification score (object classes)
            4. Final box coordinates
    :   ![img](/main_files/cs231n/11_3/6.png){: width="60%"}
    :   * __Region Proposal Network (RPN)__: is an, end-to-end, fully convolutional network that simultaneously predicts object bounds and objectness scores at each position.  
        RPNs work by passing a sliding window over the CNN feature map and at each window, outputting k potential bounding boxes and scores for how good each of those boxes is expected to be. 
    :   * __Structure__:  
            * _Input_: Image vector  
            * _Output_: A vector of bounding boxes coordinates and a class prediction for each box     
    :   * __Key Insights__:  
            1. Replace __Selective Search__ for finding RoIs by a __Region Proposal Network__ that shares the features, and thus reduces the computation and time, of the pipeline.  
    :   * __Algorithm__:   
            * Pass the entire image to a modified version of _AlexNet_ to _extract image features_ by creating an _image feature map_ for the __entire image__.  
            * Pass the __CNN Feature Map__ to the RPN to generate bounding boxes and a score for each bounding box 
            * Pass each such bounding box that is likely to be an object into Fast R-CNN to generate a classification and tightened bounding boxes. 
    :   _Ren et al, (2015). ‚ÄúFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks‚Äù_  

***

## Methods, Approaches and Algorithms in Training DL Models
{: #content4}


***
***

TITLE: Image Classification and Localization <br /> with Deep Learning
LINK: research/dl/cv/img_clsfc+loc_11_2.md


## Image Localization
{: #content1}

1. **Image Localization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   __Localization__ is the task of finding a single object in an image.  
    :   ![img](/main_files/cs231n/11_2/1.png){: width="25%"}  

2. **Image Classification+Localization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   __Localization__ can be combined with classification to, not only find the location of an object but, also, to classify it into one of different classes. 
    :   ![img](/main_files/cs231n/11_2/2.png){: width="35%"}  

3. **Structure:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   * __Input__: Image  
        * __Output__: A vector of 4 coordinates of the bounding box.  

4. **Applications:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   * Smart Cropping
        * Regular Object Extraction (as a pre-processing step)  
        * Human Pose Estimation: Represent pose as a set of 14 joint positions 

***

## Approaches
{: #content2}

1. **Localization as a Regression Problem:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    :   Since we are concerned with returning real-valued numbers (the bounding box coordinates), we use a method that is suitable for the task, __Regression__.   
    :   ![img](/main_files/cs231n/11_2/3.png){: width="80%"}
    :   * __Algorithm__:    
            * Use any classification architecture  
            * Attach two _Fully Connected Layers_, one for __Classification__ and one for __Localization__  
            * Backpropagate through the whole network using _cross-entropy loss_ and _L2 loss_ respectively.  
    :   * __Evaluation Metric:__ Intersection over Union.  

***

## Training Methods, Approaches and Algorithms 
{: #content4}

### Updated Soon!

***
***

TITLE: CNN Architectures
LINK: research/dl/cv/cnn_arcts.md


*__LeNet-5__*: _(LeCun et al., 1998)_  
![img](/main_files/cs231n/9/1.png){: width="70%"}  
* __Architecture__: [CONV-POOL-CONV-POOL-FC-FC]  
* __Parameters__: 
    * CONV: F=5, S=1
    * POOL: F=2, S=2

## AlexNet _(Krizhevsky et al. 2012)_
{: #content1}

![img](/main_files/cs231n/9/2.png){: width="70%"}  

1. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   [CONV1-MAX-POOL1-NORM1-CONV2-MAX-POOL2-NORM2-CONV3-CONV4-CONV5-Max-POOL3-FC6-FC7-FC8]  

2. **Parameters:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   * __First Layer (CONV1)__: 
            * __F__: second     
    :   ![img](/main_files/cs231n/9/3.png){: width="50%"}  

3. **Key Insights:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   * first use of ReLU
        * used Norm layers (not common anymore)
        * heavy data augmentation
        * dropout 0.5
        * batch size 128
        * SGD Momentum 0.9
        * Learning rate 1e-2, reduced by 10
        manually when val accuracy plateaus
        * L2 weight decay 5e-4
        * 7 CNN ensemble: 18.2% -> 15.4%

4. **Results:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   ![img](/main_files/cs231n/9/4.png){: width="70%"}  

5. **ZFNet _(Zeiler and Fergus, 2013)_:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    :   ![img](/main_files/cs231n/9/5.png){: width="70%"}
    :   ![img](/main_files/cs231n/9/6.png){: width="70%"}

***

## VGGNet _(Simonyan and Zisserman, 2014)_
{: #content2}

![img](/main_files/cs231n/9/7.png){: width="60%"}  

2. **Parameters:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    :   * __CONV__: F=1, S=1, P=1
        * __POOL__: F=2, S=2  
        > For all layers   
    :   ![img](/main_files/cs231n/9/8.png){: width="80%"}  
    :   > Notice:  
            __Parameters__ are mostly in the *__FC Layers__*  
            __Memory__ mostly in the *__CONV Layers__*

3. **Key Insights:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    :   * Smaller Filters
        * Deeper Networks  
        * Similar Training as AlexNet
        * No LRN Layer
        * Both __VGG16__ and __VGG19__
        * Uses Ensembles for Best Results

4. **Smaller Filters Justification:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    :   * A Stack of three 3x3 conv (stride 1) layers has same effective receptive field as one 7x7 conv layer  
        * However, now, we have deeper nets and more non-linearities
        * Also, fewer parameters:  
            3 * (3^2*C^2 ) vs. 7^2*C^2 for C channels per layer

5. **Properties:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    :   * __FC7__ Features *__generalize__* well to other tasks

6. **VGG16 vs VGG19:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    :   __VGG19__ is only slightly better and uses _more memory_ 

7. **Results:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  
    :   ILSVRC‚Äô14 2nd in classification, 1st in localization  
    :   ![img](/main_files/cs231n/9/9.png){: width="70%"}  

***

## GoogLeNet _(Szegedy et al., 2014)_
{: #content3}

![img](/main_files/cs231n/9/13.png){: width="80%"}  

1. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   ![img](/main_files/cs231n/9/14.png){: width="85%"}  

2. **Parameters:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   Parameters as specified in the Architecture and the Inception Modules

3. **Key Insights:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    :   * (Even) Deeper Networks
        * Computationally Efficient
        * 22 layers
        * Efficient ‚ÄúInception‚Äù module
        * No FC layers
        * Only 5 million parameters: 12x less than AlexNet

4. **Inception Module:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    :   * __Idea__: design a good local network topology (network within a network) and then stack these modules on top of each other   
    :   ![img](/main_files/cs231n/9/10.png){: width="50%"}
    :   * __Architecture__:  
            * Apply _parallel filter operations_ on the input from previous layer:  
                * Multiple receptive field sizes for convolution (1x1, 3x3, 5x5)   
                * Pooling operation (3x3)  
            * Concatenate all filter outputs together depth-wise  
    :   * __Issue__: *__Computational Complexity__* is very high  
        ![img](/main_files/cs231n/9/12.png){: width="70%"}  
        * __Solution__: use *__BottleNeck Layers__* that use 1x1 convolutions to reduce feature depth   
            > preserves spatial dimensions, reduces depth!
    :   ![img](/main_files/cs231n/9/11.png){: width="50%"}  

7. **Results:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents37}  
    :   ILSVRC‚Äô14 classification winner 
    :   ![img](/main_files/cs231n/9/9.png){: width="70%"}

***

## ResNet _(He et al., 2015)_
{: #content4}

1. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    :   ![img](/main_files/cs231n/9/17.png){: width="85%"}

3. **Key Insights:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}  
    :   * Very Deep Network: 152-layers
        * Uses Residual Connections
        * Deep Networks have very bad performance __NOT__ because of overfitting but because of a lack of adequate optimization  

4. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents44}  
    :   ![img](/main_files/cs231n/9/15.png){: width="70%"}  
    :   * __Observation__: Deeper Networks perform badly on the test error *__but also on the training error__*  
        * __Assumption__: Deep Layers should be able to perform at least as well as the shallower models   
        * __Hypothesis__: the problem is an optimization problem, deeper models are harder to optimize  
        * __Solution (work-around)__: Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping   

5. **Residuals:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents45}  
    :   ![img](/main_files/cs231n/9/16.png){: width="70%"}

6. **BottleNecks:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents46}  
    :   ![img](/main_files/cs231n/9/18.png){: width="70%"}

7. **Training:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents47}  
:      * Batch Normalization after every CONV layer
        * Xavier/2 initialization from He et al.
        * SGD + Momentum (0.9)
        * Learning rate: 0.1, divided by 10 when validation error plateaus
        * Mini-batch size 256
        * Weight decay of 1e-5
        * No dropout used

8. **Results:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents48}  
    :   * ILSVRC‚Äô15 classification winner (3.57% top 5 error)  
        * Swept all classification and detection competitions in ILSVRC‚Äô15 and COCO‚Äô15  
        * Able to train very deep networks without degrading (152 layers on ImageNet, 1202 on Cifar)
        * Deeper networks now achieve lowing training error as expected
    :   ![img](/main_files/cs231n/9/19.png){: width="60%"}
    :   ![img](/main_files/cs231n/9/20.png){: width="64%"}

***

## Comparisons
{: #content5}

1. **Complexity:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents51}
    :   ![img](/main_files/cs231n/9/21.png){: width="70%"}
    :   ![img](/main_files/cs231n/9/22.png){: width="70%"}

2. **Forward-Pass Time and Power Consumption:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents52}  
    :   ![img](/main_files/cs231n/9/23.png){: width="70%"}
    :   ![img](/main_files/cs231n/9/24.png){: width="70%"}


*** 

## Interesting Architectures
{: #content6}

1. **Network in Network (NiN) _[Lin et al. 2014]_:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents61}  
    :   * Mlpconv layer with ‚Äúmicronetwork‚Äù within each conv layer to compute more abstract features for local patches
        * Micronetwork uses multilayer perceptron (FC, i.e. 1x1 conv layers)
        * Precursor to GoogLeNet and ResNet ‚Äúbottleneck‚Äù layers
        * Philosophical inspiration for GoogLeNet  
    :   ![img](/main_files/cs231n/9/25.png){: width="70%"}

2. **Identity Mappings in Deep Residual Networks (Improved ResNets) _[He et al. 2016]_:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents62}  
    :   * Improved ResNet block design from creators of ResNet
        * Creates a more direct path for propagating information throughout network (moves activation to residual mapping pathway)
        * Gives better performance
    :   ![img](/main_files/cs231n/9/26.png){: width="40%"}

3. **Wide Residual Networks (Improved ResNets) _[Zagoruyko et al. 2016]_:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents63}  
    :   * Argues that residuals are the important factor, not depth
        * User wider residual blocks (F x k filters instead of F filters in each layer)
        * 50-layer wide ResNet outperforms 152-layer original ResNet
        * Increasing width instead of depth more computationally efficient (parallelizable)
    :   ![img](/main_files/cs231n/9/27.png){: width="50%"}

4. **Aggregated Residual Transformations for Deep Neural Networks (ResNeXt) _[Xie et al. 2016]_:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents64}  
    :   * Also from creators of ResNet
        * Increases width of residual block through multiple parallel pathways (‚Äúcardinality‚Äù)
        * Parallel pathways similar in spirit to Inception module
    :   ![img](/main_files/cs231n/9/28.png){: width="60%"}

5. **Deep Networks with Stochastic Depth (Improved ResNets) _[Huang et al. 2016]_:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents65}  
    :   * Motivation: reduce vanishing gradients and training time through short networks during training
        * Randomly drop a subset of layers during each training pass
        * Bypass with identity function
        * Use full deep network at test time
    :   ![img](/main_files/cs231n/9/29.png){: width="30%"}

#### Beyond ResNets

6. **FractalNet: Ultra-Deep Neural Networks without Residuals _[Larsson et al. 2017]_:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents66}  
    :   * Argues that key is transitioning effectively from shallow to deep and residual representations are not necessary
        * Fractal architecture with both shallow and deep paths to output
        * Trained with dropping out sub-paths 
        * Full network at test time
    :   ![img](/main_files/cs231n/9/30.png){: width="70%"}

7. **Densely Connected Convolutional Networks _[Huang et al. 2017]_:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents67}  
    :   * Dense blocks where each layer is connected to every other layer in feedforward fashion
        * Alleviates vanishing gradient, strengthens feature propagation, encourages feature reuse
    ;   ![img](/main_files/cs231n/9/31.png){: width="65%"}

8. **SqueezeNet (Efficient NetWork) _[Iandola et al. 2017]_:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents68}  
    :   *  AlexNet-level Accuracy With 50x Fewer Parameters and <0.5Mb Model Size
        * Fire modules consisting of a ‚Äòsqueeze‚Äô layer with 1x1 filters feeding an ‚Äòexpand‚Äô layer with 1x1 and 3x3 filters
        * Can compress to 510x smaller than AlexNet (0.5Mb)
    :   ![img](/main_files/cs231n/9/32.png){: width="60%"}


***
***

TITLE: Generative Compression
LINK: research/dl/cv/compression.md



[Feature Extraction (Notes - Docs)](https://docs.google.com/document/d/12yb9bhZfr84e6tPJwwJrKXpNKVEhhbmoMgOGl_gGbBY/edit)  
[Audio Compression](https://docs.google.com/document/d/1TUHWxU3TPR1mRCDF1kUM9xgNmvVHe5f9KxvBb4sPu_Q/edit)  


## Introduction
{: #content1}
 

5. **ML-based Compression:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    :   The main idea behind ML-based compression is that structure is __automatically discovered__ instead of __manually engineered__.  
    :   * __Examples__: 
            * *__DjVu__*: employs segmentation and K-means clustering to separate foreground from background and analyze the documents contents.     

***

## WaveOne 
{: #content2}

[Further Reading](https://arxiv.org/pdf/1705.05823.pdf)

WaveOne is a machine learning-based approach to lossy image compression.  

1. **Main Idea:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    :   An ML-based approach to compression that utilizes the older techniques for quantization but with an encoder-decoder model that depends on adversarial training for a higher quality reconstruction.   

2. **Model:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    :   The model includes three main steps that are layered together in one pipeline:  
        * __Feature Extraction__: an approach that aims to recognize the different types of structures in an image.  
            * __Structures__: 
                * Across input channels
                * Within individual scales
                * Across Scales
            * __Methods__: 
                * *__Pyramidal Decomposition__*: for analyzing individual scales 
                * *__Interscale Alignment Procedure__*:  for exploiting structure shared across scales   
        * __Code Computation and Regularization__: a module responsible for further compressing the extracted features by *quantizing the features and encoding them via two methods.  
            * __Methods__: 
                * *__Adaptive Arithmetic Coding Scheme__*: applied on the features binary expansions
                * *__Adaptive Codelength Regularization__*:  to penalize the entropy of the features to achieve better compression   
        * __Adversarial Training (Discriminator Loss)__: a module responsible for enforcing realistic reconstructions.
            * __Methods__: 
                * *__Adaptive Arithmetic Coding Scheme__*: applied on the features binary expansions
                * *__Adaptive Codelength Regularization__*:  to penalize the entropy of the features to achieve better compression   


3. **Feature Extraction:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    :   * __Pyramidal Decomposition__:   
            Inspired by _the use of wavelets for multiresolution analysis, in which an input is analyzed recursively via feature extraction and downsampling operators_, the pyramidal decomposition encoder generalizes the wavelet decomposition idea to _learn optimal, nonlinear extractors individually for each scale_.  
            For each input $$\mathbf{x}$$ to the model, and a total of $$M$$ scales, denote the input to scale $$m$$ by $$\mathbf{x}_m$$.   
            * *__Algorithm__*:  
               * Set input to first scale $$\mathbf{x}_1 = \mathbf{x}$$
               * For each scale $$m$$:  
                    * Extract coefficients $$\mathbf{c}_m = \mathbf{f}_m(\mathbf{x}_m) \in \mathbb{R}^{C_m \times H_m \times W_m}$$ via some parametrized function $$\mathbf{f}_m(\dot)$$ for output channels $$C_m$$, height $$H_m$$ and width $$W_m$$  
                    * Compute the input to the next scale as $$\mathbf{x}_{m+1} = \mathbf{D}_m(\mathbf{x}_m)$$, where $$\mathbf{D}_m(\dot)$$ is some _downsampling operator_ (either fixed or learned)

            Typically, $$M$$ is chosen to be $$ = 6$$ scales.  
            The __feature extractors for the individual scales__ are composed of *__a sequence of convolutions with kernels $$3 \times 3$$ or $$1 \times 1$$ and ReLUs with a leak of $$0.2$$__*.  
            All _downsamplers_ are learned as $$4 \times 4$$ convolutions with a stride of $$2$$. 
    :   ![img](/main_files/cv/compression/1.png){: width="80%"}  
    :   * __Interscale Alignment__:   
            Designed to leverage information shared across different scales ‚Äî a benefit not offered by the classic wavelet analysis.  
            * *__Structure__*:  
                * __Input__: the set of coefficients extracted from the different scales $$\{\mathbf{c}_m\}_{m=1}^M \subset \mathbb{R}^{C_m \times H_m \times W_m}$$     
                * __Output__: a tensor $$\mathbf{y} \in \mathbb{R}^{C \times H \times W}$$
            * *__Algorithm__*:  
                * Map each input tensor $$\mathbf{c}_m$$ to the target dimensionality via some parametrized function $$\mathbf{g}_m(¬∑)$$:  this involves ensuring that this function spatially resamples $$\mathbf{c}_m$$ to the appropriate output map size $$H \times W$$, and ouputs the appropriate number of channels $$C$$  
                * Sum $$\mathbf{g}_m(\mathbf{c}_m) = 1, \ldots, M$$, and apply another parameterized non-linear transformation $$\mathbf{g}(¬∑)$$ for _joint processing_  
        $$\mathbf{g}_m(¬∑)$$ is chosen as a __convolution__ or a __deconvolution__ with an appropriate stride to produce the target spatial map size $$H \times W$$.  
        $$\mathbf{g}(¬∑)$$ is choses as a __sequence of $$3 \times 3$$ convolutions__.  


4. **Code Computation and Regularization:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    :   Given the output tensor $$\mathbf{y} \in \mathbb{R}^{C \times H \times W}$$ of the _feature extraction step_ (namely alignment), we proceed to __quantize and encode__ it.  
    :   * __Quantization__: the tensor $$\mathbf{y}$$ is quantized to bit precision $$B$$:    
            ![img](/main_files/cv/compression/2.png){: width="50%"}  
            Given a desired precision of $$B$$ bits, we quantize the feature tensor into $$2^B$$ equal-sized bins as:  
            ![img](/main_files/cv/compression/3.png){: width="80%"}  
            For the special case $$B = 1$$, this reduces exactly to a binary quantization scheme.  
            In-practice $$B = 6$$ is chosen as a smoother quantization method.  
            * __Reason__:   
                Mapping the _continuous input values_ representing the image signal to a _smaller countable set_ to achieve a desired precision of $$B$$ bits
    :   * __Bitplane Decomposition__: we transform the _quantized tensor_ $$\mathbf{\hat{y}}$$ into a _binary tensor_ suitable for encoding via a lossless bitplane decomposition:  
            ![img](/main_files/cv/compression/4.png){: width="80%"}  
            $$\mathbf{\hat{y}}$$ is decomposed into _bitplanes_ by a transformation that maps each value $$\hat{y}_{chw}$$ into its _binary expansion_ of $$B$$ bits.  
            Hence, each of the $$C$$ spatial maps $$\mathbf{\hat{y}}_c \in \mathbb{R}^{H \times W}$$ of $$\mathbf{\hat{y}}$$ expands into $$B$$ _binary bitplanes_.  

            * __Reason__:   
                This decomposition enables the entropy coder to exploit structure in the distribution of the activations in $$\mathbf{y}$$ to achieve a compact representation.  
    :   * __Adaptive Arithmetic Encoding__: encodes $$\mathbf{b}$$ into its final _variable-length binary sequence_ $$\mathbf{s}$$ of length $$\mathcal{l}(\mathbf{s})$$:  
            ![img](/main_files/cv/compression/5.png){: width="65%"}  
            * The _binary tensor_ $$\mathbf{b}$$ that is produced by the bitplane decomposition contains significant structure (e.g. higher bitplanes are sparser, and spatially neighboring bits often have the same value).  
                This structure can be exploited by using Adaptive Arithmetic Encoding.  
            * __Method__:  
                * *__Encoding__*:     
                    Associate each bit location in the _binary tensor_ $$\mathbf{b}$$ with a _context_, which comprises a set of features indicative of the bit value.  
                    The _features_ are based on the _position of the bit_ and the _values of neighboring bits.  
                    To predict the value of each bit from its context features, we _train a classifier_ and use _its output probabilities_ to compress $$\mathbf{b}$$ via _arithmetic coding_.   
                * *__Decoding__*:    
                    At decoding time, we perform the _inverse operation_ to _decompress the code_.  
                    We interleave between:  
                    * Computing the context of a particular bit using the values of previously decoded bits  
                    * Using this context to retrieve the activation probability of the bit and decode it  
                    > This operation constrains the context of each bit to only include features composed of bits already decoded   
            * __Reason__:   
                We aim to leverage the structure in the data, specifically in the _binary tensor_ $$\mathbf{b}$$ produced by the _bitplane decomposition_ which has _low entropy_ 
    :   * __Adaptive Codelength Regularization__: modulates the distribution of the quantized representation $$\mathbf{\hat{y}}$$ to achieve a target expected bit count across inputs:  
            ![img](/main_files/cv/compression/6.png){: width="60%"}  
            * __Goal__: regulate the expected codelength $$\mathbb{E}_x[\mathcal{l}(\mathbf{s})]$$ to a target value $$\mathcal{l}_{\text{target}}$$.
            * __Method__:  
                We design a penalty that encourages a structure that the __AAC__ is able to encode.  
                Namely, we *__regularize__* the _quantized tensor_ $$\mathbf{\hat{y}}$$ with:  
                ![img](/main_files/cv/compression/7.png){: width="60%"}  
                for iteration $$t$$ and difference index set $$S = \{(0,1), (1,0), (1,1), (-1,1)\}$$.  
                > The __first term__ _penalizes the magnitude of each tensor element_  
                > The __Second Term__ _penalizes deviations between spatial neighbors_  

            * __Reason__:   
                The _Adaptive Codelength Regularization_ is designed to solve one problem; the non-variability of the latent space code, which is what controls (defines) the bitrate.  
                It, essentially, allows us to have latent-space codes with different lengths, depending on the complexity of the input, *by enabling better prediction by the __AAX__*  
            In practice, a __total-to-target ratio__ $$ = BCHW/\mathcal{l}_{\text{target}} = 4$$ works well.  

5. **Adversarial Train:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    :   * __GAN Architecture__:   
            * *__Generator__*: Encoder-Decoder Pipeline  
            * *__Discriminator__*: Classification ConvNet 
    :   * __Discriminator Design__:  




7. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  
    :   * __Problem__: Regression Problem
        * __Goal__: Learn a function $$\psi(x;\theta)$$ that is trained and used to regress to a pose vector.   
        * __Estimation__: $$\psi$$ is based on (learned through) Deep Neural Net
        * __Deep Neural Net__: is a Convolutional Neural Network; namely, __AlexNet__  
            * *__Input__*: image with pre-defined size $$ = \:$$ #-pixels $$\times 3$$-color channels  
                > $$(220 \times 220)$$ with a stride of $$4$$  
            * *__Output__*: target value of the regression$$ = 2k$$ joint coordinates  
    :   > Denote by $$\mathbf{C}$$ a convolutional layer, by $$\mathbf{LRN}$$ a local response normalization layer, $$\mathbf{P}$$ a pooling layer and by $$\mathbf{F}$$ a fully connected layer  
    :   > For $$\mathbf{C}$$ layers, the size is defined as width $$\times$$ height $$\times$$ depth, where the first two dimensions have a spatial meaning while the depth defines the number of filters.  
    :   * __Alex-Net__: 
            * *__Architecture__*:     $$\mathbf{C}(55 \times 55 \times 96) ‚àí \mathbf{LRN} ‚àí \mathbf{P} ‚àí \mathbf{C}(27 \times 27 \times 256) ‚àí \mathbf{LRN} ‚àí \mathbf{P} ‚àí \\\mathbf{C}(13 \times 13 \times 384) ‚àí \mathbf{C}(13 \times 13 \times 384) ‚àí \mathbf{C}(13 \times 13 \times 256) ‚àí \mathbf{P} ‚àí \mathbf{F}(4096) ‚àí \mathbf{F}(4096)$$   
            * *__Filters__*:  
                * $$\mathbf{C}_{1} = 11 \times 11$$,  
                * $$\mathbf{C}_{2} = 5 \times 5$$,  
                * $$\mathbf{C}_{3-5} = 3 \times 3$$.
            * *__Total Number of Parameters__* $$ = 40$$M   
            * *__Training Dataset__*:  
                Denote by $$D$$ the training set and $$D_N$$ the normalized training set:   
                $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$ $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $$  $$D_N = \{(N(x),N(\mathbf{y}))\vert (x,\mathbf{y}) \in D\}$$   
            * *__Loss__*: the Loss is modified; instead of a _classification loss_, we train a linear regression on top of the last network layer to predict a pose vector by minimizing $$L_2$$ distance between the prediction and the true pose vector,  
    :   $$\arg \min_\theta \sum_{(x,y) \in D_N} \sum_{i=1}^k \|\mathbf{y}_i - \psi_i(x;\theta)\|_2^2$$  
    :   * __Optimization__:  
            * *__BackPropagation__* in a distributed online implementation
            * *__Adaptive Gradient Updates__*
            * *__Learning Rate__* $$ = 0.0005 = 5\times 10^{-4}$$
            * *__Data Augmentation__*: randomly translated image crops, left/right flips
            * *__DropOut Regularization__* for the $$\mathbf{F}$$ layers $$ = 0.6$$

9. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents29}  
    :   * __Motivation__:   
            Although, the pose formulation of the DNN has the advantage that the joint estimation is based on the full image and thus relies on context, due its fixed input size of $$220 \times 220$$, the network has _limited capacity to look at detail_ - it _learns filters capturing pose properties at coarse scale_.  
            The _pose properties_ are necessary to _estimate rough pose_ but __insufficient__ to always _precisely localize the body joints_.  
            Increasing the input size is infeasible since it will increase the already large number of parameters.  
            Thus, a _cascade of pose regressors_ is used to achieve better precision.  
        * __Structure and Training__:   
            At the first stage: 
            * The cascade starts off by estimating an initial pose as outlined in the previous section.  
            At subsequent stages:  
            * Additional DNN regressors are trained to predict a displacement of the joint locations from previous stage to the true location.  
                > Thus, each subsequent stage can be thought of as a refinement of the currently predicted pose.   
            * Each subsequent stage uses the predicted joint locations to focus on the relevant parts of the image ‚Äì subimages are cropped around the predicted joint location from previous stage and the pose displacement regressor for this joint is applied on this sub-image.  
                > Thus, subsequent pose regressors see higher resolution images and thus learn features for finer scales which ultimately leads to higher precision  
        * __Method and Architecture__:  
            * The same network architecture is used for all stages of the cascade but learn different parameters.   
            * Start with a bounding box $$b^0$$: which either encloses the full image or is obtained by a person detector
            * Obtain an initial pose:  
                Stage 1: $$\mathbf{y}^1 \leftarrow N^{-1}(\psi(N(x;b^0);\theta_1);b^0)$$  
            * At stages $$s \geq 2$$, for all joints:
                * Regress first towards a refinement displacement $$\mathbf{y}_i^s - \mathbf{y}_i^{(s-1)}$$ by applying a regressor on the sub image defined by $$b_i^{(s-1)}$$ 
                * Estimate new joint boxes $$b_i^s$$:  
                Stage $$s$$: $$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{y}_i^s \leftarrow \mathbf{y}_i^{(2-1)} + N^{-1}(\psi(N(x;b^0);\theta_s);b)  \:\: (6)  \\
 \ \ \ \ \ \ \ \ \ \ \ \ \ \                  \:\:\:\: \text{for } b = b_i^(s-1) \\
  \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
                b_i^s \leftarrow (\mathbf{y}_i^s, \sigma diam(\mathbf{y}^s), \sigma diam(\mathbf{y}^s))) \:\: (7)$$  
                where we considered a joint bounding box $$b_i$$ capturing the sub-image around $$\mathbf{y}_i: b_i(\mathbf{y}; \sigma) = (\mathbf{y}_i, \sigma diam(\mathbf{y}), \sigma diam(\mathbf{y}))$$ having as center the i-th joint and as dimension the pose diameter scaled by $$\sigma$$, to refine a given joint location $$\mathbf{y}_i$$.    
            * Apply the cascade for a fixed number of stages $$ = S$$  
        * __Loss__: (at each stage $$s$$)   
    :  $$\theta_s = \arg \min_\theta \sum_{(x,\mathbf{y}_i) \in D_A^s} \|\mathbf{y}_i - \psi_i(x;\theta)\|_2^2 \:\:\:\:\: (8)$$


6. **Advantages:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    :   * The DNN is capable of capturing the full context of each body joint  
        * The approach is simpler to formulate than graphical-models methods - no need to explicitly design feature representations and detectors for parts or to explicitly design a model topology and interactions between joints.   
            > Instead a generic ConvNet learns these representations

8. **Notes:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}  
    :   * The use of a generic DNN architecture is motivated by its outstanding results on both classification and localization problems and translates well to pose estimation  
        * Such a model is a truly holistic one ‚Äî the final joint location estimate is based on a complex nonlinear transformation of the full image  
        * The use of a DNN obviates the need to design a domain specific pose model
        * Although the regression loss does not model explicit interactions between joints, such are implicitly captured by all of the 7 hidden layers ‚Äì all the internal features are shared by all joint regressors  

***

## Proposed Changes
{: #content3}

1. **Gated-Matrix Selection for Latent Space dimensionality estimation and Dynamic bit-rate modification:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   


2. **Conditional Generative-Adversarial Training with Random-Forests for Generalizable domain-compression:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   


3. **Adversarial Feature Learning for Induced Natural Representation and Artifact Removal:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    :   


***

__Papers:__  
* __Generative Compression__
    * WaveOne: https://arxiv.org/pdf/1705.05823.pdf
    * MIT Generative Compression: https://arxiv.org/pdf/1703.01467.pdf

* __Current Standards__
    * An overview of the JPEG2000 still image compression standard
        * Paper:http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.128.9040&rep=rep1&type=pdf
        * Notes: Pretty in-depth, by Eastman Kodak Company, from early 2000s (maybe improvements since then?)

1. WaveOne
    Paper: https://arxiv.org/pdf/1705.05823.pdf
    Site: http://www.wave.one/
    Post: http://www.wave.one/icml2017
2. Generative Compression -- Santurker, Budden, Shavit (MIT)
    Paper: https://arxiv.org/pdf/1703.01467.pdf
3. Toward Conceptual Compression -- DeepMind
    Paper: https://papers.nips.cc/paper/6542-towards-conceptual-compression.pdf


* Generative Compression:  
    Generative Compression (https://arxiv.org/pdf/1703.01467.pdf and http://www.wave.one/icml2017/ ), think about streaming videos with orders of magnitude better compression. The results are pretty insane, and this could possibly be the key to bringing AR/VR into the everyday market. If we can figure out how to integrate this into real-time systems, like lets say a phone, you could take hidef video, buffer it and encode it to compress it (the above waveone model can compress 100 img/sec from the Kodak dataset -- not shabby at all), we could save massive amounts of data with order of magnitude less storage. We could easily create a mobile app as a proof of concept, but this shit could be huge. These can be also trained to be domain specific, because they are learned not hardcoded. We could create an API allowing any device to connect to it and dynamically compress data, think drones, etc. We can also build in encryption into the system, which adds a layer of security.




***
***

TITLE: CNNs <br /> Convolutional Neural Networks
LINK: research/dl/cv/Intro_cnns_1(uncomplete).md


## Image Classification
{: #content1}

1. **The Problem:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11} 
    :   Assigning a semantic label from a fixed set of categories to a sub-grid of an image.
    :   The problem is often referred to as __The Semantic Gap__.

2. **The Challenges:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12} 
    :   1. Viewpoint Variation  
        ![img](/main_files/cs231n/2/1.png){: width="50%"}  
        2. Illumination Conditions  
        ![img](/main_files/cs231n/2/2.png){: width="50%"}  
        3. Deformation  
        ![img](/main_files/cs231n/2/3.png){: width="50%"}  
        4. Occlusion  
        ![img](/main_files/cs231n/2/4.png){: width="50%"}  
        5. Background Clutter  
        ![img](/main_files/cs231n/2/5.png){: width="50%"}  
        6. Intra-class variation  
        ![img](/main_files/cs231n/2/6.png){: width="50%"}  
        7. Scale Variation  
        ![img](/main_files/cs231n/2/7.png){: width="50%"}  

3. **Attempts:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13} 
    :   1. 

4. **The Data-Driven Approach:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14} 
    :   1. Collect a dataset of images and labels.  
        2. Use Machine Learning to train a classifier.  
        3. Evaluate the classifier on new images.  





***

## Classifiers
{: #content2}

1. **K-Nearest-Neighbors:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21} 
    :   

    :   __Complexity__:  
        :   * _Training_: $$\:\:\:\:\mathcal{O}(1)$$   
            * _Predict_: $$\:\:\:\:\mathcal{O}(N)$$ 








***

## Metrics
{: #content3}

1. **L1 Distance:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31} 
    :   $$d_1(I_1, I_2) = \sum_p{\|I_1^p - I_2^p\|}$$  
    :   Pixel-wise absolute value differences.  

2. **L2 Distance:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32} 
    :   $$d_2 (I_1, I_2) = \sqrt{\sum_{p} \left( I^p_1 - I^p_2 \right)^2}$$
    :   

3. **L1 vs. L2:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33} 
    :   The L2 distance penalizes errors (pixel differences) much more than the L1 metric does.  
    The L2 distnace will be small iff there are man small differences in the two vectors but will explode if there is even one big difference between them.  
    :   Another difference we highlight is that the L1 distance is dependent on the corrdinate system frame, while the L2 distance is coordinate-invariant.







***
***

TITLE: Auto-Encoders
LINK: research/dl/archt/auto_encdr.md


## Introduction and Architecture
{: #content1}

0. **From PCA to Auto-Encoders:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents10}   
    High dimensional data can often be represented using a much lower dimensional code.  
    This happens when  the data lies near a linear manifold in the high dimensional space.  
    Thus, if we can this _linear manifold_, we can project the data on the manifold and, then, represent the data by its position on the manifold without losing much information because in the directions orthogonal to the manifold there isn't much variation in the data.  

    Often, __PCA__ is used as a method to determine this _linear manifold_ to reduce the dimensionality of the data from $$N$$-dimensions to, say, $$M$$-dimensions, where $$M < N$$.  
    However, what if the manifold that the data is close to, is non-linear?  
    Obviously, we need someway to find this non-linear manifold.  

    Deep-Learning provides us with Deep __AutoEncoders__.  
    __Auto-Encoders__ allows us to deal with _curved manifolds_ un the input space by using deep layers, where the _code_ is a _non-linear function_ of the input, and the _reconstruction_ of the data from the code is, also, a _non-linear function_ of the code.  

    __Using Backpropagation to implement PCA (inefficiently):__{: style="color: red"}  
    * Try to make the output be the same as the input in a network with a central bottleneck.  
        ![img](https://cdn.mathpix.com/snip/images/1mzm8wkDbwLtru-N98SJYffFfFJdjiTv1sl2fczwIGM.original.fullsize.png){: width="30%"}  
    * The activities of the hidden units in the bottleneck form an efficient code.  
    * If the hidden and output layers are linear, it will learn hidden units that are a linear function of the data and minimize the squared reconstruction error.  
        * This is exactly what PCA does.  
    * The M hidden units will span the same space as the first $M$ components found by PCA  
        * Their weight vectors may not be orthogonal.  
        * They might be skews or rotations of the PCs.  
        * They will tend to have equal variances.  

    The reason to use backprop to implement PCA is that it allows us to <span>generalize PCA</span>{: style="color: goldenrod"}.  
    With non-linear layers before and after the code, it should be possible to efficiently represent data that lies on or near a non- linear manifold.  
    <br>

1. **Auto-Encoders:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   An __AutoEncoder__ is an artificial neural network used for unsupervised learning of efficient codings.   
        It aims to learn a representation (encoding) for a set of data, typically for the purpose of _dimensionality reduction_.
    :   ![img](/main_files/cs231n/aencdrs/1.png){: width="50%"}  

22. **Deep Auto-Encoders:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents122}  
    They provide a really nice way to do <span>_non-linear_ dimensionality reduction</span>{: style="color: goldenrod"}:  
    * They provide flexible mappings _both_ ways
    * The <span>learning time is linear</span>{: style="color: goldenrod"} (or better) in the number of training examples  
    * The final encoding model is fairly compact and fast.  
    <br>

33. **Advantages of Depth:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents133}  
    Autoencoders are often trained with only a single layer encoder and a single layer decoder, but using deep encoders and decoders offers many advantages:  
    * Depth can __exponentially reduce the computational cost__ of representing some functions
    * Depth can __exponentially decrease the amount of training data__ needed to learn some functions
    * Experimentally, deep Autoencoders yield __better compression__ compared to shallow or linear Autoencoders  
    <br>

44. **Learning Deep Autoencoders:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents144}  
    Training Deep Autoencoders is very challenging:  
    * It is difficult to optimize deep Autoencoders using backpropagation  
    * With small initial weights the backpropagated gradient dies  
    
    There are two main methods for training:  
    * Just initialize the weights carefully as in Echo-State Nets. (No longer used)  
    * Use unsupervised layer-by- layer pre-training. (_Hinton_)  
        This method involves treating each neighbouring set of two layers as a restricted Boltzmann machine so that the pretraining approximates a good solution, then using a backpropagation technique to fine-tune the results. This model takes the name of __deep belief network__.  
    * Joint Training (most common)  
        This method involves training the whole architecture together with a single global reconstruction objective to optimize.  

    A study published in 2015 empirically showed that the joint training method not only learns better data models, but also learned more representative features for classification as compared to the layerwise method.  
    The success of joint training, however, is mostly attributed (depends heavily) on the __regularization strategies__ adopted in the modern variants of the model.  

    [Is Joint Training Better for Deep Auto-Encoders? (paper)](https://arxiv.org/pdf/1405.1380.pdf)  
    <br>

2. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   An auto-encoder consists of:  
        * An Encoding Function 
        * A Decoding Function 
        * A Distance Function  
    :   We choose the _encoder_ and _decoder_ to be  parametric functions (typically neural networks), and to be differentiable with respect to the distance function, so the parameters of the encoding/decoding functions can be optimize to minimize the reconstruction loss, using Stochastic Gradient Descent.  
    :   The simplest form of an autoencoder is a feedforward neural network similar to the multilayer perceptron (MLP) ‚Äì having an input layer, an output layer and one or more hidden layers connecting them ‚Äì, but with the output layer having the same number of nodes as the input layer, and with the purpose of reconstructing its own inputs (instead of predicting the target value $${\displaystyle Y}$$ given inputs $${\displaystyle X}$$).  

3. **Structure and Mathematics:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   The _encoder_ and the _decoder_ in an auto-encoder can be defined as transitions $$\phi$$ and $$ {\displaystyle \psi ,}$$ such that:  
    :   $$ {\displaystyle \phi :{\mathcal {X}}\rightarrow {\mathcal {F}}} \\
    {\displaystyle \psi :{\mathcal {F}}\rightarrow {\mathcal {X}}} \\
    {\displaystyle \phi ,\psi =\arg \min_{\phi ,\psi }\|X-(\psi \circ \phi )X\|^{2}}$$
    :   where $${\mathcal {X} = \mathbf{R}^d}$$ is the input space, and $${\mathcal {F} = \mathbf{R}^p}$$ is the latent (feature) space, and $$ p < d$$.   
    :   The encoder takes the input $${\displaystyle \mathbf {x} \in \mathbb {R} ^{d}={\mathcal {X}}}$$ and maps it to $${\displaystyle \mathbf {z} \in \mathbb {R} ^{p}={\mathcal {F}}} $$:

    :   $${\displaystyle \mathbf {z} =\sigma (\mathbf {Wx} +\mathbf {b} )}$$  
    :   * The image $$\mathbf{z}$$ is referred to as _code_, _latent variables_, or _latent representation_.  
        *  $${\displaystyle \sigma }$$ is an element-wise activation function such as a sigmoid function or a rectified linear unit.
        * $${\displaystyle \mathbf {W} }$$ is a weight matrix
        * $${\displaystyle \mathbf {b} }$$ is the bias.
    :   The Decoder maps  $${\displaystyle \mathbf {z} }$$ to the reconstruction $${\displaystyle \mathbf {x'} } $$  of the same shape as $${\displaystyle \mathbf {x} }$$:  
    :   $${\displaystyle \mathbf {x'} =\sigma '(\mathbf {W'z} +\mathbf {b'} )}$$
    :   where $${\displaystyle \mathbf {\sigma '} ,\mathbf {W'} ,{\text{ and }}\mathbf {b'} } $$ for the decoder may differ in general from those of the encoder.  
    :   Autoencoders minimize  reconstruction errors, such as the L-2 loss:  
    :   $${\displaystyle {\mathcal {L}}(\mathbf {x} ,\psi ( \phi (\mathbf {x} ) ) ) =  {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )=\|\mathbf {x} -\mathbf {x'} \|^{2}=\|\mathbf {x} -\sigma '(\mathbf {W'} (\sigma (\mathbf {Wx} +\mathbf {b} ))+\mathbf {b'} )\|^{2}}$$
    :   where $${\displaystyle \mathbf {x} }$$ is usually averaged over some input training set.

4. **Applications:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   The applications of auto-encoders have changed overtime.  
        This is due to the advances in the fields that auto-encoders were applied in, or to the incompetency of the auto-encoders.  
    :   Recently, auto-encoders are applied to:  
        * __Data-Denoising__ 
        * __Dimensionality Reduction__ (for data visualization)
    :   >  With appropriate dimensionality and sparsity constraints, autoencoders can learn data projections that are more interesting than PCA or other basic techniques.
    :   > For 2D visualization specifically, t-SNE is probably the best algorithm around, but it typically requires relatively low-dimensional data. So a good strategy for visualizing similarity relationships in high-dimensional data is to start by using an autoencoder to compress your data into a low-dimensional space (e.g. 32 dimensional) (by an auto-encoder), then use t-SNE for mapping the compressed data to a 2D plane.  

    :   * [Deep autoencoders for document retrieval (Hinton)](https://www.youtube.com/watch?v=ARQ6PZh8vgE&list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&index=71)  
        * [Semantic Hashing (Hinton)](https://www.youtube.com/watch?v=swjncYpcLsk&list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&index=72)  
        * [Learning binary codes for image retrieval (Hinton)](https://www.youtube.com/watch?v=MSYmyJgYOnU&list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&index=73)  
        * [Shallow autoencoders for pre-training (Hinton)](https://www.youtube.com/watch?v=e_n2hht9Yc8&list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&index=74)  

5. **Types of Auto-Encoders:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    * Vanilla Auto-Encoder
    * Sparse Auto-Encoder
    * Denoising Auto-Encoder
    * Variational Auto-Encoder (VAE)
    * Contractive Auto-Encoder
    <br>

6. **Auto-Encoders for initializing Neural-Nets:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    After training an auto-encoder, we can use the _encoder_ to compress the input data into it's latent representation (which we can view as _features_) and input those to the neural-net (e.g. a classifier) for prediction.  
    ![img](/main_files/cs231n/aencdrs/2.png){: width="70%"} 
    <br>

7. **Representational Power, Layer Size and Depth:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    the universal approximator theorem guarantees that a feedforward neural network with at least one hidden layer can represent an approximation of any function (within a broad class) to an arbitrary degree of accuracy, provided that it has enough hidden units. This means that an autoencoder with a single hidden layer is able to represent the identity function along the domain of the data arbitrarily well.  
    However, __the mapping from input to code is shallow__. This means that we are not able to enforce arbitrary constraints, such as that the code should be sparse.  
    A deep autoencoder, with at least one additional hidden layer inside the encoder itself, can approximate any mapping from input to code arbitrarily well, given enough hidden units.  
    <br>

__Notes:__{: style="color: red"}  
{: #lst-p}
* Progression of AEs (in CV?):  
    * _Originally:_ Linear + nonlinearity (sigmoid)  
    * _Later:_ Deep, fully-connected  
    * _Later:_ ReLU CNN (UpConv)  



***

## DL Book - AEs
{: #content2}

1. **Undercomplete Autoencoders:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    An __Undercomplete Autoencoder__ is one whose code dimension is less than the input dimension.  
    Learning an undercomplete representation forces the autoencoder to capture the most salient features of the training data.  
    <br>

2. **Challenges:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    * If an autoencoder succeeds in simply learning to set $$\psi(\phi (x)) = x$$ everywhere, then it is not especially useful.  
        Instead, autoencoders are designed to be unable to learn to copy perfectly. Usually they are restricted in ways that allow them to copy only approximately, and to copy only input that resembles the training data.  
    * In [__Undercomplete Autoencoders__](#bodyContents21) If the encoder and decoder are allowed too much capacity, the autoencoder can learn to perform the copying task without extracting useful information about the distribution of the data.  
        Theoretically, one could imagine that an autoencoder with a one-dimensional code but a very powerful nonlinear encoder could learn to represent each training example $$x^{(i)}$$ with the code $$i$$. This specific scenario does not occur in practice, but it illustrates clearly that an autoencoder trained to perform the copying task can fail to learn anything useful about the dataset if the capacity of the autoencoder is allowed to become too great.  
    * A similar problem occurs in __complete AEs__  
    * As well as in the __overcomplete__ case, in which the hidden code has dimension greater than the input.  
        In complete and overcomplete cases, even a linear encoder and linear decoder can learn to copy the input to the output without learning anything useful about the data distribution.  
    <br>

3. **Regularized AutoEncoders:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    To address the challenges in learning useful representations; we introduce __Regularized Autoencoders__.  

    __Regularized Autoencoders__ allows us to train any architecture of autoencoder successfully, choosing the code dimension and the capacity of the encoder and decoder based on the complexity of distribution to be modeled.  

    Rather than limiting the model capacity by keeping the encoder and decoder shallow and the code size small, regularized autoencoders use a loss function that encourages the model to have other properties besides the ability to copy its input to its output:   
    * Sparsity of the representation 
    * Smallness of the derivative of the representation
    * Robustness to noise or to missing inputs.  

    A regularized autoencoder can be __nonlinear__ and __overcomplete__ but still learn something useful about the data distribution even if the model capacity is great enough to learn a trivial identity function.  
    <br>

4. **Generative Models as (unregularized) Autoencoders:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    In addition to the traditional AEs described here, nearly any __generative model__ with __latent variables__ and equipped with an __inference procedure__ (for computing latent representations given input) may be viewed as a particular form of autoencoder; most notably the descendants of the __Helmholtz machine__ _(Hinton et al., 1995b)_, such as:  
    * Variational Autoencoders  
    * Generative Stochastic Networks  

    These models naturally learn _high-capacity_, _overcomplete encodings_ of the input and do NOT require regularization for these encodings to be useful. Their <span>encodings are naturally useful</span>{: style="color: goldenrod"} because the models were <span>trained to _approximately maximize the probability of the training data_ rather than to _copy the input to the output_</span>{: style="color: goldenrod"}.  
    <br>

5. **Stochastic Encoders and Decoders:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    <button>Discussion</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](/main_files/cs231n/aencdrs/3.png){: width="100%" hidden=""}  
    <br>


***

## Regularized Autoencoders
{: #content3}

1. **Sparse Autoencoders:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    __Sparse Autoencoders__ are simply autoencoders whose training criterion involves a sparsity penalty $$\Omega(\boldsymbol{h})$$ on the code layer $$\boldsymbol{h},$$ in addition to the reconstruction error:  
    <p>$${\displaystyle {\mathcal {L}}(\mathbf {x} ,\psi ( \phi (\mathbf {x} ) ) ) + \Omega(\boldsymbol{h})}$$</p>  
    where typically we have $$\boldsymbol{h}=\phi(\boldsymbol{x})$$, the encoder output.   


    __Regularization Interpretation:__{: style="color: red"}  
    We can think of the penalty $$\Omega(\boldsymbol{h})$$ simply as a regularizer term added to a feedforward network whose primary task is to copy the input to the output (unsupervised learning objective) and possibly also perform some supervised task (with a supervised learning objective) that depends on these sparse features.  


    __Bayesian Interpretation of Regularization:__{: style="color: red"}  
    Unlike other regularizers such as weight decay, there is not a straightforward Bayesian interpretation to this regularizer.  
    Regularized autoencoders defy such an interpretation because __the regularizer depends on the data__ and is therefore by definition not a prior in the formal sense of the word.  
    We can still think of these regularization terms as _implicitly expressing a preference over functions_.   


    __Latent Variable Interpretation:__{: style="color: red"}  
    Rather than thinking of the sparsity penalty as a regularizer for the copying task, we can think of the entire sparse autoencoder framework as <span>approximating maximum likelihood training of a generative model that has latent variables</span>{: style="color: goldenrod"}.  

    __Correspondence between Sparsity and a Directed Probabilistic Model:__  
    Suppose we have a model with visible variables $$\boldsymbol{x}$$ and latent variables $$\boldsymbol{h},$$ with an explicit joint distribution $$p_{\text {model }}(\boldsymbol{x}, \boldsymbol{h})=p_{\text {model }}(\boldsymbol{h}) p_{\text {model }}(\boldsymbol{x} \vert \boldsymbol{h}) .$$ We refer to $$p_{\text {model }}(\boldsymbol{h})$$ as the model's prior distribution over the latent variables, representing the model's beliefs prior to seeing $$\boldsymbol{x}$$[^1].  
    The log-likelihood can be decomposed as:  
    <p>$$\log p_{\text {model }}(\boldsymbol{x})=\log \sum_{\boldsymbol{h}} p_{\text {model }}(\boldsymbol{h}, \boldsymbol{x})$$</p>  
    We can think of the autoencoder as approximating this sum with a point estimate for just one highly likely value for $$\boldsymbol{h}$$.  
    This is similar to the __sparse coding generative model__ (section 13.4), but with $$\boldsymbol{h}$$ being the _output of the parametric encoder_ rather than the result of an optimization that infers the most likely $$\boldsymbol{h}$$. From this point of view, with this chosen $$\boldsymbol{h}$$, we are maximizing:  
    <p>$$\log p_{\text {model }}(\boldsymbol{h}, \boldsymbol{x})=\log p_{\text {model }}(\boldsymbol{h})+\log p_{\text {model }}(\boldsymbol{x} \vert \boldsymbol{h})$$</p>  
    The $$\log p_{\text {model }}(\boldsymbol{h})$$ term can be sparsity-inducing. For example, the __Laplace prior__,  
    <p>$$p_{\text {model }}\left(h_{i}\right)=\frac{\lambda}{2} e^{-\lambda\left|h_{i}\right|}$$</p>  
    __corresponds to an absolute value sparsity penalty__.  
    Expressing the log-prior as an absolute value penalty, we obtain  
    <p>$$\begin{aligned} \Omega(\boldsymbol{h}) &=\lambda \sum_{i}\left|h_{i}\right| \\-\log p_{\text {model }}(\boldsymbol{h}) &=\sum_{i}\left(\lambda\left|h_{i}\right|-\log \frac{\lambda}{2}\right)=\Omega(\boldsymbol{h})+\text { const } \end{aligned}$$</p>  
    where the constant term depends only on $$\lambda$$ and not $$\boldsymbol{h} .$$ We typically treat $$\lambda$$ as a hyperparameter and discard the constant term since it does not affect the parameter learning.  
    Other priors such as the __Student-t prior__ can also induce sparsity.  
    From this point of view of __sparsity__ as <span>resulting from the effect of $$p_{\text {model}}(\boldsymbol{h})$$ on approximate maximum likelihood learning</span>{: style="color: goldenrod"}, the sparsity penalty is __not a regularization term at all__. It is just a <span>consequence of the model‚Äôs distribution over its latent variables</span>{: style="color: goldenrod"}. This view provides a __different motivation for training an autoencoder__: <span>it is a way of approximately training a generative model</span>{: style="color: goldenrod"}. It also provides a different __reason for why the features learned by the autoencoder are useful__: <span>they describe the latent variables that explain the input</span>{: style="color: goldenrod"}.  

    __Correspondence between Sparsity and an Undirected Probabilistic Model:__  
    Early work on sparse autoencoders _(Ranzato et al., 2007a, 2008)_ explored various forms of sparsity and proposed a connection between the sparsity penalty and the log $$Z$$ term that arises when applying maximum likelihood to an undirected probabilistic model $$p(\boldsymbol{x})=\frac{1}{Z} \tilde{p}(\boldsymbol{x})$$.  
    The idea is that __minimizing $$\log Z$$ prevents a probabilistic model from having high probability everywhere__, and __imposing sparsity on an autoencoder prevents the autoencoder from having low reconstruction error everywhere__. In this case, the connection is on the _level of an intuitive understanding of a general mechanism_ rather than a _mathematical correspondence_.  

    The interpretation of the sparsity penalty as corresponding to $$\log p_{\text {model }}(\boldsymbol{h})$$ in a $$\left.\text { directed model } p_{\text {model }}(\boldsymbol{h}) p_{\text {model }} \boldsymbol{x} \vert \boldsymbol{h}\right)$$ is more mathematically straightforward.   

    __Achieving actual zeros in $$\boldsymbol{h}$$:__  
    One way to achieve actual zeros in $$\boldsymbol{h}$$ for sparse (and denoising) autoencoders was introduced in Glorot et al. (2011b). The idea is to use rectified linear units to produce the code layer. With a prior that actually pushes the representations to zero (like the absolute value penalty), one can thus indirectly control the average number of zeros in the representation.  


    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * [Sparse Autoencoder and Unsupervised Feature Learning #1 (Ng)](https://www.youtube.com/watch?v=vfnxKO2rMq4)  
    * [Sparse Autoencoder and Unsupervised Feature Learning #2 (Ng)](https://www.youtube.com/watch?v=wqhZaWR-J94)  
    <br>


2. **Denoising Autoencoders:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    __Denoising Autoencoders (DAEs)__ is an autoencoder that receives a corrupted data point as input and is trained to predict the original, uncorrupted data point as its output.  
    It minimizes:  
    <p>$$L(\boldsymbol{x}, g(f(\tilde{\boldsymbol{x}})))$$</p>  
    where $$\tilde{\boldsymbol{x}}$$ is a copy of $$\boldsymbol{x}$$ that has been corrupted by some form of noise.  

    Denoising autoencoders must therefore learn to __undo this corruption__ rather than simply copying their input.  
    Denoising training forces $$\psi$$ and $$\phi$$ to implicitly learn the structure of $$p_{\text {data}}(\boldsymbol{x}),$$ as shown by _Alain and Bengio (2013)_ and _Bengio et al. (2013c)_.  
    * Provide yet another example of how useful properties can emerge as a byproduct of minimizing reconstruction error.  
    * Also an example of, how overcomplete, high-capacity models may be used as autoencoders so long as care is taken to prevent them from learning the identity function.  

    We introduce a __corruption process__ $$C(\tilde{\mathbf{x}} \vert \mathbf{x})$$ which represents a __conditional distribution over corrupted samples $$\tilde{\boldsymbol{x}}$$__, given a data sample $$\boldsymbol{x}$$.  
    The autoencoder then learns a __reconstruction distribution__ $$p_{\text {reconstruct }}(\mathrm{x} \vert \tilde{\mathrm{x}})$$ estimated from training pairs $$(\boldsymbol{x}, \tilde{\boldsymbol{x}}),$$ as follows:  
    {: #lst-p}
    * Sample a training example $$\boldsymbol{x}$$ from the training data.
    * Sample a corrupted version $$\tilde{\boldsymbol{x}}$$ from $$C(\tilde{\mathbf{x}} \vert \mathbf{x}=\boldsymbol{x})$$
    * Use $$(\boldsymbol{x}, \tilde{\boldsymbol{x}})$$ as a training example for estimating the autoencoder reconstruction distribution $$p_{\text {reconstruct }}(\boldsymbol{x} \vert \tilde{\boldsymbol{x}})=p_{\text {decoder }}(\boldsymbol{x} \vert \boldsymbol{h})$$ with $$\boldsymbol{h}$$ the output of encoder $$f(\tilde{\boldsymbol{x}})$$ and $$p_{\text {decoder}}$$ typically defined by a decoder $$g(\boldsymbol{h})$$.  

    __Learning:__  
    Typically we can simply perform gradient-based approximate minimization (such as minibatch gradient descent) on the negative log-likelihood $$-\log p_{\text {decoder }}(\boldsymbol{x} \vert \boldsymbol{h})$$ So long as the encoder is deterministic, the denoising autoencoder is a feedforward network and may be trained with exactly the same techniques as any other FFN.  
    We can therefore view the DAE as __performing stochastic gradient descent on the following expectation__:  
    <p>$$-\mathbb{E}_{\mathbf{x} \sim \hat{p}_{\text {data }}(\mathbf{x})} \mathbb{E}_{\tilde{\mathbf{x}} \sim C(\tilde{\mathbf{x}} \vert \boldsymbol{x})} \log p_{\text {decoder }}(\boldsymbol{x} \vert \boldsymbol{h}=f(\tilde{\boldsymbol{x}}))$$</p>  
    where $$\hat{p}_ {\text {data}}(\mathrm{x})$$ is the training distribution.  


    __Score Matching - Estimating the Score:__{: style="color: red"}{: #bodyContents32sm}  
    __Score Matching__ _(Hyv√§rinen, 2005)_ is an alternative to maximum likelihood. It provides a __consistent estimator of probability distributions__ based on _encouraging the model to have the same score as the data distribution at every training point $$\boldsymbol{x}$$_. In this context, the score is a particular __gradient field__    
    <p>$$\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})$$</p>   
    For autoencoders, it is sufficient to understand that <span>learning the gradient field of $$\log p_{\text {data}}$$ is one way to learn the structure of $$p_{\text {data itself}}$$</span>{: style="color: goldenrod"}.  

    A very important property of DAEs is that <span>their training criterion (with $$\text { conditionally Gaussian } p(\boldsymbol{x} \vert \boldsymbol{h}))$$ makes the autoencoder learn a __vector field__ $$(f(\boldsymbol{x}))-\boldsymbol{x}$$) that estimates the __score of the data distribution__</span>{: style="color: goldenrod"}.  
    <button>Vector Field Learning and the Score of $$p_{\text{data}}$$</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/QSRS3G3sT_oO--ZqNJMU8Oub6_etW0n4gHg-0EPSRd8.original.fullsize.png){: width="100%" hidden=""}   
    __Continuous Valued $$\boldsymbol{x}$$:__  
    For continuous-valued $$\boldsymbol{x}$$, the denoising criterion with __Gaussian corruption__ and __reconstruction distribution__ yields an __estimator of the score that is applicable to general encoder and decoder parametrizations__ _(Alain and Bengio, 2013)_.  
    This means a _generic encoder-decoder architecture_ may be made to _estimate the score_ by training with the __squared error criterion__,   
    <p>$$\|g(f(\tilde{x}))-x\|^{2}$$</p>  
    and __corruption__,  
    <p>$$C(\tilde{\mathbf{x}}=\tilde{\boldsymbol{x}} \vert \boldsymbol{x})=\mathcal{N}\left(\tilde{\boldsymbol{x}} ; \mu=\boldsymbol{x}, \Sigma=\sigma^{2} I\right)$$</p>  
    with noise variance $$\sigma^{2}$$.  
    <button>Illustration for continuous $$\boldsymbol{x}$$</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/nboAnloEXDY7mMvx9vXkkm5Zt-jf46fRNPieG_mAh78.original.fullsize.png){: width="100%" hidden=""}   
    __Guarantees:__  
    In general, there is no guarantee that the reconstruction $$g(f(\boldsymbol{x}))$$ minus the input $$\boldsymbol{x}$$ corresponds to the gradient of any function, let alone to the score. That is why the early results (Vincent, 2011) are specialized to particular parametrizations where $$g(f(\boldsymbol{x}))-\boldsymbol{x}$$ may be obtained by taking the derivative of another function. Kamyshanska and Memisevic $$(2015)$$ generalized the results of Vincent $$(2011)$$ by identifying a family of shallow autoencoders such that $$g(f(\boldsymbol{x}))-\boldsymbol{x}$$ corresponds to a score for all members of the family.  


    __DAEs as representing Probability Distributions and Variational AEs:__{: style="color: red"}  
    So far we have described only how the DAE learns to represent a probability distribution. More generally, one may want to use the autoencoder as a generative model and draw samples from this distribution. This is knows as the __Variational Autoencoder__.  


    __Denoising AutoEncoders and RBMs:__{: style="color: red"}  
    Denoising training of a specific kind of autoencoder (sigmoidal hidden units, linear reconstruction units) using Gaussian noise and mean squared error as the reconstruction cost is equivalent (Vincent, 2011) to training a specific kind of undirected probabilistic model called an RBM with Gaussian visible units. This kind of model will be described in detail in section 20.5.1; for the present discussion it suffices to know that it is a model that provides an explicit $$p_{\text {model }}(\boldsymbol{x} ; \boldsymbol{\theta})$$. When the RBM is trained using __denoising score matching__ _(Kingma and LeCun, 2010)_, its learning algorithm is equivalent to denoising training in the corresponding autoencoder. With a fixed noise level, regularized score matching is not a consistent estimator; it instead recovers a blurred version of the distribution. However, if the noise level is chosen to approach $$0$$ when the number of examples approaches infinity, then consistency is recovered. Denoising score matching is discussed in more detail in section 18.5.  
    Other connections between autoencoders and RBMs exist. Score matching applied to RBMs yields a cost function that is identical to reconstruction error combined with a regularization term similar to the contractive penalty of the CAE (Swersky et al., 2011). Bengio and Delalleau (2009) showed that an autoencoder gradient provides an approximation to contrastive divergence training of RBMs.  


    __Historical Perspective:__{: style="color: red"}  
    <button>Discussion</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/w3Nopzr4Q_GY4rYMFqN1s8eq-aiGy47eYyOCOcLHQqo.original.fullsize.png){: width="100%" hidden=""}  
    <br>


3. **Regularizing by Penalizing Derivatives:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    Another strategy for regularizing an autoencoder is to use a penalty $$\Omega$$ as in sparse autoencoders,  
    <p>$$L(\boldsymbol{x}, g(f(\boldsymbol{x})))+\Omega(\boldsymbol{h}, \boldsymbol{x})$$</p>  
    but with a different form of $$\Omega$$:  
    <p>$$\Omega(\boldsymbol{h}, \boldsymbol{x})=\lambda \sum_{i}\left\|\nabla_{\boldsymbol{x}} h_{i}\right\|^{2}$$</p>  

    This forces the model to learn a function that does not change much when $$\boldsymbol{x}$$ changes slightly. Because this penalty is applied only at training examples, it forces the autoencoder to learn features that capture information about the training distribution.  
    An autoencoder regularized in this way is called a [__contractive autoencoder (CAE)__](#bodyContents34). This approach has theoretical connections to denoising autoencoders, manifold learning and probabilistic modeling.  
    <br>


4. **Contractive Autoencoders:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    __Contractive Autoencoders (CAEs)__ _(Rifai et al., 2011a,b)_ introduces an explicit regularizer on the code $$\boldsymbol{h}=\phi(\boldsymbol{x}),$$ encouraging the derivatives of $$\phi$$ to be as small as possible:  
    <p>$$\Omega(\boldsymbol{h})=\lambda\left\|\frac{\partial \phi(\boldsymbol{x})}{\partial \boldsymbol{x}}\right\|_ {F}^{2}$$</p>  
    The __penalty__ $$\Omega(\boldsymbol{h})$$ is the *__squared Frobenius norm__*  (sum of squared elements) of __the Jacobian matrix__ of partial derivatives associated with the encoder function.  

    __Connection to Denoising Autoencoders:__{: style="color: red"}  
    There is a connection between the denoising autoencoder and the contractive autoencoder: Alain and Bengio _(2013)_ showed that in the limit of small Gaussian input noise, the denoising reconstruction error is equivalent to a contractive penalty on the reconstruction function that maps $$\boldsymbol{x}$$ to $$\boldsymbol{r}=\psi(\phi(\boldsymbol{x}))$$.  
    In other words:  
    \- Denoising Autoencoders: make the reconstruction function resist small but finite-sized perturbations of the input  
    \- Contractive Autoencoders: make the feature extraction function resist infinitesimal perturbations of the input.  
    When using the Jacobian-based contractive penalty to pretrain features $$\phi(\boldsymbol{x})$$ for use with a classifier, the best classification accuracy usually results from applying the contractive penalty to $$\phi(\boldsymbol{x})$$ rather than to $$\psi(\phi(\boldsymbol{x}))$$.  
    A contractive penalty on $$\phi(\boldsymbol{x})$$ also has close [connections to __score matching__](#bodyContents32sm).  

    __Contractive - Definition and Analysis:__{: style="color: red"}  
    The name __contractive__ arises from the way that the CAE _warps space_. Specifically, because the CAE is trained to resist perturbations of its input, it is encouraged to map a neighborhood of input points to a smaller neighborhood of output points. We can think of this as contracting the input neighborhood to a smaller output neighborhood.  

    To clarify, the CAE is contractive only *__locally__*-all perturbations of a training point $$\boldsymbol{x}$$ are mapped near to $$\phi(\boldsymbol{x})$$. *__Globally__*, two different points $$\boldsymbol{x}$$ and $$\boldsymbol{x}^{\prime}$$ may be mapped to $$\phi(\boldsymbol{x})$$ and $$\psi\left(\boldsymbol{x}^{\prime}\right)$$ points that are farther apart than the original points.  

    __As a linear operator:__  
    We can think of the _Jacobian matrix_ $$J$$ at a point $$x$$ as <span>_approximating_ the __nonlinear encoder $$\phi(x)$$__ as being a __linear operator__</span>{: style="color: goldenrod"}. This allows us to use the word _"contractive"_ more formally.  
    In the theory of linear operators, a linear operator is said to be _contractive_ if the norm of $$J x$$ remains less than or equal to 1 for all unit-norm $$x$$. In other words, __$$J$$ is contractive if it shrinks the unit sphere__.  
    We can think of the CAE as _penalizing the Frobenius norm of the local linear approximation of $$\phi(x)$$ at every training point $$x$$_ in order _to encourage each of these local linear operator to become a **contraction**_.  


    __Manifold Learning:__{: style="color: red"}  
    Regularized autoencoders learn manifolds by balancing two opposing forces.  
    In the case of the CAE, these two forces are reconstruction error and the contractive penalty $$\Omega(\boldsymbol{h}) .$$ Reconstruction error alone would encourage the CAE to learn an identity function. The contractive penalty alone would encourage the CAE to learn features that are constant with respect to $$\boldsymbol{x}$$.  
    The compromise between these two forces yields an autoencoder whose derivatives $$\frac{\partial f(\boldsymbol{x})}{\partial \boldsymbol{x}}$$ are mostly tiny. Only a small number of hidden units, corresponding to a small number of directions in the input, may have significant derivatives.  

    The __goal__ of the CAE is to _learn the manifold structure of the data_.  
    Directions $$x$$ with large $$J x$$ rapidly change $$h,$$ so these are likely to be directions which approximate the tangent planes of the manifold.  
    Experiments by _Rifai et al. (2011 a,b)_ show that training the CAE results in:  
    {: #lst-p}
    1. Most singular values of $$J$$ dropping below $$1$$ in magnitude and therefore becoming _contractive_  
    2. However, some singular values remain above $$1,$$ because the reconstruction error penalty encourages the CAE to encode the directions with the most local variance.  

    * The directions corresponding to the largest singular values are interpreted as the tangent directions that the contractive autoencoder has learned.  
    * Visualizations of the experimentally obtained singular vectors do seem to correspond to meaningful transformations of the input image.  
        Since, Ideally, these tangent directions should correspond to real variations in the data.  
        For example, a CAE applied to images should learn tangent vectors that show how the image changes as objects in the image gradually change pose.  
        <button>Estimated Tangent Vectors of the Manifold</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](https://cdn.mathpix.com/snip/images/2KaOUhQBjXy2N_cP2DgVDA5PI1ggqOK1StFn2NR8t0c.original.fullsize.png){: width="100%" hidden=""}  


    __Issues with Contractive Penalties:__{: style="color: red"}  
    Although it is cheap to compute the CAE regularization criterion, in the case of a single hidden layer autoencoder, it becomes much _more expensive_ in the case of _deeper_ autoencoders.  
    The strategy followed by _Rifai et al. (2011a)_ is to:  
    \- Separately train a series of single-layer autoencoders, each trained to reconstruct the previous autoencoder‚Äôs hidden layer.  
    \- The composition of these autoencoders then forms a deep autoencoder.  
    * Because each layer was separately trained to be locally contractive, the deep autoencoder is contractive as well.  
    * The result is not the same as what would be obtained by _jointly training_ the entire architecture with a penalty on the Jacobian of the deep model, but it captures many of the desirable qualitative characteristics.  

    Another issue is that the contractive penalty can obtain _useless results_ if we do not impose some sort of *__scale__* on the _decoder_.  
    \- For example, the encoder could consist of multiplying the input by a small constant $$\epsilon$$ and the decoder could consist of dividing the code by $$\epsilon$$.  
    As $$\epsilon$$ approaches $$0$$, the encoder drives the contractive penalty $$\Omega(\boldsymbol{h})$$ to approach $$0$$ without having learned anything about the distribution.  
    Meanwhile, the decoder maintains perfect reconstruction.  
    \- In _Rifai et al. (2011a)_, this is prevented by __tying the weights__ of $$\phi$$ and $$\psi$$. Both $$\phi$$ and $$\psi$$ are standard neural network layers consisting of an affine transformation followed by an element-wise nonlinearity, so it is straightforward to set the weight matrix of $$\psi$$ to be the transpose of the weight matrix of $$\phi$$.  



5. **Predictive Sparse Decomposition:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}  
    <button>Discussion</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](/main_files/cs231n/aencdrs/4.png){: width="100%" hidden=""}  


***

## Learning Manifolds with Autoencoders
{: #content4}  

<button>PDF</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
<iframe hidden="" src="/main_files/cs231n/aencdrs/chapter-14-manifold-learning.pdf" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>





[^1]: This is different from the way we have previously used the word "prior," to refer to the distribution $$p(\boldsymbol{\theta})$$ encoding our beliefs about the model's parameters before we have seen the training data.  

***
***

TITLE: Latent Variable Models
LINK: research/dl/archt/latent_variable_models.md



* [OneTab](https://www.one-tab.com/page/ZBsF69s-QzOY6Eb5uDha_Q)  
* [ICA (Stanford Notes)](http://cs229.stanford.edu/notes/cs229-notes11.pdf)  
* [Deep ICA (+code)](https://towardsdatascience.com/deep-independent-component-analysis-in-tensorflow-manual-back-prop-in-tf-94602a08b13f)  



## Latent Variable Models
{: #content1}

1. **Latent Variable Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __Latent Variable Models__ are statistical models that relate a set of observable variables (so-called manifest variables) to a set of latent variables.  

    __Core Assumption - Local Independence:__{: style="color: red"}  
    __Local Independence:__  
    The observed items are conditionally independent of each other given an individual score on the latent variable(s). This means that the latent variable *__explains__* why the observed items are related to another.  

    In other words, the targets/labels on the observations are the result of an individual's position on the latent variable(s), and that the observations have nothing in common after controlling for the latent variable.  

    <p>$$p(A,B\vert z) = p(A\vert z) \times (B\vert z)$$</p>  


    <button>Example of Local Independence</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/wnxPRKkVBA88V1k3i4HdWBTtn0NQFBi5gdNkTLcCeFk.original.fullsize.png){: width="100%" hidden=""}  


    __Methods for inferring Latent Variables:__{: style="color: red"}  
    <button>Show List</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
    * Hidden Markov models (HMMs)
    * Factor analysis
    * Principal component analysis (PCA)
    * Partial least squares regression
    * Latent semantic analysis and probabilistic latent semantic analysis
    * EM algorithms
    * Pseudo-Marginal Metropolis-Hastings algorithm
    * Bayesian Methods: LDA  
    {: hidden=""}



    __Notes:__{: style="color: red"}    
    {: #lst-p}
    * Latent Variables *__encode__*  information about the data  
        e.g. in compression, a 1-bit latent variable can encode if a face is Male/Female.  
    * __Data Projection:__  
        You *__"hypothesis"__* how the data might have been generated (by LVs).  
        Then, the LVs __generate__ the data/observations.  
        <button>Visualisation with Density (Generative) Models</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](https://cdn.mathpix.com/snip/images/ctljXHCOfIzpttSIOCsFbQxjFmjrEcf4a5Dr9KbWnTI.original.fullsize.png){: width="100%" hidden=""}  
    * [**Latent Variable Models/Gaussian Mixture Models**](https://www.youtube.com/embed/I9dfOMAhsug){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/I9dfOMAhsug"></a>
        <div markdown="1"> </div>    
    * [**Expectation-Maximization/EM-Algorithm for Latent Variable Models**](https://www.youtube.com/embed/lMShR1vjbUo){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/lMShR1vjbUo"></a>
        <div markdown="1"> </div>    
    <br>



***

## Linear Factor Models
{: #content2}

1. **Linear Factor Models:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    __Linear Factor Models__ are *__generative models__* that are the simplest class of *__latent variable models__*[^1].  
    A linear factor model is defined by the use of a *__stochastic__*, *__linear__* __decoder__ function that <span>*generates* $$\boldsymbol{x}$$ by adding __noise__ to a __linear transformation__ of $$\boldsymbol{h}$$</span>{: style="color: goldenrod"}.  


    __Applications/Motivation:__{: style="color: red"}  
    {: #lst-p}
    * Building blocks of __mixture models__ _(Hinton et al., 1995a; Ghahramani and Hinton, 1996; Roweis et al., 2002)_   
    * Building blocks of larger, __deep probabilistic models__ _(Tang et al., 2012)_  
    * They also show many of the basic approaches necessary to build __generative models__ that the more advanced deep models will extend further.  
    * These models are interesting because they allow us to discover explanatory factors that have a simple joint distribution.  
    * The simplicity of using a __linear decoder__ made these models some of the first latent variable models to be extensively studied.  

    __LFTs as Generative Models:__{: style="color: red"}  
    Linear factor models are some of the simplest __generative models__ and some of the simplest models that <span>learn a __representation__ of data</span>{: style="color: purple"}.  

    __Data Generation Process:__{: style="color: DarkRed"}    
    A linear factor model describes the data generation process as follows:  
    {: #lst-p}
    1. __Sample__ the *__explanatory factors__* $$\boldsymbol{h}$$ from a __distribution__:  
        <p>$$\mathbf{h} \sim p(\boldsymbol{h}) \tag{1}$$</p>  
        where $$p(\boldsymbol{h})$$ is a factorial distribution, with $$p(\boldsymbol{h})=\prod_{i} p\left(h_{i}\right),$$ so that it is easy to sample from.  
    2. __Sample__ the *real-valued* *__observable variables__* *given* the __factors__:  
        <p>$$\boldsymbol{x}=\boldsymbol{W} \boldsymbol{h}+\boldsymbol{b}+ \text{ noise} \tag{2}$$</p>  
        where the noise is typically __Gaussian__ and __diagonal__ (independent across dimensions).  

    <button>Illustration</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/rrWlVKLy8vrKZTdgB-eLoAFHVMFAc39GG7nXAO80a3Q.original.fullsize.png){: width="100%" hidden=""}<br>

2. **Factor Analysis:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    __Probabilistic PCA (principal components analysis)__, __Factor Analysis__ and other __linear factor models__ are special cases of the above equations (1 and 2) and only differ in the choices made for the *__noise distribution__* and the model‚Äôs *__prior over latent variables__* $$\boldsymbol{h}$$ before observing $$\boldsymbol{x}$$.  

    __Factor Analysis:__{: style="color: red"}  
    In __factor analysis__ *(Bartholomew, 1987; Basilevsky, 1994)*, the *__latent variable prior__* is just the <span>__unit variance Gaussian__</span>{: style="color: purple"}:  
    <p>$$\mathbf{h} \sim \mathcal{N}(\boldsymbol{h} ; \mathbf{0}, \boldsymbol{I})$$</p>  
    while the __observed variables__ $$x_i$$ are assumed to be *__conditionally independent__*, given $$\boldsymbol{h}$$.  
    Specifically, the *__noise__* is assumed to be drawn from a <span>__diagonal covariance Gaussian distribution__</span>{: style="color: purple"}, with covariance matrix $$\boldsymbol{\psi}=\operatorname{diag}\left(\boldsymbol{\sigma}^{2}\right),$$ with $$\boldsymbol{\sigma}^{2}=\left[\sigma_{1}^{2}, \sigma_{2}^{2}, \ldots, \sigma_{n}^{2}\right]^{\top}$$ a vector of <span>*__per-variable__* __variances__</span>{: style="color: purple"}.  

    The _**role**_ of the __latent variables__ is thus to <span>capture the *__dependencies__* between the different observed variables $$x_i$$</span>{: style="color: purple"}.  
    Indeed, it can easily be shown that $$\boldsymbol{x}$$ is just a <span>__multivariate normal random variable__</span>{: style="color: purple"}, with:   
    <p>$$\mathbf{x} \sim \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{b}, \boldsymbol{W} \boldsymbol{W}^{\top}+\boldsymbol{\psi}\right)$$</p>  


3. **Probabilistic PCA:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    __Probabilistic PCA (principal components analysis)__, __Factor Analysis__ and other __linear factor models__ are special cases of the above equations (1 and 2) and only differ in the choices made for the *__noise distribution__* and the model‚Äôs *__prior over latent variables__* $$\boldsymbol{h}$$ before observing $$\boldsymbol{x}$$.  

    * <button>__Motivation__</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        * Addresses limitations of regular PCA
        * PCA can be used as a general Gaussian density model in addition to reducing dimensions
        * Maximum-likelihood estimates can be computed for elements associated with principal components
        * Captures dominant correlations with few parameters     
        * Multiple PCA models can be combined as a probabilistic mixture
        * Can be used as a base for Bayesian PCA  
        {: hidden=""}

    __Probabilistic PCA:__{: style="color: red"}  
    In order to _cast_ __PCA__ in a *__probabilistic framework__*, we can make a slight _modification_ to the __factor analysis model__, making the __conditional variances__ $$\sigma_i^2$$ <span>equal to each other</span>{: style="color: purple"}.  
    In that case the covariance of $$\boldsymbol{x}$$ is just $$\boldsymbol{W} \boldsymbol{W}^{\top}+\sigma^{2} \boldsymbol{I}$$, where $$\sigma^2$$ is now a __scalar__.  
    This yields the __conditional distribution__:  
    <p>$$\mathbf{x} \sim \mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{b}, \boldsymbol{W} \boldsymbol{W}^{\top}+\sigma^{2} \boldsymbol{I}\right)$$</p>  
    or, equivalently,  
    <p>$$\mathbf{x}=\boldsymbol{W} \mathbf{h}+\boldsymbol{b}+\sigma \mathbf{z}$$</p>  
    where $$\mathbf{z} \sim \mathcal{N}(\boldsymbol{z} ; \mathbf{0}, \boldsymbol{I})$$ is __Gaussian noise__.  

    Notice that $$\boldsymbol{b}$$ is the <span>__mean__ value (over all data) on the directions that are not captured/represented</span>{: style="color: purple"}.  

    This probabilistic PCA model takes advantage of the observation that <span>most variations in the data can be captured by the latent variables $$\boldsymbol{h},$$ up to some small residual</span>{: style="color: goldenrod"} <span>__reconstruction error__</span>{: style="color: goldenrod"} $$\sigma^2$$.   
    
    __Learning (parameter estimation):__{: style="color: DarkRed"}  
    _Tipping and Bishop (1999)_ then show an *__iterative__* __EM__ algorithm for estimating the parameters $$\boldsymbol{W}$$ and $$\sigma^{2}$$.  

    __Relation to PCA - Limit Analysis:__{: style="color: DarkRed"}  
    _Tipping and Bishop (1999)_ show that probabilistic PCA becomes $$\mathrm{PCA}$$ as $$\sigma \rightarrow 0$$.  
    In that case, the conditional expected value of $$\boldsymbol{h}$$ given $$\boldsymbol{x}$$ becomes an orthogonal projection of $$\boldsymbol{x} - \boldsymbol{b}$$  onto the space spanned by the $$d$$ columns of $$\boldsymbol{W}$$, like in PCA.  

    As $$\sigma \rightarrow 0,$$ the density model defined by probabilistic PCA becomes very sharp around these $$d$$ dimensions spanned by the columns of $$\boldsymbol{W}$$.  
    This can make the model assign very low likelihood to the data if the data does not actually cluster near a hyperplane.  


    __PPCA vs Factor Analysis:__{: style="color: red"}  
    {: #lst-p}
    * Covariance
        * __PPCA__ (& PCA) is covariant under rotation of the original data axes
        * __Factor analysis__ is covariant under component-wise rescaling
    * Principal components (or factors)
        * __PPCA__: different principal components (axes) can be found incrementally
        * __Factor analysis__: factors from a two-factor model may not correspond to those from a one-factor model


    __Manifold Interpretation of PCA:__{: style="color: red"}  
    Linear factor models including PCA and factor analysis can be interpreted as <span>learning a __manifold__</span>{: style="color: goldenrod"} _(Hinton et al., 1997)_.  
    We can view __PPCA__ as <span>defining a __thin pancake-shaped region of high probability__</span>{: style="color: purple"}‚Äîa __Gaussian distribution__ that is very narrow along some axes, just as a pancake is very flat along its vertical axis, but is elongated along other axes, just as a pancake is wide along its horizontal axes.  
    <button>Illustration</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/ayo7yf-CBpBA38gdTlc0sCPWk9gG0PhFSpXzfSOp1HU.original.fullsize.png){: width="100%" hidden=""}  
    __PCA__ can be interpreted as <span>aligning this pancake with a linear manifold in a higher-dimensional space</span>{: style="color: goldenrod"}.  
    This interpretation applies not just to traditional PCA but also to any __linear autoencoder__ that learns matrices $$\boldsymbol{W}$$ and $$\boldsymbol{V}$$ with the goal of making the reconstruction of $$x$$ lie as close to $$x$$ as possible:  
    {: #lst-p}
    * Let the __Encoder__ be:  
        <p>$$\boldsymbol{h}=f(\boldsymbol{x})=\boldsymbol{W}^{\top}(\boldsymbol{x}-\boldsymbol{\mu})$$</p>  
        The encoder computes a __low-dimensional representation__ of $$h$$.  
    * With the __autoencoder view__, we have a __decoder__ computing the *__reconstruction__*:  
        <p>$$\hat{\boldsymbol{x}}=g(\boldsymbol{h})=\boldsymbol{b}+\boldsymbol{V} \boldsymbol{h}$$</p>  
    * The choices of linear encoder and decoder that minimize __reconstruction error__:  
        <p>$$\mathbb{E}\left[\|\boldsymbol{x}-\hat{\boldsymbol{x}}\|^{2}\right]$$</p>  
        correspond to $$\boldsymbol{V}=\boldsymbol{W}, \boldsymbol{\mu}=\boldsymbol{b}=\mathbb{E}[\boldsymbol{x}]$$ and the columns of $$\boldsymbol{W}$$ form an orthonormal basis which spans the same subspace as the principal eigenvectors of the covariance matrix:  
        <p>$$\boldsymbol{C}=\mathbb{E}\left[(\boldsymbol{x}-\boldsymbol{\mu})(\boldsymbol{x}-\boldsymbol{\mu})^{\top}\right]$$</p>  
    * In the case of PCA, the columns of $$\boldsymbol{W}$$ are these eigenvectors, ordered by the magnitude of the corresponding eigenvalues (which are all real and non-negative).  
    * __Variances:__  
        One can also show that eigenvalue $$\lambda_{i}$$ of $$\boldsymbol{C}$$ corresponds to the variance of $$x$$ in the direction of eigenvector $$\boldsymbol{v}^{(i)}$$.  
    * __Optimal Reconstruction:__  
        * If $$\boldsymbol{x} \in \mathbb{R}^{D}$$ and $$\boldsymbol{h} \in \mathbb{R}^{d}$$ with $$d<D$$, then the <span>__*optimal*__ __reconstruction error__</span>{: style="color: goldenrod"}  (choosing $$\mu, b, V$$ and $$W$ as above) is:  
            <p>$$\min \mathbb{E}\left[\|\boldsymbol{x}-\hat{\boldsymbol{x}}\|^{2}\right]=\sum_{i=d+1}^{D} \lambda_{i}$$</p>  
        * Hence, if the __covariance__ has *__rank__* $$d,$$ the __eigenvalues__ $$\lambda_{d+1}$$ to $$\lambda_{D}$$ are $$0$$ and __reconstruction error__ is $$0$.  
        * Furthermore, one can also show that the above solution can be obtained by <span>__*maximizing* the variances of the elements__ of $$\boldsymbol{h},$$ under *orthogonal* $$\boldsymbol{W}$$</span>{: style="color: purple"}, instead of *__minimizing reconstruction error__*.  



    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * [PPCA - Probabilistic PCA Slides](https://people.cs.pitt.edu/~milos/courses/cs3750-Fall2007/lectures/class17.pdf)  /  [PPCA Better Slides](https://www.cs.ubc.ca/~schmidtm/Courses/540-W16/L12.pdf)  
    * [Probabilistic PCA (Original Paper!)](http://www.robots.ox.ac.uk/~cvrg/hilary2006/ppca.pdf)  
    * EM Algorithm for PCA is more advantageous than MLE (closed form).  
    * __Mixtures of probabilistic PCAs__: can be defined and are a combination of local probabilistic PCA models.  
    * PCA can be generalized to the __nonlinear Autoencoders__.  
    * ICA can be generalized to a __nonlinear generative model__, in which we use a nonlinear function $$f$$ to generate the observed data.  
    <br>

4. **Independent Component Analysis (ICA):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  

5. **Slow Feature Analysis:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}

6. **Sparse Coding:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    __Sparse Coding__ _(Olshausen and Field, 1996)_ is a *__linear factor model__* that has been heavily studied as an <span>unsupervised feature learning</span>{: style="color: purple"} and <span>feature extraction</span>{: style="color: purple"} mechanism.  
    In Sparse Coding the *__noise distribution__* is <span>__Gaussian noise__</span>{: style="color: purple"} with <span>isotropic precision</span>{: style="color: purple"} $$\beta$$:  
    <p>$$p(\boldsymbol{x} \vert \boldsymbol{h})=\mathcal{N}\left(\boldsymbol{x} ; \boldsymbol{W} \boldsymbol{h}+\boldsymbol{b}, \frac{1}{\beta} \boldsymbol{I}\right)$$</p>  

    The *__latent variable prior__* $$p(\boldsymbol{h})$$ is chosen to be one with sharp peaks near $$0$$.  
    Common choices include:  
    {: #lst-p}
    * __factorized Laplace__:  
        <p>$$p\left(h_{i}\right)=$ Laplace $\left(h_{i} ; 0, \frac{2}{\lambda}\right)=\frac{\lambda}{4} e^{-\frac{1}{2} \lambda\left|h_{i}\right|}$$</p>  
    * __factorized Student-t distributions__:  
        <p>$$p\left(h_{i}\right) \propto \frac{1}{\left(1+\frac{h_{i}^{2}}{\nu}\right)^{\frac{\nu+1}{2}}}$$</p>  
    * __Cauchy__  

    __Learning/Training:__{: style="color: red"}  
    {: #lst-p}
    * Training sparse coding with __maximum likelihood__ is __*intractable*__{: style="color: purple"}.  
    * Instead, the training _alternates_ between <span>__encoding__ the data</span>{: style="color: purple"} and <span>training the decoder to __better reconstruct the data__ given the encoding</span>{: style="color: purple"}.  
        This is a [principled approximation to Maximum-Likelihood](/work_files/research/dl/concepts/inference#bodyContents15map_sc).  
        * Minimization wrt. $$\boldsymbol{h}$$ 
        * Minimization wrt. $$\boldsymbol{W}$$ 

    __Architecture:__{: style="color: red"}  
    {: #lst-p}
    * __Encoder__:  
        * <span>Non-parametric</span>{: style="color: purple"}.  
        * It is an <span>optimization algorithm</span>{: style="color: goldenrod"} that solves an __optimization problem__ in which we seek the <span>*__single most likely code value__*</span>{: style="color: purple"}:  
            <p>$$\boldsymbol{h}^{* }=f(\boldsymbol{x})=\underset{\boldsymbol{h}}{\arg \max } p(\boldsymbol{h} vert \boldsymbol{x})$$</p>   
            * Assuming a __Laplace Prior__ on $$p(\boldsymbol{h})$$:  
                <p>$$\boldsymbol{h}^{* }=\underset{h}{\arg \min } \lambda\|\boldsymbol{h}\|_{1}+\beta\|\boldsymbol{x}-\boldsymbol{W h}\|_{2}^{2}$$</p>  
                where we have taken a log, dropped terms not depending on $$\boldsymbol{h}$$, and divided by positive scaling factors to simplify the equation.  
            * __Hyperparameters:__  
                Both $$\beta$$ and $$\lambda$$ are hyperparameters.  
                However, $$\beta$$ is usually set to $$1$$ because its role is shared with $$\lambda$$.  
                It could also be treated as a parameter of the model and _"learned"_[^2].  


    __Variations:__{: style="color: red"}  
    Not all approaches to sparse coding explicitly build a $$p(\boldsymbol{h})$$ and a $$p(\boldsymbol{x} \vert \boldsymbol{h})$$.  
    Often we are just interested in learning a dictionary of features with activation values that will often be zero when extracted using this inference procedure.  


    __Sparsity:__{: style="color: red"}  
    {: #lst-p}
    * Due to the imposition of an $$L^{1}$$ norm on $$\boldsymbol{h},$$ this procedure will yield a sparse $$\boldsymbol{h}^{* }$$.  
    * If we sample $$\boldsymbol{h}$$ from a Laplace prior, it is in fact a <span>zero probability event</span>{: style="color: purple"} for an element of $$\boldsymbol{h}$$ to actually be zero.  
        <span>The __generative model__ itself is *__not__* especially __sparse__, *__only__* the __feature extractor__ is</span>{: style="color: purple"}.  
        * _Goodfellow et al. (2013d)_ describe approximate inference in a different model family, the spike and slab sparse coding model, for which samples from the prior usually contain true zeros.  


    __Properties:__{: style="color: red"}  
    {: #lst-p}
    * <span>__Advantages:__</span>{: style="color: purple"}  
        * The sparse coding approach combined with the use of the *__non-parametric__* __encoder__  can in principle minimize the combination of reconstruction error and log-prior better than any specific parametric encoder.  
        * Another advantage is that there is no generalization error to the encoder.  
            Thus, resulting in better generalization when sparse coding is used as a feature extractor for a classifier than when a parametric function is used to predict the code.  
            * A parametric encoder must learn how to map $$\boldsymbol{x}$$ to $$\boldsymbol{h}$$ in a way that generalizes. For unusual $$\boldsymbol{x}$$ that do not resemble the training data, a learned, parametric encoder may fail to find an $$\boldsymbol{h}$$ that results in accurate reconstruction or a sparse code.  
            * For the vast majority of formulations of sparse coding models, where the inference problem is convex, the optimization procedure will always find the optimal code (unless degenerate cases such as replicated weight vectors occur).  
            * Obviously, the sparsity and reconstruction costs can still rise on unfamiliar points, but this is due to generalization error in the decoder weights, rather than generalization error in the encoder.  
            * Thus, the lack of generalization error in sparse coding‚Äôs optimization-based encoding process may result in better generalization when sparse coding is used as a feature extractor for a classifier than when a parametric function is used to predict the code.  
                <button>Results</button>{: .showText value="show" onclick="showText_withParent_PopHide(event);"}
                * _Coates and Ng (2011)_ demonstrated that sparse coding features generalize better for object recognition tasks than the features of a related model based on a parametric encoder, the linear-sigmoid autoencoder.  
                * _Goodfellow et al. (2013d)_ showed that a variant of sparse coding generalizes better than other feature extractors in the regime where extremely few labels are available (twenty or fewer labels per class).  
                {: hidden=""}
    * <span>__Disadvantages:__</span>{: style="color: purple"}  
        * The primary disadvantage of the __*non-parametric* encoder__ is that it requires greater time to compute $$\boldsymbol{h}$$ given $$\boldsymbol{x}$$ because the non-parametric approach requires running an iterative algorithm.  
            * The parametric autoencoder approach uses only a fixed number of layers, often only one.  
        * It is not straight-forward to back-propagate through the non-parametric encoder: which makes it difficult to pretrain a sparse coding model with an unsupervised criterion and then fine-tune it using a supervised criterion.  
            * Modified versions of sparse coding that permit approximate derivatives do exist but are not widely used _(Bagnell and Bradley, 2009)_.  

    __Generation (Sampling):__{: style="color: red"}  
    {: #lst-p}
    * Sparse coding, like other linear factor models, often produces poor samples.  
        <button>Examples</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](https://cdn.mathpix.com/snip/images/dnnZUyAMmMg__pGi1O1os8yQXGUu0lY3LcpuWtWKTok.original.fullsize.png){: width="100%" hidden=""}  
    * This happens even when the model is able to reconstruct the data well and provide useful features for a classifier.  
        * The __reason__ is that <span>each individual feature may be learned well</span>{: style="color: purple"}, but the <span>__factorial prior__ on the __hidden code__ results in the model including __*random* subsets__ of __*all*__ of the __features__ in each generated sample</span>{: style="color: goldenrod"}.  
    * __Motivating Deep Models:__{: style="color: red"}  
        This motivates the development of deeper models that can <span>impose a __non-factorial distribution__ on the *__deepest code layer__*</span>{: style="color: goldenrod"}, as well as the development of more sophisticated shallow models.  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * [Sparse Coding (Hugo Larochelle!)](https://www.youtube.com/watch?v=7a0_iEruGoM&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&index=60)  
    <br>




[^1]: __Probabilistic Models__, with __latent variables__.  
[^2]: some terms depending on $$\beta$$ omitted from above equation\* which are needed to learn $$\beta$$.  

***
***

TITLE: Variational Auto-Encoders
LINK: research/dl/archt/vae.md



__Resources:__{: style="color: red"}  
{: #lst-p}
* [VAEs (pdf)](https://deepgenerativemodels.github.io/notes/vae/)  
* [Scalable semi-supervised learning with deep variational autoencoders (Code)](https://github.com/clinicalml/vae_ssl)  
* [Tutorial: Categorical Variational Autoencoders using Gumbel-Softmax (+Code)](https://blog.evjang.com/2016/11/)  
* [Variational Autoencoders Pursue PCA Directions [by Accident] (paper!)](https://arxiv.org/abs/1812.06775)  
* [Latent Variable Models and AutoEncoders (Intuition Blog!)](https://medium.com/datadriveninvestor/latent-variable-models-and-autoencoders-97c44858caa0)  
* [Tutorial: Categorical Variational Autoencoders using Gumbel-Softmax (Kingma)](https://blog.evjang.com/2016/11/)  



## Variational Auto-Encoders
{: #content4}

[__Auto-Encoders__](http://ahmedbadary.ml/work_files/research/dl/aencdrs) (_click to read more_) are unsupervised learning methods that aim to learn a representation (encoding) for a set of data in a smaller dimension.  
Auto-Encoders generate __Features__ that capture _factors of variation_ in the training data.

0. **Auto-Regressive Models VS Variational Auto-Encoders:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents40}  
    :   __Auto-Regressive Models__ defined a *__tractable__* (discrete) density function and, then, optimized the likelihood of training data:   
    :   $$p_\theta(x) = p(x_0) \prod_1^n p(x_i | x_{i<})$$  
    :   On the other hand, __VAEs__ defines an *__intractable__* (continuous) density function with latent variable $$z$$:  
    :   $$p_\theta(x) = \int p_\theta(z) p_\theta(x|z) dz$$
    :   but cannot optimize directly; instead, derive and optimiz a lower bound on likelihood instead.  

1. **Variational Auto-Encoders (VAEs):**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    :   __Variational Autoencoder__ models inherit the autoencoder architecture, but make strong assumptions concerning the distribution of latent variables.  
    :   They use variational approach for latent representation learning, which results in an additional loss component and specific training algorithm called Stochastic Gradient Variational Bayes (SGVB).  

2. **Assumptions:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}  
    :   VAEs assume that: 
        * The data is generated by a directed __graphical model__ $$p(x\vert z)$$ 
        * The encoder is learning an approximation $$q_\phi(z|x)$$ to the posterior distribution $$p_\theta(z|x)$$  
            where $${\displaystyle \mathbf {\phi } }$$ and $${\displaystyle \mathbf {\theta } }$$ denote the parameters of the encoder (recognition model) and decoder (generative model) respectively.  
        * The training data $$\left\{x^{(i)}\right\}_{i=1}^N$$ is generated from underlying unobserved (latent) representation $$\mathbf{z}$$

3. **The Objective Function:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}    
    <p>$${\displaystyle {\mathcal {L}}(\mathbf {\phi } ,\mathbf {\theta } ,\mathbf {x} )=D_{KL}(q_{\phi }(\mathbf {z} |\mathbf {x} )||p_{\theta }(\mathbf {z} ))-\mathbb {E} _{q_{\phi }(\mathbf {z} |\mathbf {x} )}{\big (}\log p_{\theta }(\mathbf {x} |\mathbf {z} ){\big )}}$$</p>  
    where $${\displaystyle D_{KL}}$$ is the __Kullback‚ÄìLeibler divergence__ (KL-Div).  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * $$\boldsymbol{z}$$ is some latent vector (representation); where each element is capturing how much of some factor of variation that we have in our training data.  
        e.g. attributes, orientations, position of certain objects, etc.  

4. **The Generation Process:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents44}  
    ![img](/main_files/cs231n/13/5.png){: width="40%"}  


5. **The Goal:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents45}  
    The goal is to estimate the true parameters $$\theta^\ast$$ of this generative model.  
    <br>

6. **Representing the Model:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents46}  
    \- To represent the __prior $$p(z)$$__, we choose it to be simple, usually __Gaussian__  
    \- To represent the __conditional $$p_{\theta^{*}}\left(x | z^{(i)}\right)$$__  (which is very complex), we use a neural-network  
    <br>

7. **Intractability:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents47}  
    The __Data Likelihood__:  
    <p>$$p_\theta(x) = \int p_\theta(z) p_\theta(x|z) dz$$</p>  
    is intractable to compute for every $$z$$.  

    Thus, the __Posterior Density__:  
    <p>$$p_\theta(z|x) = \dfrac{p_\theta(x|z) p_\theta(z)}{p_\theta(x)} = \dfrac{p_\theta(x|z) p_\theta(z)}{\int p_\theta(z) p_\theta(x|z) dz}$$</p>   
    is, also, intractable.
    <br>

8. **Dealing with Intractability:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents48}  
    In addition to decoder network modeling $$p_\theta(x\vert z)$$, define additional encoder network $$q_\phi(z\vert x)$$ that approximates $$p_\theta(z\vert x)$$.  
    This allows us to derive a __lower bound__ on the data likelihood that is _tractable_, which we can optimize.
    <br>

9. **The Model:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents49}  
    The __Encoder__ (recognition/inference) and __Decoder__ (generation) networks are probabilistic and output means and variances of each the conditionals respectively:  
    ![img](/main_files/cs231n/13/6.png){: width="70%"}   
    
    The __generation (forward-pass)__ is done via *__sampling__* as follows:  
    ![img](/main_files/cs231n/13/7.png){: width="72%"}   


10. **The Log-Likelihood of Data:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents410}  
    __Deriving the Log-Likelihood:__  
    ![img](/main_files/cs231n/13/8.png){: width="100%"}   

11. **Training:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents411}  
    __Computing the bound (forward pass) for a given minibatch of input data:__  
    ![img](https://cdn.mathpix.com/snip/images/AqnvjBWU8hztHMIccTxpe8mMqP4GTJ6b5Rfv79r8Lzk.original.fullsize.png){: width="80%"}  

12. **Generation:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents412}  
    ![img](https://cdn.mathpix.com/snip/images/UvH6YyerATQDxb9M3fUbaWhbi9axEdZ5Z-1w_HCR0zM.original.fullsize.png){: width="40%"}  

    \- Diagonal prior on $$\boldsymbol{z} \implies$$ independent latent variables  
    \- Different dimensions of $$\boldsymbol{z}$$ encode interpretable factors of variation  
    {: #lst-p}
    * Also good feature representation that can be computed using $$\mathrm{q}_ {\phi}(\mathrm{z} \vert \mathrm{x})$$!  

    __Examples:__  
    {: #lst-p} 
    * __MNIST:__  
        ![img](https://cdn.mathpix.com/snip/images/B0FCHMcL0yTM4eJ7si-sE23Z8GEoXDh7XU1XVstRzZ0.original.fullsize.png){: width="30%"}  
    * __CelebA__:  
        ![img](/main_files/cs231n/13/9.png){: width="30%"}  

13. **Pros, Cons and Research:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents413}  
    * __Pros__:  
        * Principled approach to generative models
        * Allows inference of $$q(z\vert x)$$, can be useful feature representation for other tasks  
    * __Cons__:  
        * Maximizing the lower bound of likelihood is okay, but not as good for evaluation as Auto-regressive models
        * Samples blurrier and lower quality compared to state-of-the-art (GANs)  
    * __Active areas of research__:   
        * More flexible approximations, e.g. richer approximate posterior instead of diagonal Gaussian
        * Incorporating structure in latent variables, e.g., Categorical Distributions  

***
***

TITLE: CNNs <br /> Convolutional Neural Networks
LINK: research/dl/archt/convnets.md



[CNNs in CV](/work_files/research/dl/cnnx)  
[CNNs in NLP](/work_files/research/dl/nlp/cnnsNnlp)  
[CNNs Architectures](/work_files/research/dl/arcts)  
[Convnet Ch.9 Summary (blog)](https://medium.com/inveterate-learner/deep-learning-book-chapter-9-convolutional-networks-45e43bfc718d)  




## Introduction
{: #content1}

1. **CNNs:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}   
    In machine learning, a convolutional neural network (CNN, or ConvNet) is a class of deep, feed-forward artificial neural networks that has successfully been applied to analyzing visual imagery.  
    In general, it works on data that have _grid-like topology._  
    > E.g. Time-series data (1-d grid w/ samples at regular time intervals), image data (2-d grid of pixels).  

    Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.  


2. **The Big Idea:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}   
    CNNs use a variation of multilayer Perceptrons designed to require minimal preprocessing. In particular, they use the [Convolution Operation](#bodyContents31).   
    The Convolution leverage _three important ideas_ that can help improve a machine learning system:  
    1. __Sparse Interactions/Connectivity/Weights:__  
        Unlike FNNs, where every input unit is connected to every output unit, CNNs have sparse interactions. This is accomplished by making the kernel smaller than the input.  
        __Benefits:__   
        * This means that we need to _store fewer parameters_, which both,  
            * _Reduces the memory requirements_ of the model and  
            * _Improves_ its _statistical efficiency_  
        * Also, Computing the output requires fewer operations  
        * In deep CNNs, the units in the deeper layers interact indirectly with large subsets of the input which allows modelling of complex interactions through sparse connections.  

        > These improvements in efficiency are usually quite large.  
        If there are $$m$$ inputs and $$n$$ outputs, then matrix multiplication requires $$m \times n$$ parameters, and the algorithms used in practice have $$\mathcal{O}(m \times n)$$ runtime (per example). If we limit the number of connections each output may have to $$k$$, then the sparsely connected approach requires only $$k \times n$$ parameters and $$\mathcal{O}(k \times n)$$ runtime.   
        
        <button>Figure: Sparse Connectivity from Below</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](/main_files/dl/cnn/14.png){: width="70%" hidden=""}  
        <button>Figure: Sparse Connectivity from Above</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](/main_files/dl/cnn/15.png){: width="70%" hidden=""}  

    2. __Parameter Sharing:__   
        refers to using the same parameter for more than one function in a model.  

        __Benefits:__{: style="color: red"}  
        {: #lst-p}
        * This means that rather than learning a separate set of parameters for every location, we _learn only one set of parameters_.  
            * This does not affect the runtime of forward propagation‚Äîit is still $$\mathcal{O}(k \times n)$$  
            * But it does further reduce the storage requirements of the model to $$k$$ parameters ($$k$$ is usually several orders of magnitude smaller than $$m$$)  

        Convolution is thus dramatically more efficient than dense matrix multiplication in terms of the memory requirements and statistical efficiency.  
        <button>Figure: Parameter Sharing</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](/main_files/dl/cnn/16.png){: width="70%" hidden=""}  

    3. __Equivariant Representations:__  
        For convolutions, the particular form of parameter sharing causes the layer to have a property called __equivariance to translation__.   
        > A function is __equivariant__ means that if the input changes, the output changes in the same way.  
            Specifically, a function $$f(x)$$ is equivariant to a function $$g$$ if $$f(g(x)) = g(f(x))$$.   

        Thus, if we move the object in the input, its representation will move the same amount in the output.  
        
        __Benefits:__{: style="color: red"}  
        {: #lst-p}  
        * It is most useful when we know that some function of a small number of neighboring pixels is useful when applied to multiple input locations (e.g. edge detection)  
        * Shifting the position of an object in the input doesn't confuse the NN  
        * Robustness against translated inputs/images   

        Note: Convolution is __not__ naturally equivariant to some other transformations, such as _changes in the scale_ or _rotation_ of an image.  


    Finally, the convolution provides a means for working with __inputs of variable sizes__ (i.e. data that cannot be processed by neural networks defined by matrix multiplication with a fixed-shape matrix).  

3. **Inspiration Model:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    Convolutional networks were inspired by biological processes in which the connectivity pattern between neurons is inspired by the organization of the animal visual cortex.  
    Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field. 


***

## Architecture and Design
{: #content2}


0. **Design:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents20}  
    A CNN consists of an input and an output layer, as well as multiple hidden layers.  
    The hidden layers of a CNN typically consist of convolutional layers, pooling layers, fully connected layers and normalization layers.  

1. **Volumes of Neurons:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}   
    Unlike neurons in traditional Feed-Forward networks, the layers of a ConvNet have neurons arranged in 3-dimensions: **width, height, depth**.  
    > Note: __Depth__ here refers to the third dimension of an activation volume, not to the depth of a full Neural Network, which can refer to the total number of layers in a network.  

2. **Connectivity:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}   
    The neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner.

3. **Functionality:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}   
    A ConvNet is made up of Layers.  
    Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.  
    
    ![img](/main_files/dl/cnn/1.png){: width="100%"}

    
4. **Layers:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}   
    We use three main types of layers to build ConvNet architectures:  
    * Convolutional Layer:  
        * Convolution (Linear Transformation)   
        * Activation (Non-Linear Transformation; e.g. ReLU)  
            > Known as __Detector Stage__  
    * Pooling Layer  
    * Fully-Connected Layer  

41. **Process:**{: style="color: SteelBlue"}{: .bodyContents2  #bodyContents241}  
    :   ConvNets transform the original image layer by layer from the original pixel values to the final class scores. 

5. **Example Architecture (CIFAR-10):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    :   Model: [INPUT - CONV - RELU - POOL - FC]
    :   * **INPUT:** [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.   
        * **CONV-Layer** will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume.    
        This may result in volume such as [$$32\times32\times12$$] if we decided to use 12 filters.  
        * **RELU-Layer:**  will apply an element-wise activation function, thresholding at zero. This leaves the size of the volume unchanged ([$$32\times32\times12$$]).  
        * **POOL-Layer:** will perform a down-sampling operation along the spatial dimensions (width, height), resulting in volume such as [$$16\times16\times12$$].  
        * **Fully-Connected:** will compute the class scores, resulting in volume of size [$$1\times1\times10$$], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10.  
        As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.

6. **Fixed Functions VS Hyper-Parameters:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    :   Some layers contain parameters and other don‚Äôt.
    :   * **CONV/FC layers** perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons).
    :   * **RELU/POOL** layers will implement a fixed function. 
    :   > The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image.  

7. **[Summary](http://cs231n.github.io/convolutional-networks/):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  
    * A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores)  
    * There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)  
    * Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function  
    * Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don‚Äôt)  
    * Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn‚Äôt)  
> [Click this for Credits](http://cs231n.github.io/convolutional-networks/)  

    ![img](/main_files/dl/cnn/2.png){: width="100%"}


***

## The Convolutional Layer
{: #content3}

1. **Convolutions:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}   
    In its most general form, the convolution is a __Linear Operation__ on two functions of real-valued arguments.  

    Mathematically, a __Convolution__ is a mathematical operation on two functions ($$f$$ and $$g$$) to produce a third function, that is typically viewed as a modified version of one of the original functions, giving the integral of the point-wise multiplication of the two functions as a function of the amount that one of the original functions is translated.  
    The convolution could be thought of as a __weighting function__ (e.g. for taking the weighted average of a series of numbers/function-outputs).  

    The convolution of the __continuous__ functions $$f$$ and $$g$$:  
    <p>$${\displaystyle {\begin{aligned}(f * g)(t)&\,{\stackrel {\mathrm {def} }{=}}\ \int _{-\infty }^{\infty }f(\tau )g(t-\tau )\,d\tau \\&=\int_{-\infty }^{\infty }f(t-\tau )g(\tau )\,d\tau .\end{aligned}}}$$</p>  

    The convolution of the __discreet__ functions f and g: 
    <p>$${\displaystyle {\begin{aligned}(f * g)[n]&=\sum_{m=-\infty }^{\infty }f[m]g[n-m]\\&=\sum_{m=-\infty }^{\infty }f[n-m]g[m].\end{aligned}}} (commutativity)$$</p>  
    In this notation, we refer to:  
    * The function $$f$$ as the __Input__  
    * The function $$g$$ as the __Kernel/Filter__  
    * The output of the convolution as the __Feature Map__  

    __Commutativity:__  
    Can be achieved by flipping the kernel with respect to the input; in the sense that as increases, the index into the $$m$$ input increases, but the index into the kernel decreases.  
    While the commutative property is useful for writing proofs, it is not usually an important property of a neural network implementation.  
    Moreover, in a CNN, the convolution is used simultaneously with other functions, and the combination of these functions __does not commute__ regardless of whether the convolution operation flips its kernel or not.  
    Because convolutional networks usually use multichannel convolution, the linear operations they are based on are not guaranteed to be commutative, even if kernel flipping is used. These multichannel operations are only commutative if each operation has the same number of output channels as input channels.

                

2. **Cross-Correlation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}   
    :   Cross-Correlation is a measure of similarity of two series as a function of the displacement of one relative to the other.
    :   The __continuous__ cross-correlation on continuous functions f and g:  
    :   $$(f\star g)(\tau )\ {\stackrel {\mathrm {def} }{=}}\int _{-\infty }^{\infty }f^{*}(t)\ g(t+\tau )\,dt,$$
    :   The __discrete__ cross-correlation on discreet functions f and g:  
    :   $$(f\star g)[n]\ {\stackrel {\mathrm {def} }{=}}\sum _{m=-\infty }^{\infty }f^{*}[m]\ g[m+n].$$  

3. **Convolutions and Cross-Correlation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}   
    * Convolution is similar to cross-correlation.  
    * _For discrete real valued signals_, they differ only in a time reversal in one of the signals.  
    * _For continuous signals_, the cross-correlation operator is the **adjoint operator** of the convolution operator.

4. **CNNs, Convolutions, and Cross-Correlation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}   
    The term Convolution in the name "Convolution Neural Network" is unfortunately a __misnomer__.  
    CNNs actually __use Cross-Correlation__ instead as their similarity operator.  
    The term 'convolution' has stuck in the name by convention.  


15. **Convolution in DL:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents315}   
    The Convolution operation:  
    <p>$$s(t)=(x * w)(t)=\sum_{a=-\infty}^{\infty} x(a) w(t-a)$$</p>   
    we usually assume that these functions are zero everywhere but in the finite set of points for which we store the values.  


16. **Convolution Over Two Axis:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents316}   
    If we use a two-dimensional image $$I$$ as our input, we probably also want to use a two-dimensional kernel $$K$$:  
    <p>$$S(i, j)=(I * K)(i, j)=\sum_{m} \sum_{n} I(m, n) K(i-m, j-n)$$</p>  

    In practice we use the following formula instead (commutativity):  
    <p>$$S(i, j)=(K * I)(i, j)=\sum_{m} \sum_{n} I(i-m, j-n) K(m, n)$$</p>  
    Usually the latter formula is more straightforward to implement in a machine learning library, because there is less variation in the range of valid values of $$m$$ and $$n$$.  


    The Cross-Correlation is usually implemented by ML-libs:  
    <p>$$S(i, j)=(K * I)(i, j)=\sum_{m} \sum_{n} I(i+m, j+n) K(m, n)$$</p>  


17. **The Mathematics of the Convolution Operation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents317}    
    * The operation can be broken into matrix multiplications using the Toeplitz matrix representation for 1D and block-circulant matrix for 2D convolution:  
        * __Discrete convolution__ can be viewed as __multiplication by a matrix__, but the matrix has several entries constrained to be equal to other entries.  
        > For example, for __univariate discrete convolution__, each row of the matrix is constrained to be equal to the row above shifted by one element. This is known as a *__Toeplitz matrix__*.  
            A __Toeplitz matrix__ has the property that values along all diagonals are constant.  

        
            ![img](/main_files/dl/cnn/20.png){: width="80%"}  
        * In __two dimensions__, a __doubly block circulant matrix__ corresponds to convolution.  
            > A matrix which is circulant with respect to its sub-matrices is called a __block circulant matrix__. If each of the submatrices is itself circulant, the matrix is called __doubly block-circulant matrix__.  
                
                ![img](/main_files/dl/cnn/21.png){: width="60%"}  
    * Convolution usually corresponds to a __very sparse matrix__ (a matrix whose entries are mostly equal to zero).  
        This is because the kernel is usually much smaller than the input image.  
    * Any neural network algorithm that works with matrix multiplication and does not depend on specific properties of the matrix structure should work with convolution, without requiring any further changes to the neural network.  
    * Typical convolutional neural networks do make use of further specializations in order to deal with large inputs efficiently, but these are not strictly necessary from a theoretical perspective.  


5. **The Convolution operation in a CONV Layer:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}   
    * The CONV layer‚Äôs __parameters__ consist of __a set of learnable filters__.  
        * Every filter is small spatially (along width and height), but extends through the full depth of the input volume.  
        >  For example, a typical filter on a first layer of a ConvNet might have size 5x5x3 (i.e. 5 pixels width and height, and 3 because images have depth 3, the color channels).  
    * In the __forward pass__, we slide (convolve) each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position.  
        * As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position.  
        > Intuitively, the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the first layer, or eventually entire honeycomb or wheel-like patterns on higher layers of the network. 
        * Now, we will have an entire set of filters in each CONV layer (e.g. 12 filters), and each of them will produce a separate 2-dimensional activation map.   
    * We will __stack__ these activation maps along the depth dimension and produce the output volume.  

    <p style="color: red">As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input. </p>    
    
6. **The Brain Perspective:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    Every entry in the 3D output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially.  

7. **Local Connectivity:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  
    * Convolutional networks exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers: 
        * Each neuron is connected to only a small region of the input volume.
    * The __Receptive Field__ of the neuron defines the extent of this connectivity as a hyperparameter.  
    >  For example, suppose the input volume has size $$[32\times32\times3]$$ and the receptive field (or the filter size) is $$5\times5$$, then each neuron in the Conv Layer will have weights to a $$[5\times5\times3]$$ region in the input volume, for a total of $$5*5*3 = 75$$ weights (and $$+1$$ bias parameter).  

    <p style="color: red">Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.</p>

8. **Spatial Arrangement:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}  
    There are __three__ hyperparameters control the size of the output volume:  
    1. __The Depth__ of the output volume is a hyperparameter that corresponds to the number of filters we would like to use (each learning to look for something different in the input).  
    2. __The Stride__ controls how depth columns around the spatial dimensions (width and height) are allocated.  
        > e.g. When the stride is 1 then we move the filters one pixel at a time.  

        * The __Smaller__ the stride, the __more overlapping regions__ exist and the __bigger the volume__.  
        * The __bigger__ the stride, the __less overlapping regions__ exist and the         __smaller the volume__.  

    3. The __Padding__ is a hyperparameter whereby we pad the input the input volume with zeros around the border.   
        This allows to _control the spatial size_ of _the output_ volumes.  


9. **The Spatial Size of the Output Volume:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents29}  
    We compute the spatial size of the output volume as a function of:  
    * **$$W$$**: The input volume size.  
    * **$$F$$**: $$\:\:$$The receptive field size of the Conv Layer neurons.  
    * **$$S$$**: The stride with which they are applied.  
    * **$$P$$**: The amount of zero padding used on the border.  
    Thus, the __Total Size of the Output__:  
    <p>$$\dfrac{W‚àíF+2P}{S} + 1$$</p>  

    __Potential Issue__: If this number is not an integer, then the strides are set incorrectly and the neurons cannot be tiled to fit across the input volume in a symmetric way.  
    * __Fix__: In general, setting zero padding to be $${\displaystyle P = \dfrac{K-1}{2}}$$ when the stride is $${\displaystyle S = 1}$$ ensures that the input volume and output volume will have the same size spatially.  

10. **Calculating the Number of Parameters:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents210}   
    Given:  
    * __Input Volume__:  $$32\times32\times3$$  
    * __Filters__:  $$10 5\times5$$  
    * __Stride__:  $$1$$  
    * __Pad__:  $$2$$  
    
    The number of parameters equals the number of parameters in each filter $$ = 5*5*3 + 1 = 76$$ (+1 for __bias__) times the number of filters $$ 76 * 10 = 760$$.  
            


11. **The Convolution Layer:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents311}   
    ![img](/main_files/dl/cnn/3.png){: width="70%"}  
    ![img](/main_files/dl/cnn/8.png){: width="70%"}  

    __The Conv Layer and the Brain:__  
    ![img](/main_files/dl/cnn/10.png){: width="70%"}  
    ![img](/main_files/dl/cnn/11.png){: width="70%"}  
    ![img](/main_files/dl/cnn/12.png){: width="70%"}  


12. **From FC-layers to Conv-layers:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents312}   
    ![img](/main_files/dl/cnn/4.png){: width="70%"}  
    ![img](/main_files/dl/cnn/5.png){: width="70%"}  
    ![img](/main_files/dl/cnn/6.png){: width="70%"}  


13. **$$1\times1$$ Convolutions:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents313}   
    ![img](/main_files/dl/cnn/9.png){: width="70%"}  



0. **Notes:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents30}   
    * __Summary__:  
        * ConvNets stack CONV,POOL,FC layers 
        * Trend towards smaller filters and deeper architectures 
        * Trend towards getting rid of POOL/FC layers (just CONV) 
        * Typical architectures look like [(CONV-RELU) * N-POOL?] * M-(FC-RELU) * K, SOFTMAX  
            where $$N$$ is usually up to \~5, $$M$$ is large, $$0 <= K <= 2$$.  
            But recent advances such as ResNet/GoogLeNet challenge this paradigm 
    * __Effect of Different Biases__:  
        Separating the biases may slightly reduce the statistical efficiency of the model, but it allows the model to correct for differences in the image statistics at different locations. For example, when using implicit zero padding, detector units at the edge of the image receive less total input and may need larger biases.  
    * In the kinds of architectures typically used for classification of a single object in an image, the greatest reduction in the spatial dimensions of the network comes from using pooling layers with large stride.  


***


## The Pooling Layer
{: #content4}

1. **The Pooling Operation/Function:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}   
    The pooling function calculates a __summary statistic__ of the nearby pixels at the point of operation.  
    Some common statistics are _max, mean, weighted average_ and _$$L^2$$ norm_ of a surrounding rectangular window.  


2. **The Key Ideas/Properties:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}   
    In all cases, pooling helps to make the representation approximately __invariant to small translations__ of the input.   
    > Invariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs do not change.  
    Invariance to local translation can be a useful property if we care more about whether some feature is present than exactly where it is.  

    <button>Figure: MaxPooling</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](/main_files/dl/cnn/17.png){: width="70%" hidden=""}   

    __(Learned) Invariance to other transformations:__  
    Pooling over spatial regions produces invariance to translation, but if we _pool over the outputs of separately parametrized convolutions_, the features can learn which transformations to become invariant to.  
    This property has been used in __Maxout networks__.  
    <button>Figure: Examples of Learned Invariance</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](/main_files/dl/cnn/18.png){: width="70%" hidden=""}  


    For many tasks, pooling is essential for handling inputs of varying size.  
    > This is usually accomplished by varying the size of an offset between pooling regions so that the classification layer always receives the same number of summary statistics regardless of the input size. For example, the final pooling layer of the network may be defined to output four sets of summary statistics, one for each quadrant of an image, regardless of the image size.  

    One can use fewer pooling units than detector units, since they provide a summary; thus, by reporting summary statistics for pooling regions spaced $$k$$ pixels apart rather than $$1$$ pixel apart, we can improve the computational efficiency of the network because the next layer has roughly $$k$$ times fewer inputs to process.  
    This reduction in the input size can also result in improved statistical efficiency and reduced memory requirements for storing the parameters.  
    <button>Figure: Downsampling</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](/main_files/dl/cnn/19.png){: width="70%" hidden=""}  


3. **Theoretical Guidelines for choosing the pooling function:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}   
    [Link](http://www.di.ens.fr/willow/pdfs/icml2010b.pdf)  

4. **Variations:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents44}   
    __Dynamical Pooling:__  
    It is also possible to dynamically pool features together, for example, by running a clustering algorithm on the locations of interesting features (Boureau et al., 2011) [link](http://yann.lecun.com/exdb/publis/pdf/boureau-iccv-11.pdf). This approach yields a different set of pooling regions for each image.  

    __Learned Pooling:__  
    Another approach is to learn a single pooling structure that is then applied to all images (Jia et al., 2012).  

5. **Pooling and Top-Down Architectures:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents45}   
    Pooling can complicate some kinds of neural network architectures that use top-down information, such as Boltzmann machines and autoencoders.  

6. **The Pooling Layer (summary):**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents46}   
    ![img](/main_files/dl/cnn/13.png){: width="100%"}  





**Notes:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents48}  
* __Pooling Layer__:  
    * Makes the representations smaller and more manageable
    * Operates over each activation map independently (i.e. preserves depth)  
* You can use the stride instead of the pooling to downsample  

***

## Convolution and Pooling as an Infinitely Strong Prior
{: #content5}
            
1. **A Prior Probability Distribution:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents51}  
    This is a probability distribution over the parameters of a model that encodes our beliefs about what models are reasonable, before we have seen any data.  

2. **What is a weight prior?:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents52}  
    Assumptions about the weights (before learning) in terms of acceptable values and range are encoded into the prior distribution of the weights.  
    * A __Weak Prior__:  has a high _entropy_, and thus, variance and shows that there is low confidence in the initial value of the weight.  
    * A __Strong Prior__: in turn has low entropy/variance, and shows a narrow range of values about which we are confident before learning begins.  
    * A __Infinitely Strong Prior__: demarkets certain values as forbidden completely, assigning them zero probability.  

3. **Convolutional Layer as a FC Layer:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents53}  
    If we view the conv-layer as a FC-layer, the:  
    * __Convolution__: operation imposes an *__infinitely strong prior__* by making the following restrictions on the weights:  
        * Adjacent units must have the same weight but shifted in space.  
        * Except for a small spatially connected region, all other weights must be zero.  
    * __Pooling__: operation imposes an *__infinitely strong prior__* by:  
        * Requiring features to be __Translation Invariant__.   
        
        

4. **Key Insights/Takeaways:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents54}  
    * Convolution and pooling can cause underfitting if the priors imposed are not suitable for the task. When a task involves incorporating information from very distant locations in the input, then the prior imposed by convolution may be inappropriate.  
    > As an example, consider this scenario. We may want to learn different features for different parts of an input. But the compulsion to used tied weights (enforced by standard convolution) on all parts of an image, forces us to either compromise or use more kernels (extract more features).  

    * Convolutional models should only be compared with other convolutional models. This is because other models which are permutation invariant can learn even when input features are permuted (thus loosing spatial relationships). Such models need to learn these spatial relationships (which are hard-coded in CNNs).


*** 

## Variants of the Basic Convolution Function and Structured Outputs
{: #content6}

1. **Practical Considerations for Implementing the Convolution Function:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents61}  
    * In general a convolution layer consists of application of *__several different kernels to the input.__* This allows the extraction of several different features at all locations in the input. This means that in each layer, a single kernel (filter) isn‚Äôt applied. Multiple kernels (filters), usually a power of 2, are used as different feature detectors.  
    * The _input_ is generally not real-valued but instead *__vector valued__* (e.g. RGB values at each pixel or the feature values computed by the previous layer at each pixel position). Multi-channel convolutions are commutative only if number of output and input channels is the same.  
    * __Strided Convolutions__ are a means to do *__DownSampling__*; they are used to reduce computational cost, by calculating features at a *__coarser level__*. The effect of strided convolution is the same as that of a convolution followed by a downsampling stage. This can be used to reduce the representation size.  
        <p>$$Z_{i, j, k}=c(\mathrm{K}, \mathrm{V}, s)_{i, j, k}=\sum_{l, m, n}\left[V_{l,(j-1) \times s+m,(k-1) \times s+n} K_{i, l, m, n}\right] \tag{9.8}$$</p>  
    * __Zero Padding__ is used to make output dimensions and kernel size independent (i.e. to control the output dimension regardless of the size of the kernel). There are three types:  
        1. __Valid__: The output is computed only at places where the entire kernel lies inside the input. Essentially, _no zero padding_ is performed. For a kernel of size $$k$$ in any dimension, the input shape of $$m$$ in the direction will become $$m-k+1$$ in the output. This shrinkage restricts architecture depth.   
        2. __Same__: The input is zero padded such that the _spatial size of the input and output is **same**_. Essentially, for a dimension where kernel size is $$k$$, the input is padded by $$k-1$$ zeros in that dimension. Since the number of output units connected to border pixels is less than that for center pixels, it may under-represent border pixels.  
        3. __Full__: The input is padded by enough zeros such that _each input pixel is connected to the same number of output units_.  
        > The optimal amount of Zero-Padding usually lies between "valid" and "same" convolution.  

    * __Locally Connected Layers__/__Unshared Convolution__: has the same connectivity graph as a convolution operation, but *__without parameter sharing__* (i.e. each output unit performs a linear operation on its neighbourhood but the parameters are not shared across output units.).  
        This allows models to capture local connectivity while allowing different features to be computed at different spatial locations; at the _expense_ of having _a lot more parameters_.      
        <p>$$Z_{i, j, k}=\sum_{l, m, n}\left[V_{l, j+m-1, k+n-1} w_{i, j, k, l, m, n}\right] \tag{9.9}$$</p>  
        > They're useful when we know that each feature should be a function of a small part of space, but there is no reason to think that the same feature should occur across all of space.  
        For example, if we want to tell if an image is a picture of a face, we only need to look for the mouth in the bottom half of the image.  
    * __Tiled Convolution__: offers a middle ground between Convolution and locally-connected layers. Rather than learning a separate set of weights at _every_ spatial location, it learns/uses a set of kernels that are cycled through as we move through space.  
        This means that immediately neighboring locations will have different filters, as in a locally connected layer, but the memory requirements for storing the parameters will increase only by a factor of the size of this set of kernels, rather than by the size of the entire output feature map.  
        <p>$$Z_{i, j, k}=\sum_{l, m, n} V_{l, j+m-1, k+n-1} K_{i, l, m, n, j \% t+1, k \% t+1} \tag{9.10}$$</p>  
    * __Max-Pooling, and Locally Connected Layers and Tiled Layers__: When max pooling operation is applied to locally connected layer or tiled convolution, the model has the ability to become transformation invariant because adjacent filters have the freedom to learn a transformed version of the same feature.  
    > This essentially similar to the property leveraged by pooling over channels rather than spatially.  
    * __Different Connections__: Besides locally-connected layers and tiled convolution, another extension can be to restrict the kernels to operate on certain input channels. One way to implement this is to connect the first m input channels to the first n output channels, the next m input channels to the next n output channels and so on. This method decreases the number of parameters in the model without decreasing the number of output units.  
    * __Other Operations__: The following three operations‚Äîconvolution, backprop from output to weights, and backprop from output to inputs‚Äîare sufficient to compute all the gradients needed to train any depth of feedforward convolutional network, as well as to train convolutional networks with reconstruction functions based on the transpose of convolution. 
    > See Goodfellow (2010) for a full derivation of the equations in the fully general multidimensional, multiexample case.  
    * __Bias__:  Bias terms can be used in different ways in the convolution stage.  
        * For locally connected layer and tiled convolution, we can use a bias per output unit and kernel respectively.  
        * In case of traditional convolution, a single bias term per output channel is used.  
        * If the _input size is fixed_, a bias per output unit may be used to _counter the effect of regional image statistics and smaller activations at the boundary due to zero padding_.  
        
2. **Structured Outputs:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents62}  
    Convolutional networks can be trained to output high-dimensional structured output rather than just a classification score.  
    A good example is the task of __image segmentation__ where each pixel needs to be associated with an object class.  
    Here the output is the same size (spatially) as the input. The model outputs a tensor $$S$$  where $$S_{i, j, k}$$ is the probability that pixel $$(j,k)$$ belongs to class $$i$$.  
    * __Problem__: One issue that often comes up is that the output plane can be smaller than the input plane.  
    * __Solutions__:  
        * To produce an output map as the same size as the input map, only same-padded convolutions can be stacked.  
        * Avoid Pooling Completely _(Jain et al. 2007)_  
        * Emit a lower-Resolution grid of labels _(Pinheiro and Collobert, 2014, 2015)_ 
        * __Recurrent-Convolutional Models__: The output of the first labelling stage can be refined successively by another convolutional model. If the models use tied parameters, this gives rise to a type of recursive model.  
        * Another model that has gained popularity for segmentation tasks (especially in the medical imaging community) is the [U-Net](https://arxiv.org/abs/1505.04597). The up-convolution mentioned is just a direct upsampling by repetition followed by a convolution with same padding.  

    The output can be further processed under the assumption that contiguous regions of pixels will tend to belong to the same label. Graphical models can describe this relationship. Alternately, [CNNs can learn to optimize the graphical models training objective](https://www.robots.ox.ac.uk/~vgg/rg/papers/tompson2014.pdf).  



***

## Distinguishing features
{: #contentx}


2. **Image Features:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents92} 
    :   are certain quantities that are calculated from the image to _better describe the information in the image_, and to _reduce the size of the input vectors_. 
    :   * Examples:  
            * __Color Histogram__: Compute a (bucket-based) vector of colors with their respective amounts in the image.  
            * __Histogram of Oriented Gradients (HOG)__: we count the occurrences of gradient orientation in localized portions of the image.   
            * __Bag of Words__: a _bag of visual words_ is a vector of occurrence counts of a vocabulary of local image features.  
                > The __visual words__ can be extracted using a clustering algorithm; K-Means.  


***
***

TITLE: Neural Networks
LINK: research/dl/archt/neural_archits.md



__Resources:__{: style="color: red"}  
{: #lst-p}
* [Types of artificial neural networks (wiki!)](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks)  
* [Artificial Neural Networks Complete Description (wiki!)](https://en.wikipedia.org/wiki/Artificial_neural_network)  
* [History of ANNs (wiki!)](https://en.wikipedia.org/wiki/History_of_artificial_neural_networks)  
* [Mathematics of ANNs (wiki)](https://en.wikipedia.org/wiki/Mathematics_of_artificial_neural_networks)  
* [Deep Learning vs Probabilistic Graphical Models vs Logic (Blog!)](http://www.computervisionblog.com/2015/04/deep-learning-vs-probabilistic.html)  


__Interpretation of NNs:__{: style="color: red"}  
{: #lst-p}
* Schmidhuber was a pioneer for the view of "neural networks as programs", which is claimed in his blog post. As opposed to the "representation learning view" by Hinton, Bengio, and other people, which is currently dominant in deep learning.   




## Neural Architectures
{: #content1}


2. **Neural Architectures - Graphical and Probabilistic Properties:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  

    
    <button>Properties</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    * __FFN__ 
        * Directed
        * Acyclic
        * ?
    * __MLP__ 
        * Directed
        * Acyclic
        * Fully Connected (Complete?)
    * __CNN__ 
        * Directed
        * Acyclic
        * ?
    * __RNN__ 
        * Directed
        * Cyclic
        * ?
    * __Hopfield__ 
        * Undirected
        * Cyclic
        * Complete
    * __Boltzmann Machine__ 
        * Undirected
        * Cyclic
        * Complete
    * __RBM__ 
        * Undirected
        * Cyclic
        * Bipartite
    * __Bayesian Networks__ 
        * Directed
        * Acyclic
        * ?
    * __HMMs__ 
        * Directed
        * Acyclic
        * ?
    * __MRF__ 
        * Undirected
        * Cyclic
        * ?
    * __CRF__ 
        * Undirected
        * Cyclic
        * ?
    * __DBN__ 
        * Directed
        * Acyclic
        * ?
    * __GAN__ 
        * ?
        * ?
        * Bipartite-Complete
    {: hidden=""}

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * [NNs Graphical VS Functional view (wiki!)](https://en.wikipedia.org/wiki/Mathematics_of_artificial_neural_networks#Neural_networks_as_functions)  
    <br>

3. **Neural Networks and Graphical Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    __Deep NNs as PGMs:__{: style="color: red"}  
    You can view a deep neural network as a graphical model, but here, the CPDs are not probabilistic but are deterministic. Consider for example that the input to a neuron is $$\vec{x}$$ and the output of the neuron is $$y .$$ In the CPD for this neuron we have, $$p(\vec{x}, y)=1,$$ and $$p(\vec{x}, \hat{y})=0$$ for $$\hat{y} \neq y .$$ Refer to the section 10.2 .3 of Deep Learning Book for more details.  
    <br>

3. **Neural Networks as Gaussian Processes:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    It's long been known that these deep tools can be related to Gaussian processes, the ones I mentioned above. Take a neural network (a recursive application of weighted linear functions followed by non-linear functions), put a probability distribution over each weight (a normal distribution for example), and with infinitely many weights you recover a Gaussian process (see [Neal](http://www.cs.toronto.edu/pub/radford/thesis.pdf) or [Williams](http://papers.nips.cc/paper/1197-computing-with-infinite-networks.pdf) for more details).  

    We can think about the finite model as an approximation to a Gaussian process.  
    When we optimise our objective, we minimise some "distance" (KL divergence to be more exact) between your model and the Gaussian process.  
    
    <button>Illustration</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/9AhiNOhdojUkjscIHxrF6VDwDB6C87sJQk0te27ahY4.original.fullsize.png){: width="100%" hidden=""}  








***
***

TITLE: Hopfield Networks
LINK: research/dl/archt/hopfield.md



[Hopfield Networks Exercises](https://neuronaldynamics-exercises.readthedocs.io/en/latest/exercises/hopfield-network.html)  
[Hopfield Networks Example and Code (medium)](https://medium.com/100-days-of-algorithms/day-80-hopfield-net-5f18d3dbf6e6)  
[INTRODUCTION TO HOPFIELD NEURAL NETWORKS (blog)](https://www.doc.ic.ac.uk/~sd4215/hopfield.html)  
[The Hopfield Model (paper!)](http://page.mi.fu-berlin.de/rojas/neural/chapter/K13.pdf)  
[Hopfield Network Demo (github)](https://github.com/drussellmrichie/hopfield_network)  
[Hopfield Networks Tutorial + Code (blog)](http://koaning.io/intro-to-hopfield-networks.html)  
[Why learn Hopfield Nets and why they work (blog)](https://towardsdatascience.com/hopfield-networks-are-useless-heres-why-you-should-learn-them-f0930ebeadcd)  
[Hopfield Networks (Quantum ML Book)](https://www.sciencedirect.com/topics/computer-science/hopfield-network)  
* [A Tutorial on Energy-Based Learning (LeCun)](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf)  
* [Hopfield Nets (Hinton Lecs)](https://www.youtube.com/watch?v=DS6k0PhBjpI&list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&index=50&t=0s)  
* [Hopfield Nets (CMU Lecs!)](https://www.youtube.com/watch?v=yl8znINLXdg)  
* [Hopfield Nets - Proof of Decreasing Energy (vid)](https://www.youtube.com/watch?v=gfPUWwBkXZY)  
* [On the Convergence Properties of the Hopfield Model (paper)](http://www.paradise.caltech.edu/CNS188/bruck90-conv.pdf)  




## Hopfield Networks
{: #content1}

1. **Hopfield Networks:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __Hopfield Networks__ are a form of _recurrent_ artificial neural networks that serve as <span>content-addressable ("associative") memory</span>{: style="color: purple"} systems with binary threshold nodes.  

    They are __energy models__ i.e. their properties derive from a global __energy function__.  
    * A Hopfield net is composed of binary threshold units with recurrent connections between them.  

    __Motivation:__{: style="color: red"}  
    __Recurrent networks of *non-linear* units__ are generally very hard to analyze.  
    They can behave in many different ways:  
    {: #lst-p}
    * Settle to a stable state
    * Oscillate
    * Follow chaotic trajectories that cannot be predicted far into the future  

    __Main Idea:__{: style="color: red"}  
    _John Hopfield_ realized that <span>if the __connections are__ *__symmetric__*, there is a __global *energy* function__</span>{: style="color: goldenrod"}:  
    {: #lst-p}
    * Each binary "configuration" of the whole network has an energy.  
        * __Binary Configuration:__ is an assignment of binary values to each neuron in the network.  
            Every neuron has a particular binary value in a configuration.  
    * The <span>binary threshold decision rule</span>{: style="color: purple"} causes the network to <span>settle to a __minimum__ of this energy function</span>{: style="color: purple"}.  
        The rule causes the network to go downhill in energy, and by repeatedly applying the rule, the network will end-up in an energy minimum.  


    __The Energy Function:__{: style="color: red"}  
    {: #lst-p}
    * The global energy is the sum of many contributions:  
        <p>$$E=-\sum_{i} s_{i} b_{i}-\sum_{i< j} s_{i} s_{j} w_{i j}$$</p>  
        * Each contribution depends on:  
            * *One* __connection weight__: $$w_{i j}$$  
                A *__symmetric__* connection between two neurons; thus, have the following restrictions:  
                * $$w_{i i}=0, \forall i$$ (no unit has a connection with itself)  
                * $$w_{i j}=w_{j i}, \forall i, j$$ (connections are symmetric)  
            and 
            * The __binary states__ of *two* __neurons__: $$s_{i}$$ and $$s_{j}$$
                where $$s_{j} \in \{-1, 1\}$$ (or $$\in \{0, 1\}$$) is the state of unit $$j$$, and $$\theta_{j}$$ is the threshold of unit $$j$$.  
        * To make up the following terms:  
            * The __quadratic term__ $$s_{i} s_{j} \in \{-1, 1\}$$, involving the states of *__two__* units and  
            * The __bias term__ $$s_i b_i \in \{-\theta, \theta\}$$, involving the states of individual units.  
    * This simple *__quadratic__* energy function makes it possible for each unit to compute __locally__ how it's state affects the global energy:  
        The __Energy Gap__ is the difference in the global energy of the whole configuration depending on whether $$i$$ is on:  
        <p>$$\begin{align}
            \text{Energy Gap} &= \Delta E_{i} \\
              &= E\left(s_{i}=0\right)-E\left(s_{i}=1\right) \\
              &= b_{i}+\sum_{j} s_{j} w_{i j} 
            \end{align}
            $$</p>  
        i.e. the difference between the __energy when $$i$$ is *on*__ and the __energy when $$i$$ is *off*__.  
        * __The Energy Gap and the Binary Threshold Decision Rule:__  
            * This difference (__energy gap__) is exactly what the __binary threshold decision rule__ computes.  
            * <span>The Binary Decision Rule is the</span>{: style="color: goldenrod"} __*derivative* of the energy gap wrt the state of the $$i$$-th unit $$s_i$$__{: style="color: goldenrod"}.  

    __Settling to an Energy Minimum:__{: style="color: red"}  
    To find an energy minimum in this net:  
    {: #lst-p}
    * Start from a _random state_, then  
    * Update units <span>one at a time</span>{: style="color: purple"} in _random_ order:  
        * Update each unit to whichever of its two states gives the lowest global energy.  
            i.e. use <span>__binary threshold units__</span>{: style="color: goldenrod"}.  

    __A Deeper Energy Minimum:__{: style="color: red"}  
    The net has two triangles in which the three units mostly support each other.  
    {: #lst-p}
    - Each triangle mostly hates the other triangle.  
    The triangle on the left differs from the one on the right by having a weight of $$2$$ where the other one has a weight of $$3$$.  
    - So turning on the units in the triangle on the right gives the deepest minimum.  

    __Sequential Updating - Justification:__{: style="color: red"}  
    {: #lst-p}
    * If units make __simultaneous__ decisions the energy could go up.  
    * With simultaneous parallel updating we can get *__oscillations__*.  
        - They always have a __period__ of $$2$$ (bi-phasic oscillations).  
    * If the updates occur in parallel but with random timing, the oscillations are usually destroyed.  

    __Using Energy Models (with binary threshold rule) for Storing Memories:__  
    {: #lst-p}
    * _Hopfield (1982)_ proposed that <span>memories could be __energy minima__ of a neural net</span>{: style="color: goldenrod"} (w/ symmetric weights).  
        - <span>The __binary threshold decision rule__ can then be used to _"clean up"_ __incomplete__ or __corrupted__ memories</span>{: style="color: purple"}.  
            Transforms _partial_ memories to _full_ memories.   
    * The idea of memories as energy minima was proposed by _I. A. Richards (1924)_ in "Principles of Literary Criticism".  
    * Using energy minima to represent memories gives a <span>__content-addressable ("associative") memory__</span>{: style="color: purple"}:  
        - An item can be accessed by just knowing part of its content.  
            - This was really amazing in the year 16 BG (Before Google).  
        - It is robust against hardware damage. 
        - It's like reconstructing a dinosaur from a few bones.  
            Because you have an idea about how the bones are meant to fit together.  


    __Storing memories in a Hopfield net:__{: style="color: red"}  
    {: #lst-p}
    * If we use activities of $$1$$ and $$-1$$ we can store a binary state vector by incrementing the weight between any two units by the product of their activities.  
        <p>$$\Delta w_{i j}=s_{i} s_{j}$$</p>   
        * This is a very simple rule that is __*not* error-driven__ (i.e. does not learn by correcting errors).  
            That is both its strength and its weakness:  
            * It is an __online__ rule  
            * It is not very efficient to store things  
        * We treat __biases__ as weights from a __*permanently on* unit__.    
    * With states of $$0$$ and $$1$$ the rule is slightly more complicated:  
        <p>$$\Delta w_{i j}=4\left(s_{i}-\frac{1}{2}\right)\left(s_{j}-\frac{1}{2}\right)$$</p>  



    __Summary - Big Ideas of Hopfield Networks:__{: style="color: red"}  
    {: #lst-p}
    * __Idea #1__: we can find a local energy minimum by using a network of symmetrically connected binary threshold units.  
    * __Idea #2__: these local energy minima might correspond to memories.  


    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * They were responsible for resurgence of interest in Neural Networks in 1980s
    * They can be used to store memories as distributed patterns of activity
    * The constraint that weights are symmetric guarantees that the energy function decreases monotonically while following the activation rules  
    * The Hopfield Network is a *__non-linear dynamical system__* that converges to an *__attractor__*.  
    <br>


2. **Structure:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    The __Hopfield Network__ is formally described as a __complete Undirected graph__{: style="color: goldenrod"} $$G=\langle V, f\rangle,$$ where $$V$$ is a set of McCulloch-Pitts neurons and $$f : V^{2} \rightarrow \mathbb{R}$$ is a function that links pairs of units to a real value, the connectivity weight.  
    {: #lst-p}
    * The __Units__:  
        The units in a Hopfield Net are __binary threshold units__,  
        i.e. the <span>units only take on __two different values__ for their states</span>{: style="color: purple"} and the <span>value is determined by whether or not the units' __input__ *__exceeds__* __their threshold__</span>{: style="color: purple"}.  
    * The __States:__  
        The state $$s_i$$ for unit $$i$$ take on values of $$1$$ or $$-1$$,  
        i.e. $$s_i \in \{-1, 1\}$$.  
    * The __Weights:__  
        Every pair of units $$i$$ and $$j$$ in a Hopfield network has a connection that is described by the __connectivity weight__ $$w_{i j}$$.   
        * __Symmetric Connections (weights)__:  
            * The connections in a Hopfield net are constrained to be symmetric by making the following restrictions:  
                * $$w_{i i}=0, \forall i$$ (no unit has a connection with itself)  
                * $$w_{i j}=w_{j i}, \forall i, j$$ (connections are symmetric)  
            * The <span>constraint that weights are *__symmetric__* guarantees that the __energy function decreases monotonically__ while following the activation rules</span>{: style="color: purple"}.  
                A network with __*asymmetric* weights__ may exhibit some periodic or chaotic behaviour; however, Hopfield found that this behavior is confined to relatively small parts of the phase space and does not impair the network's ability to act as a content-addressable associative memory system.  



3. **Update Rule:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    Updating one unit (node in the graph simulating the artificial neuron) in the Hopfield network is performed using the following rule:  
    <p>$$s_{i} \leftarrow\left\{\begin{array}{ll}{+1} & {\text { if } \sum_{j} w_{i j} s_{j} \geq \theta_{i}} \\ {-1} & {\text { otherwise }}\end{array}\right.$$</p>  

    Updates in the Hopfield network can be performed in two different ways:  
    {: #lst-p}
    * __Synchronous:__ All units are updated at the same time. This requires a central clock to the system in order to maintain synchronization.  
        This method is viewed by some as less realistic, based on an absence of observed global clock influencing analogous biological or physical systems of interest.  

    __Neural Attraction and Repulsion (in state-space):__{: style="color: red"}  
    Neurons "attract or repel each other" in state-space.  
    The weight between two units has a powerful impact upon the values of the neurons. Consider the connection weight $$w_{ij}$$ between two neurons $$i$$ and $$j$$.  
    If $$w_{{ij}}>0$$, the updating rule implies that:  
    {: #lst-p}
    * when $$s_{j}=1,$$ the contribution of $$j$$ in the weighted sum is positive. Thus, $$s_{i}$$ is pulled by $$j$$ towards its value $$s_{i}=1$$
    * when $$s_{j}=-1,$$ the contribution of $$j$$ in the weighted sum is negative. Then again, $$s_{i}$$ is pushed by $$j$$ towards its value $$s_{i}=-1$$  
    
    Thus, the __values of neurons $$i$$ and $$j$$__ will <span>__converge__ if the weight between them is *positive*</span>{: style="color: purple"}.  
    Similarly, they will <span>__diverge__ if the weight is *negative*</span>{: style="color: purple"}.  
    <br>

4. **Energy:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    Hopfield nets have a scalar value associated with each state of the network, referred to as the __"energy", $$E$$,__ of the network, where:  
    <p>$$E=-\frac{1}{2} \sum_{i, j} w_{i j} s_{i} s_{j}+\sum_{i} \theta_{i} s_{i}$$</p>  
    <button>Energy Landscape of Hopfield Net</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/VgVFJGoT4ogbRvkb1SO-HbUS6i0M94Xte6gIgcYJUbw.original.fullsize.png){: width="100%" hidden=""}  
    This quantity is called *__"energy"__* because it either decreases or stays the same upon network units being updated.  
    Furthermore, under repeated updating the network will eventually converge to a state which is a local minimum in the energy function.  
    Thus, <span>if a state is a __*local minimum* in the energy function__ it is a __*stable state* for the network__</span>{: style="color: purple"}.  
    
    __Relation to Ising Models:__{: style="color: red"}  
    Note that this energy function belongs to a general class of models in physics under the name of [__Ising models__](https://en.wikipedia.org/wiki/Ising_model).  
    These in turn are a special case of [__Markov Random Fields (MRFs)__](https://en.wikipedia.org/wiki/Markov_random_field), since the associated probability measure, the __Gibbs measure__, has the *__Markov property__*.  
    <br>

44. **Initialization and Running:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents144}  
    __Initialization__ of the Hopfield Networks is done by <span>setting the values of the units to the desired __start pattern__</span>{: style="color: purple"}.  
    Repeated updates are then performed until the network converges to an __attractor pattern__.  
    <br>

55. **Convergence:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents155}  
    The Hopfield Network converges to an __attractor pattern__ describing a stable state of the network (as a non-linear dynamical systems).  
    
    __Convergence__ is generally __assured__, as Hopfield proved that the attractors of this nonlinear dynamical system are <span>stable</span>{: style="color: goldenrod"}, <span>non-periodic</span>{: style="color: goldenrod"} and <span>non-chaotic</span>{: style="color: goldenrod"} as in some other systems.  

    Therefore, in the context of Hopfield Networks, an __attractor pattern__ is a final *__stable state__*, a pattern that cannot change any value within it under updating.  
    <br>


66. **Training:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents166}  
    * Training a Hopfield net involves __*lowering the energy* of states that the net should "remember"__.  
    * This allows the net to serve as a __content addressable memory system__, 
        I.E. the network will converge to a "remembered" state if it is given only part of the state.  
    * The net can be used to recover from a distorted input to the trained state that is most similar to that input.  
        This is called __associative memory__ because it recovers memories on the basis of similarity.  
    * Thus, the network is properly trained when the energy of states which the network should remember are local minima.  
    * Note that, in contrast to __Perceptron training__, the thresholds of the neurons are never updated.  
    <br>


5. **Learning Rules:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    There are various different learning rules that can be used to store information in the memory of the Hopfield Network.  

    __Desirable Properties:__{: style="color: red"}  
    {: #lst-p}
    * __Local:__ A learning rule is _local_ if each weight is updated using information available to neurons on either side of the connection that is associated with that particular weight.  
    * __Incremental:__ New patterns can be learned without using information from the old patterns that have been also used for training.  
        That is, when a new pattern is used for training, the new values for the weights only depend on the old values and on the new pattern.  

    These properties are desirable, since a learning rule satisfying them is more biologically plausible.  
    > For example, since the human brain is always learning new concepts, one can reason that human learning is incremental. A learning system that were not incremental would generally be trained only once, with a huge batch of training data.  

    __Hebbian Learning Rule:__{: style="color: red"}  
    The Hebbian rule is both __local__ and __incremental__.  
    For the Hopfield Networks, it is implemented in the following manner, when learning $$n$$ binary patterns:  
    <p>$$w_{i j}=\frac{1}{n} \sum_{\mu=1}^{n} \epsilon_{i}^{\mu} \epsilon_{j}^{\mu}$$</p>  
    where $$\epsilon_{i}^{\mu}$$ represents bit $$i$$ from pattern $$\mu$$.  

    \- If the bits corresponding to neurons $$i$$ and $$j$$ are equal in pattern $$\mu,$$ then the product $$\epsilon_{i}^{\mu} \epsilon_{j}^{\mu}$$ will be positive.  
    This would, in turn, have a positive effect on the weight $$w_{i j}$$ and the values of $$i$$ and $$j$$ will tend to become equal.  
    \- The opposite happens if the bits corresponding to neurons $$i$$ and $$j$$ are different.  

    __The Storkey Learning Rule:__{: style="color: red"}  
    This rule was introduced by _Amos Storkey (1997)_ and is both __local__ and __incremental__.  
    The weight matrix of an attractor neural network is said to follow the Storkey learning rule if it obeys:  
    <p>$$w_{i j}^{\nu}=w_{i j}^{\nu-1}+\frac{1}{n} \epsilon_{i}^{\nu} \epsilon_{j}^{\nu}-\frac{1}{n} \epsilon_{i}^{\nu} h_{j i}^{\nu}-\frac{1}{n} \epsilon_{j}^{\nu} h_{i j}^{\nu}$$</p>  
    where $$h_{i j}^{\nu}=\sum_{k=1}^{n} \sum_{i \neq k \neq j}^{n} w_{i k}^{\nu-1} \epsilon_{k}^{\nu}$$ is a form of __local field__ at neuron $$i$$.  

    This learning rule is __local__, since the <span>synapses take into account only neurons at their sides</span>{: style="color: purple"}.  


    __Storkey vs Hebbian Learning Rules:__  
    Storkey showed that a Hopfield network trained using this rule has a __greater capacity__ than a corresponding network trained using the Hebbian rule.  
    The Storkey rule makes use of more information from the patterns and weights than the generalized Hebbian rule, due to the __effect of the *local field*__.  
    <br>

6. **Spurious Patterns:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    * Patterns that the network uses for training (called __retrieval states__) become *__attractors__* of the system.  
    * Repeated updates would eventually lead to convergence to one of the retrieval states.  
    * However, sometimes the network will converge to spurious patterns (different from the training patterns).  
    * __Spurious Patterns__ arise due to *__spurious minima__*.  
        The energy in these spurious patterns is also a local minimum:  
        * For each stored pattern $$x,$$ the negation $$-x$$ is also a spurious pattern.  
        * A spurious state can also be a linear combination of an odd number of retrieval states. For example, when using $$3$$ patterns $$\mu_{1}, \mu_{2}, \mu_{3},$$ one can get the following spurious state:  
        <p>$$\epsilon_{i}^{\operatorname{mix}}=\pm \operatorname{sgn}\left( \pm \epsilon_{i}^{\mu_{1}} \pm \epsilon_{i}^{\mu_{2}} \pm \epsilon_{i}^{\mu_{3}}\right)$$</p>  
    * Spurious patterns that have an *__even__* __number of states__ cannot exist, since they might sum up to zero.  

    * Spurious Patterns (memories) occur when two nearby energy minima combine to make a new minimum in the wrong place.  
    * Physicists, in trying to increase the capacity of Hopfield nets, rediscovered the __Perceptron convergence procedure__.  
    <br>

7. **Capacity:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    The Network capacity of the Hopfield network model is determined by neuron amounts and connections within a given network. Therefore, the number of memories that are able to be stored is dependent on neurons and connections.  
    
    __Capacity:__  
    {: #lst-p}
    * It was shown that the recall accuracy between vectors and nodes was $$0.138$$ (approximately $$138$$ vectors can be recalled from storage for every $$1000$$ nodes) _(Hertz et al. 1991)_.  
    * Using __Hopfield's storage rule__ the capacity of a totally connected net with $$N$$ units is only about $$0.15N$$ memories:  
        * At $$N$$ bits per memory the __total information stored__ is only $$0.15 N^{2}$$ bits.  
        * This does not make efficient use of the bits required to store the weights.  
        * It <span>depends on a constant $$0.15$$</span>{: style="color: purple"} 
    * __Capacity Requirements for Efficient Storage__: 
        * The net has $$N^{2}$$ weights and biases.  
        * After storing $$M$$ memories, each connection weight has an integer value in the range $$[-M, M]$$.  
        * So the __number of bits required to *Efficiently* store the weights and biases__ is:  
            <p>$$ N^{2} \log (2 M+1)$$</p>  
        * It <span>scales __logarithmically__ with the number of stored memories $$M$$</span>{: style="color: purple"}.  

    __Effects of Limited Capacity:__  
    {: #lst-p}
    * Since the capacity of Hopfield Nets is limited to $$\approx 0.15N$$, it is evident that many mistakes will occur if one tries to store a large number of vectors.  
        When the Hopfield model does not recall the right pattern, it is possible that an intrusion has taken place, since semantically related items tend to confuse the individual, and recollection of the wrong pattern occurs.  
        Therefore, the Hopfield network model is shown to confuse one stored item with that of another upon retrieval.  
    * Perfect recalls and high capacity, $$>0.14$$, can be loaded in the network by __Storkey learning method__.  
        Ulterior models inspired by the Hopfield network were later devised to raise the storage limit and reduce the retrieval error rate, with some being capable of one-shot learning.  
        * [A study of retrieval algorithms of sparse messages in networks of neural cliques](https://hal.archives-ouvertes.fr/hal-01058303/)  

    __Spurious Minima Limit the Capacity:__{: style="color: red"}  
    {: #lst-p}
    * Each time we memorize a configuration, we hope to create a new energy minimum.  
    * The problem is if <span>two nearby minima *__merge__* to create a minimum at an intermediate location</span>{: style="color: purple"}[^1]:  
        ![img](https://cdn.mathpix.com/snip/images/UL_iey6sV1ZUu36LhhJrMDAivSzfUaZMyuxNMc2Y8BE.original.fullsize.png){: width="60%"}  
        * Then we would get a blend of them rather than individual memories.  
        * The __Merging of Nearby Minima__ limits the capacity of a Hopfield Net.  


    __Avoiding Spurious Minima by Unlearning:__{: style="color: red"}  
    __Unlearning__ is a strategy proposed by _Hopfield, Feinstein and Palmer_ to avoid spurious minima.  
    It involves applying the opposite of the storage rule of the binary state the network settles to.  
    
    __Strategy:__  
    {: #lst-p}
    * Let the net settle from a random initial state and then do __Unlearning__.  
        Whatever binary state it settles to, apply the opposite of the storage rule.  
        Starting from _red_ merged minimum, doing unlearning will produce the two separate minima:  
        ![img](https://cdn.mathpix.com/snip/images/PNO7sSTFKv3MGDiAE5jxf3nTr4h7CBCNGS2rtI3erxA.original.fullsize.png){: width="40%"}  
    * This will get rid of deep, spurious minima and increase memory capacity.    
    * The strategy was shown to work but with no good analysis.  

    __Unlearning and Biological Dreaming:__  
    The question of why do we dream/what is the function of dreaming is a long standing question:  
    {: #lst-p}
    * When dreaming, the state of the brain is extremely similar to the state of the brain when its awake; except its not driven by real input, rather, its driven by a relay station.  
    * We dream for several hours a day, yet we actually don't remember most if not all of our dreams at all.  

    Crick and Mitchison proposed unlearning as a model of what dreams are for:  
    {: #lst-p}
    * During the day, we store a lot of things and get spurious minima.  
    * At night, we put the network (brain) in a random state, settle to a minimum, and then __unlearn__ what we settled to.  
    * The function of dreams is to get rid of those spurious minima.  
    * That's why we don't remember them, even though we dream for many hours (unless we wake up during the dream).  
        I.E. __We don't *store* our dreams__{: style="color: goldenrod"}.  
    
    __Optimal Amount of Unlearning:__  
    From a mathematical pov, we want to derive exactly how much unlearning we need to do.  
    Unlearning is part of the process of fitting a model to data, and doing maximum likelihood fitting of that model, then unlearning should automatically come out of fitting the model AND the amount of unlearning needed to be done.   
    Thus, the solution is to <span>derive unlearning as the right way to minimize some cost function</span>{: style="color: purple"}, where the cost function is "_how well your network models the data that you saw during the day_".  


    __The Maximal Capacity of a given network architecture, over all possible learning rules:__{: style="color: red"}  
    _Elizabeth Gardner_ showed that the capacity of _fully connected networks_ of _binary neurons_ with _dense patterns_ [__scales as $$2N$$__](https://pdfs.semanticscholar.org/7346/d681807bf0852695caa42dbecae5265b360a.pdf), a storage capacity which is much larger than the one of the Hopfield model.  
    __Learning Rules that are able to saturate the Gardner Bound:__  
    A simple learning rule that is guaranteed to achieve this bound is the __Perceptron Learning Algorithm (PLA)__ <span>applied to each neuron independently</span>{: style="color: purple"}.  
    However, unlike the rule used in the Hopfield model, PLA is a *__supervised__* rule that needs an explicit ‚Äúerror signal‚Äù in order to achieve the Gardner bound.  

    __Increasing the Capacity of Hopfield Networks:__{: style="color: red"}  
    Elizabeth Gardner showed that there was a much better storage rule that uses the full capacity of the weights:  
    {: #lst-p}
    * Instead of trying to store vectors in _one shot_, <span>cycle through the training set many times</span>{: style="color: purple"}.  
    * Use the __Perceptron Learning Algorithm (PLA)__ to train each unit to have the correct state given the states of all the other units in that vector.  
    * It loses the __online learning__ property in the interest of more __efficient storage__.  
    * Statisticians call this technique *__"pseudo-likelihood"__*.  
    * __Procedure Description__:  
        * Set the network to the memory state you want to store
        * Take each unit separately and check if this unit adopts the state we want for it given the states of all the other units: 
            * if it would you, leave its incoming weights alone  
            * if it wouldn't, you change its incoming weights in the way specified by the PLA   
                Notice those will be __*integer* changes__ to the weights  
        * Repeat several times, as needed.  
    * __Convergence:__  
        * If there are too many memories the Perceptron Convergence Procedure won't converge  
        * PLA __converges__, only, if there is a set of weights that will solve the problem   
            Assuming there is, this is a much more efficient way to store memories.  
    <br>

8. **Hopfield Networks with Hidden Units:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    We add some __hidden units__ to the network with the goal of <span>making the states of those hidden units represent an interpretation of the perceptual input that's shown on the visible units</span>{: style="color: purple"}.  
    The idea is that the <span>__weights__ between units represent __*constraints* on good interpretations__</span>{: style="color: purple"} and <span>by finding a __low energy state__ we find a __good interpretation of the input data__</span>{: style="color: purple"}.  

    __Different Computational Role for Hopfield Nets:__{: style="color: red"}{: #bodyContents18dcr}  
    Instead of using the Hopfield Net to store memories, we can use it to <span>construct interpretations of sensory input</span>{: style="color: purple"}, where  
    {: #lst-p}
    * The __input__ is represented by the _visible units_.  
    * The __interpretation__ is represented by the _states_ of the _hidden units_.  
    * The __Quality__ of the interpretations is represented by the (negative) _Energy_ function.  

    ![img](https://cdn.mathpix.com/snip/images/xkAmrsy0dyoXGJBvHVi4d1LDQKlWdZ3K_bP41Qvc7LI.original.fullsize.png){: width="40%"}  

    [__Example - Interpreting a Line Drawing__](https://www.youtube.com/watch?v=vVEju0zMCaA&list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&index=52)  

    __Two Difficult Computational Issues:__{: style="color: red"}  
    Using the states of the hidden units to represent an interpretation of the input raises two difficult issues:  
    {: #lst-p}
    * __Search__: How do we avoid getting trapped in poor local minima of the energy function?  
        Poor minima represent sub-optimal interpretations.  
    * __Learning__: How do we learn the weights on the connections to the hidden units? and between the hidden units?  
        Notice that there is no supervision in the problem.  


9. **Stochastic Units to improve Search:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents19}  
    Adding noise helps systems escape local energy minima.  

    __Noisy Networks Find Better Energy Minima:__{: style="color: red"}  
    {: #lst-p}
    * A Hopfield net always makes decisions that reduce the energy.
        - This makes it impossible to escape from local minima.  
    * We can use random noise to escape from poor minima.
        - Start with a lot of noise so its easy to cross energy barriers.
        - Slowly reduce the noise so that the system ends up in a deep minimum.  
            This is __"simulated annealing"__ _(Kirkpatrick et.al. 1981)_.  

    __Effects of Temperature on the Transition Probabilities:__  
    The temperature in a physical system (or a simulated system with an Energy Function) affects the transition probabilities.  
    {: #lst-p}
    * __High Temperature System__:  
        ![img](https://cdn.mathpix.com/snip/images/X1dg3rSIy6tVYNCyni5L1yVBoghnLIg88MFCCixV3Gc.original.fullsize.png){: width="60%"}  
        * The __probability of *crossing barriers*__ is <span>high</span>{: style="color: purple"}.  
            I.E. The __probability__ of going *__uphill__* from $$B$$ to $$A$$ is lower than the probability of going *__downhill__* from $$A$$ to $$B$$; but not much lower.  
            * In effect, the temperature _flattens the energy landscape_.    
        * So, the __Ratio of the Probabilities__ is <span>low</span>{: style="color: purple"}.  
            Thus,  
            * It is easy to cross barriers  
            * It is hard to stay in a deep minimum once you've got there  
    * __Low Temperature System__:  
        ![img](https://cdn.mathpix.com/snip/images/slyGgdTvzQogdm6EhkfEUNolozpN8S9gmbTusznV714.original.fullsize.png){: width="60%"}  
        * The __probability of *crossing barriers*__ is <span>much __smaller__</span>{: style="color: purple"}.  
            I.E. The __probability__ of  going *__uphill__* from $$B$$ to $$A$$ is _much_ lower than the probability of going *__downhill__* from $$A$$ to $$B$$.  
        * So, the __Ratio of the Probabilities__ is <span>much __higher__</span>{: style="color: purple"}.  
            Thus,  
            * It is harder to cross barriers  
            * It is easy to stay in a deep minimum once you've got there  
        * Thus, if we run the system long enough, we expect all the particles to end up in $$B$$.  
            However, if we run it at a low temperature, it will take a very long time for particles to escape from $$A$$.  
        * To increase the speed of convergence, starting at a high temperature then gradually decreasing it, is a good compromise.  

    __Stochastic Binary Units:__{: style="color: red"}  
    To <span>__inject noise__ in a Hopfield Net</span>{: style="color: purple"}, we replace the binary threshold units with binary __binary stochastic units__ that make _biased random decisions_.  
    \- The __Temperature__ controls the _amount of noise_.  
    \- <span>__*Raising* the noise level__ is equivalent to __*decreasing* all the energy gaps__ between configurations</span>{: style="color: purple"}.  
    <p>$$p\left(s_{i}=1\right)=\frac{1}{1+e^{-\Delta E_{i} / T}}$$</p>  
    * This is a normal __logistic equation__, but with the <span>__energy gap__ *__scaled__* by a __temperature__ $$T$$</span>{: style="color: purple"}:  
        * __High Temperature:__ the exponential will be $$\approx 0$$ and $$p\left(s_{i}=1\right)= \dfrac{1}{2}$$.  
            I.E. the __probability of a unit turning *on*__ is about a <span>half</span>{: style="color: purple"}.  
            It will be in its _on_ and _off_ states, equally often.  
        * __Low Temperature__: depending on the sign of $$\Delta E_{i}$$, the unit will become _more firmly on_ or _more firmly off_.  
        * __Zero Temperature__: (e.g. in Hopfield Nets) the sign of $$\Delta E_{i}$$ determines whether RHS is $$0$$ or $$1$$.  
            I.E. the unit will behave *__deterministically__*; a standard binary threshold unit, that will always adopt whichever of the two states gives the lowest energy.  
    * __Boltzmann Machines__ use stochastic binary units, with temperature $$T=1$$ (i.e. standard logistic equation).    


    __Thermal Equilibrium at a fixed temperature $$T=1$$:__{: style="color: red"}  
    {: #lst-p}
    * Thermal Equilibrium does not mean that the system has settled down into the lowest energy configuration.  
        I.E. not the states of the individual units that settle down.  
        The individual units still rattle around at Equilibrium, unless the temperature is zero $$T=0$$.  
    * What settles down is the <span>__probability distribution__ over configurations</span>{: style="color: purple"}.  
        * It settles to the __stationary distribution__.  
            * The stationary distribution is determined by the __energy function__ of the system.  
            * In the stationary distribution, the __probability of any configuration__ is $$\propto e^{-E}$$.  
    * __Intuitive Interpretation of Thermal Equilibrium__:  
        - Imagine a huge ensemble of systems that all have exactly the same energy function.  
        - The __probability of a configuration__ is just the _fraction of the systems_ that have that configuration.  
    * __Approaching Thermal Equilibrium__:  
        * Start with any distribution we like over all the identical systems.  
            - We could start with all the systems in the same configuration (__Dirac distribution__).  
            - Or with an equal number of systems in each possible configuration (__uniform distribution__).  
        * Then we keep applying our stochastic update rule to pick the next configuration for each individual system.  
        * After running the systems stochastically in the right way, we may eventually reach a situation where the fraction of systems in each configuration remains constant.  
            - This is the stationary distribution that physicists call thermal equilibrium.  
            - Any given system keeps changing its configuration, but the <span>__fraction of systems__ in each configuration does not change</span>{: style="color: purple"}.  
    * __Analogy__:  
        <button>Analogy - Card Shuffling</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        * Imagine a casino in Las Vegas that is full of card dealers (we need many more than $$52!$$ of them).  
        * We start with all the card packs in standard order and then the dealers all start shuffling their packs.  
            - After a few shuffling steps, the king of spades. still has a good chance of being next to the queen of spades. The packs have not yet forgotten where they stated.  
            - After prolonged shuffling, the packs will have forgotten where they! started. There will be an equal number of packs in each of the $$52!$$ possible orders.  
            - Once equilibrium has been reached, the number of packs that leave a configuration at each time step will be equal to the number that enter the configuration.  
        * The only thing wrong with this analogy is that all the configurations have equal energy, so they all end up with the same probability.  
            * We are generally interested in reaching equilibrium for systems where certain configurations have lower energy than others.  
        {: hidden=""}


    __As Boltzmann Machines:__{: style="color: red"}  
    __Boltzmann Machines__ are just _Stochastic Hopfield Nets_ with _Hidden Units_.  
    <br>



***
***
***


How a Boltzmann machine models a set of binary data vectors  
Why model a set of binary data vectors and what we could do with such a model if we had it
The probabilities assigned to binary data vectors are determined by the weights in a Boltzmann machine

__BMs__ are good at modeling binary data 

__Modeling Binary Data:__{: style="color: red"}  
Given a training set of binary vectors, fit a model that will assign a probability to every possible binary vector.  
- This is useful for deciding if other binary vectors come from the same distribution (e.g. documents represented by binary features that represents the occurrence of a particular word).  
- It can be used for monitoring complex systems to detect unusual behavior.  
- If we have models of several different distributions it can be used to compute the posterior probability that a particular distribution produced the observed data:  
    <p>$$p(\text {Model}_ i | \text { data })=\dfrac{p(\text {data} | \text {Model}_ i)}{\sum_{j} p(\text {data} | \text {Model}_ j)}$$</p>  


__Models for Generating Data:__{: style="color: red"}  
There are different kinds of models to generate data:  
{: #lst-p}
* __Causal Models__  
* __Energy-based Models__  



__How a Causal Model Generates Data:__{: style="color: red"}  
{: #lst-p}
* In a __Causal Model__ we generate data in two _sequential_ steps:  
    * First pick the hidden states from their prior distribution.  
        > in causal models, often __independent__ in the prior.  
    * Then pick the visible states from their conditional distribution given the hidden states.  
    ![img](https://cdn.mathpix.com/snip/images/caikJVJyPI9RUHOvw_XhbndekKG1XMBtELTSeVDFC54.original.fullsize.png){: width="50%"}  
* The probability of generating a visible vector, $$\mathrm{v},$$ is computed by summing over all possible hidden states. Each hidden state is an 'explanation" of $$\mathrm{v}$$:  
    <p>$$p(\boldsymbol{v})=\sum_{\boldsymbol{h}} p(\boldsymbol{h}) p(\boldsymbol{v} | \boldsymbol{h})$$</p>  

> Generating a binary vector: first generate the states of some latent variables, and then use the latent variables to generate the binary vector.  


__How a Boltzmann Machine Generates Data:__{: style="color: red"}  
{: #lst-p}
* It is __not__ a causal generative model.  
* Instead, everything is defined in terms of the <span>__energies of joint configurations__ of the visible and hidden units</span>{: style="color: purple"}. 
* The __energies of joint configurations__ are <span>related</span>{: style="color: purple"} to their __probabilities__ in two ways:  
    * We can simply define the probability to be:  
        <p>$$p(\boldsymbol{v}, \boldsymbol{h}) \propto e^{-E(\boldsymbol{v}, \boldsymbol{h})}$$</p>  
    * Alternatively, we can define the probability to be the probability of finding the network in that joint configuration after we have updated all of the stochastic binary units many times (until thermal equilibrium).  

    These two definitions agree - analysis below.  
* __The Energy of a joint configuration__:  
    <p>$$\begin{align}
        E(\boldsymbol{v}, \boldsymbol{h}) &= - \sum_{i \in v_{i s}} v_{i} b_{i}-\sum_{k \in h_{i d}} h_{k} b_{k}-\sum_{i< j} v_{i} v_{j} w_{i j}-\sum_{i, k} v_{i} h_{k} w_{i k}-\sum_{k< l} h_{k} h_{l} w_{k l} \\
        &= -\boldsymbol{v}^{\top} \boldsymbol{R} \boldsymbol{v}-\boldsymbol{v}^{\top} \boldsymbol{W} \boldsymbol{h}-\boldsymbol{h}^{\top} \boldsymbol{S} \boldsymbol{h}-\boldsymbol{b}^{\top} \boldsymbol{v}-\boldsymbol{c}^{\top} \boldsymbol{h}   
        \end{align}
        $$</p>  
    where $$v_{i} b_{i}$$ __binary state__ of _unit $$i$$_ in $$\boldsymbol{v}$$, $$h_k b_k$$ is the __bias__ of _unit $$k$$_, $$i< j$$ indexes every non-identical pair of $$i$$ and $$j$$ once (avoid self-interactions and double counting), and $$w_{i k}$$ is the __weight__ between visible unit $$i$$ and hidden unit $$k$$.  
* __Using Energies to define Probabilities__:  
    * The __probability of a *joint* configuration__ over both _visible_ and _hidden_ units depends on the energy of that joint configuration compared with the energy of all other joint configurations:  
        <p>$$p(\boldsymbol{v}, \boldsymbol{h})=\dfrac{e^{-E(\boldsymbol{v}, \boldsymbol{h})}}{\sum_{\boldsymbol{u}, \boldsymbol{g}} e^{-E(\boldsymbol{u}, \boldsymbol{g})}}$$</p>  
    * The __probability of a configuration of the *visible* units__ is the sum of the probabilities of all the joint configurations that contain it:  
        <p>$$p(\boldsymbol{v})=\dfrac{\sum_{\boldsymbol{h}} e^{-E(\boldsymbol{v}, \boldsymbol{h})}}{\sum_{\boldsymbol{u}, \boldsymbol{g}} e^{-E(\boldsymbol{u}, \boldsymbol{g})}}$$</p>  

    where the _denomenators_ are the __partition function__ $$Z$$.  
    * <button>Example - How Weights define a Distribution</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](https://cdn.mathpix.com/snip/images/FlB10QoVtoitDn8bOQAx4Muo-Myp2aTQAKXFc_BPZEo.original.fullsize.png){: width="100%" hidden=""}  
* __Sampling from the *Model*__:  
    * If there are <span>more than a few hidden units</span>{: style="color: purple"}, we cannot compute the normalizing term (the partition function) because it has *__exponentially__* many terms i.e. __intractable__.  
    * So we use Markov Chain Monte Carlo to get samples from the model starting from a random global configuration:  
        - Keep picking units at random and allowing them to stochastically update their states based on their energy gaps.  
    * Run the Markov chain until it reaches its stationary distribution (thermal equilibrium at a temperature of $$1$$).  
        - The probability of a global configuration is then related to its energy by the, __Boltzmann Distribution:__   
            <p>$$p(\mathbf{v}, \mathbf{h}) \propto e^{-E(\mathbf{v}, \mathbf{h})}$$</p>  
* __Sampling from the *Posterior distribution* over *hidden* configurations (for a given Data vector)__:  
    * The __number of possible hidden configurations__ is *__exponential__* so we need __MCMC__ to sample from the *posterior*.  
        - It is just the same as getting a sample from the model, except that we <span>keep the visible units _clamped_ to the given data vector</span>{: style="color: purple"}.  
            I.E. Only the __hidden units__ are allowed to change states (updated)  
    * Samples from the posterior are required for learning the weights.  
        Each __hidden configuration__ is an <span>_"explanation"_</span>{: style="color: goldenrod"} of an observed __visible configuration__.  
        Better explanations have lower energy.  


__The Goal of Learning:__{: style="color: red"}  
We want to maximize the product of the probabilities (sum of log-probabilities) that the Boltzmann Machine assigns to the binary vectors in the training set.  
This is Equivalent to maximizing the probability of obtaining exactly $$N$$ training cases if we ran the BM as follows:  
{: #lst-p}
* For $$i$$ in $$[1, \ldots, N]$$:  
    * Run the network with __no external input__ and let it settle to its *__stationary distribution__*    
    * Sample the *__visible vector__*  

__Possible Difficulty in Learning - Global Information:__{: style="color: red"}  
Consider a chain of units with visible units at the ends:  
![img](https://cdn.mathpix.com/snip/images/vEznkdbLrzZMR8Tn76VEAv-iymx2azcZtYlDl76vYYQ.original.fullsize.png){: width="60%"}  
If the training set is $$(1,0)$$ and $$(0,1)$$ we want the product of all the weights to be negative.  
So to know how to change w1 or w5 we must know w3.  

__Learning with Local Information:__{: style="color: red"}  
A very surprising fact is the following:  
<span>Everything that one weight needs to know about the other weights and the data is contained in the difference of two correlations</span>{: style="color: purple"}.  
The __derivative of *log probability* of one training vector__ wrt. one weight $$w_{ij}$$:  
<p>$$\dfrac{\partial \log p(\mathbf{v})}{\partial w_{i j}}=\left\langle s_{i} s_{j}\right\rangle_{\mathbf{v}}-\left\langle s_{i} s_{j}\right\rangle_{\text {free}}$$</p>  
where:  
{: #lst-p}
* $$\left\langle s_{i} s_{j}\right\rangle_{\mathbf{v}}$$: is the expected value of product of states at thermal equilibrium when the training vector is clamped on the visible units.  
    This is the __positive phase__{: style="color: goldenrod"} of learning.
    * __Effect:__ Raise the weights in proportion to the product of the activities the units have when you are presenting data.  
    * __Interpretation:__ similar to the <span>__storage__ term</span>{: style="color: purple"} for a Hopfield Net.   
        It is a __Hebbian Learning Rule__{: style="color: goldenrod"}.  
* $$\left\langle s_{i} s_{j}\right\rangle_{\text {free}}$$: is the expected value of product of states at thermal equilibrium when nothing is clamped.  
    This is the __negative phase__{: style="color: goldenrod"} of learning.
    * __Effect:__ Reduce the weights in proportion to _"how often the two units are **on** together when sampling from the **models distribution**"_.  
    * __Interpretation:__ similar to the <span>__Unlearning__ term</span>{: style="color: purple"} i.e. the <span>_opposite of the storage rule_</span>{: style="color: purple"} for <span>avoiding</span>{: style="color: goldenrod"} (getting rid of) <span>spurious minima</span>{: style="color: goldenrod"}.  
        Moreover, this rule specifies the __exact amount of *unlearning*__ to be applied.  

So, the __change in the weight__ is _proportional to_ the expected product of the activities averaged over all visible vectors in the training set that's what we call data MINUS the product of the same two activities when there is no clamping and the network has reached thermal equilibrium with no external interference:  
<p>$$\Delta w_{i j} \propto\left\langle s_{i} s_{j}\right\rangle_{\text{data}}-\left\langle s_{i} s_{j}\right\rangle_{\text{model}}$$</p>  

<span class="borderexample">Thus, the learning algorithm only requires __local information__.</span>  


__Effects of the Positive and Negative Phases of Learning:__{: style="color: red"}  
Given the __probability of a training data vector $$\boldsymbol{v}$$__:  
<p>$$p(\boldsymbol{v})=\dfrac{\sum_{\boldsymbol{h}} e^{-E(\boldsymbol{v}, \boldsymbol{h})}}{\sum_{\boldsymbol{u}} \sum_{\boldsymbol{g}} e^{-E(\boldsymbol{u}, \boldsymbol{g})}}$$</p>   
and the __log probability__:  
<p>$$\begin{align}
    \log p(\boldsymbol{v}) &= \log \left(\dfrac{\sum_{\boldsymbol{h}} e^{-E(\boldsymbol{v}, \boldsymbol{h})}}{\sum_{\boldsymbol{u}} \sum_{\boldsymbol{g}} e^{-E(\boldsymbol{u}, \boldsymbol{g})}}\right) \\
    &= \log \left(\sum_{\boldsymbol{h}} e^{-E(\boldsymbol{v}, \boldsymbol{h})}\right) - \log \left(\sum_{\boldsymbol{u}} \sum_{\boldsymbol{g}} e^{-E(\boldsymbol{u}, \boldsymbol{g})}\right) \\
    &= \left( \sum_{\boldsymbol{h}} \log e^{-E(\boldsymbol{v}, \boldsymbol{h})}\right) - \left(\sum_{\boldsymbol{u}} \sum_{\boldsymbol{g}} \log e^{-E(\boldsymbol{u}, \boldsymbol{g})}\right) \\
    &= \left(\sum_{\boldsymbol{h}} -E(\boldsymbol{v}, \boldsymbol{h})\right) - \left(\sum_{\boldsymbol{u}} \sum_{\boldsymbol{g}} -E(\boldsymbol{u}, \boldsymbol{g})\right)
    \end{align}
    $$</p>   
{: #lst-p}
* __Positive Phase:__   
    * The first term is <span>decreasing the energy of terms in that sum that are already large</span>{: style="color: purple"}. 
        * It finds those terms by settling to thermal equilibrium with the vector $$\boldsymbol{v}$$ clamped so they can find an $$\boldsymbol{h}$$ that __produces a low energy__ with $$\boldsymbol{v}$$).  
    * Having sampled those vectors $$\boldsymbol{h}$$, it then <span>changes the weights to make that energy even lower</span>{: style="color: purple"}.  
    * __Summary:__  
        The positive phase finds hidden configurations that work well with $$\boldsymbol{v}$$ and lowers their energies.  
* __Negative Phase__:  
    * The second term is <span>doing the same thing but for the __partition function__</span>{: style="color: purple"}.  
        It's <span>finding global configurations (combinations of visible and hidden states) that give low energy and therefore are large contributors to the partition function</span>{: style="color: purple"}.   
    * Having found those global configurations $$(\boldsymbol{v}', \boldsymbol{h}')$$, it <span>tries to raise their energy so that they contribute less</span>{: style="color: purple"}.    
    * __Summary:__  
        The negative phase finds the joint configurations that are the best competitors and raises their energies.  

Thus, the positive term is making the top term in $$p(\boldsymbol{v})$$ __bigger__ and the negative term is making the bottom term in $$p(\boldsymbol{v})$$ __smaller__.  

<button>Effects of only using the Hebbian Rule (Positive phase)</button>{: .showText value="show" onclick="showTextPopHide(event);"}
<span hidden="">If we only use the Hebbian rule (positive phase) without the unlearning term (negative phase) the synapse strengths will keep getting stronger and stronger, the weights will all become very positive, and the whole system will blow up.  
The Unlearning counteracts the positive phase's tendency to just add a large constant to the unnormalized probability everywhere.</span>


__Learning Rule Justification - Why the Derivative is so simple:__{: style="color: red"}  
{: #lst-p}
* The __probability of a *global* configuration__ <span>at __thermal equilibrium__</span>{: style="color: purple"} is an <span>__exponential function__ of its energy</span>{: style="color: goldenrod"}.  
    - Thus, <span>settling to equilibrium makes the __log probability__ a *__linear__* __function__ of the __energy__</span>{: style="color: goldenrod"}.  
* The __energy__ is a *__linear__* __function__ of the __weights__ and __states__:  
    <p>$$\dfrac{\partial E}{\partial w_{i j}}=s_{i} s_{j}$$</p>  
    It is a __log-linear model__.  
    This an important fact because we are trying to manipulate the log probabilities by manipulating the weights.  
* The <span>__process of settling to thermal equilibrium__ _propagates information_ about the __weights__</span>{: style="color: goldenrod"}.  
    * We don't need an explicit __back-propagation__ stage.  
    * We still need __two stages__:  
        1. Settle with the data
        2. Settle with NO data 
    * However, the network behaves, basically, in the same way in the two phases[^1]; while the forward and backprop stages are very different.  

__The Batch Learning Algorithm - An inefficient way to collect the Learning Statistics:__{: style="color: red"}  
{: #lst-p}
* __Positive phase:__  
    - Clamp a data-vector on the visible units.
    - Let the hidden units reach thermal equilibrium at a temperature of 1 (may use annealing to speed this up)  
        by updating the hidden units, one at a time.  
    - Sample $$\left\langle s_{i} s_{j}\right\rangle$$ for all pairs of units
    - Repeat for all data-vectors in the training set.
* __Negative phase:__  
    - Do not clamp any of the units
    - Set all the units to random binary states.
    - Let the whole network reach thermal equilibrium at a temperature of 1, by updating all the units, one at a time.  
        * __Difficulty__: where do we start? 
    - Sample $$\left\langle s_{i} s_{j}\right\rangle$$ for all pairs of units
    - Repeat many times to get good estimates  
        * __Difficulty__: how many times? (especially w/ multiple modes) 
* __Weight updates:__  
    - Update each weight by an amount proportional to the difference in $$\left\langle s_{i} s_{j}\right\rangle$$ in the two phases.  




[^1]: The state space is the corners of a hypercube. Showing it as a $$1-D$$ continuous space is a misrepresentation.  

***
***

TITLE: Recurrent Neural Networks <br /> Deep Learning Book Ch.10
LINK: research/dl/archt/rnns.md



[RNNs in NLP](/work_files/research/dl/nlp/rnns)  
[RNNs in CV](/work_files/research/dl/rnns_cv)  
[All of RNNs (ch.10 summary)](https://medium.com/@jianqiangma/all-about-recurrent-neural-networks-9e5ae2936f6e)  
[TRAINING RECURRENT NEURAL NETWORKS (Illya Stutskever PhD)](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)  
[Guide to RNNs and LSTMs](https://skymind.ai/wiki/lstm#a-beginners-guide-to-recurrent-networks-and-lstms)  



## Introduction
{: #content1}

1. **Recurrent Neural Networks:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __Recurrent Neural Networks (RNNs)__ are a family of neural networks for processing __sequential data__.  

    In an RNN, the connections between units form a _directed cycle_, allowing it to exhibit dynamic temporal behavior.  

    The standard RNN is a __nonlinear dynamical system__ that maps sequences to sequences.  


2. **Big Idea:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    RNNs __share parameters across different positions__/index of time/time-steps of the sequence, which allows it to _generalize well to examples of different sequence length_.  
    * Such sharing is particularly important when a specific piece of information can occur at multiple positions within the sequence.  

    > A related idea, is the use of convolution across a 1-D temporal sequence (_time-delay NNs_). This convolution operation allows the network to share parameters across time but is _shallow_.  
    The output of convolution is a sequence where each member of the output is a function of a small number of neighboring members of the input.  


3. **Dynamical Systems:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    __Classical Form of a Dynamical System:__{: style="color: red"}  
    <p>$$\boldsymbol{s}^{(t)}=f\left(\boldsymbol{s}^{(t-1)} ; \boldsymbol{\theta}\right) \tag{10.1}$$</p>  
    where $$\boldsymbol{s}^{(t)}$$  is called the state of the system.  

    ![img](/main_files/dl/archits/rnns/1.png){: width="100%"}  

    __A Dynamical System driven by an external signal $$\boldsymbol{x}^{(t)}$$__:  
    <p>$$\boldsymbol{s}^{(t)}=f\left(\boldsymbol{s}^{(t-1)}, \boldsymbol{x}^{(t)} ; \boldsymbol{\theta}\right) \tag{10.4}$$</p>  
    the state now contains information about the whole past sequence.   

    Basically, any function containing __recurrence__ can be considered an RNN.  

    __The RNN Equation (as a Dynamical System):__{: style="color: red"}  
    <p>$$\boldsymbol{h}^{(t)}=f\left(\boldsymbol{h}^{(t-1)}, \boldsymbol{x}^{(t)} ; \boldsymbol{\theta}\right) \tag{10.5}$$</p>  
    where the variable $$\mathbf{h}$$ represents the __state__.  

    ![img](/main_files/dl/archits/rnns/2.png){: width="100%"}  


4. **Unfolding the Computation Graph:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    __Unfolding__ maps the left to the right in the figure below (from _figure 10.2_) (both are computational graphs of a RNN without output $$\mathbf{o}$$):  
    ![img](/main_files/dl/archits/rnns/3.png){: width="100%"}  
    where the black square indicates that an interaction takes place with a delay of $$1$$ time step, from the state at time $$t$$  to the state at time $$t + 1$$.  

    We can represent the unfolded recurrence after $$t$$ steps with a function $$g^{(t)}$$:  
    <p>$$\begin{aligned} \boldsymbol{h}^{(t)} &=g^{(t)}\left(\boldsymbol{x}^{(t)}, \boldsymbol{x}^{(t-1)}, \boldsymbol{x}^{(t-2)}, \ldots, \boldsymbol{x}^{(2)}, \boldsymbol{x}^{(1)}\right) \\ &=f\left(\boldsymbol{h}^{(t-1)}, \boldsymbol{x}^{(t)} ; \boldsymbol{\theta}\right) \end{aligned}$$</p>  
    The function $$g^{(t)}$$ takes the whole past sequence $$\left(\boldsymbol{x}^{(t)}, \boldsymbol{x}^{(t-1)}, \boldsymbol{x}^{(t-2)}, \ldots, \boldsymbol{x}^{(2)}, \boldsymbol{x}^{(1)}\right)$$ as input and produces the current state, but the unfolded recurrent structure allows us to factorize $$g^{(t)}$$ into _repeated applications of a function $$f$$_.  
    
    The unfolding process, thus, introduces two major advantages:  
    {: #lst-p}
    1. Regardless of the sequence length, the learned model always has the same input size.  
        Because it is specified in terms of transition from one state to another state, rather than specified in terms of a variable-length history of states.  
    2. It is possible to use the _same_ transition function $$f$$ with the same parameters at every time step.  
    Thus, we can learn a single shared model $$f$$ that operates on all time steps and all sequence lengths, rather than needing to learn a separate model $$g^{(t)}$$ for all possible time steps  

    __Benefits:__   
    {: #lst-p}
    * Allows generalization to sequence lengths that did _not_ appear in the training set
    * Enables the model to be estimated to be estimated with far fewer training examples than would be required without parameter sharing.  
    <br>

5. **The State of the RNN $$\mathbf{h}^{(t)}$$:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    The network typically learns to use $$\mathbf{h}^{(t)}$$ as a kind of _lossy summary_ of the task-relevant aspects of the past sequence of inputs up to $$t$$.  
    This summary is, in general, _necessarily lossy_, since it maps an arbitrary length sequence $$\left(\boldsymbol{x}^{(t)}, \boldsymbol{x}^{(t-1)}, \boldsymbol{x}^{(t-2)}, \ldots, \boldsymbol{x}^{(2)}, \boldsymbol{x}^{(1)}\right)$$  to a fixed length vector $$h^{(t)}$$.  

    The most demanding situation (the extreme) is when we ask $$h^{(t)}$$ to be rich enough to allow one to approximately recover/reconstruct the input sequence, as in __AutoEncoders__.  

6. **RNN Architectures/Design Patterns:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    We will be introducing three variations of the RNN, and will be analyzing _variation 1_, the basic form of the RNN.  
    1. __Variation 1; The Standard RNN (basic form):__{: #bodyContents161}  
        <button>Architecture</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](/main_files/dl/archits/rnns/4.png){: width="100%" hidden=""}  
        * __Architecture__:  
            * Produces an output at each time-step
            * Recurrent connections between hidden units  
        * __Equations__:  
            The standard RNN is __parametrized__ with three weight matrices and three bias vectors:  
            <p>$$\theta=\left[W_{h x} = U, W_{h h} = W, W_{o h} = V, b_{h}, b_{o}, h_{0}\right]$$</p>  
            Then given an input sequence $$\left(\boldsymbol{x}^{(t)}, \boldsymbol{x}^{(t-1)}, \boldsymbol{x}^{(t-2)}, \ldots, \boldsymbol{x}^{(2)}, \boldsymbol{x}^{(1)}\right)$$ the RNN performs the following computations for every time step:   
            <p>$$\begin{aligned} \boldsymbol{a}^{(t)} &=\boldsymbol{b}+\boldsymbol{W h}^{(t-1)}+\boldsymbol{U} \boldsymbol{x}^{(t)} \\ \boldsymbol{h}^{(t)} &=\tanh \left(\boldsymbol{a}^{(t)}\right) \\ \boldsymbol{o}^{(t)} &=\boldsymbol{c}+\boldsymbol{V} \boldsymbol{h}^{(t)} \\ \hat{\boldsymbol{y}}^{(t)} &=\operatorname{softmax}\left(\boldsymbol{o}^{(t)}\right) \end{aligned}$$</p>  
            where the parameters are the bias vectors $$\mathbf{b}$$ and $$\mathbf{c}$$ along with the weight matrices $$\boldsymbol{U}$$, $$\boldsymbol{V}$$ and $$\boldsymbol{W}$$, respectively, for input-to-hidden, hidden-to-output and hidden-to-hidden connections.  
            We, also, Assume the hyperbolic tangent activation function, and that the output is discrete[^1].  
        * __The (Total) Loss__:  
            The __Total Loss__ for a given sequence of $$\mathbf{x}$$ values paired with a sequence of $$\mathbf{y}$$ values is the _sum of the losses over all the time steps_. Assuming $$L^{(t)}$$ is the __negative log-likelihood__ of $$y^{(t)}$$ given $$\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(t)}$$, then:  
            <p>$$\begin{aligned} & L\left(\left\{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(\tau)}\right\},\left\{\boldsymbol{y}^{(1)}, \ldots, \boldsymbol{y}^{(\tau)}\right\}\right) \\=& \sum_{t} L^{(t)} \\=& -\sum_{t} \log p_{\text { model }}\left(y^{(t)} |\left\{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(t)}\right\}\right) \end{aligned}$$</p>  
            where $$p_{\text { model }}\left(y^{(t)} |\left\{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(t)}\right\}\right)$$ is given by reading the entry for $$y^{(t)}$$ from the model's output vector $$\hat{\boldsymbol{y}}^{(t)}$$.  
        * __Complexity__:  
            * __Forward Pass__:  
                The runtime is $$\mathcal{O}(\tau)$$ and cannot be reduced by parallelization because the forward propagation graph is inherently sequential; each time step may only be computed after the previous one.  
            * __Backward Pass__:  
                The standard algorithm used is called __Back-Propagation Through Time (BPTT)__, with a runtime of $$\mathcal{O}(\tau)$$  
                                 
        * __Properties__:  
            * The Standard RNN is __Universal__, in the sense that any function computable by a __Turing Machine__ can be computed by such an RNN of a _finite size_.  
                > The functions computable by a Turing machine are discrete, so these results regard exact implementation of the function, not approximations.  
                The RNN, when used as a Turing machine, takes a binary sequence as input, and its outputs must be discretized to provide a binary output.  

            * The output can be read from the RNN after a number of time steps that is asymptotically linear in the number of time steps used by the Turing machine and asymptotically linear in the length of the input (_Siegelmann and Sontag, 1991; Siegelmann, 1995; Siegelmann and Sontag, 1995; Hyotyniemi, 1996_).  
            * The theoretical RNN used for the proof can simulate an __unbounded stack__ by representing its activations and weights with rational numbers of unbounded precision.  

    2. __Variation 2:__{: #bodyContents162}  
        <button>Architecture</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](/main_files/dl/archits/rnns/5.png){: width="100%" hidden=""}  
        * __Architecture__:  
            * Produces an output at each time-step  
            * Recurrent connections _only_ from the output at one time step to the hidden units at the next time step  
        * __Equations__:  
            <p>$$\begin{aligned} \boldsymbol{a}^{(t)} &=\boldsymbol{b}+\boldsymbol{W o}^{(t-1)}+\boldsymbol{U} \boldsymbol{x}^{(t)} \\
            \boldsymbol{h}^{(t)} &=\tanh \left(\boldsymbol{a}^{(t)}\right) \\
            \boldsymbol{o}^{(t)} &=\boldsymbol{c}+\boldsymbol{V} \boldsymbol{h}^{(t)} \\
            \hat{\boldsymbol{y}}^{(t)} &=\operatorname{softmax}\left(\boldsymbol{o}^{(t)}\right) \end{aligned}$$</p>  
        * __Properties__:  
            * Strictly __less powerful__ because it _lacks hidden-to-hidden recurrent connections_.  
                It __cannot__ simulate a _universal Turing Machine_.  
            * It requires that the output units capture all the information about the past that the network will use to predict the future; due to the lack of hidden-to-hidden recurrence.  
                But, since the outputs are trained to match the training set targets, they are unlikely to capture the necessary information about the past history.  
            * The __Advantage__ of eliminating hidden-to-hidden recurrence is that all the time steps are __de-coupled__[^2]. Training can thus be parallelized, with the gradient for each step $$t$$ computed in isolation.  
                Thus, the model can be trained with [__Teacher Forcing__](#bodyContents17).  
                

    3. __Variation 3:__{: #bodyContents163}  
        <button>Architecture</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        ![img](/main_files/dl/archits/rnns/6.png){: width="100%" hidden=""}  
        * __Architecture__:  
            * Produces a _single_ output, after reading entire sequence
            * Recurrent connections between hidden units
        * __Equations__:  
            <p>$$\begin{aligned} \boldsymbol{a}^{(t)} &=\boldsymbol{b}+\boldsymbol{W h}^{(t-1)}+\boldsymbol{U} \boldsymbol{x}^{(t)} \\
            \boldsymbol{h}^{(t)} &=\tanh \left(\boldsymbol{a}^{(t)}\right) \\
            \boldsymbol{o} = \boldsymbol{o}^{(T)} &=\boldsymbol{c}+\boldsymbol{V} \boldsymbol{h}^{(T)} \\
            \hat{\boldsymbol{y}} = \hat{\boldsymbol{y}}^{(T)} &=\operatorname{softmax}\left(\boldsymbol{o}^{(T)}\right) \end{aligned}$$</p>  


7. **Teacher Forcing:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    Teacher forcing is a procedure that emerges from the maximum likelihood criterion, in which during training the model receives the ground truth output $$y^{(t)}$$ as input at time $$t + 1$$.  
    
    Models that have recurrent connections from their _outputs_ leading _back into the model_ may be trained with teacher forcing.  

    Teacher forcing may still be applied to models that have hidden-to-hidden connections as long as they have connections from the output at one time step to values computed in the next time step. As soon as the hidden units become a function of earlier time steps, however, the BPTT algorithm is necessary. Some models may thus be trained with both teacher forcing and BPTT.  

    The __disadvantage__ of strict teacher forcing arises if the network is going to be later used in an __closed-loop__ mode, with the network outputs (or samples from the output distribution) fed back as input. In this case, the fed-back inputs that the network sees during training could be quite different from the kind of inputs that it will see at test time.  
    
    __Methods for Mitigation:__  
    {: #lst-p}
    1. Train with both teacher-forced inputs and free-running inputs, for example by predicting the correct target a number of steps in the future through the unfolded recurrent output-to-input paths[^3].  
    2. Another approach (_Bengio et al., 2015b_) to mitigate the gap between the inputs seen at training time and the inputs seen at test time randomly chooses to use generated values or actual data values as input. This approach exploits a curriculum learning strategy to gradually use more of the generated values as input.

    > proof: p.377, 378  

9. **Computing the Gradient in an RNN:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents19}  
    > Note: The computation is, as noted before, w.r.t. the [__standard RNN (variation 1)__](#bodyContents161)  

    Computing the gradient through a recurrent neural network is straightforward. One simply applies the generalized back-propagation algorithm of _section 6.5.6_ to the unrolled computational graph. No specialized algorithms are necessary.  

    <button>Derivation</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](/main_files/dl/archits/rnns/7.png){: width="100%" hidden=""}  
    Once the gradients on the internal nodes of the computational graph are obtained, we can obtain the gradients on the parameter nodes, which have descendents at all the time steps:  
    <button>Derivation Cont'd</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](/main_files/dl/archits/rnns/8.png){: width="100%" hidden=""}  

    __Notes:__  
    {: #lst-p}
    * We do not need to compute the gradient with respect to $$\mathbf{x}^{(t)}$$ for training because it does not have any parameters as ancestors in the computational graph defining the loss.  

    
    [^1]: A natural way to represent discrete variables is to regard the output $$\mathbf{o}$$ as giving the unnormalized log probabilities of each possible value of the discrete variable. We can then apply the softmax operation as a post-processing step to obtain a vector $$\hat{\boldsymbol{y}}$$ of normalized probabilities over the output.  
    [^2]: for any loss function based on comparing the prediction at time $$t$$ to the training target at time $$t$$.  
    [^3]: In this way, the network can learn to take into account input conditions (such as those it generates itself in the free-running mode) not seen during training and how to map the state back toward one that will make the network generate proper outputs after a few steps.  
    

10. **Recurrent Networks as Directed Graphical Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents110}  
    Since we wish to interpret the _output_ of an RNN as a _probability distribution_, we usually use the __cross-entropy__ associated with that distribution to define the _loss_.  
    > E.g. Mean squared error is the cross-entropy loss associated with an output distribution that is a unit Gaussian.  

    When we use a _predictive log-likelihood training objective_, such as equation 10.12, we train the RNN to _estimate the conditional distribution_ of the next sequence element $$\boldsymbol{y}^{(t)}$$ given the past inputs. This may mean that we maximize the log-likelihood:  
    <p>$$\log p\left(\boldsymbol{y}^{(t)} | \boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(t)}\right) \tag{10.29}$$</p>  
    or, if the model includes connections from the output at one time step to the nexttime step,  
    <p>$$\log p\left(\boldsymbol{y}^{(t)} | \boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(t)}, \boldsymbol{y}^{(1)}, \ldots, \boldsymbol{y}^{(t-1)}\right) \tag{10.30}$$</p>  
    Decomposing the joint probability over the sequence of $$\mathbf{y}$$ values as a series of one-step probabilistic predictions is one way to capture the _full joint distribution_ across the whole sequence. When we do not feed past $$\mathbf{y}$$ values as inputs that condition the next step prediction, the outputs $$\mathbf{y}$$ are __conditionally independent__ given the sequence of $$\mathbf{x}$$ values.  



    __Summary:__{: style="color: red"}  
    This section is useful for understanding RNN from a _probabilistic graphical model_ perspective. The main point is to show that __RNN provides a very efficient parametrization of the *joint distribution* over the observations $$y^{(t)}$$.__  
    The introduction of _hidden state_ and _hidden-to-hidden_ connections can be motivated as reducing [fig 10.7]() to [fig 10.8](); which have $$\mathcal{O}(k^{\tau})$$ and $$\mathcal{O}(1)\times \tau$$ parameters, respectively (where $$\tau$$ is the length of the sequence).  


18. **Backpropagation Through Time:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents118}  
    * We can think of the recurrent net as a layered, feed-forward net with shared weights and then train the feed-forward net with (linear) weight constraints.
    * We can also think of this training algorithm in the time domain:
        * The forward pass builds up a stack of the activities of all the units at each time step
        * The backward pass peels activities off the stack to compute the error derivatives at each time step
        * After the backward pass we add together the derivatives at all the different times for each weight.  
    <br>


19. **Downsides of RNNs:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents119}  
    * RNNs are not __Inductive__: They memorize sequences extremely well, but they don‚Äôt necessarily always show convincing signs of generalizing in the correct way.  
    * They unnecessarily __couple their representation size to the amount of computation per step__: if you double the size of the hidden state vector you‚Äôd quadruple the amount of FLOPS at each step due to the matrix multiplication.  
        > Ideally, we‚Äôd like to maintain a huge representation/memory (e.g. containing all of Wikipedia or many intermediate state variables), while maintaining the ability to keep computation per time step fixed.  
    <br>


20. **RNNs as a model with Memory \| Comparison with other Memory models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents120}  
    [Modeling Sequences](/concepts_#bodyContents23)  


__NOTES:__  
{: #lst-p}
* RNNs may also be applied in two dimensions across spatial data such as images  
* A __Deep RNN in vertical dim (stacking up hidden layers)__ increases the memory representational ability with _linear scaling in computation_ (as opposed to increasing the size of the hidden layer -> quadratic computation).  
* A __Deep RNN in time-dim (add extra pseudo-steps for each real step)__ increase ONLY the representational ability (efficiency) and NOT memory.  
* __Dropout in Recurrent Connections__: dropout is ineffective when applied to recurrent connections as repeated random masks zero all hidden units in the limit. The most common solution is to only apply dropout to non-recurrent connections.  
* **Different Connections in RNN Architectures:**  
    1. __PeepHole Connection:__  
        is an addition on the equations of the __LSTM__ as follows:  
        <p>$$ \Gamma_o = \sigma(W_o[a^{(t-1)}, x^{(t)}] + b_o) \\
        \implies 
        \sigma(W_o[a^{(t-1)}, x^{(t)}, c^{(t-1)}] + b_o)$$</p>  
        Thus, we add the term $$c^{(t-1)}$$ to the output gate.  
* __Learning Long-Range Dependencies in RNNs/sequence-models__:  
    One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies.  
* __LSTM (simple) Implementation__: [github](https://github.com/nicodjimenez/lstm), [blog](http://nicodjimenez.github.io/2014/08/08/lstm.html)
* [**Sampling from RNNs**](https://medium.com/machine-learning-at-petiteprogrammer/sampling-strategies-for-recurrent-neural-networks-9aea02a6616f){: value="show" onclick="iframePopA(event)"}
<a href="https://medium.com/machine-learning-at-petiteprogrammer/sampling-strategies-for-recurrent-neural-networks-9aea02a6616f"></a>
    <div markdown="1"> </div>    
* __Gradient Clipping Intuition__:  
    ![img](/main_files/concepts/1.png){: width="55%"}   
    * The image above is that of the __Error Surface__ of a _single hidden unit RNN_  
    * The observation here is that there exists __High Curvature Walls__.   
        This Curvature Wall will move the gradient to a very different/far, probably less useful area. 
        Thus, if we clip the gradients we will avoid the walls and will remain in the more useful area that we were exploring already.   
    Draw a line between the original point on the Error graph and the End (optimized) point then evaluate the Error on points on that line and look at the changes $$\rightarrow$$ this shows changes in the curvature.  



***
***

TITLE: PGMs <br /> Probabilistic Graphical Models
LINK: research/dl/archt/pgm.md


__Resources:__{: style="color: red"}  
{: #lst-p}
* [An Introduction to Probabilistic Graphical Models: Conditional Independence and Factorization (M Jordan)](http://people.eecs.berkeley.edu/~jordan/prelims/chapter2.pdf)  
* [An Intro to PGMs: The Elimination Algorithm (M Jordan)](http://people.eecs.berkeley.edu/~jordan/prelims/chapter3.pdf)  
* [An Intro to PGMs: Probability Propagation and Factor Graphs](http://people.eecs.berkeley.edu/~jordan/prelims/chapter4.pdf)  
* [An Intro to PGMs: The EM algorithm](http://people.eecs.berkeley.edu/~jordan/prelims/chapter11.pdf)  
* [An Intro to PGMs: Hidden Markov Models](http://people.eecs.berkeley.edu/~jordan/prelims/chapter12.pdf)  
* [A Brief Introduction to Graphical Models and Bayesian Networks (Paper!)](https://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html)  
* [Deep Learning vs Probabilistic Graphical Models vs Logic (Blog!)](http://www.computervisionblog.com/2015/04/deep-learning-vs-probabilistic.html)  
* [Probabilistic Graphical Models CS-708 (CMU!)](https://sailinglab.github.io/pgm-spring-2019/)  
* [Graphical Models, Exponential Families, and Variational Inference (M. Jordan)](https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf)  


## Graphical Models
{: #content1}

<button>Taxonomy of Graphical Models</button>{: .showText value="show" onclick="showTextPopHide(event);"}
![img](https://cdn.mathpix.com/snip/images/_wohJaHgQzjzawD16m9vaHl-7jvkyn4IcpBHGDltoKc.original.fullsize.png){: width="100%" hidden=""}  


0. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents10}  
    Machine learning algorithms often involve probability distributions over a very large number of random variables. Often, these probability distributions involve direct interactions between relatively few variables. Using a single function to describe the entire joint probability distribution can be very inefficient (both computationally and statistically).  

    > A description of a probability distribution is _exponential_ in the number of variables it models.  


1. **Graphical Model:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    A __graphical model__ or __probabilistic graphical model (PGM)__ or __structured probabilistic model__ is a probabilistic model for which a graph expresses the conditional dependence structure (factorization of a probability distribution) between random variables.  
    > Generally, this is one of the most common _statistical models_  

    __Properties:__{: style="color: red"}  
    {: #lst-p}
    * __Factorization__  
    * __Independence__  

    __Graph Structure:__  
    A PGM uses a graph $$\mathcal{G}$$ in which each _node_ in the graph corresponds to a _random variable_, and an _edge_ connecting two r.vs means that the probability distribution is able to _represent interactions_ between those two r.v.s.  

    
    __Types:__  
    {: #lst-p}
    * __Directed__:  
        Directed models use graphs with directed edges, and they represent factorizations into conditional probability distributions.  
        They contain one factor for every random variable $$x_i$$ in the distribution, and that factor consists of the conditional distribution over $$x_i$$ given the parents of $$x_i$$.  
    * __Undirected__:  
        Undirected models use graphs with undirected edges, and they represent factorizations into a set of functions; unlike in the directed case, these functions are usually not probability distributions of any kind.  


    __Core Idea of Graphical Models:__{: style="color: red"}  
    <span>The probability distribution factorizes according to the cliques in the graph, with the potentials usually being of the exponential family (and a graph expresses the conditional dependence structure between random variables).</span>{: style="color: goldenrod"}    
    <br>
            
2. **Neural Networks and Graphical Models:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    __Deep NNs as PGMs:__{: style="color: red"}  
    You can view a deep neural network as a graphical model, but here, the CPDs are not probabilistic but are deterministic. Consider for example that the input to a neuron is $$\vec{x}$$ and the output of the neuron is $$y .$$ In the CPD for this neuron we have, $$p(\vec{x}, y)=1,$$ and $$p(\vec{x}, \hat{y})=0$$ for $$\hat{y} \neq y .$$ Refer to the section 10.2 .3 of Deep Learning Book for more details.  
    <br>
 

***

## Bayesian Network
{: #content2}

1. **Bayesian Network:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    A __Bayesian network__, __Bayes network__, __belief network__, or __probabilistic directed acyclic graphical model__ is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (__DAG__).  
    > E.g. a Bayesian network could represent the probabilistic relationships between diseases and symptoms.  


    __Bayes Nets (big picture):__{: style="color: red"}  
    __Bayes Nets:__ a technique for describing complex joint distributions (models) using simple, local distributions (conditional probabilities).  

    In other words, they are a device for describing a complex distribution, over a large number of variables, that is built up of small pieces (_local interactions_); with the assumptions necessary to conclude that the product of those local interactions describe the whole domain.  

    __Formally,__ a Bayes Net consists of:  
    {: #lst-p}
    1. A __directed acyclic graph of nodes__, one per variable $$X$$ 
    2. A __conditional distribution for each node $$P(X\vert A_1\ldots A_n)$$__, where $$A_i$$ is the $$i$$th parent of $$X$$, stored as a *__conditional probability table__* or *__CPT__*.  
        Each CPT has $$n+2$$ columns: one for the values of each of the $$n$$ parent variables $$A_1 \ldots A_n$$, one for the values of $$X$$, and one for the conditional probability of $$X$$.  

    Each node in the graph represents a single random variable and each directed edge represents one of the conditional probability distributions we choose to store (i.e. an edge from node $$A$$ to node $$B$$ indicates that we store the probability table for $$P(B\vert A)$$).  
    <span>Each node is conditionally independent of all its ancestor nodes in the graph, given all of its parents.</span>{: style="color: goldenrod"} Thus, if we have a node representing variable $$X$$, we store $$P(X\vert A_1,A_2,...,A_N)$$, where $$A_1,\ldots,A_N$$ are the parents of $$X$$.   


    __The _local probability tables (of conditional distributions)_ and the _DAG_ together encode enough information to compute any probability distribution that we could have otherwise computed given the entire joint distribution.__{: style="color: goldenrod"}  


    __Motivation:__{: style="color: red"}  
    There are problems with using full join distribution tables as our probabilistic models:  
    {: #lst-p}
    * Unless there are only a few variables, the joint is WAY too big to represent explicitly
    * Hard to learn (estimate) anything empirically about more than a few variables at a time  


    __Examples of Bayes Nets:__{: style="color: red"}  
    {: #lst-p}
    * __Coin Flips__:  
        img1
    * __Traffic__:  
        img2
    * __Traffic II__:  
        img3
    * __Alarm Network__:  
        img4


    __Probabilities in BNs:__{: style="color: red"}  
    {: #lst-p}
    * Bayes Nets *__implicitly__* encode <span>__joint distributions__</span>{: style="color: purple"}:  
        Encoded as a <span>*__product__* of _local_ __conditional distributions__</span>{: style="color: purple"}:  
        <p>$$p(x_1, x_2, \ldots, x_n) = \prod_{i=1}^{n} p(x_i \vert \text{parents}(X_i)) \tag{1.1}$$</p>  
    * We are guaranteed that $$1.1$$ results in a proper joint distribution:  
        1. Chain Rule is valid for all distributions:  
            <p>$$p(x_1, x_2, \ldots, x_n) = \prod_{i=1}^{n} p(x_i \vert x_1, \ldots, x_{i-1})$$</p>  
        2. Conditional Independences Assumption:  
            <p>$$p(x_1, x_2, \ldots, x_{i-1}) = \prod_{i=1}^{n} p(x_i \vert \text{parents}(X_i))$$</p>
            $$\implies$$  
            <p>$$p(x_1, x_2, \ldots, x_n) = \prod_{i=1}^{n} p(x_i \vert \text{parents}(X_i)) \tag{1.1}$$</p>  

    Thus (from above) Not every BN can represent every joint distribution.  
    The topology enforces certain conditional independencies that need to be met.  
    > e.g. Only distributions whose variables are _absolutely independent_ can be represented by a Bayes Net with no arcs  


    __Causality:__{: style="color: red"}  
    Although the structure of the BN might be in a way that encodes causality, it is not necessary to define the joint distribution graphically. The two definitions below are the same:  
    img5  
    To summarize:  
    {: #lst-p}
    * When BNs reflect the true causal patterns:  
        * Often simpler (nodes have fewer parents)
        * Often easier to think about
        * Often easier to elicit from experts 
    * BNs need NOT be causal:  
        * Sometimes no causal net exists over the domain (especially if variables are missing)  
            * e.g. consider the variables $$\text{Traffic}$$ and $$\text{Drips}$$  
        * Results in arrows that reflect __correlation__, not __causation__  
    * The meaning of the arrows:  
        * The topology may happen to encode causal structure
        * But, the <span>topology really encodes conditional independence </span>{: style="color: goldenrod"}   
            <p>$$p(x_1, x_2, \ldots, x_{i-1}) = \prod_{i=1}^{n} p(x_i \vert \text{parents}(X_i))$$</p>
    <br>

    __Questions we can ask__{: style="color: red"}  
    Since a BN encodes a __joint distribution__ we can ask any questions a joint distribution can answer:  
    {: #lst-p}
    * __Inference:__ given a fixed BN, what is $$P(X \vert \text { e)? }$$
    * __Representation:__ given a BN graph, what kinds of distributions can it encode?
    * __Modeling:__ what BN is most appropriate for a given domain?  


    __Size of a BN:__{: style="color: red"}  
    {: #lst-p}
    - Size of a full __joint distribution__ over $$N$$ (boolean) variables: $$2^N$$  
    - Size of an __$$N$$-node net__ with nodes having up to $$k$$ parents: $$\mathcal{O}(N \times 2^{k+1}$$  


    __Advantages of BN over full joint distribution:__{: style="color: red"}  
    {: #lst-p}
    * Both will model (calculate) $$p(X_1, X_2, \ldots, X_N)$$  
    * BNs give you huge space savings  
    * It is easier to elicit local CPTs  
    * Is faster to answer queries      
    <br>


    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * The __acyclicity__ gives an order to the (order-less) chain-rule of conditional probabilities  
    * Think of the conditional distribution for each node as a _description of a noisy "causal" process_  
    * The graph of the BN represents certain independencies directly, but also contains extra independence assumptions that can be "inferred" from the shape of the graph  
        ![img](https://cdn.mathpix.com/snip/images/kZDOC0zRNk2bHppVoSor5oVJXp1YkcVdem6Sqd77bCE.original.fullsize.png){: width="34%"}  
    * There could be extra independence relationships in the distribution that are not represented by the BN graph structure but can be read in the CPT. This happens when the structure of the BN is not "optimal" for the assumptions between the variables.  
    <br>

2. **Independence / D-Separation:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    Goal is to find a graph algorithm that can show independencies between variables in BNs. Steps:    
    1. Study independence properties for triples  
    2. Analyze complex cases (configurations) in terms of member triples  
    3. D-separation: a condition / algorithm for answering such queries  

    
    __Causal Chains:__{: style="color: red"}  
    ![img](https://cdn.mathpix.com/snip/images/C14nDRwzxp3kdGjCGvBCfdT5wQT3-5ejjNUWa6aCElg.original.fullsize.png){: width="34%"}  
    A Causal Chain is a configuration of __three nodes__ that expresses the following representation of the joint distribution over $$X$$, $$Y$$ and $$Z$$:  
    <p>$$P(x, y, z)=P(z \vert y) P(y \vert x) P(x)$$</p>  

    Let's try to see if we can guarantee independence between $$X$$ and $$Z$$:  
    * __No Observations:__  
        ![img](https://cdn.mathpix.com/snip/images/vCusDjGbyWit9iVXoCw-ZDdK5JGdx32uk7qsKe0xCAc.original.fullsize.png){: width="38%"}  
        $$X$$ and $$Z$$ are __not guaranteed independent__.  
        * __Proof__:  
            By Counterexample:  
            <p>$$P(y \vert x)=\left\{\begin{array}{ll}{1} & {\text { if } x=y} \\ {0} & {\text { else }}\end{array} \quad P(z \vert y)=\left\{\begin{array}{ll}{1} & {\text { if } z=y} \\ {0} & {\text { else }}\end{array}\right.\right.$$</p>  
            $$\text { In this case, } P(z \vert x)=1 \text { if } x=z \text { and } 0 \text { otherwise, so } X \text { and } Z \text { are not independent.}$$  

    * __$$Y$$ Observed:__  
        ![img](https://cdn.mathpix.com/snip/images/W1ji7bFRCqCmTeO_sFPz3BZXnIaY2zRlLtpv1vIKiUw.original.fullsize.png){: width="38%"}  
        $$X$$ and $$Z$$ are __independent given $$Y$$__. i.e. $$P(X \vert Z, Y)=P(X \vert Y)$$  
        * __Proof:__  
            <p>$$\begin{aligned} P(X \vert Z, y) &=\frac{P(X, Z, y)}{P(Z, y)}=\frac{P(Z \vert y) P(y \vert X) P(X)}{\sum_{x} P(X, y, Z)}=\frac{P(Z \vert y) P(y \vert X) P(X)}{P(Z \vert y) \sum_{x} P(y \vert x) P(x)} \\ &=\frac{P(y \vert X) P(X)}{\sum_{x} P(y \vert x) P(x)}=\frac{P(y \vert X) P(X)}{P(y)}=P(X \vert y) \end{aligned}$$</p>  
    An analogous proof can be used to show the same thing for the case where X has multiple parents.  

    To summarize, <span>in the causal chain chain configuration, $$X \perp Z \vert Y$$ </span>{: style="color: goldenrod"}.  

    > Evidence along the chain "blocks" the influence.  

    * [**Causal Chains (188)**](https://www.youtube.com/embed/FUnOdyZZAaE?start=1698){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/FUnOdyZZAaE?start=1698"></a>
        <div markdown="1"> </div>    

    
    __Common Cause:__{: style="color: red"}  
    __Common Cause__ is another configuration for a triple. It expresses the following representation:  
    <p>$$P(x, y, z)=P(x \vert y) P(z \vert y) P(y)$$</p>  
    ![img](https://cdn.mathpix.com/snip/images/CduaPDNSTAmqr_VCjz80jm0h8KUAPQogTozW5-zxe8s.original.fullsize.png){: width="34%"}  

    Let's try to see if we can guarantee independence between $$X$$ and $$Z$$:  
    * __No Observations:__  
        ![img](https://cdn.mathpix.com/snip/images/5WPHMjwgo5l-on6MzOi4urmfDQixe1crSlo38pSfhDY.original.fullsize.png){: width="38%"}  
        $$X$$ and $$Z$$ are __not guaranteed independent__.  
        * __Proof__:  
            By Counterexample:  
            <p>$$P(x \vert y)=\left\{\begin{array}{ll}{1} & {\text { if } x=y} \\ {0} & {\text { else }}\end{array} \quad P(z \vert y)=\left\{\begin{array}{ll}{1} & {\text { if } z=y} \\ {0} & {\text { else }}\end{array}\right.\right.$$</p>  
            \text { Then } P(x \vert z)=1 \text { if } x=z \text { and } 0 \text { otherwise, so } X \text { and } Z \text { are not independent. }  

    * __$$Y$$ Observed:__  
        ![img](https://cdn.mathpix.com/snip/images/Uu8LonpCanIwyab_QKVW_DDx6GR8z7d2fvl1oh5uNtc.original.fullsize.png){: width="38%"}  
        $$X$$ and $$Z$$ are __independent given $$Y$$__. i.e. $$P(X \vert Z, Y)=P(X \vert Y)$$  
        * __Proof:__  
            <p>$$P(X \vert Z, y)=\frac{P(X, Z, y)}{P(Z, y)}=\frac{P(X \vert y) P(Z \vert y) P(y)}{P(Z \vert y) P(y)}=P(X \vert y)$$</p>  

    > Observing the cause blocks the influence.  

    * [**Common Cause (188)**](https://www.youtube.com/embed/FUnOdyZZAaE?start=1922){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/FUnOdyZZAaE?start=1922"></a>
        <div markdown="1"> </div>    


    __Common Effect:__{: style="color: red"}  
    __Common Effect__ is the last configuration for a triplet. Expressing the representation:  
    <p>$$P(x, y, z)=P(y \vert x, z) P(x) P(z)$$</p>  
    ![img](https://cdn.mathpix.com/snip/images/bHoFq74CVLvVJNrj13GmsoiVlJrVuskee1qaDPB5ZZc.original.fullsize.png){: width="34%"}  

    Let's try to see if we can guarantee independence between $$X$$ and $$Z$$:  
    * __No Observations:__  
        ![img](https://cdn.mathpix.com/snip/images/CTuUmWXjwo4kqxrotATNnYossKNqwl6mfjB2H7OPQi8.original.fullsize.png){: width="38%"}  
        $$X$$ and $$Z$$ are, readily, __guaranteed to be independent__: $$X \perp Z$$.  

    * __$$Y$$ Observed:__  
        ![img](https://cdn.mathpix.com/snip/images/wfOTQbsydA4ZTpN0tW0tQtRbfDyNGrokp1xk9nC8JfE.original.fullsize.png){: width="38%"}  
        $$X$$ and $$Z$$ are __not necessarily independent given $$Y$$__. i.e. $$P(X \vert Z, Y)\neqP(X \vert Y)$$  
        * __Proof:__  
            By Counterexample:  
            $$\text { Suppose all three are binary variables. } X \text { and } Z \text { are true and false with equal probability: }$$  
            <p>$$\begin{array}{l}{P(X=\text {true})=P(X=\text { false })=0.5} \\ {P(Z=\text {true})=P(Z=\text { false })=0.5}\end{array}$$</p>  
            $$ \text { and } Y \text { is determined by whether } X \text { and } Z \text { have the same value: } $$  
            <p>$$P(Y \vert X, Z)=\left\{\begin{array}{ll}{1} & {\text { if } X=Z \text { and } Y=\text { true }} \\ {1} & {\text { if } X \neq Z \text { and } Y=\text { false }} \\ {0} & {\text { else }}\end{array}\right.$$</p>  
            $$ \text { Then } X \text { and } Z \text { are independent if } Y \text { is unobserved. But if } Y \text { is observed, then knowing } X \text { will }\\ 
            \text { tell us the value } {\text { of } Z, \text { and vice-versa. } \text{So } X \text { and } Z \text { are } \text {not} \text { conditionally independent given } Y \text {. }} $$  

    Common Effect can be viewed as __"opposite" to Causal Chains and Common Cause__-$$X$$ and $$Z$$ are guaranteed to be independent if $$Y$$ is not conditioned on. But when conditioned on $$Y, X$$ and $$Z$$ may be dependent depending on the specific probability values for $$P(Y \vert X, Z)$$).  

    This same logic applies when conditioning on descendants of $$Y$$ in the graph. If one of $$Y$$ 's descendent nodes is observed, as in Figure $$7, X$$ and $$Z$$ are not guaranteed to be independent.  
    ![img](https://cdn.mathpix.com/snip/images/UBploFCx8ITyj_Yp20s9iAS_tXmj-0-DmF2neJp2alY.original.fullsize.png){: width="34%"}  

    > Observing an effect activates influence between possible causes.  

    * [**Common Effect (188)**](https://www.youtube.com/embed/FUnOdyZZAaE?start=2115){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/FUnOdyZZAaE?start=2115"></a>
        <div markdown="1"> </div>    
    <br>

    __General Case, and D-Separation:__{: style="color: red"}  
    We can use the previous three cases as building blocks to help us answer conditional independence questions on an arbitrary Bayes‚Äô Net with more than three nodes and two edges.  We formulate the problem as follows:  
    __Given a Bayes Net $$G,$$ two nodes $$X$$ and $$Y,$$ and a (possibly empty) set of nodes $$\left\{Z_{1}, \ldots Z_{k}\right\}$$ that represent observed variables, must the following statement be true: $$X \perp Y |\left\{Z_{1}, \ldots Z_{k}\right\} ?$$__  

    __D-Separation:__ (directed separation) is a property of the structure of the Bayes Net graph that implies this conditional independence relationship, and generalizes the cases we‚Äôve seen above. If a set of variables $$Z_{1}, \cdots Z_{k} d-$$ -separates $$X$$ and $$Y,$$ then $$X \perp Y \vert\left\{Z_{1}, \cdots Z_{k}\right\}$$ in all possibutions that can be encoded by the Bayes net.  


    __D-Separation Algorithm:__  
    1. Shade all observed nodes $$\left\{Z_{1}, \ldots, Z_{k}\right\}$$ in the graph.
    2. Enumerate all undirected paths from $$X$$ to $$Y$$ .
    3. For each path:
        * Decompose the path into triples (segments of 3 nodes).
        * If all triples are active, this path is active and $$d$$ -connects $$X$$ to $$Y$$.
    4. If no path d-connects $$X$$ and $$Y$$ and $$Y$$ are d-separated, so they are conditionally independent
    given $$\left\{Z_{1}, \ldots, Z_{k}\right\}$$  

    Any path in a graph from $$X$$ to $$Y$$ can be decomposed into a set of 3 consecutive nodes and 2 edges - each of which is called a triple. A triple is active or inactive depending on whether or not the middle node is observed. If all triples in a path are active, then the path is active and $$d$$ -connects $$X$$ to $$Y,$$ meaning $$X$$ is not guaranteed to be conditionally independent of $$Y$$ given the observed nodes. If all paths from $$X$$ to $$Y$$ are inactive, then $$X$$ and $$Y$$ are conditionally independent given the observed nodes.  

    __Active Triples:__ We can enumerate all possibilities of active and inactive triples using the three canonical graphs we presented above in Figure 8 and 9.  
    ![img](https://cdn.mathpix.com/snip/images/-833dBM_DnuIFc9csn1KJYNu-IH-4TntVXDfprkEvTk.original.fullsize.png){: width="80%" .center-image}  

    * [**General Case and D-Separation (188)**](https://www.youtube.com/embed/FUnOdyZZAaE?start=2331){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/FUnOdyZZAaE?start=2331"></a>
        <div markdown="1"> </div>    


    __Examples:__  
    {: #lst-p}
    * <button>Ex.1</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/vveQfEygnrmDXO3u1dmq5qIg0WFuLlgUwuli1UuWGWE.original.fullsize.png){: width="100%" hidden=""}  
    * <button>Ex.2</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/6kxQgkDxD1oQVhNBNOdEG9Knv0aVcsL6lbHBOpB-opk.original.fullsize.png){: width="100%" hidden=""}  
    * <button>Ex.3</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/LpfPhSioro_8eMua-mQ6oQ9yFtbmHEnaXhJWaOmXGss.original.fullsize.png){: width="100%" hidden=""}  
    * [**Examples (188)**](https://www.youtube.com/embed/FUnOdyZZAaE?start=2840){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/FUnOdyZZAaE?start=2840"></a>
        <div markdown="1"> </div>    
    <br>  

    __Structure Implications:__{: style="color: red"}  
    Given a Bayes net structure, can run d-separation algorithm to build a complete list of conditional independences that are necessarily true of the form.  
    <p>$$X_{i} \perp X_{j} |\left\{X_{k_{1}}, \ldots, X_{k_{n}}\right\}$$</p>  
    This list determines the set of probability distributions that can be represented.  

    __Topology Limits Distributions:__{: style="color: red"}  
    {: #lst-p}
    ![img](https://cdn.mathpix.com/snip/images/gyXj8tjBdUlkWuBYJvVScXjnh_Qsvb7cj6LKTqQL9pc.original.fullsize.png){: width="60%"}  
    * Given some graph topology $$G$$, only certain joint distributions can be encoded  
    * The graph structure guarantees certain (conditional) independences
    * (There might be more independence)
    * Adding arcs increases the set of distributions, but has several costs
    * Full conditioning can encode any distribution  
    * The more assumptions you make the fewer the number of distributions you can represent  

    __Bayes Nets Representation Summary:__{: style="color: red"}  
    {: #lst-p}
    * Bayes nets compactly encode joint distributions
    * Guaranteed independencies of distributions can be deduced from BN graph structure
    * D-separation gives precise conditional independence guarantees from graph alone
    * A Bayes nets joint distribution may have further (conditional) independence that is not detectable until you inspect its specific distribution

    <br>

3. **Inference:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    [Lecture (188)](https://www.youtube.com/watch?v=A1hYXGAUdmU&list=PL7k0r4t5c108AZRwfW-FhnkZ0sCKBChLH&index=15)  

    __Inference__ is the process of calculating the joint PDF for some set of query variables based on some set of observed variables.  
    For example:  
    {: #lst-p}
    * __Posterior Probability__ inference:  
        <p>$$P\left(Q \vert E_{1}=e_{1}, \ldots E_{k}=e_{k}\right)$$</p>  
    * __Most Likely Explanation__ inference:  
        <p>$$\operatorname{argmax}_{q} P\left(Q=q \vert E_{1}=e_{1} \ldots\right)$$</p>  

    __Notation - General Case:__  
    <p>$$ \left.\begin{array}{ll}{\textbf { Evidence variables: }} & {E_{1} \ldots E_{k}=e_{1} \ldots e_{k}} \\ {\textbf { Query}^{* } \textbf { variable: }} & {Q} \\ {\textbf { Hidden variables: }} & {H_{1} \ldots H_{r}}\end{array}\right\} \begin{array}{l}{X_{1}, X_{2}, \ldots X_{n}} \\ {\text { All variables }}\end{array} $$</p>  


    __Inference by Enumeration:__{: style="color: red"}  
    We can solve this problem _naively_ by forming the joint PDF and using __inference by enumeration__ as described above. This requires the creation of and iteration over an exponentially large table.   
    __Algorithm:__  
    * Select the entries consistent with the evidence
    * Sum out $$\mathrm{H}$$ to get join of Query and evidence  
    * Normalize: $$\times \dfrac{1}{Z} = \dfrac{1}{\text{sum of entries}}$$  
    * [**Inference by Enumeration (188)**](https://www.youtube.com/embed/A1hYXGAUdmU?start=980){: value="show" onclick="iframePopA(event)"}
    <a href="https://www.youtube.com/embed/A1hYXGAUdmU?start=980"></a>
        <div markdown="1"> </div>    



    __Variable Elimination:__{: style="color: red"}  
    Alternatively, we can use __Variable Elimination__: __eliminate__ variables one by one.  
    To eliminate a variable $$X$$, we:  
    {: #lst-p}
    1. Join (multiply together) all factors involving $$X$$.
    2. Sum out $$X$$.  

    A __factor__ is an unnormalized probability; represented as a multidimensional array:  
    {: #lst-p}
    * __Joint Distributions__: $$P(X,Y) \in \mathbb{R}^2$$  
        * Entries $$P(x, y)$$ for all $$x, y$$  
        * Sums to $$1$$  
    * __Selected Joint__: $$P(x,Y) \in \mathbb{R}$$ 
        * A slice of the joint distribution
        * Entries $${P}({x}, {y})$$ for fixed $${x},$$ all $${y}$$  
        * Sums to $$P(x)$$  

    * __Single Conditional__: $$P(Y \vert x)$$  
        * Entries $${P}({y} \vert {x})$$ for fixed $${x},$$ all $${y}$$  
        * Sums to $$1$$ 
    * __Family of Conditionals__: $$P(Y \vert X)$$ 
        * Multiple Conditionals
        * Entries $${P}({y} \vert {x})$$ for all $${x}, {y}$$  
        * Sums to $$\vert X\vert$$  

    * __Specified family__: $$P(y \vert X)$$
        * Entries $$P(y \vert x)$$ for fixed $$y$$ but for all $$x$$
        * Sums to random number; not a distribution  

    > For Joint Distributions, the \# __capital variables__ dictates the _"dimensionality"_ of the array.  


    At all points during variable elimination, each factor will be proportional to the probability it corresponds to but the underlying distribution for each factor won‚Äôt necessarily sum to $$1$$ as a probability distribution should.  

    __Inference by Enumeration vs. Variable Elimination:__  
    __Inference by Enumeration__ is very *__slow__*: You must join up the whole joint distribution before you sum out the hidden variables.  
    __Variable Elimination:__ Interleave __joining__ and __marginalization__.  
    Still NP-hard, but usually much faster.  
    ![img](https://cdn.mathpix.com/snip/images/X9wU1le8_K5c9X2lm59mNGaEkJW8DZnnWHiz-3h4eVw.original.fullsize.png){: width="70%"}  
    Notice that $$\sum_r P(r) P(t \vert r) = P(t)$$, thus, in VE, you end up with $$\sum_{t} P(L \vert t) P(t)$$.   


    __General Variable Elimination - Algorithm:__  
    ![img](/main_files/ml/kmeans/12.png){: width="90%"}  



    __VE - Computational and Space Complexity:__  
    {: #lst-p}
    * The computational and space complexity of variable elimination is determined by the largest factor.  
    * The __elimination ordering__ can greatly affect the size of the largest factor.  
    * There does NOT always exist an ordering that only results in small factors.  
    * __VE__ is <span>NP-Hard</span>{: style="color: goldenrod"}:  
        * __Proof__:  
            We can reduce *__3-Sat__* to a BN-Inference problem.  
            We can encode a _Constrained Satisfiability Problem (CSP)_ in a BN and use it to give a solution to the CSP; if the CSP consists of 3 clauses, then finding a solution for the CSP via BN-Inference is equivalent to solving 3-Sat.  
            <button>Analysis</button>{: .showText value="show" onclick="showTextPopHide(event);"}
            ![img](https://cdn.mathpix.com/snip/images/8ZMywFLFKbZr7mhjmbryM_Mr7sny3j2Bmjb_-gr2kS0.original.fullsize.png){: width="100%" hidden=""}  
    * Thus, __inference in Bayes‚Äô nets is NP-hard__{: style="color: goldenrod"}.  
        __No known efficient probabilistic inference in general.__  

    __Polytrees:__  
    A __Polytree__ is a directed graph with no undirected cycles.  
    For polytrees we can always find an ordering that is efficient.  
    * __Cut-set conditioning for Bayes‚Äô net inference:__ Choose set of variables such that if removed only a polytree remains.   
    <br>  


4. **Sampling:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    __Sampling__ is the process of generating observations/_samples_ from a distribution.  
    \- Sampling is like doing _repeated_ (probabilistic) __simulation__.  
    \- Sampling could be used for __learning__ (e.g. RL). But in the context of BNs, it is used for __Inference__.  
    \- Sampling provides a way to do _efficient_ inference, by presenting us with a <span>tradeoff between accuracy and computation (time)</span>{: style="color: goldenrod"}.  
    \- The __Goal__ is to prove that as the number of samples you generate $$N$$ goes to $$\infty$$, the approximation converges to the true probability you are trying to compute.  
    \- Using sampling in a BN from the entire network is necessary, because listing all the outcomes is too expensive even if we can create them given infinite time.  

    __Idea/Algorithm for Inference:__  
    {: #lst-p}
    * Draw $$N$$ samples from a sampling distribution $$S$$  
    * Compute an approximate posterior probability
    * Show this converges to the true probability $$P$$   

    __Sampling from a given distribution:__  
    {: #lst-p}
    * Get sample $$u$$ from __uniform distribution__ over $$[0,1)$$  
    * Convert this sample $$u$$ into an outcome for the given distribution by having each target outcome associated with a sub-interval of $$[0,1)$$ with sub-interval size equal to probability of the outcome  
    ![img](https://cdn.mathpix.com/snip/images/64k37lx3GpRcNnLpuyn9WgIzIN6GUi1kC3GisMc-iEo.original.fullsize.png){: width="70%"}  

    __Sampling Algorithms in BNs:__  
    {: #lst-p}
    * Prior Sampling
    * Rejection Sampling
    * Likelihood Weighting
    * Gibbs Sampling


    __Prior Sampling:__{: style="color: red"}  


    __Algorithm:__  
    {: #lst-p}
    * For $$i=1,2, \ldots, n$$  
        * Sample $$x_{i}$$ from $$P\left(X_{i} \vert \text { Parents }\left(X_{i}\right)\right)$$  
    * Return $$\left(x_{1}, x_{2}, \ldots, x_{n}\right)$$  

    __Notes:__  
    {: #lst-p}
    * This process generates samples with probability:  
        <p>$$
        \begin{aligned} S_{P S}\left(x_{1} \ldots x_{n}\right)=\prod_{i=1}^{n} P\left(x_{i} \vert \text { Parents }\left(X_{i}\right)\right)=P\left(x_{1} \ldots x_{n}\right) \\ \text { ...i.e. the BN's joint probability } \end{aligned}
        $$</p>  
    * Let the number of samples of an event be $$N_{P S}\left(x_{1} \cdots x_{n}\right)$$  
    * Thus,  
        <p>$$\begin{aligned} \lim _{N \rightarrow \infty} \widehat{P}\left(x_{1}, \ldots, x_{n}\right) &=\lim _{N \rightarrow \infty} N_{P S}\left(x_{1}, \ldots, x_{n}\right) / N \\ &=S_{P S}\left(x_{1}, \ldots, x_{n}\right) \\ &=P\left(x_{1} \ldots x_{n}\right) \end{aligned}$$</p>   
    * I.e., the sampling procedure is *__consistent__*   


    __Rejection Sampling:__{: style="color: red"}  
    __Rejection Sampling__  

    It is also *__consistent__*.  

    __Idea:__  
    {: #lst-p}
    Same as Prior Sampling, but no point in keeping all of the samples. We just tally the outcomes that match our evidence and __reject__ the rest.  

    __Algorithm:__  
    {: #lst-p}
    * Input: evidence instantiation  
    * For $$i=1,2, \ldots, n$$  
        * Sample $$x_{i}$$ from $$P\left(X_{i} \vert \text { Parents }\left(X_{i}\right)\right)$$  
        * If $x_{i}$ not consistent with evidence  
            * Reject: return - no sample is generated in this cycle  
    * Return $$\left(x_{1}, x_{2}, \ldots, x_{n}\right)$$   



    __Likelihood Weighting:__{: style="color: red"}  
    __Likelihood Weighting__  


    __Key Ideas:__  
    {: #lst-p}
    Fixes a problem with Rejection Sampling:  
    * If evidence is unlikely, rejects lots of samples  
    * Evidence not exploited as you sample  
    __Idea__: __*fix* evidence variables__ and sample the rest.  
    * __Problem:__ sample distribution not consistent!  
    * __Solution__: weight by probability of evidence given parents.  


    __Algorithm:__  
    {: #lst-p}
    * Input: evidence instantiation  
    * $$w=1.0$$  
    * for $$i=1,2, \dots, n$$  
        * if $$\mathrm{x}_ {\mathrm{i}}$$ is an evidence variable  
            * $$\mathrm{n} \mathrm{x} _  {\mathrm{i}}=$$ observation $$\mathrm{x}_ {\mathrm{i}}$$ for $$\mathrm{x}_ {\mathrm{i}}$$  
            * $$\operatorname{set} \mathrm{w}=\mathrm{w} * \mathrm{P}\left(\mathrm{x}_ {\mathrm{i}} \vert \text { Parents(X.) }\right.$$    
        * else  
            * Sample $$x_ i$$ from $$P\left(X _ {i} \vert \text { Parents }\left(X _ {i}\right)\right)$$  
    * Return $$\left(\mathrm{x}_ {1}, \mathrm{x}_ {2}, \ldots, \mathrm{x}_ {\mathrm{n}}\right), \mathrm{w}$$  


    __Notes:__  
    {: #lst-p}
    * Sampling distribution if $$z$$ sampled and $$e$$ fixed evidence  
        <p>$$S_{W S}(\mathbf{z}, \mathbf{e})=\prod_{i=1}^{l} P\left(z_{i} \vert \text { Parents }\left(Z_{i}\right)\right)$$</p>  
    * Now, samples have weights  
        <p>$$w(\mathbf{z}, \mathbf{e})=\prod_{i=1}^{m} P\left(e_{i} \vert \text { Parents }\left(E_{i}\right)\right)$$</p>  
    * Together, weighted sampling distribution is consistent  
        <p>$$\begin{aligned} S_{\mathrm{WS}}(z, e) \cdot w(z, e) &=\prod_{i=1}^{l} P\left(z_{i} \vert \text { Parents }\left(z_{i}\right)\right) \prod_{i=1}^{m} P\left(e_{i} \vert \text { Parents }\left(e_{i}\right)\right) \\ &=P(\mathrm{z}, \mathrm{e}) \end{aligned}$$</p>   
    * Likelihood weighting is good
        * We have taken evidence into account as we generate the sample  
        * E.g. here, $$W$$‚Äôs value will get picked based on the evidence values of $$S$$, $$R$$  
        * More of our samples will reflect the state of the world suggested by the evidence   
    * Likelihood weighting doesn‚Äôt solve all our problems  
        * Evidence influences the choice of downstream variables, but not upstream ones (C isn‚Äôt more likely to get a value matching the evidence)  
    * We would like to consider evidence when we sample every variable (leads to __Gibbs sampling__)   


    __Gibbs Sampling:__{: style="color: red"}  
    __Gibbs Sampling__  

    * Procedure: keep track of a full instantiation $$x_1, x_2, \ldots, x_n$$. Start with an arbitrary instantiation consistent with the evidence. Sample one variable at a time, conditioned on all the rest, but keep evidence fixed. Keep repeating this for a long time.
    * Property: in the limit of repeating this infinitely many times the resulting samples come from the correct distribution (i.e. conditioned on evidence).
    * Rationale: both upstream and downstream variables condition on evidence.
    * In contrast: likelihood weighting only conditions on upstream evidence, and hence weights obtained in likelihood weighting can sometimes be very small. Sum of weights over all samples is indicative of how many ‚Äúeffective‚Äù samples were obtained, so we want high weight.   

    * Gibbs sampling produces sample from the query distribution $$P(Q \vert \text { e })$$ in limit of re-sampling infinitely often  
    * Gibbs sampling is a special case of more general methods called __Markov chain Monte Carlo (MCMC)__ methods  
        * __Metropolis-Hastings__ is one of the more famous __MCMC__ methods (in fact, Gibbs sampling is a special case of Metropolis-Hastings)  
    * __Monte Carlo Methods__ are just sampling  

    __Algorithm by Example:__  
    {: #lst-p}
    ![img](https://cdn.mathpix.com/snip/images/KSV3YwhhYal5g8aKVludJcV2vA6BYyBsUPJxVmHP_h4.original.fullsize.png){: width="80%"}  

    __Efficient Resampling of One Variable:__   
    {: #lst-p}
    * Sample from $$\mathrm{P}(\mathrm{S} \vert+\mathrm{c},+\mathrm{r},-\mathrm{w})$$:  
        <p>$$\begin{aligned} P(S \vert+c,+r,-w) &=\frac{P(S,+c,+r,-w)}{P(+c,+r,-w)} \\ &=\frac{P(S,+c,+r,-w)}{\sum_{s} P(s,+c,+r,-w)} \\ &=\frac{P(+c) P(S \vert+c) P(+r \vert+c) P(-w \vert S,+r)}{\sum_{s} P(+c) P(s \vert+c) P(+r \vert+c) P(-w \vert s,+r)} \\ &=\frac{P(+c) P(S \vert+c) P(+r \vert+c) P(-w \vert S,+r)}{P(+c) P(+r \vert+c) \sum_{s} P(s \vert+c)} \\ &=\frac{P(S \vert+c) P(-w \vert S,+r)}{\sum_{s} P(s \vert+c) P(-w \vert s,+r)} \end{aligned}$$</p>  
    * Many things cancel out ‚Äì only CPTs with $$S$$ remain!  
    * More generally: only CPTs that have resampled variable need to be considered, and joined together  


    __Bayes‚Äô Net Sampling Summary:__{: style="color: red"}  
    <button>Summary</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/U2mYDfFzMnaqL7fI4uA1hSlp5XWAq2OXLYW9RN9JlsA.original.fullsize.png){: width="100%" hidden=""}  
    <br>

5. **Decision Networks / VPI (Value of Perfect Information):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    [Decision Networks / VPI (188)](https://www.youtube.com/watch?v=19sr7yKV56I&list=PL7k0r4t5c108AZRwfW-FhnkZ0sCKBChLH&index=17)  
    <br>




***

## Random Field Techniques
{: #content4}

1. **Random Field:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    A __Random Field__ is a random function over an arbitrary domain (usually a multi-dimensional space such as $$\mathbb{R}^{n}$$ ). That is, it is a function $$f(x)$$ that takes on a random value at each point $$x \in \mathbb{R}^{n}$$ (or some other domain). It is also sometimes thought of as a synonym for a stochastic process with some restriction on its index set. That is, by modern definitions, a random field is a generalization of a stochastic process where the underlying parameter need no longer be real or integer valued "but can instead take values that are multidimensional vectors on some manifold.  

    __Formally__  
    Given a probability space $$(\Omega, \mathcal{F}, P),$$ an $$X$$ -valued random field is a collection of $$X$$ -valued random variables indexed by elements in a topological space $$T$$. That is, a random field $$F$$ is a collection  
    <p>$$\left\{F_{t} : t \in T\right\}$$</p>  
    where each $$F_{t}$$ is an $$X$$-valued random variable.  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * [Random Field (wiki)](https://en.wikipedia.org/wiki/Random_field)  
    <br>




***
***

TITLE: Deep Generative Models
LINK: research/dl/archt/dgms.md



* [Probabilistic Models and Generative Neural Networks (paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4943066/)  
* [Latent Variable Model Intuition (slides!)](http://mlvis2016.hiit.fi/latentVariableGenerativeModels.pdf)  
* [Generative Models (OpenAI Blog!)](https://openai.com/blog/generative-models/)  




__In situations respecting the following assumptions, Semi-Supervised Learning should *improve performance*:__{: style="color: red"}  :  
{: #lst-p}
* Semi-supervised learning works <span>when $$p(\mathbf{y} \vert \mathbf{x})$$ and $$p(\mathbf{x})$$ are __tied__ together</span>{: style="color: goldenrod"}.  
    * This happens when $$\mathbf{y}$$ is closely associated with one of the causal factors of $$\mathbf{x}$$.  
* The <span>__best possible model__ of $$\mathbf{x}$$ (wrt. __generalization__)</span>{: style="color: goldenrod"} is the one that <span>*__uncovers__* the above __"true" structure__</span>{: style="color: goldenrod"}, with <span>$$\boldsymbol{h}$$ as a __latent variable__ that *__explains__* the __observed variations__ in $$\boldsymbol{x}$$</span>{: style="color: goldenrod"}.  
    * Since we can write the __Marginal Probability of Data__ as:  
        <p>$$p(\boldsymbol{x})=\mathbb{E}_ {\mathbf{h}} p(\boldsymbol{x} \vert \boldsymbol{h})$$</p>  
        * Because the __"true" generative process__ can be conceived as <span>*__structured__* according to this __directed graphical model__</span>{: style="color: purple"}, with $$\mathbf{h}$$ as the __parent__ of $$\mathbf{x}$$:  
            <p>$$p(\mathbf{h}, \mathbf{x})=p(\mathbf{x} \vert \mathbf{h}) p(\mathbf{h})$$</p>  
    * Thus, __The "ideal" representation learning discussed above should recover these latent factors__.  
* The <span>__marginal__ $$p(\mathbf{x})$$ is *__intimately tied__* to the __conditional__ $$p(\mathbf{y} \vert \mathbf{x})$$, and knowledge of the structure of the former should be helpful to learn the latter</span>{: style="color: purple"}.  
    * Since the __conditional distribution__ of $$\mathbf{y}$$ given $$\mathbf{x}$$ is <span>tied by *Bayes' rule* to the __components in the above equation__</span>{: style="color: purple"}:  
        <p>$$p(\mathbf{y} \vert \mathbf{x})=\frac{p(\mathbf{x} \vert \mathbf{y}) p(\mathbf{y})}{p(\mathbf{x})}$$</p>  





## Introduction and Preliminaries
{: #content9}

1. **Unsupervised Learning:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents91}  
    __Unsupervised Learning__ is the task of making inferences, by learning a better representation from some datapoints that do not have any labels associated with them.  
    It intends to learn/infer an __*a priori* probability distribution__ $$p_{X}(x)$$; I.E. it solves a __density estimation problem__.  
    It is a type of *__self-organized__* __Hebbian learning__ that helps find previously unknown patterns in data set without pre-existing labels.   
    ![img](/main_files/dl/archits/dgms/1.png){: width="100%"}  


2. **Density Estimation:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents92}  
    __Density Estimation__ is a problem in Machine Learning that requires learning a function $$p_{\text {model}} : \mathbb{R}^{n} \rightarrow \mathbb{R}$$, where $$p_{\text {model}}(x)$$ can be interpreted as a __probability  density function__ (if $$x$$ is continuous) or a __probability mass function__ (if $$x$$ is discrete) on the space that the examples were drawn from.  

    To perform such a task well, an algorithm needs to <span>learn the __structure of the data__</span>{: style="color: purple"} it has seen. It must know where examples cluster tightly and where they are unlikely to occur.  

    * __Types__ of Density Estimation:  
        * *__Explicit__*: Explicitly define and solve for $$p_\text{model}(x)$$  
        * *__Implicit__*: Learn model that can sample from $$p_\text{model}(x)$$ without explicitly defining it     

    <br>


3. **Generative Models (GMs):**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents93}  
    A __Generative Model__ is a _statistical model_ of the <span>__joint__ probability distribution</span>{: style="color: purple"} on $$X \times Y$$:  
    <p>$${\displaystyle P(X,Y)}$$</p>   
    where $$X$$ is an _observable_ variable and $$Y$$ is a _target_ variable.  

    In __supervised settings__, a __Generative Model__ is a model of the <span>__conditional__ probability</span>{: style="color: purple"} of the observable $$X,$$ given a target $$y,$$:  
    <p>$$P(X | Y=y)$$</p>   


    __Application - Density Estimation:__{: style="color: red"}  
    Generative Models address the __Density Estimation__ problem, a core problem in unsupervised learning, since they model   
    Given training data, GMs will generate new samples from the same distribution.   
    * __Types__ of Density Estimation:  
        * *__Explicit__*: Explicitly define and solve for $$p_\text{model}(x)$$  
        * *__Implicit__*: Learn model that can sample from $$p_\text{model}(x)$$ without explicitly defining it     


    __Examples of Generative Models:__{: style="color: red"}  
    {: #lst-p}
    * Gaussian Mixture Model (and other types of mixture model)
    * Hidden Markov Model
    * Probabilistic context-free grammar
    * Bayesian network (e.g. Naive Bayes, Autoregressive Model)
    * Averaged one-dependence estimators
    * Latent Dirichlet allocation (LDA)
    * Boltzmann machine (e.g. Restricted Boltzmann machine, Deep belief network)
    * Variational autoencoder
    * Generative Adversarial Networks
    * Flow-based Generative Model

    <button>A generative model for generative (graphical) models - Diagram</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    ![img](https://cdn.mathpix.com/snip/images/ZIHj9KE3aa7i-o5jSaxLD8fP-tDPzFtBf7ymVT8g2JQ.original.fullsize.png){: width="100%" hidden=""}  

    

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * Generative Models are __Joint Models__.  
    * Latent Variables are __Random Variables__{: style="color: goldenrod"}.  
    <br>


***

## Deep Generative Models
{: #content1}

1. **Generative Models (GMs):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    

2. **Deep Generative Models (DGMs):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    
    * DGMs __represent *probability distributions* over multiple variables__ in some way:  
        * Some allow the probability distribution function to be evaluated explicitly. 
        * Others do not allow the evaluation of the probability distribution function but support operations that implicitly require knowledge of it, such as drawing samples from the distribution.  
    * __Structure/Representation:__  
        * Some of these models are structured probabilistic models described in terms of graphs and factors, using the language of (probabilistic) graphical models.  
        * Others cannot be easily described in terms of factors but represent probability distributions nonetheless.  







***

## SECOND
{: #content2}









***

## THIRD
{: #content3}











***
***

TITLE: Gated Units <br /> RNN Architectures
LINK: research/dl/archt/gated_recurrent_units.md



* [Building an LSTM from Scratch in PyTorch](http://mlexplained.com/category/fromscratch/)  


## GRUs
{: #content2}

1. **Gated Recurrent Units:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21} 
    :   __Gated Recurrent Units (GRUs)__ are a class of modified (_**Gated**_) RNNs that allow them to combat the _vanishing gradient problem_ by allowing them to capture more information/long range connections about the past (_memory_) and decide how strong each signal is.  

2. **Main Idea:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22} 
    :   Unlike _standard RNNs_ which compute the hidden layer at the next time step directly first, __GRUs__ computes two additional layers (__gates__):  
        > Each with different weights
    :   * *__Update Gate__*:  
    :   $$z_t = \sigma(W^{(z)}x_t + U^{(z)}h_{t-1})$$  
    :   * *__Reset Gate__*:  
    :   $$r_t = \sigma(W^{(r)}x_t + U^{(r)}h_{t-1})$$  
    :   The __Update Gate__ and __Reset Gate__ computed, allow us to more directly influence/manipulate what information do we care about (and want to store/keep) and what content we can ignore.  
        We can view the actions of these gates from their respecting equations as:  
    :   * *__New Memory Content__*:  
            at each hidden layer at a given time step, we compute some new memory content,  
            if the reset gate $$ = ~0$$, then this ignores previous memory, and only stores the new word information.  
    :   $$ \tilde{h}_t = \tanh(Wx_t + r_t \odot Uh_{t-1})$$
    :   * *__Final Memory__*:  
            the actual memory at a time step $$t$$, combines the _Current_ and _Previous time steps_,  
            if the _update gate_ $$ = ~0$$, then this, again, ignores the _newly computed memory content_, and keeps the old memory it possessed.  
    :   $$h_t = z_t \odot h_{t-1} + (1-z_t) \odot \tilde{h}_t$$  

***

## Long Short-Term Memory
{: #content3}

1. **LSTM:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31} 
    :   The __Long Short-Term Memory__ (LSTM) Network is a special case of the Recurrent Neural Network (RNN) that uses special gated units (a.k.a LSTM units) as building blocks for the layers of the RNN.  

2. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32} 
    :   The LSTM, usually, has four gates:  
    :   * __Input Gate__: 
            The input gate determines how much does the _current input vector (current cell)_ matters      
    :   $$i_t = \sigma(W^{(i)}x_t + U^{(i)}h_{t-1})$$ 
    :   * __Forget Gate__: 
            Determines how much of the _past memory_, that we have kept, is still needed   
    :   $$i_t = \sigma(W^{(i)}x_t + U^{(i)}h_{t-1})$$ 
    :   * __Output Gate__: 
            Determines how much of the _current cell_ matters for our _current prediction (i.e. passed to the sigmoid)_
    :   $$i_t = \sigma(W^{(i)}x_t + U^{(i)}h_{t-1})$$  
    :   * __Memory Cell__: 
            The memory cell is the cell that contains the _short-term memory_ collected from each input
    :   $$\begin{align}
            \tilde{c}_t & = \tanh(W^{(c)}x_t + U^{(c)}h_{t-1}) & \text{New Memory} \\
            c_t & = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t & \text{Final Memory}
        \end{align}$$
    :   The __Final Hidden State__ is calculated as follows:  
    :   $$h_t = o_t \odot \sigma(c_t)$$
     

3. **Properties:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33} 
    :   * __Syntactic Invariance__:  
            When one projects down the vectors from the _last time step hidden layer_ (with PCA), one can observe the spatial localization of _syntacticly-similar sentences_  
            ![img](/main_files/dl/nlp/9/5.png){: width="100%"}  


***
***

TITLE: Boltzmann Machines
LINK: research/dl/archt/bms.md


__Resources:__{: style="color: red"}  
{: #lst-p}
* [A Thorough Introduction to Boltzmann Machines](http://willwolf.io/2018/10/20/thorough-introduction-to-boltzmann-machines/)  
* [RBMs Developments (Hinton Talk)](https://www.youtube.com/watch?v=VdIURAu1-aU&t=0s)  
* [A Tutorial on Energy-Based Learning (LeCun)](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf)  
* [DBMs (paper Hinton)](http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf)  
* [Generative training of quantum Boltzmann machines with hidden units (paper)](https://arxiv.org/abs/1905.09902)  
* [Binary Stochastic Neurons in TF](https://r2rt.com/binary-stochastic-neurons-in-tensorflow.html)  


## Preliminaries
{: #content9}



2. **The Boltzmann Distribution:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents92}  
    The __Boltzmann Distribution__ is a probability distribution (or probability measure) that gives the _probability that a system will be in a certain state_ as a function of that _state's energy_ and the _temperature of the system_:  
    <p>$$p_{i} = \dfrac{1}{Z} e^{-\frac{\varepsilon_{i}}{k_B T}}$$</p>  
    where $$p_{i}$$ is the probability of the system being in state $$i$$, $$\varepsilon_{i}$$ is the energy of that state, and a constant $$k_B T$$ of the distribution is the product of __Boltzmann's constant__ $$k_B$$ and __thermodynamic temperature__ $$T$$, and $$Z$$ is the __partition function__.    

    The distribution shows that <span>states with __*lower* energy__ will always have a __*higher* probability__ of being occupied</span>{: style="color: goldenrod"}.  
    The __*ratio* of probabilities of two states__ (AKA __Boltzmann factor__) only depends on the states' energy difference (AKA __Energy Gap__):{: #bodyContents92BF}  
    <p>$$\frac{p_{i}}{p_{j}}=e^{\frac{\varepsilon_{j}-\varepsilon_{i}}{k_B T}}$$</p>  

    __Derivation:__{: style="color: red"}  
    The Boltzmann distribution is the distribution that __maximizes the entropy__:  
    <p>$$H\left(p_{1}, p_{2}, \cdots, p_{M}\right)=-\sum_{i=1}^{M} p_{i} \log_{2} p_{i}$$</p>  
    subject to the constraint that $$\sum p_{i} \varepsilon_{i}$$ equals a particular mean energy value.  

    [This is a simple __Lagrange Multipliers__ maximization problem (can be found here).](https://bouman.chem.georgetown.edu/S98/boltzmann/boltzmann.htm)  


    __Applications in Different Fields:__{: style="color: red"}  
    {: #lst-p}
    * __Statistical Mechanics__{: style="color: purple"}  
        The __canonical ensemble__ is a probability distribution with the form of the Boltzmann distribution.  
        It gives the probabilities of the various possible states of a closed system of fixed volume, in thermal equilibrium with a heat bath.  
    * __Measure Theory__{: style="color: purple"}  
        The Boltzmann distribution is also known as the __Gibbs Measure__.  
        The __Gibbs Measure__ is a probability measure, which is a generalization of the canonical ensemble to infinite systems.  
    * __Statistics/Machine-Learning__{: style="color: purple"}  
        The Boltzmann distribution is called a __log-linear model__.  
    * __Probability Theory/Machine-Learning__{: style="color: purple"}  
        The Boltzmann distribution is known as the __softmax function__.  
        The __softmax function__ is used to represent a __categorical distribution__.  
    * __Deep Learning__{: style="color: purple"}  
        The Boltzmann distribution is the [__sampling distribution__](https://en.wikipedia.org/wiki/Sampling_distribution) of __stochastic neural networks__ (e.g. RBMs).  



    <br>




***
***


## Boltzmann Machines
{: #content1}

1. **Boltzmann Machines (BMs):**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    A __Boltzmann Machine (BM)__ is a type of <span>stochastic recurrent neural network</span>{: style="color: purple"} and <span>Markov Random Field (MRF)</span>{: style="color: purple"}.    

    __Goal - What do BMs Learn:__{: style="color: red"}  
    Boltzmann Machines were originally introduced as a general **_‚Äúconnectionist‚Äù_ approach** to learning <span> arbitrary probability distributions over binary vectors</span>{: style="color: goldenrod"}.  
    They are capable of learning <span>internal representations of data</span>{: style="color: goldenrod"}.  
    They are also able to <span>represent and solve (difficult) combinatoric problems</span>{: style="color: goldenrod"}.  

    __Structure:__{: style="color: red"}  
    ![img](https://cdn.mathpix.com/snip/images/GcOUC--gwjuIgl1bAzFcW77LqLOD3siTNaol-pbyhV8.original.fullsize.png){: width="20%"}
    {: #lst-p}
    * __Input__:  
        BMs are defined over a $$d$$-dimensional __binary random vector__ $$\mathrm{x} \in\{0,1\}^{d}$$.  
    * __Output__:  
        The units produce __binary results__.  
    * __Units:__  
        * __Visible__ Units: $$\boldsymbol{v}$$  
        * __Hidden__ Units: $$\boldsymbol{h}$$  
    * __Probabilistic Model__:  
        It is an __energy-based model__; it defines the __joint probability distribution__ using an __energy function__:  
        <p>$$P(\boldsymbol{x})=\frac{\exp (-E(\boldsymbol{x}))}{Z}$$</p>    
        where $$E(\boldsymbol{x})$$ is the energy function and $$Z$$ is the partition function.  
    * __The Energy Function:__  
        * With only __visible units__:  
            <p>$$E(\boldsymbol{x})=-\boldsymbol{x}^{\top} \boldsymbol{U} \boldsymbol{x}-\boldsymbol{b}^{\top} \boldsymbol{x}$$</p>  
            where $$U$$ is the "weight" matrix of model parameters and $$\boldsymbol{b}$$ is the vector of bias parameters.  
        * With both, __visible and hidden units__:  
            <p>$$E(\boldsymbol{v}, \boldsymbol{h})=-\boldsymbol{v}^{\top} \boldsymbol{R} \boldsymbol{v}-\boldsymbol{v}^{\top} \boldsymbol{W} \boldsymbol{h}-\boldsymbol{h}^{\top} \boldsymbol{S} \boldsymbol{h}-\boldsymbol{b}^{\top} \boldsymbol{v}-\boldsymbol{c}^{\top} \boldsymbol{h}$$</p>  


    __Approximation Capabilities:__{: style="color: red"}  
    A BM with only __visible units__ is limited to modeling <span>linear relationships</span>{: style="color: purple"} between variables as described by the weight matrix[^2].  
    A BM with __hidden units__ is a <span>universal approximator of probability mass functions over discrete variables</span>{: style="color: goldenrod"} _(Le Roux and Bengio, 2008)_.  


    __Relation to Hopfield Networks:__{: style="color: red"}  
    A Boltzmann Machine is just a <span>__Stochastic__ Hopfield Network with __Hidden Units__</span>{: style="color: purple"}.  
    BMs can be viewed as the __stochastic__, __generative__ counterpart of Hopfield networks.  

    <button>Comparison and Discussion</button>{: .showText value="show" onclick="showTextPopHide(event);"}
    <div markdown="1">
    It is important to note that although Boltzmann Machines bear a strong resemblance to Hopfield Networks, they are actually nothing like them in there functionality.  
    {: #lst-p}
    * __Similarities__:  
        * They are both networks of __binary units__.  
        * They both are __energy-based__ models with the same __energy function__ 
        * They both have the same __update rule/condition__ (of estimating a unit‚Äôs output by the sum of all weighted inputs).  
    * __Differences__:  
        * __Goal:__ BMs are NOT memory networks. They are not trying to store things. Instead, they employ a [different computational role](/work_files/research/dl/archits/hopfield#bodyContents18dcr); they are trying to learn <span>__latent representations__ of the data</span>{: style="color: purple"}.  
            The goal is __representation learning__.   
        * __Units__: BMs have an extra set of units, other than the visible units, called __hidden units__. These units represent __latent variables__ that are not observed but learned from the data.  
            These are necessary for representation learning.  
        * __Objective__: BMs have a different objective; instead of minimizing the energy function, they <span>minimize the error (__KL-Divergence__) between the *"real"* distribution over the data and the *model* distribution over global states</span>{: style="color: purple"} (marginalized over hidden units).  
            Interpreted as the error between the input data and the reconstruction produced by the hidden units and their weights.  
            This is necessary to capture the training data probability distribution.  
        * __Energy Minima__: energy minima were useful for Hopfield Nets and served as storage points for our input data (memories). However, they are very harmful for BMs since there is a _global objective_ of finding the best distribution that approximates the real distribution.  
            This is necessary to capture the training data probability distribution "well".   
        * __Activation Functions__: the activation function for a BM is just a *__stochastic__* version of the __binary threshold__ function. The unit would still update to a binary state according to a threshold value but with the <span>update to the unit state being governed by a probability distribution (__Boltzmann distribution__)</span>{: style="color: purple"}.  
            This is necessary (important$$^{ * }$$) to escape energy minima.  
    </div>


    __Relation to the Ising Model:__{: style="color: red"}  
    The global energy $$E$$ in a Boltzmann Machine is identical in form to that of the Ising Model.  




    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * __Factor Analysis__ is a <span>__Causal__ Model</span>{: style="color: purple"} with _continuous_ variables.  
    <br>


2. **Unit-State Probability:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    * The __units__ in a BM are *__binary units__*.  
    * Thus, they have __*two* states__ $$s_i \in \{0,1\}$$ to be in:  
        1. __On__: $$s_i = 1$$   
        2. __Off__: $$s_i = 0$$  
    * The __probability that the $$i$$-th unit will be *on* ($$s_i = 1$$)__ is:  
        <p>$$p(s_i=1)=\dfrac{1}{1+ e^{-\Delta E_{i}/T}}$$</p>  
        where the scalar $$T$$ is the __temperature__ of the system.  
        * The RHS is just the __logistic function__. Rewriting the probability:  
        <p>$$p(s_i=1)=\sigma(\Delta E_{i}/T)$$</p>  

        <button>Derivation</button>{: .showText value="show" onclick="showTextPopHide(event);"}
        * Using the [__Boltzmann Factor__](#bodyContents92BF) (ratio of probabilities of states):  
            <p>$$\begin{align}
                \dfrac{p(s_i=0)}{p(s_i=1)} &= e^{\frac{E\left(s_{i}=0\right)-E\left(s_{i}=1\right)}{k T}} \\
                \dfrac{1 - p(s_i=1)}{p(s_i=1)} &= e^{\frac{-(E\left(s_{i}=1\right)-E\left(s_{i}=0\right))}{k T}} \\
                \dfrac{1}{p(s_i=1)} - 1 &= e^{\frac{-\Delta E_i}{k T}} \\ 
                p(s_i=1) &= \dfrac{1}{1 + e^{-\Delta E_i/T}} 
                \end{align}
                $$</p> 
            where we absorb the Boltzmann constant $$k$$ into the artificial Temperature constant $$T$$.   
            {: hidden=""}  

    








***

## Restricted Boltzmann Machines (RBMs)
{: #content2}

1. **Restricted Boltzmann Machines (RBMs):**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    __Restricted Boltzmann Machines (RBMs)__ 








***

## Deep Boltzmann Machines (DBNs)
{: #content3}

1. **Deep Boltzmann Machines (DBNs):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    __Deep Boltzmann Machines (DBNs)__ 








[^1]: The unit deep within the network is doing the same thing, but with different boundary conditions.  
[^2]: Specifically, the probability of one unit being on is given by a linear model (__logistic regression__) from the values of the other units.  

***
***

TITLE: FeedForward Neural Networks and Multilayer Perceptron
LINK: research/dl/archt/fnn&mlp.md


## FeedForward Neural Network
{: #content1}

1. **FeedForward Neural Network:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    The __FeedForward Neural Network (FNN)__ is an _artificial neural network_ wherein the connections between the nodes do _not_ form a _cycle_, allowing the information to move only in one direction, forward, from the input layer to the subsequent layers.  
    <br>

2. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}    
    An FNN consists of one or more layers, each consisting of nodes (simulating biological neurons) that hold a certain _wight value $$w_{ij}$$._ Those weights are usually multiplied by the input values (in the input layer) in each node and, then, summed; finally, one can apply some sort of activation function on the multiplied values to simulate a response (e.g. 1-0 classification).  
    <br>

3. **Classes of FNNs:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}    
    There are many variations of FNNs. As long as they utilize FeedForward control signals and have a layered structure (described above) they are a type of FNN:  
    * [__Single-Layer Perceptron__](http://localhost:8889/work_files/research/ml/1_2):  
        A <span>linear binary classifier</span>{: style="color: purple"}, the __single-layer perceptron__ is the simplest feedforward neural network. It consists of a single layer of output nodes; the inputs are multiplied by a series of weights, effectively, being fed directly to the outputs where they values are summed in each node, and if the value is above some threshold (typically 0) the neuron fires and takes the activated value (typically 1); otherwise it takes the deactivated value (typically 0).  
        <p>$$f(\mathbf{x})=\left\{\begin{array}{ll}{1} & {\text { if } \mathbf{w} \cdot \mathbf{x}+b>0} \\ {0} & {\text { otherwise }}\end{array}\right.$$</p>  
        In the context of neural networks, a perceptron is an artificial neuron using the [__Heaviside step function__](https://en.wikipedia.org/wiki/Heaviside_step_function) as the activation function.   
    * [__Multi-Layer Perceptron__](#content2):  
        This class of networks consists of multiple layers of computational units, usually interconnected in a feed-forward way. Each neuron in one layer has directed connections to the neurons of the subsequent layer. In many applications the units of these networks apply a sigmoid function as an activation function.  
        
  

***

## Multilayer Perceptron
{: #content2}

1. **Multilayer Perceptron:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    The __Multilayer Perceptron (MLP)__ is a class of _FeedForward Neural Networks_ that is used for learning from data.  
    <br>

2. **Architecture:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}    
    The MLP consists of at least three layers of nodes: *__input layer__*, *__hidden layer__*, and an *__output layer__*.  

    The layers in a neural network are connected by certain weights and the MLP is known as a __fully-connected network__ where each neuron in one layer is connected with a weight $$w_{ij}$$ to every node in the following layer.  
    
    Each node (except for the input nodes) uses a __non-linear activation function__ that were developed to _model the frequency of **action potential** (firing) of biological neurons_.  
    <br>

3. **Learning:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}    
    The MLP employs a __supervised learning__ technique called __backpropagation__.  
    Learning occurs by changing the weights, connecting the layers, based on the amount of error in the output compared to the expected result. Those weights are changed by using _gradient-methods_ to optimize a, given, objective function (called the __loss function__).  
    <br>

4. **Properties:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    * Due to their _non-linearity_, MLPs can distinguish and model non-linearly-separable data    
    * According to [__Cybenko's Theorem__](https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf), MLPs are *__universal function approximators__*; thus, they can be used for _regression analysis_ and, by extension, _classification_  
    * Without the _non-linear activation functions_, MLPs will be identical to __Perceptrons__, since Linear Algebra shows that the linear transformations in many hidden layers can be collapsed into one linear-transformation  
    <br>